In todays world organizations in different branches are using more and more agile ways of working As the operational environment is constantly changing and organizations are forced to keep up the pace to stay alive they might not be able to survive by following only the old inflexible methods However thorough consideration and preparation needs to be done before changing into agile In many cases organizations are so used to follow traditional models such as waterfall that they do not realize that the organization itself needs to be changed as well not just the method they are following The number of agile pitfalls organizations are facing is endless but there are a lot of same mistakes many organizations are doing one after another These common issues are the most interesting ones and therefore highlighted in this thesis In this thesis the most common pitfalls of agile software development are investigated and suggestions how to avoid them are introduced The thesis is not related to any specific organization or technology but common issues identified by having some informal interview discussions First a preliminary literature was written in order tohave a hunch on common issues before starting interview discussions and preparing current state analysis Based on current state analysis conclusion topics for the literature review were identified After literature review initial proposal for tackling the most common agile pitfalls in advance was prepared and validated by agile professionals These agile professionals were partly representing same persons that were interviewed for the current state analysis Finally after initial proposal was validated the final proposal was written The topic for the thesis was decided based on authors own passion and interest The author has been working as a scrum master and wanted to gain more knowledge in order to develop the use of agile methods in her own job She had experienced a lot of positive implications because of agile way of working instead of traditional methods However she had faced also some severe issues and wanted to drill down to learn whether other people are having same experience and how these could be avoided This thesis is not built around any case organization and therefore people interviewed are representing couple of different organizations Interviewed people were chosen based on suitable background and their willingness to participate and they are all having agile experience Though the thesis is not done to any specific organization the outcome of it can be considered as a checklist for any person or organization that are either planning to go agile or already are using agile but facing issues and would like to improve way of working The business challenge of this thesis is that managers in software development adopt agile as some sort of cure all without consideration to the challenges that are likely to be encountered for this particular field of work The business challenge is not related to a single organization but common issues The objective of the thesis is to develop a checklist how to overcome issues in agile software development Target audience for the checklist are people like the author individuals who are using agile methods in their job and would like to improve the way of working to embrace agile benefits However the checklist could be useful also to persons and organizations that are only planning to go agile The output of the thesis is a validated proposal in a form of a checklist answering to a question how to overcome some of the most common issues in agile software development By taking the checklist into a consideration when planning to go agile organizations can avoid the most common agile pitfalls As the use of agile methods has been a rising trend in many organizations in all branches and not least in the software development agile pitfalls is very actual topic Despite the popularity of agile surprisingly many organizations do not familiarize themselves with careful preparations but are getting an illusion that agile simply means lightening or even skipping the planning and project management tasks Software development is demanding and there any many possible stumbling blocks that are not fading away by just saying that traditional methods will be replaced with agile Agile methods are not curing all the problems and not leading to a successful end without seriously going into it The output of this thesis should help organizations to understand the preconditions of agile and things to consider before going agile software development In the next chapters first the research method and material used is explained Then the summary of the preliminary literature is written following by the current state analysis After and based on the current state analysis the conceptual framework is introduced Last an initial proposal and its validation is described ending to a final proposal in addition to conclusions This chapter describes the research design and data collection methods Qualitative research method is used due to its suitability to the thesis In addition to the current state analysis and literature review also preliminary literature review is done to gain a hunch of the current issues The design of the research process is illustrated in below figure First preliminary literature review is carried out in order to get a hunch of the most common issues in agile software development Though the issues that are collected from the literature are not exactly similar to the ones identified based on interview discussions they are still directional and a good starting point In the literature issues are introduced from all over the world from different kind of organizations and different technologies Most of all the issues in the literature are mainly more generic compared to the ones identified by discussions with individuals After the preliminary literature review the current state analysis is drawn up based on informal interview discussions with people involved in agile software development Current state analysis is introducing the current strengths and weaknesses of agile software development Interviewed people are representing scrum masters and developers from different organizations In the next phase of the thesis a literature review is done the main concepts related to the summary of the current state analysis are explained such as agile software development scrum traditional software development waterfall method differences between agile and waterfall change management and agile transformation The literature review is targeting to conceptual framework that will be a base for the initial proposal a checklist how to overcome most common issues in agile software development Initial proposal is validated by couple of the interviewed persons the initial proposal is finetuned based on their comments and the outcome is the final proposal When considering the validity of the research process it can be stated that above mentioned was valid for this case because there was no case company involved Also the subject is so new and broad that discussions instead of a questionnaire were more suitable Data collection for data stage 1 was done via informal facetoface discussions with people involved in agile software development With some of the people discussions were not just onetime but continued couple of times Originally the purpose was to have few more discussions but it became obvious rather soon that the answers were started to repeat themselves Hence it did not make sense to continue discussions There were total five people discussed with representing both scrum masters and developers As illustrated in below picture four scrum masters and a developer were interviewed from couple of different organizations Discussions were done informally and incognito in order to get honest and independent opinions from people Field notes were done by the author to record the discussions Data was analysed by pickingup the main points from the answers and to coming back to those in cases where it was not clear enough what the interviewee was trying to say All the interviewees were having their own point of view a very unique way to express things and hence it required some analysis and rediscussions to be able to crystallize the main points After the main points from the answers were pickedup they were categorized under few topics to be able to identify the areas of issues This was helping to understand the big picture and the areas where the biggest issues were lying Also the identification of the literature topics was much easier after the categorization As shown in below table data stage 2 was done by introducing the thesis as a whole and especially the initial proposal to two of the interviewees participating to data stage 1 Informal discussions with two individuals were done and the author prepared field notes Their comments and suggestions were taken into account when the final proposal was prepared Comments and suggestions were compared to the theory of the thesis and the initial proposal to figure out how they could be put into practice and finetune the initial proposal In this chapter findings from the preliminary literature are introduced The purpose of this chapter is to gain preliminary information before starting the interviews and current state analysis to have a hunch of the most common agile issues In the study of Gandomani Ghani Ziaei and Zulzalil 2013 the obstacles and issues in agile software development are categorized under four themes organizational and management related challenges people related challenges process related challenges and technology and tools related challenges Many of the current challenges are stem from the culture and structure of the organization which is serving needs of traditional methods Organizational culture is affecting to agile transform Organizational culture is a vague term covering numerous things such as prevailing attitudes norms and values Iivari  Iivari 2010 Gandomani T et al 2013 are using a term “The agile transformation process when discussing about organizations moving from traditional methodologies into agile Organizations are often making a mistake by underestimating the difficulty of the agile transformation process and not investing it this is making challenges even more difficult Organizational issues in agile software development are coming from too narrow thinking of the meaning of agility Organizations are often stating they are agile though it usually means only software development The software development is failing in agility in cases where the organization around it is not agile enough The software development projects and teams cannot fully use their agile potential unless the organization is not supporting them and getting rid of traditional thinking and old habits When the agile software development team is lacking agile support from their organization it tends to lead situations where people are not feeling safe to share identified issues and mistakes this is reducing agility and impacting to end results Gothelf J 2014 According to Moczar 2013 agile is promising too much when stating that it would be a solution to problems faced with traditional methods Moczar 2013 has identified several times that agile is partly falling to same issues than with other methods Organizations are counting too much on pure agile method and forgetting the importance of agile thinking In cases where only the agile method has been followed without changing the mindset as well it has sometimes leaded even to bigger catastrophes than by using traditional methods and changed the good intentions totally upside down One of the common issues is that organizations are not considering carefully whether the use of agile is worthwhile Moczar L 2013  Since agile is all about people people related challenges are playing a significant role especially in cases where the organizations have earlier been using traditional software development methods One of the common people related weaknesses is the difficulty for people to change their mindset and behaviour into agile mode During agile transformation there is not always enough training and coaching from agile expertise though it would be needed People related issues are concerning both customers and vendors and both can have overwhelming impacts Gandomani T et al 2013 For instance the agile principle of early and continuous delivery is sometimes leading too hasty outcome in detriment of quality This principle is allowing developers to neglect to bugs The consequence of too fast delivery might be the growth of defect backlog ending up to excessive work Moczar L 2013 The manifesto for agile software development is encouraging to “development over planning This has been often an issue though the original idea has been to make things easier There are often issues because the size of the changes is varying from a tiny to huge ones Though agile is welcoming changes even late in the development it is still commonly causing problems because the development is constantly ongoing and there might be unsolved defects making it even harder to success in agile Moczar L 2013 The plan to have a totally selforganized team without a project manager who would be responsible for the whole project is not always working as desired What happens often is that the scrum master is forced to act as a project manager to keep things going on but without a project manager mandate For instance the prioritization of the tasks to be done is an issue faced in the real world often timepressure is so high that an additional prioritization is needed In practise it is difficult for developers to manage all the priorities and dependencies by themselves Moczar L 2013 The outcome of the preliminary literature review are some the most common weaknesses of the agile software development on a highlevel The weaknesses of agile software development are for instance organizations are not agile enough and therefore not able to provide support for the agile software development teams people with experience on traditional software development are not able to get rid of their old habits and mindsets and preventing the successful use of agile agile processes are not properly used due to lack of agile knowledge When reading the results of the preliminary literature review it needs to keep in mind that though the issues mentioned are partly similar than in the current state analysis they cannot totally match due to fact that CSA is done by interviewing Finnish ITprofessionals while literature is from the wider perspective Still the preliminary literature is providing a hunch a useful overview In this chapter the most common strengths and weaknesses of agile software development are being introduced The current state analysis is prepared based on informal and anonymous interview discussions Based on interview discussions the following strengths of agile software development were identified intense and good cooperation easiness to plan work in small pieces possibility to correct mistakes rather easily and quickly allocated resources if preconditions are in place the quality is usually good Though above mentioned are considered as strengths they still cannot be taken for granted but can be achieved only by treating agile method with conscious Agile strengths can turn to weaknesses in a quick manner if agile principles are not followed actively First people discussed with were having positive experience on cooperation and communication between different parties such as the project team and customers Especially when sitting at the same premises and having extended facetoface communication the cooperation has been much more informal and therefore better compared to traditional approaches Communication can be done without delays and so called Chinese whispers –effect can often be avoided also threshold to open discussion is low One of the scrum masters highlighted the easiness of the cooperation when all project members are sitting on the same premises he had experienced that good cooperation usually requires people locating on same premises and as soon as part of the scrum team is located for instance in another country communication gets poor All interviewees mentioned good cooperation and communication as the most valuable thing agile can offer However they all had experienced the fragility of good cooperation meaning it can easily be spoiled This will be elaborated more in the next subchapter Another identified strength of agile software development is the easiness to plan work in small pieces This is a great advantage because the changes in the schedule and error estimates are not causing as much issues as with traditional methods The socalled snowball effect can be avoided rather easily and the possibilities to adjust the overall schedule works better One of the scrum masters stated that it is unrealistically to even think that all the smallest details could be planned in the beginning of the project due to nature of the software development and especially regarding bigger software projects Hence he appreciated the possibility that agile is offering to plan work in pieces Third strength of the agile software development was identified to be the good possibilities to correct mistakes and bugs easily and relatively early People were having unpleasant experience on traditional methods where mistakes are not often noticed until at the end of the project but they considered agile way of working to enable faster issue fixing People noticed that for example in scrumming you are learning sprint by sprint and eventually be a master The scrum master 1 was praising agile due to its mercifulness in hes experience software development done by traditional methods is harsh and punishing people for all mistakes they are doing especially in the beginning of the project when agile method is often offering a possibility to fix mistakes during the coming sprints Hes opinion was that in agile software development the learning curve of the scrum team members is much better because it is actually possible to learn by mistakes fast within the project and not only after the project is about to end or even finished Allocated resources are also one of the agile strengths people mentioned Allocated resources are a great benefit because they know the product that is developed but also other project members enabling to proceed smoothly In perfect situations resources are allocated 100 to the agile project itself this is something that is unfortunately not always happening but when it does it makes agile life easy One of the interviewees a scrum master stated that everything is much easier by using agile because there are designated resources and they are mainly allocated to the same project Despite all the strengths there are also several weaknesses in agile software development such as agile methodology is used though there are not prerequisites lack of sufficient planning or documentation or testing too early delivery communication and cooperation issues due to resources located in different places issues due to cultural differences when projects are international resources not always able to concentrate 100 to agile work due to other responsibilities changes in staffing affecting agile projects heavier than traditional ones agile methodology and principles not known bigger risks to break existing functionalities because the big picture not always known due to constant changes done Three of the most common weaknesses are explained in detail in this chapter though there is not much difference between the answers by the interviewees Also to mention some of the weaknesses are almost overlapping One and the most common of the weaknesses observed and discussed was that in many cases all agile resources are not 100 allocated to agile work due to other responsibilities This is causing delays to the development work and makes it difficult to plan schedules Even one person with less than fulltime allocation may cause tremendous issues As the developer that was interviewed said since things are unfortunately often depending on individuals the nonattendance of even one person can spoil the whole thing and undercut the benefits of agile Even too early delivery meaning lack of sufficient planning documentation and testing is also a big issue regarding agile software development Some of the people interviewed stated this issue to be concerning the whole project covering all the steps and starting from the project planning they felt that in some cases the project team thought that the use of agile would justify defective quality Though agile is encouraging to iterations and welcoming changes over planning this was sometimes misused When using agile there is sometimes pressure to deliver outcomes earlier than what would be wise and realistic leading to careless development and lack of proper testing Especially lack of planning and documentation is sometimes making bug fixing difficult and causing too much dependency on individuals Without proper planning there are often conflicts between the development done by other people within the same agile team or even other projects Poor planning is often leading to quality issues and bugs as well In cases where also the documentation is negligible the defect fixing is even more painful and time consuming In addition the software around is constantly changing making it harder to identify the root cause for issues and corrective actions The third biggest weakness discussed was the use of agile methodology without having preconditions to adopt it People were having bad experience of projects originally planned to be done with traditional methods but for varied reasons the method was changed to agile these situations were often leading to confused situation where agile method was supposed to be followed but the organization around the project group was not acting agile at all Some of the people were considering agile as a trendy concept that is rather often used without really focusing on it and the conditions it is requiring Typically the thought is to run a project like with waterfall method but without any specifications and with minimal testing One of the scrum masters was even having experience on agile team developers not at all familiar with the agile method itself leading to waist of valuable time reserved for the development work He used a lot of time during several sprints for teaching agile principles and scrumming to other team members Strengths and weaknesses based on interview discussions are listed in below table Interviewees were overall satisfied with the quality of work in agile projects They all though in many cases agile approach works better than traditional one Due to designated resources and emphasizing the communication and cooperation risk to fail is less Especially good and intense cooperation and designated resources were appreciated However there are several weaknesses as well such as all resources may not be 100 allocated to agile work due to other responsibilities misusing agile approach by working carelessly and using agile though all the preparation work was not done As the interviewees were speculating most of the issues are due to lack of proper preparations and underestimation of agile approach Interesting observation was that people identified more agile issues than successes An interesting observation is that many of the strengths and weaknesses are opposite to each other meaning that the advantages of agile can be gained only with careful consideration and preparation and without this they can turn into weaknesses When rushing to agile without preconditions in place the results are not always positive as expected When discussing with people about what should be done differently to succeed with agile a common denominator seems to be that better change management and learning agile deeper would be needed In the next chapter literature review based on findings from the current state analysis is introduced In this chapter a conceptual framework of the thesis is being introduced Topics are identified based on conclusions of the current state analysis The purpose of this chapter is to support the understanding of the thesis and to prepare the proposal The current state analysis revealed that the most common issues are related on a highlevel to either agile transformation the differences between agile and traditional methods or change management The idea of the agile software development is to have an adaptive team which can deliver frequently and rapidly and welcome changes in the requirements The advantages of the agile software development are “the ability to respond to the changing requirements of the project Balaji S  Murugaiyan S 2012 and the improved communication between the customer and the development team Agile method is usually more profitable and suitable for smaller projects One of the issues in agile software development is the demand for seniorlevel resources agile developers should be able to do decisions and be selfimposed Balaji S  Murugaiyan S 2012 Manifesto for agile software development Individuals and interactions over processes and tools Working software “Scrum n A framework within which people can address complex adaptive problems while productively and creatively delivering products of the highest possible value Schwaber K  Sutherland J 2013 Scrum has empirical and iterative approach aiming to control risks and highlight predictability According to empirical approach there are three main principles to follow adaptation inspection and transparency The purpose of transparency is to keep the whole process visible to the people who are either performing or accepting the work Inspections are referring to the idea that scrum artifacts should be inspected enough to detect the unwanted side effects but not exaggerate Adaptation is aiming to adjustment of the artifact in case the inspection is revealing that the artifact is unacceptable Schwaber K  Sutherland J 2013 The product owner development team and a scrum master are formulating a selforganizing scrum team that should not be depending on outsiders The scrum teams are having needed competencies to deliver the artifacts incrementally and iteratively Continuous feedback is desired to develop the competence and productivity Traditional software development is approaching things from the predictive point of view Traditional software development is based on detailed plan with a complete list of items that must be developed All the changes are going through a change control management GhilicMicu B et al 2013 Traditional and one of the oldest and most popular ways of software development is the document driven sequential waterfall method The catch of the waterfall method is to follow the predefined stages and milestones and to invest on early planning An output of a stage is an input for the for the coming stage At first requirements are gathered and right after that follows the design phase After the design the implementation ie coding and testing is done and the final phase is handing to maintenance Bhuvaneswari et al 2013 Balaji S  Murugaiyan S 2012 The advantage of the waterfall method is the easiness to understand and implement it due to its linear model Waterfall is useful on mature products and weaker teams can benefit more from it However one centric pain point of the waterfall method is the unrealistic expectation that requirements in the beginning of the project could be strict and unchangeable leading to issues in the latter phases of the projects In this model issues cannot usually be solved in one phase completely leading to quality issues in the final outcome As the final deliverable ie the actual software is delivered at the end of the project possible issues are identified late leading to expensive changes Bhuvaneswari et al 2013 Balaji S  Murugaiyan S 2012 According to Kotter change management “refers to a set of basic tools or structures intended to keep any change effort under control The goal is often to minimize the distractions and impacts of the change 2011 There are several alternative approaches to change and the selection should be done case by case taking all the circumstances into account Lockitt 2014 has roughly divided change management strategies into five different approaches directive expert negotiated educative and participative However these strategies are not exclusive and can be used alongside One of the change management tasks is to make a decision what strategy or strategies to use and how and when to implement them Lockitt B 2014One of the five strategy approaches directive strategy emphasizes the authority of the managers even without other people involved in the decision making This approach is allowing fast change but not taking other involved peoples opinions into account The disadvantage of this strategy is often strong change resistance and lack of ideas from other stakeholders Lockitt B 2014 Another strategy approach expert is looking the change management from the problem solving point of view and it is suitable especially for the technical cases such as new systems being introduced There are often specialists leading this kind of changes which is bringing both advantage and issues as well though this approach is enabling rather quick implementation affected people may not share same views than experts driving the change Lockitt B 2014 Negotiating strategy approach is highlighting the negotiating between the management and people affected The management is letting stakeholders to express their views and is willing to do compromises regarding how and what is to be done By following this approach the change is having slower tempo and the predictability of the outcome is not complete however people affected are more involved and there is less change resistance Lockitt B 2014 Educative strategy is trying to change peoples way of thinking leading them to support the change Different kind of activities is used within this strategy such as training and sweet talking by experts and consultants Naturally this approach is timeconsuming but as an advantage it is involving and committing people and reducing the amount of change resistance Lockitt B 2014 In participative strategy all affected people are involved and their opinions are taken into account In case experts and consultants from the outside are used to facilitate the change management process they are not allowed to do any decisions This approach is offering a possibility to learn and grow up for both individuals and the organization around them In addition it is committing people and making them to support the change As a disadvantage this kind of change process is taking a lot of time and may be expensive Lockitt B 2014 When moving to agile a strategy for the agile change management is needed Agile transformation is sociotechnical process that requires a lot of time and patient There are three different approaches to use when moving to agile tailoring localization and adoption Tailoring is aiming to fewer changes in the organization and it was popular especially in the days when agile methods were introduced Tailoring approach may not always be the best way to implement agile but rather a way to have the disciplined process and agile side by side Gandomani T et al 2012 Instead of tailoring localization is accepting essential changes but not all agile activities Some parts of agile might be ignored totally and some are customized Especially in organizations that are taking their first steps towards agile and lacking experience some practices are still done by following traditional ways Gandomani T et al 2012 Adoption approach is emphasizing major changes to adapt organizations with agile When using adoption approach agile methods are tried to be used completely without any limitations Agile adoption is considered as the best way to achieve agile method Gandomani T et al 2012 Challenges in agile transformation have been categorized as follows management and organizational challenges people challenges process challenges and technology related challenges Impacting to peoples mindset is one of the biggest challenges it is impossible to achieve overnight and besides time it requires mentoring as well Individuals as members of a project team may cause severe issues because of their habits ambitions and different cultural backgrounds Coaching towards agile is unique comparing to other methodologies and therefore requires an experienced and professional mentor in order to succeed When changing to agile people must change and forget old habits and roles for example project managers with strong experience in traditional methods must learn new way of working and forget being a commander Also the role of a customer is changing radically because of the agile way of working forcing them to contribute in a different way Gandomani T et al 2012 From the management point of view tacit knowledge and minimal documentation are causing issues and can be treated as barriers Still one of the biggest management relates agile issues to be considered is the group decision making which is totally opposite when comparing to the traditional software development Besides group decision also letting individual project team members do selfgoverning decisions is part of agile but can sometimes be hard for the management to implement in practice Gandomani T et al 2012 In many organizations changing processes from traditional life cycle model to more iterative and evolutionary agile is difficult This change affects many levels such as strategies peoples roles and measurement practices In organizations where operations are spread to different locations process related barriers towards agile transformation are playing even a bigger role and challenges regarding communication and cultural differences needs to be taken into account as well Gandomani T et al 2012 As a conclusion transforming from the traditional software development methods to agile is never easy but a timeconsuming process that needs to be treated with a conscious and understand the importance of it Everybody involved in agile transformation needs to be aware of the challenges and sufficient training and coaching must be provided In addition as there are several different agile methods to choose organizations should carefully study them to find the most suitable one for them All in all in order to succeed agile transformation requires a professional change management strategy plan and resources Change management strategy from a wider perspective is mandatory for successful agile transformation Purely technical point of view concentrating on software development process is not sufficient but all aspects as illustrated in below picture should be taken into account Agile transition is change oriented not methodology oriented process that is touching all levels in the organization Gandomani T et al 2012 Selection of a method  selection of an approach  creating a change management strategy  creating and following the execution plan  In this chapter initial proposal to overcome issues in agile software development is introduced Initial proposal is prepared based on data 1 which is current state analysis and literature review  Initial proposal is trying to take all the previously introduced aspects in to account to offer a useful checklist Initial proposal is telling who what and when certain actions needs to be done The aspect “why is not mentioned in below figure because the lack of the case company the thesis is based on common issues and not related to a specific organization  There are several things that organizations and individuals should be taken into account when planning to go agile At first a careful consideration which one traditional or agile method would be preferable should be done Comparison between these two different methods should always be done case by case and understand the unique features in every project There are cases where agile is not suitable at all despite of all the benefits it is offering When doing the comparison also the characteristics of the organization are crucial some organizations are more traditional and rigid having a lot of bureaucracy It can be extremely challenging or even impossible to bring agility to organizations like this After careful consideration and selection of the method desired approach should be defined As introduced in earlier in the literature review there are roughly three alternatives to select from tailoring localization and adoption When selecting the approach all aspects must be considered realistically from the project and organizational point of view One major thing impacting to the selection of the approach is the former experience on agile or the lack of it A change management strategy should be created by considering all known and common challenges meaning management organizational people process and technology related aspects should be considered The creation of a change management strategy must be done in the planning phase after the method to follow and the approach has been chosen before the actual project starts As explained in the literature review first the most suitable change management strategy approach to achieve the desired change needs to be defined When defining the strategy all aspects of the change must be taken into consideration the organizational culture the scale of the change expected change resistance schedule budget and risks of the change An execution plan is needed together with the active followup It is crucial to plan in detail how the actions will be executed the plan itself is not enough but it needs to be followedup as well The initial proposal is validated and commented by two of the interviewees participating in current state analysis a developer and a scrum master 2 Validation was done via email and by having informal discussion Also comments from the thesis supervisor was received The developer commented that the initial proposal was good and realistically She is working in a software development industry and using agile methodology in her work currently Her company is struggling with same issues mentioned in this thesis and hence planning to start implementing similar phase than the selection of approach phase in the initial proposal they came into a conclusion that a phase like this is a must in order to avoid facing same agile pitfalls over and over again The company did the decision without knowing the initial proposal introduced in this thesis which is a notable example of the necessity and usefulness of this kind of a checklist The developer was thinking that the way agile methodologies are used in Finland may be different than in other countries and especially other continentals In her experience Finnish companies are not yet too familiar with agile software development and therefore the initial proposal would probably not be as usable in other countries but suitable in Finland The scrum master 2 evaluated the initial proposal as simple and doable In her experience this kind of checklists needs to be simply enough and the correlation between commonly known issues and the checklist needs to be clear to get people interested about it She stated that in case companies would not like to execute all phases they could still pickup certain phase or phases and execute them individually this is an alternative that should be highlighted and explained The thesis supervisor highlighted the lack of the named resources in the initial proposal there is only mentioned either project team or management However this is not sufficient but leaves it too vague and raise a question “how to make sure things will be done In addition the thesis supervisor was missing a more concrete checklist with actions and their subtasksSince there was not identified any major changes during the proposal validation the final proposal is rather like the initial proposal with a comment that in case companies do not want to implement all the phases they can also pickup an individual phase and execute it it is not recommended but better than ignoring the whole checklist There is also more depth added to make sure that things will be done there must be a responsible person pointedout regarding all the steps in the final proposal In the initial proposal instead of individuals there were mentioned that either a project team or management should be responsible for certain steps It was too vague definition creating a risk that things will not necessarily be done and certainly not on time In the final proposal it is suggested that named person can be either from the project team or management it is depending on the project and organization which one is more preferably A detailed checklist with all subtasks is also added to the final proposal The checklist is covering all stages of the proposal and its purpose is to offer more concreteness Selection of a method to select between traditional and agile methods Responsible person is named individual from the project team or from the management of the organization To succeed the person responsible requires sufficient knowledge of the organization Planning phase To consider what kind of change management strategy would be the most suitable The final proposal is trying to take all the previously introduced aspects into account to offer a useful checklist The final proposal is telling who what when and why certain actions needs to be done There are several things that organizations and individuals should be taken into account when planning to go agile At first a careful consideration which one traditional or agile method would be preferable should be done Comparison between these two different methods should always be done case by case and understand the unique features in every project There are cases where agile is not suitable at all despite of all the benefits it is offering When doing the comparison also the characteristics of the organization are crucial some organizations are more traditional and rigid having a lot of bureaucracy It can be extremely challenging or even impossible to bring agility to organizations like this There must be a named individual responsible for the selection of a method responsibility on selecting a method cannot be shared Naturally it is essential that responsible person is cooperating with other stakeholders and if needed also consults subject matter experts but he or she is responsible that the decision will be done appropriately and on time Without a responsible individual who is having sufficient preconditions there is an increased risk that this step will be done carelessly or ignored totally Also support from the management is needed the way the support is needed is depending on the situation but a minimum requirement is principled support Sometimes also financial support may be required Selection of a method is a big decision that should not be done without a support from the management Despite of a good plan the first mistake is already done if responsible person with management support is not pointed out After careful consideration and selection of the method desired approach should be defined As introduced in earlier in the literature review there are roughly three alternatives to select from tailoring localization and adoption When selecting the approach all aspects must be considered realistically from the project and organizational point of view One major thing impacting to the selection of the approach is the former experience on agile or the lack of it As in the first step of the proposal selection of a method also selection of approach requires an individual responsible with managerial support A change management strategy should be created by considering all known and common challenges meaning management organizational people process and technology related aspects should be considered The creation of a change management strategy must be done in the planning phase after the method to follow and the approach has been chosen before the actual project starts As explained in the literature review first the most suitable change management strategy approach to achieve the desired change needs to be defined When defining the strategy all aspects of the change must be taken into consideration the organizational culture the scale of the change expected change resistance schedule budget and risks of the change The successful creation of a change management strategy requires also a named person who is in charge Especially in this stage the management support is crucial due to fact that changes may touch all aspects of the organization and have a significant impact on its customers as well An execution plan is needed together with the active followup It is crucial to plan in detail how the actions will be executed the plan itself is not enough but it needs to be followedup as well There must also be resources enough to execute the planned actions As with previous step deep and sustainable support from the management is important The management is also needed to provide sufficient resources and finance to secure the implementation of the execution plan The thesis is not built around a case company but done from a common point of view Though the amount of people interviewed is not much it was obvious that the answers and opinions were starting to be repetitive hence there was not more interviewees involved When considering the results of this study it needs to keep in mind the preconditions such as geographically location since this study was done in a small country as Finland it is obvious that the sizes of the projects are minor meaning that the use of agile is different than globally In addition the way agile methodology is used is also depending on the organization Some organizations are more agileoriented than others and therefore better aware of the possible pitfalls Out of the five interviewees three of them were working as consultants at the time of the interview discussion this is also a fact worth to notice since consultants may have different kind of possibilities to impact their customers way of work and especially the way they are adopting agile and doing all the prework During the proposal validation the developer commented that the outcome of this thesis is probably serving best Finnish people due to fact that the current state analysis was done based on interview discussions with Finnish people and the assumption that the use of agile methodologies is not yet very advanced in Finland This is a useful view when considering the credibility of the thesis When considering the facts mentioned above it can be said that the study is credible enough but the preconditions needs to be kept in mind If a similar study would have been done in another location or in a selected case company the results may have been a bit different However the issues identified in the current state analysis are matching to the preliminary literature in a highlevel It was really educating to drawup a study like this the topic is near to my heart and I have been really interested on agile methodology and luckily have had the opportunity to use that in practise I had originally a totally another topic suggested by my employer of that time I found this original topic to be too wide and it was difficult to seize that hence I decided to do my thesis without a case company and select a topic that really fascinates me most That was at the same time a really good decision but it also felt difficult to do the thesis without a case company supporting in a background knowing there is nobody particularly ordering a study like this Still I think the outcome of the thesis – a proposal how to overcome agile issues in a form of a checklist is valuable and useful for the companies planning or going agile pment project Chapter 5 fulfils this goal even though the application of the theory is singular at the moment The secondary goal of this thesis was to examine and document the practical application of the theory of test platform prioritization for functional testing in a smartwatch application development project This goal was fulfilled by the investigation presented in Chapter 6 The findings of the investigation also support and elaborate the theory presented Test platform prioritization as presented in this thesis has practical applications but it is not viable for every project It can also be utilized in projects outside of the gaming field The investigation showed no difference in testing results between the Apple Watch device and the Xcode Apple Watch simulator in functional system and regression tests There were some functionalities of the application that could not be tested on the simulator so testing without the device would have left some gaps in the test coverage It would be a more feasible strategy to conduct unit testing tasks when possible with only the simulator Installing the application builds to the device takes considerably longer and because unit test are conducted in a phase where more changes are still made to the project this would lead to significant benefits to resources and work flow The prioritization of testing platforms can be carried out on an ad hoc basis or it can be planned ahead utilizing tools such as the ones presented in this thesis More tools can be created or discovered in the future to cover a wider range of scenarios and development frameworks Test platform prioritization for unit testing would be an interesting topic for future study since unit tests are very different in nature to functional system tests and with unit tests there is a greater possibility of affecting the time usage through prioritization Test automation would be another field where test platform prioritization could yield interesting results Running automated tests on an actual device would be significantly more challenging compared to a simulator There are frameworks for controlling mobile device functions through the desktop computer interface such as Appium which can be used for automation but maintaining the test sets and the devices in working order for executing automated scheduled regression or smoke tests would certainly present complicated issues Before setting up this kind of a system it would be important to first discover if running the automated tests on a physical device would produce greater results to justify the additional effort The increasing focus on efficiency and optimizing the way people think and work has led to a new area of serious gaming – cognitive games The rise of modern web rendering technologies has enabled the creation of visually interesting cognitive games on browser based technologies The goal of this study was to assess the applicability of using modern browser technologies to create a user centric cognitive gaming platform and the use of mathematical formulas in organic rendering The approach discusses the current market situation and the products and methods of cognitive gaming as well as the technologies involved The user centric approach is studied through user experience design as well as graphic design and animation aspects The reference implementation is project CCA a user centric cognitive gaming platform built on top of Adobe Flash that uses seemingly organic movement rendering The technical implementation is discussed from the platform clientserver aspect as well as an overview of the structure of the front end architecture The rendering engine methods go through the 2D –based rendering of mathematical formulas the use of continuous Bezier curves in organic movement and the creative ways of using Perlin noise to generate textures as well as movement Optimization of complex rendering and platform building is an essential part of the process The results show the viability of using modern browser based technologies in the creation of a cognitive gaming platform Through the use of optimization and creative mathematical solutions as well as tending to user experience needs a successful product is built The project platform is used in medical trials as well as the Science Changing the World Exhibition shown in science centers around Europe This study stands as a testament to the possibilities of cognitive end user training and a guide on the aspects of building a successful gaming platform Humanity as a whole has seen increased focus on optimizing the way we work the way we consume and the way we think This need for ever improving performance has made us consume smarter perform more efficiently at work and waste less time During recent years the world of gaming and digital entertainment has seen a growth in a new area one that was not focused solely in time consuming entertainment the coming of so called “brain fitness Gaming suites and platforms offering rather simple basic mechanics that provided the user various activities they could do to train their brains The central tenet in these games was that through exercise powered by game related reward models one could improve the way one thinks and track that improvement as well The brain games became very popular and spread throughout the entertainment ecosystem from mobile phones to modern console platforms The games varied from simplistic memory exercises to the ones based on largely theoretical premises and offered actual data as back up of their effectiveness For the most part the games have been shown to increase your ability to memorize things but the actual science proven benefit is still under much debate Aside from the more mass market oriented products the world of science started to take interest in the inherent interest  reward models and game environments and how they could be used in rehabilitation and analysis of different diseases and medical conditions that affect our cognitive functions Cognitive gaming is based on the concept of neuroplasticity the ability of the brain to physically adapt to new stimuli At the same time the front end tools for web development have improved to a level closer to full desktop experience The introduction of RIA Rich Internet Applications technologies like Adobe Flex and Microsoft Silverlight in addition to the rising support for the next W3C standard HTML5 and CSS3 have enabled high production value fully fledged user experiences across platforms within the browser This study focuses on the key issues in building a user oriented gaming platform and the applicability of organic movement and shapes with mathematic formulas in cognitive gaming The reference implementation is a consumer oriented platform for cognitive gaming built for Neuroware Group a browserran application to measure improve and develop cognitive abilities The goal of this thesis is to study how to develop a rendering system for seemingly organic movement and a fullyfledged cognitive gaming platform with a user experience focus for improving peoples lives using modern web front end technologies The present study does not explore the medical theories of cognitive gaming nor is it a study of software engineering methods themselves This study focuses mostly on the front end platform implementation where the game logic and rendering lies and only has an overview of the backend system of project CCA The first part of the study focuses on the basis of the reference project the setup of the research question and key factors and challenges The second part provides an overview of brain gaming and introduces the user experience needs and specifics in consumer market oriented platform building as well as details on CCA platform implementation The third part introduces the technological core of the platform rendering engine specifics optimization approach and implementation and how the challenges were overcome After going through the theory and reference implementation the final part discusses the merits accomplishments and future of the project CCA From current user base and use cases to possible future uses as well as the related branch of the project currently presented at the Science Changing the World exhibit The reference project is collaboration between the concept owner and CEO of Neuroware Group – Matias Palva PhD and Niilo Säämänen the author of the present thesis Neuroware Group is an innovative small company focusing on neurogaming and cognitive training games that are based on theoretical research on the fundamental workings of the human brain The project began based on the research done by Satu and Matias Palva at the University of Helsinki Neuroscience center Matias Palva had started a prototype of the creature rendering system on the LabView platform to test the possibilities of making a consumer oriented cognitive game based on his research The project CCA started in 2008 on the 18th of December with a meeting with Matias Palva Based on the prototype idea a consensus on the viability of creating the platform with modern web technologies was found The aim of the project was to create a consumer oriented platform for cognitive gaming based on the research done by Satu and Matias Palva on neuroplasticity and the application of organic shapes and movement in the realm of brain fitness The target was to make a browser based solution with a fairly large portion of the platform logic and controls coming from the backend solution enabling a modular and extendable solution for brain training Figure 1 shows the approximate funding partners and their relative contributions to the project during its lifetime The project had Tekes funding for the initial prototype and development Development was started without certainty about future budget options During the development additional funding was acquired as a grant from the Runar Bäckström Foundation and closer to the final stages of the project the trials with HermoPharma funded further development The approximate total external funding in project CCA was 70 000€ The projects first release version was finished during the spring of 2011 after 2 years of development Since the 1st release version the platform has been further developed and optimized The main goals of the project have been achieved and the whole platform had been heavily reworked and evolved to a point of maturity The project work was delegated as follows The front end platform development rendering engine development and UXdesign by Niilo Säämänen graphic design by Mikko Häkkinen and the game design mathematical theorems and backend solution by Matias Palva Mostly the project was an intense collaboration between Matias Palva and Niilo Säämänen The reference project is a cognitive gaming platform for consumer use The tool is meant to be used by end users of all ages and trades and to be easily approachable and trustworthy tool for measuring and improving cognitive processing efficiency From a users perspective the platform is a webbrowser based game system with user authentication and personal account based training programs The general user flow in CCA is straightforward users either have an account or register for one and log in to the game platform The users have free choice over which order they plays their games in and can choose from various available games to them as seen in Figure 2 The different game modes depend on the users account and targets Each user has a specific user account based training program that allows for them to play a certain amount of games per day Most games last around one minute and in each day a different set of games is played The total duration of a training program is approximately 30 minutes per day The games vary depending on the type but contain various amounts of moving or static visual objects called creatures The users task is to perceive andor memorize creatures or the target visual states thereof according to the instructions The target state of a creature is a brief contrast color andor shapechange In some of the game modes the target state change is relative based on a calibration round played before the actual measurement portion Calibration changes the size of the target state so that the subject detects around 6090 of the changes in a two creature game phase The detected amount of target states is expected to get better with training When the target state is perceived there is a limited window of opportunity for reaction within which the user must react by pressing a key Configurable but by default it is the space bar The platform measures the hit rate and reaction time of the user per target state per creature on screen The hit rate HR is the main measurement used in CCA to define user capability After a successful play the users are given a summary of their performance The performance metrics shown are based on the hit rate and reaction time of the user The main numerical feedback given is called capacity Capacity is defined as where N is the number of creatures Capacity is given as a total value for the playthrough and as a separate value for each phase of the game In addition to the numerical metrics the users are also given trophies achievements and stars based on their performance These categories provide user friendly feedback on how well the users are doing without the need for detailed metrics The detailed metrics are available but not shown as the default content The purpose behind the trophies is to empower the user and give them solid clear and maintainable goals for their training After the game end screen the user returns to the main interface of the CCA platform where he can see his statistics review his achievements change his information and play another game The projects target user groups were divided into two sections The general public and the clinical trials The general public users were divided into two sections Elderly citizens whose interests would be to both test attention and working memory as well as train to improve them and school aged children who would benefit from attention disorder testing with an automated platform The current methods for testing and diagnosing attention disorders is work intensive and expensive and automating the testing would yield significant savings These users focus was on daily life and they suffered from no known disruptions in their cognitive capabilities Because of the scientific nature of the platform and the precise data collected from our users the platform could be used as a reference in clinical trials to see how and if people improve their performance with the use of a medicine This is important in studies of new medicine and helping people with brain trauma The platform enables a way of measuring and making training programs specific to studying the target groups differences with medicine and without Since a portion of potential users is visually challenged significant emphasis was put on making the platform visually appealing and simple to use The focus was on pleasant user experience instead of pure efficiency To make the users feel safe with the program and to trust it a solid user experience was necessary The work started soon after the first meeting and the focus was on creating a prototype of the basic rendering mechanics as a proof of concept This was necessary as the rendering power of browser based solutions is still far behind native language based compilers and it was needed to see if it was possible to create the rendering engine on such platforms As seen from Figure 2 CCA was a comprehensive project spanning many years The project was divided into separate phases each phase consisting of a number of sprints The first priority was in the rendering and game engine construction to make sure it was viable to build a full scale platform with the technologies chosen After a few weeks of development a first prototype of the rendering engine was developed and published After some tweaking of parameters and bottleneck analyzing it was considered to be a viable solution and a sufficient base for building the platform Figure 4 shows the first version of the platform rendering engine The result contained only one stationary creature comprised of a nominal amount of points rendered just so one could see how it would work and what the possible performance bottlenecks were The initial version rendered the stationary creature with barely 30 FPS and was quite heavy on the CPU  After the POC was finished the full development of the platform began Development was done in an agile way with sprints of 24 weeks 1 where one aspect of the platform was tackled at a time Since the most difficult part of the project was the rendering engine itself the first 45 months were spent solely on creating optimizing and fine tuning the first version of the core rendering engine  Once the basic engine was built a fast pass over visual style was done for the project an initial GUI for test users and funders to see and test the progress Work was started also on doing reusable UI components for various user prompts and game information needs Beyond the GUI needs the first game mode logic was coded and the first actual play through of CCA was possible  As depicted in Figure 5 the first GUI version took the metaphor of the circles quite far and was oriented around cells that contained smaller cells The first version of the UI was fully implemented and contained growth animations for chosen cells and a small amount of fluid dynamics however it was quite confusing to use and more of a game in itself  After the initial GUI pass the focus turned to the game modes of the system The final version of CCA supports 4 different game modes for cognitive gaming from a single creature reaction observing to feeding multiple creatures In addition to building the initial game modes the first bug fixing and improvements for the rendering engine were implemented  Once the game modes were done a sprint was dedicated for properly introducing a full GUI to the platform and further separating UI components After the initial logic and games and UI were done the focus was on Login and register functionality as well as user support mechanisms such as account handling and performance graphs and information graphics in general  After the phase three of the project the official Beta release of CCA was ready The beta interface can be seen in Figure 6 The platform supported all the basic game modes user accounts a full play through experience and statistics to prove it Once this version was done the work for the Heureka science exhibition called Science Changing the World started It was a separate version completely independent from the backend and running a local copy of the rendering engine modified for 4 player multiplayer needs  After the Heureka version the phase 5 concentrated more on improving different parts of the platform at a time The addition of medical imaging indicator helped automated testing of the platform the improved target location in path finding enabled smoother movement and the GUI code had a thorough overhaul to separate it more from user logic  Phase 6 was the 2nd major round of refactoring and improvements for the platform The introduction of training programs changed the way the platform worked and the new game modes added a lot of variety to the package Engine code refactoring helped further development of game modes especially and the target state calculation change was done in order to make it more manageable for the server to fine tune the user account based training regime  In Phase 7 the way users were rewarded and progress communicated got a much needed overhaul changing the direction of the feedback to a much more user friendly result In Phase 8 the release version of the platform was finalized the final GUI was put in place the performance was verified and the user experience perfected  There were a lot of features and ideas in the beginning some of which were discarded along the way as unviable for us to implement with our schedule and needs among those features were evolutionary algorithms in creature generation  The technologies enabling browser based heavily mathematic rendering for the front end were evaluated before the project begun and a choice was made in the beginning on which platform to pursue the solution on Since much of the projects core engine is based on mathematics that are rather universally supported by the different platforms the porting of the platform to a different technology is not considered to be an impossible choice in the future  One of the technologies evaluated was Silverlight the RIA solution from Microsoft It is a browser plugin similar to its main competitor Adobe Flash An overview of the Silverlight technology stack is seen in Figure 7 The runtime is based on the popular C language and UI components are marked in the more human readable XAML Extensible Apllication Markup Language XAML is also used in Windows presentation foundation WPF and NET framework of which Silverlight is a sub sect of Silverlight implements the same version of the Common Language Runtime CLR as the NET framework  30  Silverlight offered similar performance to Adobe Flash and being a part of the MS solution package it comes with a wide range of solid development tools and support with Visual Studio family of products Silverlight supports the integration of multimedia graphics animations and interactivity in a single runtime environment Similar to Flash Silverlight also supports vector rendering in addition to the typical Bitmap based rendering  The main hindrance in Silverlight and one of the reasons it was not used is that it is far behind Flash in adoption rates Where the Flash Player 10 adoption is around 987 in mature markets 3 Silverlights adoption rate was around 22 4 at the time of the project start Whilst being impressive in its abilities the Silverlight technology was rather young in 2009 and sorely lacking in features  The technology chosen was the de facto standard for rich content online in 2009 the Adobe Flash platform As mentioned in relation to Silverlight it is the most widely spread browser technology in the world and has a mature and solid development environment and support Flash supports vector rendering as well as bitmaps and the newest versions from 11 onwards support using OpenGL based GPU rendering as well When the project started the player version target was 101 but the release version of CCA is targeted for the Flash Player 11 platform 7  Whilst requiring the installation of a plugin the wide spread adoption of Flash made it a clear and easy choice for a consistent user experience across platforms and browsers There are up–todate versions of the player for Windows Linux OSX and Android platforms  The heralded revolution of standards based online development – HTML5 – was not around in a prominent way back in 2008 After studying the possibilities of using HTML5 and Javascript based solution as an open alternative for the plugins it was found that the performance of Javascript VMs and especially the rendering performance was subpar when compared with the plugin based technologies  This has changed a lot from the year 2009 and the newest JITJust in Time compiler in Google Chrome is starting to surpass some of the plugin solutions in pure crunching power However the rendering capabilities of browsers vary a lot based on the versions and the adoption rate of new browsers remains one of the larger obstacles in generating modern standards based rich solutions online  Being a consumer mass market solution we aimed at all three big players in the market on consumer platforms Microsoft Windows family Apple OSX and Linux based operating systems needed to be supported The support for our decided solution platform – Adobe Flash is wide and envelopes all our target platforms  The benefit of choosing a browser ran online platform was also the generic platform independent nature of the technology With a single solution we could reach and deliver a fully functioning solution to the whole user group As seen in Figure 8 the project supports the main versions of Microsoft Windows from XP onwards OSX from 106 onwards and Linux Red Hat Enterprise 56 or later openSUSE 113 or later and Ubuntu 1004 or later in both 32bit and 64 bit varieties 7  In principle a twopersonproject the CCA was a significant undertaking The viability of using browser based technology for creating a complex mathematical rendering engine for clinical and consumer use was something not many had done before  The biggest challenges of the project were on the pure engine building level The rendering of creatures in real time without prerendering or cheating in the rendering pipe line and the performance of path finding logic and path wrapping The mathematics used were performance intensive and the rendering of several creatures with hundreds of rendering points and surfaces in addition to the path creation with continuous Bezier curves was difficult for a CPU based rendering engine  The goal of building a medical platform for consumer use had its own challenges The combination of a number based mathematics oriented solution and attractive user experience provided many obstacles Keeping the UI code separated from the engine and ensuring a solid separation of concerns was essential in building a working platform  On the platform front the separation of as much presentation and user logic to the back end for control and data based optimization of training posed several challenges to overcome The parameterization of almost all UI elements and processes from game modes and their configurations to even trophy icon generation was a complex issue to solve  Cognitive Gaming  A type of gaming and exercise that is designed to help and improve cognition Used in the aid of recovering from brain trauma also used as recreational activity believed to be beneficial to the mind  RIA  Rich Internet Applications Browser based technologies that enable creation of desktop like features in the world of internet Example technologies include Flash and Silverlight  Modern Browser technologies  A technology stack that contains all browser ran technologies HTML5JS Flash Silverlight Java   Neuroplasticity  The theory that the brain is capable of physical change and improvement based on outside stimuli  Creature  The main target in the project CCA A collection of mathematical formulas that are rendered based on a path finding engine to represent an organic “creature  A single unit target of the platform  Bezier curves  Smooth parametric curves based on 2 control points and a start point and an end point Essential in project CCA  User experience  A combination of usability user interface interaction design information architecture and animation to cre ate a complete use experience for a user  Platform  A combination of technologies that form a coherent reusable and deployable whole An extensible combination of modules that works as a basis for building content on top of  Figure 9 Key concepts of the study  Key concepts used in this study are shown in Figure 9 They cover the areas of cognitive gaming and modern browser based game development as well as the principle technological choices in organic rendering  Cognitive gaming is an exciting new area for consumers and scientists alike There have been various studies and trial projects about using games as platforms for advanced learning learning through play and using virtual worlds as class rooms for learning Cognitive gaming takes the elements of gaming such as repeatable tasks reward models user tracking and fun of play and combines them with medical research and recuperative methods for a game experience that also benefits and improves cognition  The basis for all brain exercise games and all is the concept of neuroplasticity or brain plasticity the ability of the brain to change physically throughout life in response to stimuli The times when changes happen in brains are in the beginning of life when injury hits the brain and whenever something new is learned and memorized 8  Up until recently it was believed that the connections in brains remain fixed with age and physical changes are impossible However recent studies have shown that brain keeps on changing through learning and stimuli throughout our lives 9 As an example when comparing professional musicians to amateur musicians and nonmusicians the actual physical volume of grey matter in areas involved in music such as motor regions anterior superior parietal areas and inferior temporal areas was larger with the professionals who practiced over one hour per day 10 These changes were also greater when measured over time  Another example came from a study done on extensive learning with German medical students They used medical imaging to monitor the brains of the students before their medical exam and after and compared the results to similar students who were not studying at that time The students brains showed anatomical changes in grey matter in different areas of the brain including the parietal cortex and posterior hippocampus parts of the brain known to be involved in learning and memory 11  Despite the recent studies and interest in training programs to be used there have been very few long term studies in the effects of cognitive games and training While there are studies that show the short term implications of training 12 especially on those suffering from early stages of cognitive impairment there have not been sufficient enough studies to show whether the training can postpone the effects of such impairments as dementia 13  In 2005 the size of the brain health market globally for software and biometric applications was estimated to be around 210 million dollars The estimated value of the market in 2012 is over one billion dollars and by 2020 it is estimated to reach six billion dollars in value 14 This rapid growth comes in part from recent research and in part from many medical professionals and researchers trying out the possibilities of using their research in helping people on the consumer market  As seen in Figure 10 the biggest growth expected to happen in the growth of cognitive gaming and brain fitness is the consumer market by a large margin The growth of self service training portals with training regimes directed for home use and selfimprovement are already growing fast and are expected to do so in the future as well The other big growth area is in the area of insurance and health care as well as elder living The benefits of preventive training in age related deterioration are substantial as well as in the rehabilitation of people with brain related injuries The savings generated by such actions could be monumental Aside from the main growth areas the uses of cognitive games in school systems as well as employee care are expected to grow  The cognitive game market is divided into 2 different parts the pure software products and the biometric applications that require actual hardware to measure physiological responses applications Examples of biometric products include products that measure hart rate variability or brain activity through EEG Electroencephalography the recording of the brains electronic activity over a short period of time through the scalp Our reference project belongs to the software category and does not need any hardware to function  In the area of cognitive games the reference project lies the focus on perception attention and working memory there are various competitors and products on the market To keep the subject more valid and tied to the subject of this study the focus is on online browser based cognitive gaming platforms and their use cases technology and popularity  The biggest company in the online cognitive gaming market in 2012 was the Lumositycom Lumosity is partnered with researchers at Berkley Harward and Columbia and works with numerous health care organizations to help create cognitive gaming experiences The service has over 25 million users and it provides comprehensive and personalized training programs based on user accounts 15  As seen in Figure 11 Lumosity online training program conveys facts and scientific information about what the tasks you are performing at the moment provide and manages to create a solid user experience with enough information and play to make it interesting The training program is fitted to your needs based on a simple questionnaire The first steps in Lumosity are free but after a few games you get to a point where you cannot benefit from the service without subscribing  Technology wise the frontend base portal of Lumosity is based on standards based HTML5 and CSS3 providing the general test framework and admin functionality in the portal The actual games themselves are made with Adobe Flash technology similar to the reference project The games are rather simple in function and are very event based in nature In addition to the web interface Lumosity also has a mobile application available for the Apple iOS platform  From a user experience and game design point of view the portal comes across more as a collection of different games and a framework that ties them together as seen from Figure 12 The different games use different visual cues and styles based on the subject matter and are not uniformly under the same visual design aspect as the main Lumosity portal However they do contain repeating elements in the introductory controls to provide similar functionality across games  In addition to providing a cognitive gaming service Luminosity has scientists actively working in the field of research and finding out how to best use cognitive gaming to benefit the human condition In a recent study it was shown that there was improvement in the working memory and visual attention of the target group when using a web based training application outside of a clinical trial setting 16  The other competitive platform taken as an example in this study is the My Brain Solutions portal at wwwmybrainsolutionscom Similar to Lumosity the portal provides a brain assessment in the beginning based on which it generates a user profile and a training program for you to follow Unlike Lumosity the brain assessment is a quite a comprehensive test of memory comprehension emotion and other cognitive functionality and lasts around 30 minutes up front 17  As seen from Figure 13 the My Brain Solutions contains a personalized training solution as well as charts on how the users brain and performance range on the variety of test subjects In addition to points which Lumosity used in this platform the user also has badges a reward mechanism similar to achievements to convert the arbitrary numbers and progression into more human readable terms The platform allows you to set your own goals and encourages you to set actions for yourself to keep you busy  Technology of the My Brain Solutions follows that of Lumosity and others the main site is a web portal built on web standards while the individual games are based on Adobe Flash technology In addition to the web portal My Brain Solution has various applications for different mobile platforms targeting a specific feature such as MyCalmBeat that focuses on lessening stress and increasing focus through slow breathing  In comparison to Lumosity My Brain Solutions has a more unified gaming platform with all the games showing similar introduction screens button layout and statistics  in game information when doing tasks As seen in Figure 14 the different games all feel as if they are of similar family and go well together visually with the main visual identity of the portal From initial testing the games seem to have more complex interactions in them as well  From the more entertainment oriented area of gaming recent years have shown increased popularity for titles such as Brain Age 2005 for the Nintendo DS console that sold over 1896 million copies as well as its sequel Brain Age 2 20052007 1483 million copies to date 18 While these products were very popular and studies have been done in order to evaluate their possibilities 19 Nintendo has distanced itself from the use of scientific proof of benefit in the games 20  An example of the Brain Age line of games can be seen in Figure 15 In the Brain Age games the user is expected to play a small amount each day according to a training program This is a common approach to brain fitness and is employed in the reference project as well The tasks a user performs orient around simple calculations and mathematical questions memory exercises Stroop tests as well as Sudoku puzzles  The cognitive gaming platforms focus on a certain set of games each targeted for training different parts of the cognitive system These areas are close to the ones used in clinical neuropsychology but understandably are not as exact or as precise due to the consumer market approach These target areas can be roughly divided into three categories memory attention and executive function Some platforms such as My Brain Solutions also contain tests related to emotion and human understanding  Memory games test and train our ability to memorize items words patterns and other objects There are different variations of memory games a commonly used one involves working memory or short term memory These tests are called Nback tests where a user is presented with a sequence of objects and the task is to react when the current object matches the one shown N objects before 21 The Nfactor can be adjusted to make test more or less difficult Short term memory tests are well suited for consumer market testing the test gameplay does not take overly long and the results are parsed live An example of an Nback game is seen at the top in Figure 12 as well as another type of memory exercise seen in the middle  Attention games focus on our ability to perceive and react to our perception in a given controlled environment The idea of attention tests is to train the cognitive processes of focus and visual search as well as long term attention The tests often focus on pattern recognition as well as visual attention The way to test attention implemented in the project CCA is to have multiple moving objects on screen that the user has to follow and react whenever the object achieves target state Target state is a change in the shape colour contrast or form of the object Numerous examples of attention tests are found in the reference project platform as well as the example game at the bottom of Figure 12  Executive function is an umbrella term used for various cognitive processes and sub processes working together  Executive functions are those involved in complex cognitions such as solving novel problems modifying behaviour in the light of new information generating strategies or sequencing complex actions 22  In the reference platforms for brain gaming the tests for executive function include games related to arithmetic quantitative reasoning planning verbal fluency and task switching An example of an executive function test can be seen in Figure 16 This task is about connecting a series of nodes with as much area as possible without ending in a dead node a task that requires both planning and reasoning  The reference project is mostly based on various attention games as well a testing mode for Nback tests to improve your working memory and a continuous change mode that falls in part into the domain of executive function  The term game engine came to be around mid1990s in reference to 1st person shooter games such as Doom by ID Software Game engine is a platform for game development that employs data driven architecture to create reusable software components such as a threedimensional graphics rendering system collision detection system and a physics simulation 23 The line between an engine and a game is often blurry and to date there are few engines that can adapt to more than a few genres of games  The engine defines how the game is rendered on screen and influences a lot of the design around the core concept of the game Some game engines include tools for building the actual game content on top of them such as Unity Unreal Engine or CryEngine 242526 This is often called scripting since it is most often done using a scripting language Many of the engines contain their own scripting language 26 or use a set of commonly supported high level languages Unity includes support for C Javascript and Python based Boo 24 Some engines provide only the actual rendering engine for abstracting the hardware level such as Ogre 27  Game building in general has gone through a renaissance of sorts where the middle ware is increasingly important in creating game experiences in the industry as well as in the education of game development 28 A large portion of games today use middleware such as Bink video 29 Havok physics engine 30 or the Adobe Flash harnessing Scaleform 31 for game user interface building These middle ware programs provide a necessary relief from the complexity of building a modern game each solution doing its part providing a polished and optimized way of handling one aspect of the game  In the field of medical and cognitive gaming some are using the same game engines as entertainment oriented gaming uses 32 while many use their own engines based on a higher level programming environment such as Flash Silverlight or Java to create their own base engines Many cognitive games are not yet complex enough to have the need for specific engines but with the increase in demand development budget and competition it is only a matter of time  When talking from a more conceptual level of how game interaction and gameplay is handled there are two types of game environments static and dynamic In the context of this study this distinction is defined as the way game events are controlled in the game and the way the engine handles rendering and interaction  The traditional game engine is static as in everything in the engine is defined by an artist or a developer every move you make is the result of a careful calibration iteration and concepting Static engines allow for absolute control for the games designers and are in many cases more optimal from rendering and calculations point of view In the comparison products Luminosity My Brain Solutions and the Brain Age all fall into the static game engine section They are all also very event based in their approach everything happening in the games is not based on real time calculations but on specific events happening at specific times and the reaction to those events  The downside of static game engines is that in controlling everything they lose the element of surprise in some ways When everything is designed there are no happy accidents nor odd gimmicks that a player can find that cannot be reproduced and everything in general works as it does always the same reliable way Many static engines also employ a basic physics model with hardbody physics In such an engine the physical values are tweaked and set by the designers in a way it replicates some simple form of physics but does not really allow for proper surprises  While being more obscure and harder to control dynamic game engines base the world they render on a set of rules These rules may be physics different path algorithms and so forth but they all have in common the lack of direct control over what happens inside In a dynamic engine the game designer gives the objects targets creates behavior and sets up boundaries but how the engine executes these is left for the engines internal system to decide  Some games use the dynamic game building in ways to create randomized environments procedural content based on a set of rules Games such as Diablo 33 and Minecraft 34 have used procedural generation to create whole levels or in the case of MineCraft a whole planet The traditional problems with computer generated content in games have been the repetition of content and unnatural and uninteresting combinations Some newer games use this dynamic or procedural generation in the generation of items for the player to use such as the weapons in Borderlands and Borderlands 2  35  In gaming physics a more solid way of doing physics in a dynamic way is called softbody physics where a physical object is a collection of its sub parts physics This is immensely heavy in calculation and is only now emerging with the new Cryengine 3 25 and other new game engine platforms The project CCA is not in a world of fully fledged physics nor is the world building in any way overly complex but the engine and the way the creatures work is entirely dynamic  User experience UX has many different definitions depending on the subject matter it is related to A classic example is from 1996 from the first annual ACM Interactions Design Awards  “By “experience we mean all the aspects of how people use an interactive product the way it feels in their hands how well they understand how it works how they feel about it while theyre using it how well it serves their and how well it fits into the entire context in which they are using it“ 36  The NielsenNorman group define user experience as  User experience encompasses all aspects of the endusers interaction with the company its services and its products The first requirement for an exemplary user experience is to meet the exact needs of the customer without fuss or bother Next comes simplicity and elegance that produce products that are a joy to own a joy to use True user experience goes far beyond giving customers what they say they want or providing checklist features In order to achieve highquality user experience in a companys offerings there must be a seamless merging of the services of multiple disciplines including engineering marketing graphical and industrial design and interface design 37  In principle UX is everything a user feels sees experiences when in contact with a company andor a product The point NielsenNorman make is to go beyond the needs of the user the realization that UX is far more than providing the user with what they need but how they experience it as well The joy of use aesthetics and message all combined in a thought out package  User experience design UXD is an umbrella term that covers the different facets of expertise required to create a wholesome UX The roots of UX design come from Human Centred Design HCD HCD can be summarized as  Positioning the user as a central concern in the design process  Identifying the aspects of the design that are important to the target user group  Developing the design iteratively and inviting users participation  Collecting evidence of userspecific factors to assess a design 38  In addition to the methodology of HCD UXD builds on top of HCD with more complex cultural and business factors While traditional HCD based usability factors were about performance and smooth operation UXD brings along aspects of social interaction the importance of aesthetics both very culturally complex and context requiring concepts  Since both UX and UXD are very broad subjects in the context of this study the focus of UX is confined to how our project platform uses UXD to create a suitable UX within the platform itself The areas presented in relation to the reference project are interface design  aesthetics interaction design  movement information design  communication and playability in the cognitive gaming context There are also various UX concepts such as presence immersion flow fun involvement and engagement that try to describe the UX in games 39 These theories while important are not discussed within the scope of this study  What the success of iOS and mobile platforms has taught us user experience matters People are willing to pay for better suited and thought out solutions The wakeup call provided by the mobile markets sudden rise has not gone unnoticed in the world of gaming Especially in large companies there has been a surge of focus on hiring user experience designers and front end designers to focus on the neglected parts of games the game UI and the interactions with it  Games have been on the forefront of human computer interaction for the past 20 years as the playability of a game essential to the overall experience is uniquely a quite complex endeavor from a user experience point of view But while focusing on playability inside the engines and games themselves they often forget the first thing a user sees when they enter a game the user interface of the game itself The navigation settings and saveload these top level controls affect a lot on how one perceives the game experience  Often these are badly designed riddled with flaws inconsistencies and hiding of information It is not uncommon in games that you are not sure what will happen when you adjust a setting or a parameter The inclusion of UXD methods and experts has made games more userfriendly and easy to use In building a platform for cognitive gaming the unified UX for the games within the platform as well as the overarching user interface was an important part  As has happened throughout time the advance of technology and tools for creating visual communication have changed and evolved at a rapid rate in recent decades Despite this change the essence of graphic design remains unchanged to bring order to information form to ideas and expression and feeling to artefacts that document human experience 40  The international typographic style has been a significant and influential style in graphic design for the past 50 years Its origins come from Switzerland and Germany in the 1950s and it is also known as the Swiss Design The visual characteristics of the style thrive for unity though a solid mathematically based grid objective photography sansserif typography and a solid copy that presents information in a clear and factual manner 40 More than pure style the importance of the international typographic style lies in the attitude and approach its early pioneers adopted The role of graphic design was formed more towards shaping information and communication then personal expression and artistic eccentricity  This clarity of style and information is a basis of much of modern online communication and it is the basis for the style in CCA as well In addition to a clear and unified visual communication in CCA the focus was also on the visual narrative animations movement and flow that affect how a user perceives a product  Typography is the art of type the act of arranging to make language visible In the modern day the term envelops many crafts from the traditional typesetters and compositors to graphic designers and artists In the context of this study the term is used to mean everything we do with type in a digital platform Font sizing characters and legibility  Typography exists to honor content  Like oratory music dance calligraphy – like anything that lends its grace to languagetypography is an art that can be deliberately misused It is a craft by which the meanings of a text or its absence of meaning can be clarified honored and shared or knowingly disguised 41  In modern digital communication the use of solid typography to create a unified and visually attractive legible message is an essential part It is used to both communicate efficiently as well as to add a feel personality and grace to the communique  The technological choice in CCA the Adobe Flash Platform is a good choice for working with typography as it contains a much more advanced text rendering and font support engine than the traditional browser solutions With the support of fully embeddable type the platform could take use of proprietary fonts with full fidelity and control With the recent advances in CSS type support it is even possible to use proprietary or special fonts on standards based online communication 42  In project CCA the main typography is provided by the fairly common Myriad Pro originally developed by Adobe in 1992 and widely used by companies such as Apple Walmart and Wells Fargo 43  Myriad Pro is a versatile sansserif font family designed for mostly digital use The family contains a wide variety of weights and widths to suit the needs of the CCA platform It is a simple elegant font with excellent readability An overview of Myriad Pro font rendering with examples can be seen in Figure 17  It is often a problem for digital platforms to provide similar design and typography in different languages as the Latin based languages have their own typography and the Chinese Arabic and various other languages have their own typeset Rare fonts support even the whole plethora of European language typesets from Cyrillic to Greek Myriad Pro is a good choice for a language versioned platform since it provides a complete support for Greek and Cyrillic characters which enables the use of same visual fidelity across languages  The Flash Platform also provides the tools to embed different character sets for the use of completely different language characters such as having Myriad Pro for Latin based languages and a specific font for the Chinese market Which font to use is decided during runtime when dynamic content is loaded based on the character sets involved in the UTF8 encoded content  As seen in Figure 18 CCA uses Myriad Pro in two different weights regular and bold The main headers for each screen are 36 points big allowing for a clear distinction from the sub headers and regular copy text As CCA is not a very text heavy platform the most used texts are the buttons and sub headings All elements that indicate something that can be activated or form a separate section of information are bolded for effect All regular text information notices explanations and such are in a readable 14 pt size  In interface design the essential glue that keeps a view a page a dialog together is the composition also known as the page layout Composition can be thought of as the link between art and mathematics the use of relations numerical patterns such as the golden section and geometric shapes to create a formula that organizes content in a meaningful and clear way 44 Composition can be based on various styles from a single visual to perhaps the most used element of composition grid theory An effective tool is also to use Gestalt Laws of grouping the principles of how the mind organizes visual data in creating coherent compositions 45  In CCA the whole composition is based on a centred grid with a focal point always at the horizontal and vertical centre of the display area All content containers and controls as well as all UIelements are aligned based on the centre point As seen in Figure 19 the centre based composition is followed by the main playground the user menu at the bottom and the game menu at the top as well as the main information dialog at the start of the game During the actual game the main visual time indicator is at the centre point as the most obvious visual element for a user  An important aspect of composition is the use of spatial relationships The space can be created by content using images texts icons lists logos or just plain text – or it can be created by the space between content called negative space or white space The space can be actively used to create a point or it can be passive there just because the layout process requires it 44 The use of negative space is essential in giving air and increasing legibility in digital design It can be divided into two categories macro white space – the distance between major content elements – and micro white space – the distance between elements within content elements such as lists  A challenge in composition in a digital medium is the dynamic nature of the viewer setups The layout can be viewed with a screen from 1024*768px up to 2560*1440px with varying pixel density DPI This creates quite unique issues for the use of white space and the arrangement of elements The traditional approach in digital design is to set a certain target resolution a compromise that contains reasonable resolutions and provides a good enough result for those not matching the target resolution  As Figure 20 shows the most popular screen resolution of users in Europe has grown from 1024*768 to 1366*768 in the last 4 years Beyond just the resolution the size to resolution ratio or DPI Dots per Inch has also grown and diversified The rapid change in resolution as well as the fragmented size variance has resulted in a situation where designing for just a single base resolution is not a preferable option anymore This is true even without taking into account the boom in mobile browsing and devices with smaller physical size and screen resolutions  As a response to the ever growing resolution and varying DPI the modern way of doing digital layouts is in a state of transition an ever larger amount of platforms and frontend technologies enable the use of adaptive or responsive layouts A responsive layout adapts to the viewing area by resizing reorganizing and resampling the information presented to the user 47 It is often based on set of steps within which the content scales and when a step changes the content arrangement and  or organization itself changes  A key challenge in CCA was the requirement of being completely resizable upwards from a minimum size 1024*800px The elements must fit the screen from the minimum onwards while maintaining a suitable space and visual feel to them A part of the solution for this was the centerbased grid design Because the platform can be resized both vertically and horizontally basing the user controls in the middle of the screen enabled efficient use of both axes  In CCA the whole viewport scales based on user resolution but the game area itself has a minimum and a maximum stop for both performance and playability reasons The size of the game area affects how big an area the user has to visually scan in order to notice changes This area cannot be too large otherwise it starts affecting users performance scores  In addition to the scaling layout size all the main views in the UI of CCA can be dragged around the screen by the user as needed This empowers the user to arrange the UI in efficient ways depending on their own resolution It also helps solve the problems of using extra space in the GUI Each element has an active drag area everywhere where there is no control or content presented The dragging works with slight simulated physics calculating a primitive velocity of the object when a user drags and releases it and using a static coefficient for friction decreases the elements velocity until it comes to a stop  An important part of understandable GUIdesign is the use of repeatable recognizable visual controls and containers also known as elements The use of repeatable controls is also of benefit from the development point of view as it is substantially less time consuming to create base classes for repeatable elements such as a content container or a button and button sub type Repeating elements are also essential in creating a coherent user experience where the user can predict how a certain view behaves and how to interact with controls  CCA contains a set of containers and controls for the users to use that is largely based on drag able containers that automatically centre on screen when created and later on allow the user to place them wherever they please As seen in Figure 21 the main container is the base of every content element It is vector based stretchable and contains the dragenhanced functionality  Other common elements for most of project CCAs layouts are the main header the general button element and the close button In prompts the close and the general button provide the same functionality with different messages From these base elements most of project CCAs views have been built  Aside from the floating elements a typical CCA UI contains the user control menu This element attaches itself to the bottom of the game screen and displays user specific information such as the account name and the amount of points the user has as well as providing logout functionality During a game play the UI also has a game related menu with information about the game you are playing as well as controls to get back to the main UI  For proper interaction every element needs three basic states for a solid user experience normal hover  active and clicked  reaction Examples of project CCA button elements three states can be seen from Figure 22 They all follow the same logic having a subtle highlight specific for the type of the button and a full fill recognizable action state that is clearly indicated Providing these states allows the user to always be in control to have a predictable response to every interaction  CCA contains also elements that are not meant for interaction by the user but for mere show of information During the game there is a large clock displaying how much time is left in the current game and how many sections are included in it Another element that a user can view is the indicator for performance in the current game The symbols stars and crosses show the user how well he is doing during the exercise whilst being inconspicuous enough not to distract the exercise Examples of the clock and the starcross performance indicator can be found in Figure 53  Colors contain a lot of meaning The selection of colors for a user interface and especially for branding of a platform is an important step Colors while being culturally dependent and highly subjective offer a lot of meaning and interpretation and can greatly enhance the user experience and aesthetics of a system Whilst there is a plethora of articles about color in physics psychology and other disciplines in the context of this study color is used to discuss the merits of the use of color in the reference project from a user and artistic point of view  Figure 23 shows the main color themes in CCA The main color is a green shade R 184 G 227 B 115 normally used in a radial gradient with multiple stops between the end and start points The gradient form is to create a feeling of enlightenment a subtle focus on the content in the middle The green background and color scheme is used throughout the buttons and modal dialogs wherever something related to the main game is encountered The blue R 91 G 166 B 218 theme is used in information and statistics where there is a need for more contrast with the elements and a distinct visual appearance  Other than the dominant background and highlight colors the main color for text in CCA is always white with a subtle drop shadow to bring it forward from low contrast backgrounds In buttons and other interact able controls the active color is always dark grey or black to highlight the change and alert the user to an action  In addition to the use of color in the platform interface the games themselves have a specific use of color Each creature in the game has its own color scheme containing various hues and shades to create an organic “creature  The use of shapes and colors in the game relies on feature integration theory a theory of attention suggesting that perceiving stimuli can be divided into two separate tasks features and objects Features can be registered fast and in parallel whereas objects are slower and separately identified Visual searches regarding these two tasks are called feature search and conjunction search Feature searches are fast sweeps targeting only one feature such as color or shape while conjunction searches work with a combination of features 48  In CCA there are game modes targeting both types of visual search The game types focusing on multiple creatures on a neutral light grey background focus on the use of feature search where the target mode of the creature is a change in contrast color and  or shape The game platform also has a mode for rendering a specific Perlin noise background matching the color set of the active creature aimed at the use of conjunction search The use of a background where the creature easily blends in forces the user to focus on finding the shape by combining color and shape information The task of separating the object from the background layer is called figureground separation 49  Motion is a powerful tool in the world of modern digital communication when creating a unified user experience especially for the consumer market By creating a specific visual narrative a repeating pattern of interactions and movement that builds upon the visual style and reinforces it with suitable movement one can build a memorable and effective user experience  In the realm of traditional animation there are 12 principles for believable movement 50 of which the following are relevant to transitions in digital media and the CCA platform  • Anticipation o To prepare the audience for the action and to make the action feel more realistic  • Slow in and slow out o To simulate most movement in the real world Most of human movement and gravity based movement have an inout easing curve it builds up and it builds down  • Arcs  o Trajectories are followed by most natural motion most living creatures have structures that enable them to move in certain ways follow certain paths Emulating these trajectories creates organic looking movement  • Secondary action o A complementary animation that adds to the main animation enhances its effect  • Timing o Correct timing makes objects more real like they were following the laws of physics  • Appeal  o Anything the user sees and finds likeable pleasant design a quality of charm simplicity and communication Originally about drawn characters but applies to graphics as well  The sections of movement and animation discussed in this study can be divided into three categories Timing and flow movement patterns and easing and nonlinear motion  Timing is the part of animation that gives meaning to movement 51  The key in efficient visual narrative is twofold timing and delays Timing is the essence of transactions interactions and transitions within the platform How long does an element transition how it appears how it behaves Delay is essential in combining different views and states With the combination of these two the base structure of visual narrative is achieved  As Figure 24 shows most transitions in the project platform last between 300ms and 600ms In views the time in is always 500ms and the time out varies based on view but the change trigger is always 750ms when a transition from a content state is initiated the out animation call stack is started and the 750ms delay timer is initiated at the same time Regardless of the time the view takes to animate out in 750ms the animate in of the new state is called and started This overlap makes it possible for a user to interact during transition and the loose coupling between view animations makes the user always be in control  On average for a transition to be within acceptable limits it has to be between 100 and 1000ms Immediate response for actions like clicking requires some indication of reaction within 100ms of the action If you go over the 1000ms limit people will start to think the system is sluggish and unresponsive 37 In CCA the times are based on iterations of visual aesthetics feeling of motion and professional opinion Motion is a rather tender art and the feel of a transition is based on the shapes colours graphics elements and the surrounding elements of the target There is no unified rule that can be quantified for every solution it depends greatly on the context and surrounding platform  The buttons have the same click animation time always to make the reaction based on a user intent unified The difference in animation in and out time is essential since the shapes and means are different in a menu button the line colour and shadow are animated in the close button the shape and colour are animated and in the general button the gradient shadow and text colour are animated All the buttons have different hover state animations however they are visually coherent and unified to represent the same indication for the user  Tweening or inbetweening is a method of interpolating the change in value between points A and B A basic tween is linear consisting of a predictable amount of points between A and B Dynamic tweening is adding acceleration and  or deceleration effects to animation by using easing algorithms 52 The problem with static  linear tweening is that the movement looks fabricated and clunky Dynamic tweening solves this problem by adding natural feeling movement patterns such as the slow in slow out principle of animation  In programmatic animation easing algorithms enable the feeling people perceive in physical movement This effect is done by applying an algorithm to the interpolation of movement of the object In principle the ease affects how the steps of movement are shown where there are more steps and where less along the path of the animation Figure 25 shows the most common easing algorithms used in tweening libraries on various platforms Most are based on Robert Penners work on easing and tweening  52  In CCA the use of easing algorithms is subtle and a mix of the following  Content boxes o BackeaseOut for background o QuadeaseOut for fades  Menu elements o QuadeaseOut for fades and color  o StrongeaseOut for the line  Countdowns o StrongeaseOut for scaling  Drag o Based on inertia a custom easeOut calculated based on mouse movement and velocity  Most of the transitions are elegant simple based on the subtle curve of Quad based movement The parts where there is a need for highlighting movement as in dialog boxes and the pop out of elements the Back based movement is used For fast movement and to give the feeling of control of pace the Strong movement is used  For user reactions the easing type of easeOut is almost exclusively used in CCA The easeOut means the movement starts fast and slows down as it decelerates in the end It works well for user reactions as the start of the movement is the user initiated action the response needs to be fast and clear and easeIn algorithms tend to make the transition seem sluggish EaseIn is only used when animating a background out with scaling of the height of the element Since the Back easing algorithm has the bounce effect and the background is closing it is visually appealing and logical to have it do the bounce in the beginning  After timing and easing are taken into account what is left to create a solid visual narrative is the actual movement of objects There needs to be a recognizable pattern of movement that strengthens the visual branding and solidifies the feeling of the user experience In CCA different containers have different movement but the general movement patterns are very similar and create a solid feeling of a unified platform  The button animations all contain the same click effect animation and timing When a user clicks a button the text or main shape of the button turn black in colour over 300ms The menu button is a good example of the secondary action principle of animation The main action is the colour animation to black the secondary supporting animation is the line growing from the left to right underneath the text The line animation does not dominate the visual but builds on the effect and supports the feeling of highlighting Examples of button visual states can be seen in Figure 23  Dialogs and content views all appear in a similar fashion the background animates in first by scaling the height to 100 from 0 while animating the opacity of the element to a full 10 form 0 These two combined make the background appear smoothly from thin air The content inside dialogs is animated in with a sequenced animation fading in elements and in the case of lists also animating the x coordinates of the element from –N to 0 Where N is subjective to the size of the element The sequence timings can be found from the Figure 24  The basic pattern of animation in user interface elements for project CCA is the fade in  fade out the simple vanishing and appearance of objects It is supported by secondary actions such as colour animations movement scaling of horizontal and vertical size depending on the object animated  From development point of view creating a visual narrative requires a specific way of creating your user interface classes When transitioning between states many platforms and systems make the mistake of queuing the transitions in a way that makes the user wait for interactions between states This breaks the narrative flow and lengthens the response time of the system to the users frustration  When creating a solid platform with UX in mind it is necessary to plan in advance and to create methods for handling view states concurrently In project CCA when states are transitioned every control taking part in the transition executes their hide  show mechanic There is a set delay between calling the next state when current state is transitioning out and there is no queue As soon as a control starts to form on the screen a user can interact with it The user can stop the current transition and use another control while transitions are in place as well  Creating modern user experiences requires the developer to take this into account when creating their solution It saves time and effort to solve the issue of state handling and transitioning between states already in the beginning or planning phase of the project When building individual controls it is beneficial to have a set template to start from that contains all standard functionality needed for state handling Whether to implement this via inheritance or other methods is up to the developer  In CCA every visual element from single controls such as a button or content views such as the main menu to modal dialogs and user menus all of them contain basic functionality for handling their own view state The repeated functionality in all visual controls of CCA are show hide and dispose as seen in Figure 26  A necessary requirement and in creating modern interfaces with programmatic animation is the ability to override ongoing animations Overriding enables the stopping transitions and interacting with controls mid transition to keep the user in control Modern UI technologies either contain suitable Tweening libraries inbuilt or one can use one of the many open source alternatives out there In CCA the library in use is an extremely robust and extensive tweening platform called Greensock Tweening Platform 54 by Jack Doyle wwwgreensockcom GSAP is available for Flash and HTML  Javascript based platforms  GSAP abstracts many of the verbose methods needed in creating programmatic animation such as the setting of filters timelines and sequencing by offering a simple but powerful syntax that allows for the tweening of any numeric property alongside platform specific properties such as CSS transformations It is also quite robust efficient and contains a full open documentation The platform is used by many of the top sites and digital experiences in the world such as big movie productions big brands and art projects 54  In CCA there are two different systems for showing performance and ability The user has access to pure numbers and graphical visualizations of performance and numeric data The platform provides statistics on all game modes as well as training program based data for the user to analyze The idea is to enable the user to transparently go through how well he has done and how he has improved and draw his own correlations about the program Example of the histogram approach is shown in Figure 27  The second way the user is communicated the importance and performance of the data is via a very UX oriented method – achievements The user is provided different rewards in three different categories of achievements achievements stars and trophies The different rewards are all catering to a certain part of the measured data and user response Every achievement has their own icon and a badge the user can see once they log in  The achievements provide a very human understandable way of communicating complex data oriented events The user can for example get a trophy from doing better than previous runs or for having a perfect run where there were no missed reactions The rewards vary between the different game modes In comparison to the raw data shown by the histograms that do provide a valuable way of seeing visually where you improve the achievements allow the user to gain specific feedback for specific parts of their game performance Example view of achievements in CCA is seen in Figure 28  CCA is a user experience driven cognitive gaming platform harnessing the power of the Adobe Flash Platform in the front end implementation and the LabVIEW system design software as the server side solution  The general architecture of the platform is based on typical clientserver architecture Overview of the architecture is seen in Figure 29 The client connects to the server and authenticates the user account after which the server sends a packet of data that determines what options are available for the user with the account The user interacts with the options available and a new request is sent to the server to which the server responds by providing wanted data be it user account details or a list of games  As is common for Flash based web applications there are no page loads or page requests during the application run The state of the page is managed by the application platform itself and all interactions and reactions by the user are parsed processed and handled without the browser As such the platform itself can be ran with a flash player outside of browser environments as well  Labview short for Laboratory Virtual Instrumentation Engineering Workbench is a programming and system design platform built by National Instruments It uses a dataflowbased programming language which enables users to program logic with the use of graphical block diagrams 55 The platform provides a very visual way of creating functional systems  The platform front end is based on Actionscript 30 AS3 The Ecmascript based 3rd iteration of the Actionscript language is an object oriented programming language running on the Actionscript virtual machine The version of the virtual machine that supports AS3 was built from ground up to support the new OOP nature of the language In combination with the Flash development tools like Flash Builder and Flash CS5 the AS3 is a powerful tool for interactive media  The front end is built to be modular and much of the control comes from the backend This datadriven architecture helps keep the platform modifiable for different purposes and user programs without making changes in the front end implementation 23 The server side provides lists of menu items and games available as well as statistics and user account data The front end has different tracks of interaction and progression through the solution that are triggered by the server messages  Figure 30 gives an overview of what packages views and controls the CCA front end platform contains In AS3 projects there is always a root stub that initiates the solution when started The main solution initiates the application model and main UI elements and queries the server for language versioned UI definition XML After that the navigation service handles user interactions and along with the UI state handler keeps the visual and logical state of the application in sync  The model package contains main application logic and data related functionality The application model handles application states with the navigation service and holds references to the different parts of the application The navigation is integrated tightly with the UI state handler that handles the UI state changes and controls what the user sees Data services handle all server communication and data parsing  The creatures package is the home of the main rendering engine All the heavy lifting mathematics with path generation and creature drawing is done by the package The pathfinder handles the generation of new path points and general route handling The creature handles the algorithms creature specific calibration and state information The brain is responsible for combining the input from path finder and the creature and to pass off the necessary values to the renderer which then does the actual plotting and drawing of the creatures A detailed description of how the rendering core works is in Chapter 6 of this study  The game handling package handles all game related logic Once the game data has loaded and parsed the game handler initiates the correct game mode The game mode handles the states within the game everything from info prompts to countdown to the different phases of the game data tracking user performance and the end and fail conditions Each game mode has their own conditions for game complete and their own goals  The UI package contains all UI elements views and controls Essentially the GUI is constructed via the UI package The main UI class acts as an interface for the UI elements initiating them based on UI states and disposing them as necessary The game related UI elements are contained in their own package and are used and called from the game handling classes All controls are separated as usable entities any view can use any control as needed  The utilities package contains all extra functionality and helper classes that the views game handling and data services need It has sections for animation data loaders and handlers algorithm parsing various physics and inertia handlers as well as the core Bezier classes Utilities are used through the application model by any section of the solution that needs them Most utilities are initiated only once and kept in memory to be used as a static instance  The CCA platform was built to be highly manageable from the back end solution Even though the rendering engine and game logic resides in the rendering and game engine in the front end platform as much as possible of the configuration of the games and rendering options were separated from the engine and interaction logic  The communication between the front and the back end services is done using a fairly standard REST API Every interaction is based on HTTP POST calls with structured XML Data XML is an application profile or restricted form of SGML the Standard Generalized Markup Language 56 XML is a standardsbased and well documented human readable format for communication and configuration and a suitable choice for the platform Due to the nature of the game rendering engine XML was also suitable for conveying the mathematical formulas and sets of functions needed to create the creature renders The Flash platform also contains an in built support for E4X 57 the extension for AS3 that makes XML a native primitive in the programming environment making the parsing and use of XML efficient  When the user enters the platform the front end queries the server for the latest language information After the user inputs his credentials the server is queried with the user information and a request for the main user interface details for said user As seen in Figure 31 the server parses the request matches the user login information and returns information regarding the user account The front end platform constructs the personalized user interface of the user based on said instructions  The platform supports a user based exercise program that monitors how well the user is doing and calibrates it to match the performance and improve it When a user logs in they are greeted by a personalized message based on their previous performance  The user has specific games based on his  her exercise program and a set amount of exercised per game per day or a set time period he can perform The program can be managed and calibrated to each users needs based on progression and scores from the game  Most of the data is served to front end only when needed For example when the user wants to see a certain performance graph it is loaded only on users request and rendered with the latest data Each view of the chart is requested per interaction to limit the amount of data sent and to keep response times low In principle everything in the front end is based on the data transmitted from the back end services except for the rendering and game logic and user interaction  The front end game logic is highly parameterized and the games are generated based on the instructions sent by the server in a game XML  Figure 32 provides an overview of a single game In the initialization phase the front end fetches the gameand object parameters and initializes the correct game logic engine instantiates the creature renderers per creature in the configuration file and sets the initial failure success and timing of the game During the game the front end game logic handles all user interaction game events and data gathering In the event of a game over be it via failure or success the game termination is triggered The performance data is sent to the server and in return an analysis of the performance of the users in relation to their performance program is dispatched and shown in the front end platform  The game is initialized by using a set of configuration flags for the game logic and the rendering engine via the game XML The game information is parsed from the game XML to a game value object This VO is essentially the core of what a single game contains and it is used in the rendering and game logic engine  The game XML has 2 main parts the game configuration and the creature rendering The creature rendering is covered in detail in chapter 6 A simplified hierarchy of configuration details in the game XML is shown in Figure 33 In principle the configuration construes from 4 main sections The user settings game settings optional configuration and game object data The game object is a simple set of variables that define the user id game name game id and login id for the user session and game management with the server  User settings are related to the user account of the logged in user The player information is for UI notifications mode contains game related data for the user The mode determines what kind of a game logic instance is created and also contains information about the delay and timing of the users abilities in reacting to stimuli The mode related user information also contains an optional calibration property that determines the time and requirements of a calibration round before the actual game begins Calibration is done to ensure proper user difficulty in training  Game settings define alongside the failure condition the structure of the game The periods contain depending on the game mode the different sections of the game Each section has a target time performance and creatures that are used during that period The target timing contains an array of timestamps on when the rendering engine displays a target mode effect that prompts the user to react The target timing also contains information on which creature said target timing affects  The target timing was originally done in the game logic engine with only minimum and maximum times for the frequency of the target event given by the server But during development and testing there arose a need for a more fine grained control over target states especially with the integration of the user training programs With the server in control of the target times various iterations of the same game can be made without affecting the front end platform codebase  The optional settings include modules that are used in different game modes and a medical imaging and measurement helper The foto detector is an indicator that shows whenever a creature reaches a target state Its size and color can be configured depending on the use case It is used when recording user activity via medical imaging and other instruments as a synchronizing signal The background generation defines the needed values for the background Perlin noise generation from the actual noise to the color treshholding More information about the background Perlin noise generation is found in chapter 643  54 Game rendering event  The system in game engines that controls the ongoing simulation is the main loop It is the representation of time in your engine and the layer responsible for the game life cycle 58 In CCA the front end game engine is based on asynchronous events between different modules The core of the rendering engine is the renderer class Any visual interaction or module that requires a time based rendering pass such as the path creature and target rendering and calculations subscribe to the main rendering class render event and base their rendering passes on it The event is a custom event with a unique rendering time key that is then employed in the various time based calculations in the rendering Figure 34 shows the most important renderer event related dependencies  The separation of direct calls gives the platform a manageable and simple way to add new requirements on the render pipeline During the development of the platform the rendering core started from a simple mode of having creatures running free on the screen and slowly evolved to support dynamically moving bubbles for the background time rendering visualizations and calibration events all subscribing to the main rendering thread  The separation also allows for an important feature for any gaming platform the complete control over the flow of time With this control all forms of prompts countdowns user feedback and result screens can be efficiently and smoothly incorporated to the game designs at any time  55 User reaction tracking  In order to get a complete account of how a user performs during a game in the CCA platform all user reactions are tracked even those that dont end up showing any visual signs in the user interface Every reaction be it valid or invalid is time stamped and logged alongside all creature target renders and game phase changes The data is held in memory during the game period and after the game is over it is sent to the server for analysis  The game data tracked during a play is shown in Figure 35 The data contains events divided into the game periods they belong to for precise analysis Each event contains the timestamp in the relative time of the game rendering process type of the event handled and possible tags related to the event In addition every reaction or target state handled contains a list of all visible creature positions for further analysis The data also contains information about the actual rendering resolution of the game areaTracking enables the use of more detailed information about how people play and the strategies they employ in reacting optimally within the platform games For example the difference between a user trying to optimize hit percentage by reacting at a regular interval and a honest try at seeing the reactions can be taken into consideration with the analysis of the reaction data especially with comparisons to previous user account performanceThe core of project CCA is the rendering engine It is a 2dimensional engine based on a standard XYcoordinate system The coordinate system starts from the top left corner and expands to the bottom right corner The whole rendering area is flexible and user scalable and the platform adapts to the user resolution by adjusting the speed and size of the rendered creatures for optimal playability For the sake of rendering performance and playability there are set minimum and maximum width and height values for the scaling  The engine is a so called black box system where the game is generated based on set rules and the engines own logic This means that while being very controllable the engine does not mindlessly repeat control orders nor does it follow a linear human made path The engine draws and moves the creatures based on their paths and targets trying to find an optimum route but does so within its own limits and algorithms  Games often are made based on strict artistic control and very man made worlds as it allows for a more fine grained control and precisely deterministic outcomes but it lacks the surprising elegance of simulation based engines A good example of some simulation engines are the modern physics engines in games especially the ones based on soft body physics instead of rigid body physics  The basic principles of the rendering engine revolve around the concept of a creature Each creature is an entity with its own path finding and rendering logic and all game types work with changes in the creatures The core separation of the creature object is the shape and path as seen in Figure 36 The shape is the actual form of the creature the visual end result that the user sees on screen calculated based on a series of mathematical formulas and a set of parameters The shape consists of the actual calculations for solving the formulas as well as the rendering of said formulas on screen with a set of colors and transparencies and possible dynamic movement The shape also takes care of wrapping the shape on the actual path generated by the path part  The path is the core movement of the creature itself Each creature has its own path generation the role of which is to find out target locations for the creature and calculate optimum movement paths between them The path is based on continuous cubic beziers and recalculates itself every time the creature reaches the end of an interpolated target path and embarks on the next segment  The rendering of the creatures on screen is based on two main parts the plotting of the stationary creature on its own and the path wrapping calculation and plot The creatures stationary calculation defines how the creature looks is colored and how it performs its target state The path wrapping takes the end result of the stationary phase and integrates the creatures form into the shape of the path it is travelling on The creature can also be rendered without the path as a stationary object in its prime state  During the project lifecycle there were a couple of different ideas on how to render the actual creatures With the path generation separated it was possible to use hand drawn pieces of creatures and animate them on top of the path points to make them look more artistic and use less rendering mathematics as well But for pure organic feeling and keeping full creative control in the hands of back end generation it was deemed better to go with the option of drawing the creatures completely with mathematical formulas for full customizability  The creatures are rendered based on a server served creature XML that contains settings and configuration as well as the actual formulas for creature generation Figure 37 shows an overview of the creature generation The settings define features that the creature has as a whole such as the target states and length of the creature The calculations contain the essence of the creature Each formula has a set of configuration attributes that explain how the particular set of formulas is rendered  The formula settings define the look and feel of both the creature surfaces as well as the creature main lines In addition the values related to actual plotting of the formulas are introduced the amount of points to be rendered the start value of the plot and the distance the plotting is done on Each formula is a set of two X and Y coordinate plots The main plot is the first XY pair and the return plot is the XY2 pair when these are combined they form a surface with a fill that is a part of the creature  The actual creature rendering is a stacked plotting approach to rendering mathematical expressions spreading them over a uniform scale wrapping them up with return functions to create surfaces and mirroring the end result to optimize rendering The creature form is unified on both sides so only one half of the creature form is calculated and then mirrored to create the actual creature  Figure 38 shows an overview of how a creature is rendered from the set of algorithms provided by the creature XML The first line shows the first formula being plotted first the initial plot of points without lines drawn is shown then the fill with lines and shape Then the return formula XY2 is plotted and it forms a complex shape Then the return and the first formula are filled to create the end result This is done for each formula and drawn on top of each other on the creatures canvas and the formula 4 line contains the end result creature built from the 4 mathematical expressions  The creatures are based on two types of formulas The various formulas are plotted on the screen and the 2d coordinate system with the time seed value from the rendering event time loop  The base of the creature and most of the formulas are static formulas Static formulas are plotted once per creature per formula After the initial plotting of the formula the plotted points are stored in vectors typed arrays of AS3 and used for path wrapping and physics calculations before finally being turned into visual renderings  The amount of plot points is defined per formula as are the values the formula is plotted on The definition of the target state is given as a separate formula to be rendered when triggered All the times of trigger and densities and colors and opacity levels are configurable per formula  As seen in in Figure 39 a single expression of a creature consists of a set of parameters followed by the initial and the return formula of the single shape to be rendered Each static formula has a variable that is plotted with the given attributes This variable is named t and it is parsed in the rendering engine into AS3 native value along with the text representations of the math functions  In the example expression the t is plotted from valStart to the distance provided by valDist over the course of the amount of points defined in valNum After these have been plotted the points are stretched with the length factor of the creature XML The length factor is the length of the creature in ideal circumstances and is adjusted as needed based on the resolution of the game area during play  Some creatures based on rendering type and game mode also contain dynamic formulas Dynamic formulas are plotted per render loop every time anew This creates a relatively significant overhead for calculations for the processor Dynamic formulas are used sparsely and mostly to add some flair or to make it harder to recognize the target states of the creatures Figure 40 is an example of a dynamic expression the definition in CCA for a dynamic variable is c Typically dynamic formulas are circular so they create repeating smooth movement  The third type of algorithm every creature contains is the target state calculation formula Figure 41 This formula decides the look of the creature when it reaches target state the state when a user is supposed to react to the change in the creature The target state is parameterized in size and effect based on the user accounts previous score and calibration results It is precalculated at the start of a game and contains only static formulas The difference between a normal drawing function and the state one is that the state is drawn for a set amount of time as defined in the creature head part of the creature XML  Every formula the creature is based on was originally a one way formula forming a line but not a shape After some development time there arose a need to make shapes fills and surfaces in the creatures to add visual fidelity and concretize the creatures some more To solve this need the ability to render return formulas for the various algorithms was introduced  In principle a return formula is plotted on the same time values as the initial formula and in the same scale With the main formula it creates a complete shape or surface which then can be filled with color and opacity to make it more vivid Adding the return formula to the shapes enabled the creation of complex shapes interloping with opacity and colors for unique creature designs An example of how the return formula forms the body of the creature can be seen in Figure 38 and an example of the XML can be seen in Figure 42  The creature formulas plotted on their various paths form the natural state of the creature To create life and movement it needs to be integrated with the path provided by the path creator The integration of the creature to the path is called path wrapping  Figure 43 shows a visual representation of the difference between the creature in its natural state and the path wrapped bended version of the creature The red line at the bottom is the interpolated target path of the path calculation for the creature The target path generation is explained in detail in chapter 632  When wrapping the formulas on the path the formulas are plotted as usual to create the natural stationary creature but after that the whole plot is bent around the segment path so that lengthwise the center of the creature is on the current point of the path rendering progression Then both halves are calculated from the center onwards up to the point where the length of the creature ends The interpolated target path is the size of the creature and contains a point for each plot point in the stationary creature plot  After the target path is generated the creature is wrapped around the path point by point calculating the angle at each step This calculation is done per formula per creature each formula is bent individually  A simplified principle of the creature bending can be seen in Figure 44 First the integral of the segment as well as the angle of the segment at any given point is solved while making sure that the angle doesnt go over – or  pi Then the position of the creature is interpolated on the path and the point is rotated based on the angle on the frame path at the same point The creature is wrapped point by point on the ongoing path allowing it to react to every small change in the angle of the path individually enabling the organic feel  The movement of the creature is based on the path generation created by the path creator class The path creator calculates the targets where the creature needs to go and the path itself and then attaches the creature on the path The path is based on continuous cubic Bezier curves to create a harmonic organic movement of the creatures while maintaining randomized movement paths  A rendering of path creation in CCA can be seen in Figure 45 When the path is created it consists of 5 sections of points each section has a minimum amount of 100 points between control points A creature moves between the middle control points 34 and whenever it hits the 4rd control point a new control point is generated on the path and the last control point is removed When the game begins first 2 points are generated by random and after that based on the same rules as the new points in the game  Because the middle and ongoing segment of the path needs to be unchanged every decision reaction for the creature takes the time of going through the next 2 segment paths By making sure the length of the paths per segment is never longer than 05 seconds preferably around 0304s the creatures reaction speed is between 0608s which is around the same as the reaction speed of an ordinary human being in a nontrivial task  The paths are based on cubic Bezier curves Every cubic Bezier curve has two control points and a start and an end point When making continuous Bezier curves the start point of the next curve is always the end point of the previous curve Making Bezier curves that continue from each other is relatively simple and requires no complex calculations But when making smooth movement one needs to make continuous Bezier curves that are smooth and that means each curve affects the curve before it and after it An example curve can be seen in Figure 46  For the path finding and rendering system to work And because calculating continuous Bezier curves with the path wrapping isnt cheap the active creature path at any given moment must be a static Bezier curve The path will change only when the creature reaches the end of the current path then the next point on the path is calculated and rendering continues Because of this requirement the path needs to have the five segments or six points at all times  Once the point locations are generated the path needs to be smoothed Since the motion is continuous through the points the movement cannot have jumpy or unsmooth sections To achieve this the last control point of the previous curve needs to be on a straight line to the first control point of the next curve as seen in Figure 47 The line goes through the control point and the path point  For creating cubic Bezier curves Flash contains a native library called BezierSegment It is a collection of four point objects that define a single cubic Bezier curve and has methods for getting the value of the curve at any point on the curve itself 59 The solution in CCA is based on the BezierSegment class and more specifically on the work done by Andy Woodruff on the implementation of continuous cubic beziers with AS3  60  One issue when using continuous Bezier curves as paths for movement is that when the angles of the previous and next point collide in a straight line on the x or y axis the line drawn is a straight line and by default is calculated without any steps To overcome this in CCA the straight line situation is checked after the Bezier generation and a slight variation is added to the target point to get a proper curve between the points for calculations  In CCA the continuous Bezier curve forms only the foundation for path generation on which the actual movement is built Once the base path is generated the active segment is resampled for smoother movement and simple physics  In Figure 48 the different parts of the path generation are shown The RawInterpPath is the path created by the smooth Bezier curve generation After the RawInterpPath is calculated the active segment of the RawInterpPath is separated as its own entity called InterpPath The actual path used by the creature for movement is made by calculating the velocity and displacement of the InterpPath This path is called a FramePath  Figure 49 opens up the complex logic of creating the path The actual FramePath is not needed for the current segment of the creature for movement For that a FractionalFramePath is generated from the InterpPath which contains a fractional index that is used to find the actual movement point in the end The FractionalFramePath index is used by interpolating a point on the actual curve based on said index In principle the FractionalFramePath is the creature location on the InterpPath  The physics used in the game are fairly simple The main use for physics as such is to enable the creature to have different speeds based on its mood and the curvature of the path it is on This creates the feel of organic movement as the creature slows down to curves and speeds up when going long ways along a slightly curving path  The path segment where the creature is currently moving on during the rendering is never changed Because of this the physics and velocity of the creature are calculated when the path is calculated In general the path is calculated and physics applied to the movement only at points where the path is recalculated – when the creature reaches a new target location  Aside from the mathematics of actual path generation CCA also contains methods to create path finding for the creatures and boundary methods to keep the creatures rendered inside the actual game area  There are two modes to the path finding hunting and foraging When foraging the next path point is generated without a specific goal by using pseudo random logic In foraging mode the creature finds its own way around the game area without specific incentives When in hunting mode the creature has a target array of points where it aims to move to the closest target point is evaluated and an optimal path is generated towards said point With the closest one calculated the path finder finds out if it is close enough to nab it in this path if not itll create one path segment towards it first and then redo the calculation If the target is gone the creature falls back to normal foraging mode  Figure 50 shows a rough overview of how path finding logic works in CCA The next target location is generated by combining the orientation between previous two target points with a randomized target angle and a speed based on creature length and the angle of movement The core idea is to have a similar direction of movement to prevent too big drastic changes in the creature direction  If the next natural point in a creatures path generation is outside the boundaries of the game area a bounce method is applied The bounce calculates how far outside the boundaries of the stage the creature was going for and changes the angle of movement so that the new point is inside the stage This is done to ensure the creature is always within the visible target area and does not wander off the sides of the screen  After the path generation and bounce factor have been taken into account the distance between the previous and the new point is calculated to make sure the points are not too close to each other In case of them being too close a small speed fix is added to the points location in order to keep the length of the path relatively uniform The length of the active path segment is set to be 1…2x the length of the creature to keep the reaction times in check  Perlin noise is a procedural texture or a pseudo random gradient that is used abundantly in modern computer generated art and graphics for its unique attribute of seeming organic and natural The algorithm interpolates and combines random noise functions into a single function that generates more naturalseeming random noise Perlin noise has been described as a “fractal sum of noise Developed by Ken Perlin back in 1980s for the movie Tron Perlin noise has been used in CGI and had a huge impact on computer generated graphics ever since 61  Aside from using Perlin noise to draw patterns or textures it is also very useful for creating organic smooth movement The movement is achieved by using the luminance values of a black and white Perlin noise and attaching it to the velocity of a particle engine Due to the nature of the noise one can create the noise field once and then change the offset of the x and y coordinate space of a single octave of the noise With this the noise moves without recalculating the whole noise while maintaining the same visual similarity Perlin noise is also quite optimized and suitable for performance use  Figure 51 shows a Perlin noise field used in project CCA to move the background particles around This noise field is randomized on every application run and resize event so the movement of the background pattern seems organic and never repeats itself Perlin noise is rarely used to just create visuals but rather to blend other visuals together or to generate objects landscape and other items that benefit from natural seeming patterns 61  The way Perlin noise is used in the background rendering of the CCA system is quite simple Every resize and init event a a 2dimensional Perlin noise is generated to fill the game background layer no need to make it whole screen sized It is restricted by the same resolution restriction stops as the main game area thus simplifying the sizing issue to a single place This Perlin noise is not visible to a user and is used only to generate movement  The noise field is approximately the size of the gaming area and it is populated with N amount of particles The amount of particles varies based on the users resolution to create an optimum look versus performance The particles are simple round shapes with opacity randomized to make them look like abstractions of bubbles In the start the particles are positioned randomly along the noise field  As previously mentioned the game has a single renderer that all time based actions are performed on On every render tick the position of each particle is changed according to a set offset that is unique to each instance of the particle  The opacity angle speed and scale of the particle are calculated from the brightness of the pixel at the new position of the particle Figure 52 The brighter the noise at the coordinates the faster the particle moves and the larger it becomes and vice versa In addition to this noise based movement each particle has a unique wander property that affects the navigation of the particle to make the movement a little bit unique With the smooth coherent noise we get from Perlin noise it provides a unique and performance efficient way to generate organic movement  In addition to working as background organic movement and mood setting the particles served another purpose During gameplay correct reactions triggered a coloring effect on the background particles closest to the creature whose reaction was targeted There was a set radius around the active creature within which the particles had to be  The coloring was gradual a smooth green shade with opacity stops per reaction was introduced An opposite effect was also introduced when a user repeatedly triggered reaction without a cause outside of the reaction window of the target state the particles were gradually colored with light shades of red One could get an overview of a game situation at a glance  In addition to making movement with Perlin noise Perlin noise is used in the reference project as a creator of coloured randomly generated backgrounds that fit the colour scheme of the creatures The colour match is important as it makes the spotting of creature target states harder when there is less contrast between the background and the creature itself  Figure 53 shows an example of how Perlin noise is used in the backgrounds of certain game modes It was implemented by partially replicating the Perlin noise in use in the background animator The actual effect was developed in true to platform fashion in a parameterized way where the server provides a possible array of colours to be mapped into the background noise with the creature XML These colour values are then mapped to the noise based on the brightness values of the noise  The colours are mapped to the noise by first taking the highest and lowest brightness values in the black and white noise then mapping the colours to the noise by assigning a brightness index per colour In CCA a vector of values was used since it is faster to iterate through an array of bytes than to manipulate an actual bitmap object  A small trick was used in the resizing of Perlin noise fields since they are quite heavy to instantiate and generate and there was a need to generate one anew every time the stage was resized In calculation or rendering heavy tasks it is not viable to run them every time a user or a program resizes the window but rather set a small delayed call to the refresh function and overwrite that delay every time the resize gets called With this small optimization one most of the time gets only one rerendering round even when the user holistically drags the window around  Optimization should always be done holistically Look at the big picture first and then drill down until you find the specific problem that is slowing your application When you dont optimize holistically you risk fruitless optimizations 62  The amount of computing resources available to a game is finite Optimizing code increases the amount of work the engine can achieve per unit of computational power and maximizes the efficiency of the system 62 In optimization there are many ways to speed up the code but it is essential to identify and solve the actual performance bottlenecks within the laws of diminishing returns The main tradeoff to think about when optimizing is the development time versus complexity  In a project with strict performance requirements such as the CCA one has to be quite strict and diligent in handling performance bottlenecks However untimely optimization leads to chaos and low quality code it is imperative to find out exactly what needs to be optimized before jumping in Performance optimization is a balancing act between readable solution code and pure performance gain  There are many ways to approach optimization but the fundamental basis of the optimization lifecycle is testing or benchmarking  Figure 54 shows the basic idea of the optimization lifecycle When optimizing the essential point is to measure the gains A benchmark is a point of reference in the game that serves as a comparison against future implementations Benchmark should be reliable quick and it should represent an actual gaming situation The main use of benchmarks is that they enable relative comparisons  After the comparison points are gathered the detection step starts The point of detection is to find the biggest return on investment for the optimization In detection phase it is important to start from the big picture and analyse layer by layer to the finer problem points until a suitable issue is found After the detection it is a matter of solving said issue Solving can be about fixing a bug toggling a flag rewriting the algorithm involved or changing the data structure 62  Once the solving is done the check phase begins When checking the benchmark is ran again and measured to see if the solution changed anything in the performance After checking it is all about repeating the same progress again until the biggest performance gains are figured The idea of this cycle is to find optimization hotspots and bottlenecks Hotspots are the points in your program that consume a lot of processing power Typically hotspots are small amounts of code with a big hit on performance Optimizing hotspots leads to significant performance benefits Bottlenecks are particular points in the system execution that clog down the performance of the whole system  Optimization can also be approached on a broader level by dividing it into three categories Figure 55 On system level optimization the focus is on the use of resources of the target platform The point is to find an implementation that utilises system resources with balance and efficiency System level optimization is about planning a platform that utilizes the available resources without overusing in a sustainable way  Application level optimization can also be called algorithm level optimization It is the choices made in the data structures and algorithms that make up the platform The idea is to use a good profiler tool to find out call hierarchies and time and iteration amounts of systems most used parts Algorithm level optimisations are the crucial backbone of optimizing code The identifying of a key part of code that uses a lot of processing time and optimizing it or the node that calls it with a more efficient solution is of the best return on development time  Micro level optimizations are the most common stereotypical types of optimisation The small hacks on as low level as possible to get a bit of a performance boost out of a specific thing The optimization of inner loops or pixel rendering routines that are called extremely often and that benefit from the most miniscule of an improvement because of the sheer amount of times they are called during an application execution  In project CCA the concentration on optimization was made on all three levels A structural planning oriented performance review was conducted in the initial phases of the engine building The algorithmic level was benchmarked and reiterated throughout the application development cycle and the low level micro approach with rendering and mathematics was made in the engine building phase to smooth out the heaviest of operations in the engine  Beyond the processes of optimization one must be wary of the pitfalls along the way It is easy to get stuck on assumptions about performance problems and optimizing prematurely without knowing if it is a big performance sink Optimizing based on your own machine or with debug builds gives misleading results as debug builds are often slower and riddled with processes not found in the final release build In CCA during development there was a point where the focus was on micro optimizations on the engine rendering loop for too long and without the aid of a profiler the real optimization pitfalls would have remained hidden  A common trait in game rendering engines is to render only what is needed and reduce the level of detail dynamically whenever possible In fact many games could not function without such optimizations due to the sheer size of the game worlds and the amount of rendering In CCA the amount of rendering is relatively small compared to many of the 3dimensional engines but due to the measurements and the nature of the medical side of the platform it was necessary to render everything on screen without any reduction of rendering quality or level of detail Because of this in the creation of the rendering engine it was necessary to try and use all the tricks possible with a managed language to make it as fast as possible  A good approach to optimizing holistically is to focus on detecting and solving issues on the system level whenever possible then the algorithmic level and only if no solution is found resort to micro level optimization  The optimization of the platform can be divided into 3 main sections seen in Figure 56 In data optimization the focus was on the issues of latency and communications with the clientserver architecture The point of data optimization was to minimize the latency of backend calls and limit the amount of data sent between the back and front end platforms The front end requests data for a view be it the main user interface a data visualization component or a single game only when initializing the view for the first time  When non changing data such as the main user interface localization and configuration is loaded it is cached on the front end and not refetched until the user next logs in again When a response from the system takes between 200ms and 1000ms a user is prone to lose the feeling of flow and the user experience of the system is hampered 37 For the sake of responsiveness and continuous feedback the user interface shows a loading indicator whenever a data request takes longer than 400ms The indicator is not shown on shorter loading times to keep the user flow uninterrupted  When optimizing the application level the focus was on making the rendering engine and game logic separate and to manage garbage collection and memory handling with specific initialization and dispose functionality The separation of the engine and the game logic optimized not only the development time of the platform but the way responsibilities were divided between the two  The initialization of objects in managed languages is costly and can easily cause variation in the FPS of the system as well as random hangs in program execution Also the disposing of objects for garbage collection GC can cause the GC to run at times when the user is performing an important part of the platform The ways of observing smoothness in a system are somewhat immaterial and cannot be found purely from profilers Extensive testing and visual assessing is needed to make sure the user experience flow remains unaltered More information about garbage collection and GUI elements can be found in the Flash specific section 73  The deepest iteration of optimization was done on the micro level thinking about the mathematics used and testing different operators and in lining function calls and other hacks that produce less readable code but provide a performance boost in a time critical section of the platform Mostly micro optimizations were done in the core rendering engine on the calculation and drawing of the creatures and their movement  The way to find important performance issues in project CCA was done with using the Flash Builder profiling tool for performance profiling Figure 57 Profilers are the most common tools for optimization gathering information about resource usage most often the CPU load during a session Profilers typically provide information about the amount of calls selftime and total time of function calls as well as memory used in them 62 In CCA the profiler helped in generating memory footprints and locating high resource hogging sections of the rendering code as well as finding bugs in disposing of objects  In CCA the creature rendering model is a very versatile implementation of a data driven architecture The creature parameters define how they each formula in them is plotted on screen and the detail level of each creature can be adjusted per formula as well Each mathematical formula has an amount of points that are calculated and plotted to render out its true form and these can be adjusted and manipulated with relative ease The lower the amount points rendered on screen the less fidelity in the creature but on low end machines or situations where there are many creatures on screen it increases performance smoothly  Figure 58 shows the clear difference in rendering quality based on the amount of control points or dots used in the creature generation The overall shape and visual style of the creature is the same between the fill render and the 4x fill render but the subtle details of the creature are more smooth and round in the 4x fill render than the normal fill render The dot render shows the inflated dots generated by the plotting of the formulas the dots themselves are used in the rendering only as the control points for the lines that shape the creature itself  The difference in cumulative CPU calculation time in milliseconds between the 1x and 4x dot renders is shown in Figure 59 The values benchmarked by using the Flash Builder profiling tool set show that the rendering of 4x the amount of points on the same creature increase the calculation time approximately 32 The difference is to be expected as the fidelity increases so does all the major computation in the creature rendering and path bending More difference could be obtained by adding a formula for the creature generation itself instead of adjusting the points of the formulas calculated  Different creatures can have different dot amounts for visual fidelity Some formulas work better with low amounts of dots while others require much more to look and behave properly When the creatures are moving the tighter turns they take the more clear the complexity of the creature if the creature consists of a low amount of dots the lines between start to show when doing above 90 degree turns quite visibly There is a small implementation in the path creator that tries to optimize creature movement in a way that it never does such high degree turns  Due to the data centered design of the rendering the creatures can be optimized based on user accounts and user machine performance to create the best possible outcome For most formulas in CCA the conversion of the formulas to native code could be done in the initialization and the calculations for the necessary values out of them only once and then reuse the plotted model of the creature in all the path bending operations without recalculating the plot The performance gains were significant In addition to the fidelity of the creatures additional features can be optimized such as the background Perlin noise particle animation can also be toggled as needed  The Flash platform or more specifically the Actionscript 30 AS3 execution model is single threaded Because of the lack of threading all sufficiently heavy actions on the UI thread slow down or stop the rendering of the whole UI layer While being robust for a web technology it is a severe limitation of processor use AS3 is also a managed language interpreted by a runtime which leads to system managed memory and garbage collection  There are various issues when profiling with managed languages the nature of the byte code leads to more native level calls than pure native languages The way AS3 handles array actions is more involved than native languages every array is implemented via a generic data structure so every array load means hitting that data structure and querying it for type information 62 Because of the cost of even basic operations can be much more varied than in native code it is beneficial to set a good baseline for performance data to know what to avoid  The basic approach in optimizing for AS3 is to always use strictly typed operators everywhere in your code it provides a significant runtime performance boost Another way of optimizing what is needed in AS3 came with the Flash Player 10 release Flash had no history of supporting typed arrays before the introduction of Vectors in FP10 Vectors have a significant overhead in declaring and wiping them but they are extremely robust when iterated over significant amounts of values 63 The difference between array and vector use is significant as seen from Figure 60 The reading speed of vectors is roughly 40 faster than with traditional arrays The writing speed difference is even greater roughly 170  In AS3 the construction of new objects is very costly When creating the creatures and parsing the formulas from XML to AS3 native math functions it was useful to use a technique called object pooling In object pooling you declare your objects only once and store them in a list Instead of creating new objects when you need them you use the objects already declared and stored from the list Figure 61 The benefits of object pooling are greater when using more complex primitives the increase in performance is 5x when using a basic type such as a Point and over 80x when using a complex Sprite type 62 In CCA each creature is object pooled but so is each set of formulas that form a part of the creature  To prevent the UI from slowing down in CCA the initialization of objects into the object pool was also optimized In the game handling initialization the creature classes belonging to the game are instantiated The Perlin noise backgrounds are created as well as all the creatures When creatures are not on screen or do not belong in the current play mode they are hidden from view and removed from the display render loop Since the game core is about constant perception of the target moving it is imperative not to have high changes in frame rate on average even if the rendering frame rate is not the full required 30fps the users arent as phased by the difference as long as it maintains a steady pace  Alongside the optimizations mentioned here there are great many smaller point solution optimizations used in CCA and available for the Flash platform Links to further reading on the subject can be found from the associated references in the end of this thesis  One of the basic optimizations to do is to remove loop invariant code When doing large quantities of iterations the cost of declaring iterators within loops and inner loops can be quite high especially in managed languages such as AS3 It is useful to try and minimize the amount of variables declared inside loops when it is not necessary even predeclaring the loop iterators let alone variables used in the loops saves performance 62 64  Another optimization that tends to be bad for code readability but that can help optimize code that is heavily used during rendering is the inlining of functions in your code When moving functions inline one must be wary of the cost for code readability it significantly hampers the future development of the code base Inlining calculation heavy functions can amount to a performance increase of 400 in AS3 64  Other uses for micro optimization are the manual redo of language specific mathematical libraries There are many ways to solve basic things such as the absolute of a number via casting to int or using a ternary operator that can be faster than using the platform inbuilt Mathabs Small optimizations in AS3 can also be gained by using the right iterators for the right loops for example when doing while loops it is faster to loop through in reverse order than forward looping 64  In CCA the micro level optimizations were used solely in the core rendering and path finding engine There was an effort to keep the unmanageable only in the areas identified by profiling to require significant tuning and optimization Function inlining is used in some processor heavy mathematical operations and reimplementations of things like Pointdistance and Mathabs to improve low level efficiency For most of the platform code algorithm and system level optimizations were preferred  Another way of optimizing performance heavy bottlenecks in calculations and iterations is to use bitwise operators instead of verbose solutions Since computers operate in binary using bitwise operators can be a powerful programming tool Often a few quick operations can easily replace what would otherwise be complex and heavy code In certain situations using bitwise operators can even amount to clearer code 62  Common bitwise techniques in AS3 revolve around using the inherent quality of binary operations multiplying and dividing by the power of two integer conversions sign conversions modulo absolute value minimum and maximum seeking and color conversions Figure 62 shows common alternatives to operations in AS3 by using bitwise operators for efficiency  Performance benefits for using bitwise operators in AS3 can be immense Figure 63 shows the benefits of using bitwise instead of natural logic in common logical operations The benefit is especially clear when using the bitwise method instead of the Mathabs an increase of roughly 2500 can be achieved Basic functions such as multiplication and dividing run around 300350 faster than the basic solution When testing the modulus of a number the performance is around 600 similar to testing if a number is even or uneven  When optimizing the CCA rendering engine the approach was to benchmark key rendering situations and find out the most performance intensive mathematical operations and optimize them The most used and heavy functions were optimized with various micro optimizations including removing loop invariant code declaring variables together and bitwise operations  Flash supports both Vector Not to be confused with the Vector of typed arrays in Flash and bitmap rendering Both of them have their own benefits drawing with vectors enables crisp smooth and scalable rendering as well as quite handy tools for handling the graphics Whereas drawing with bitmaps allows for easy manipulation and bitmap specific filters such as PixelBender and especially with complex and large graphical content performance benefits 64  There are many ways to optimize the drawing of content in Flash One good tool is to use the redraw regions debug option to visibly observe what regions of the screen are “dirty and redrawn on every frame Even though content is opaque if it is on the display list it can still trigger hit tests and other runtime performance hogging tests as long as its visibility is set to true One can also set the platform level toggle on movie quality to a lower amount this affects antialiasing smoothing of scaled bitmaps fonts and animations  Other performance heavy operations in native Flash drawing are the use of filters and blend modes Filters such as drop shadow and blur need to be applied on the whole object they affect during render time unless specifically set to cached Efficient use of cached effects and minimizing the amount of alpha blending can also help rendering performance especially on lower end hardware  Most vector content in Flash can be cached as bitmaps and reused even changing the objects x and y coordinates does not affect the caching benefit However caching cannot be used for any content that needs to be redrawn every frame such as the creatures in CCA  In CCA both bitmap and vector based drawing methods were studied The target was to find which would be most ideal for rendering the CCA creatures Vector based rendering gives one smooth controlled geometrics in a fully scalable way enabling graphical fidelity and an ease of creation As opposed to vectors bitmap rendering is purely pixel based and as rendering goes it is typically significantly faster than vector based rendering Handling bitmap rendering is somewhat trickier than doing it in vectors especially if you go with just pixel painting since you need to manually take into account antialiasing and corners of the creatures  Since the creatures are plotted from mathematical formulas into vector shapes in CCA as the basis of rendering using pure vector based rendering on the Flash platform level was an obvious choice In the initial engine tests there were no significant increases in performance from using bitmap based rendering over vector based rendering as most of the CPU power went into path finding and creature calculations  8 Results and future implications  81 Current situation  After the past two and a half years of development project CCA is a mature bug free platform and currently deployed in clinical trials as well as touring the Science Changing the World Exhibit It has proven to be a useful tool for cognitive gaming in preclinical alpha version testing users were generally pleased with the tool There were no harmful effects or discomfort reported and 20 users out of 21 found the game appealing  Despite the fact that the project got off to a good start it was a large piece of software for two people to handle There were many ideas presentations and funding rounds to try and ramp up a company built around it to make it a viable consumer market platform for cognitive gaming but none of them have succeeded so far  Regardless of it not being as big as originally hoped it would be at this point it is a solid work of engineering user experience science and hard work CCA has performed well under tests and clinical trials It is currently being translated to other languages and has been developed to be fully multilingual based on different language resource XML files that are easily configured via the server interaction There is a single simple API for language versioning that can be triggered on any back end call so users can change the language at any point during platform use  After the current clinical trials there have been rough plans and ideas about the possibilities of reworking the core engine for mobile platforms as well as ventures into the world of 3D but the future of the platform remains open The online version of CCA can be found at wwwmybraincapacitycom  The current version of project CCA is being used mainly in ongoing clinical trials in conjunction with HermoPharma wwwhermopharmacom in their amblyopia studies There have been plans for a mass market version and an open account creation but for now the plans for large scale deployment are on hold  Possible uses for the project CCA platform range from helping aged people hone their cognitive abilities to helping the rehabilitation of people after experiencing brain trauma It has proven its capability as a cognitive gaming platform and can be used for accurate user statistics and ongoing analysis of users progress as well as measuring the effectiveness of drugs by providing detailed data about gameplay and improvement  The ongoing clinical trial at the moment of writing this Thesis is related to ambylopia also known as the lazy eye A fairly common disorder characterized as vision deficiency in an eye that is physically normal and able Ambylopia is estimated to affect the lives of between 15 of the population 66 Hermopharma are in the process of developing a drug based on antidepressants to prevent or heal damage caused by ambylopia wwwambylopiafi  In the trials project CCA is being used as a measurement and a training program for the test users to gain accurate information about how the medicine works and to see if the training of the affected is efficient in improving results Figure 64 shows the campaign for the clinical trial In addition to being used as a measurement device for the actual clinical trials a standalone implementation of a single game mode was made to support testing for one eye at a time The purpose for this version was to act as a campaign game for people interested in the possibility of amblyopia affecting their lives giving them a preview of what the trial was about and to raise interest and awareness for it  On top of the main game platform build we were approached by Heureka wwwheurekafi to do a multiplayer version of the game for a new exhibition called Science changing the world they were doing in conjunction with 3 other leading science centers The exhibition is a testament to science a playful series of exhibits to interact with and learn about new things in science and the ways health life quality even the planet can be improved with scientific methods The show started at Heureka from 1542010 Figure 65 up until 1512011 After that it has toured the Museon the Hague in 2011 the Cité des sciences Paris in 20112012 and it is in the Pavillion of Knowledge in Lisbon until 1572013 After Lisbon the exhibition is up for rent  What Heureka wanted was an offline version of the rendering engine with a specific game type and about 10 different game configurations and creature combinations It differed from the main game in a few important aspects It was not a training schedule based system having no server interaction or user accounts but a pure front end implementation and it was to be played by up to four people at the same time competing in cognition around who achieves the highest score The game was also in 3 different languages and the whole user interface of the game was redone to fit the resolution and language requirements  Due to the modular nature of the platform and engine code it was possible to separate the rendering engine with a game logic part and implement the new multiplayer requirements without too much trouble on top of the custom version It was mutually beneficial as some of the improvements made in the Heureka version ended up being ported back to the main game platform  An additional other challenge of the Heureka version was that it was run on a 46 touch screen on a full high definition television 1920*1080px That meant that the whole game area that the game runs on was relatively bigger than the typical CCA platform game area around 1280*800px Because of this it required extensive optimization and calibration of the points rendering and creature styles to make it perform well enough for show use  The multiplayer mode was a single mode of the Nobject capacity test the game featured 1 to 5 creatures at the same time on screen and the users had to react to each target state of any of the creatures within a set time frame Each successful reaction per player was colored with its unique shade of color glow around the creature the reaction affected to keep it clear on who did the reaction  Example gameplay of the Heureka CCA version can be seen in Figure 66 The game was played in a customized stand with the 46 screen facing upwards All controls for users were abstracted from the traditional keyboard and each player had an arcade style control button to use to react The custom controls were connected to a keyboard input and could be mapped into the CCA Flash application via normal keyboard events  In addition to the changes in the game mechanics the multiplayer mode required its own user interface In the multiplayer version there were between 1 to 4 players and the game needed be playable with any number of people between that range The UI had to work in four directions matching the four player slots around the stand  To enable the users to play at the same time a separate UI based on the same logic and graphical design of project CCA was implemented The UI was built so a separate instance of for each direction was created and shown based on user participation Each side of the game had an initial information screen informing the users about what they are supposed to do and the game started by anyone pressing the start button The game allowed hot seat style multiplayer where anyone could jump in during the game and start reacting to the creatures albeit coming in later would affect your score in the end of the game  Only the players who were active during the game run were shown the game end screen with detailed information about how they did In addition to the statistics a crown reward system was implemented the best user was rewarded with a gold crown the second player a silver one 3rd player got a bronze crown and the 4th one had to settle for nothing  As opposed to the online version of the main platform the Heureka version had to be a standalone installed package with everything needed inside a single installer file To do this with traditional Flash was impossible and third party extensions were not reliable enough Luckily Adobe itself had come up with a solution to the problem Adobe AIR or the Adobe Integrated Runtime is a wrapper for the Flash player that allows it to handle native functionality from file system access to running its own instance 69  Adobe AIR enables the use of web standard HTML and Javascript as well as Actionscript 30Flash to create native applications for all major desktop environments as well as Android iOS and certain smart TVs 69 It is widely used in creating casual games for mobile platforms as well as a lot of entertainment and ad campaign applications Air has full support for Stage 3D OpenGL API for accelerated graphics as well as a lot of native integration to iOS and Android services  Developing for AIR brought about some small changes to the CCA codebase because it handles certain things like stage events and window events as well as keyboard interaction a bit differently but all in all around 90 of the code could be ported to AIR within days The inbuilt installer application visuals such as icons and automatic updating functionality made the AIR version completely standalone and enabled a solid experience for the people keeping up the show at the various science centers  The biggest issue to solve in the Heureka version was the requirement for the game to run on its own for a minimum of 16 hours in a row The game was automatically started at the beginning of a day and the whole machine was shut down at the end of a show day The application needed to be maintenance free during uptime and it should be trustworthy and robust throughout the whole life time of the show  The approach to the Heureka version required a lot of profiling and optimization Before deploying the build in the actual environment a series of test scripts were implemented The scripts made the game play itself for a long time changing the game parameters and reactions to simulate a real user situation A logging of memory footprint and performance details was also implemented These features enabled the testing of the game before actual deployment  But as optimizations go it is rarely beneficial to test only in development environments as was the case in Heureka Using the scripts the game seemed to work fine for the required 16 hours without memory leaks or slowing down but during the actual show run it started to show problems We kept getting reports that the game randomly crashes after 2 hours depending on how many users have been using it and the rate of use during the day  Al lot of time was spent trying to track down what caused this elaborate bug by the Flash logs it showed to be running fine and that memory use was not stacking up and rendering times were normal But from the OS point of view the app slowly ate away at the memory space allocated and crashed in the end when it had consumed too much of the system memory  The reason this fatal bug was not noticed in the internal tests was because the tests always ran the game to completion to the end screen and then began a new round It became apparent that the crash happened when time the game was started but abandoned midway through without playing it to the end numerous times The game had a timeout function if it was not played for a bit it started showing instructions on what it does and encouraging people to play  In the end after some serious time spent with the profiler and taking memory snapshots in different benchmark situations a memory leak was found A creature rendering class in the core engine rendering system was being reset for garbage collection with a call to not the correct level of inheritance but the super class of it This left the game with a small amount of assets each game creation that werent being cleaned up and slowly started to eat away at the memory available  This problem had not been found during the main CCA engine development due to the fact that the main game was never supposed to be used for more than a maximum of about an hour at a time A lesson was learnt here profiling and being rigorous about testing the memory footprint is imperative and extremely valuable when creating continuous running software  Despite the problems during the development and the rework needed to create the Heureka version it is up and running around Europe at the moment The branching was a success and even if nothing else big will come of the project CCA at least it got to educate people around Europe about science and cognition  At the writing of this thesis Matias is doing the clinical trials with HermoPharma and the language versioning system enables fast and easy porting to other language areas The Heureka version is still circling Europe The current version is complete but there are a few routes that have been pondered about for where to take CCA next  One obvious platform for brain gaming and cognitive use that appeared during the development of the CCA platform and that offers lucrative amount of users and more importantly users that might benefit from the platform is the mobile space especially the rise of the tablets Tablets fit perfectly with the way the project CCA core works and could be easily a good place to start spreading out to  Due to the fact that Flash platform is not available for mobile except for Android it is not a viable option for creating the project CCA for mobile Even though the Air platform would support building to mobile environments the computing power required on top of the managed language would most likely be too high for mobile CPUs The heavy rendering and calculations required would need a native application to be built for CCA platform On the other hand the core of the CCA engine is reasonably easily converted since it is very mathematics based  The other way aside from mobile platforms is to evolve the CCA forwards towards the 3D space The same engine would not work in a similar fashion in 3 dimensions as it is currently a very much 2 dimensional solution but the same ideas and approach could be viable in 3D The serious gaming around platforms such as Unity wwwunitycom is growing at a significant pace enabling efficient solutions for beautiful yet useful gaming  There are some initial steps towards looking into the possibility of building a next version of CCA with the Unity engine toolkit Unity would provide a unified platform for all native computers as well as a way into consoles and mobile gaming Then again the advances in web standards and the rise of WebGL could enable a remake of CCA with Javascript and HTML5 Canvas based approach The future remains open  Cognitive gaming is a rising area of serious gaming The approach that a brain can be trained and moulded to perform better at tasks we perceive to be meaningful is a exciting thought In this day and age efficiency is one of the top most factors working life focuses on and the ability to improve not only working methods or tools but people themselves provides valuable and genuine possibilities  With the rising popularity of mobile platforms and the improvements in traditional web the focus has shifted from creating features to providing good user experience and servicing user needs Going beyond usability the importance of brand and aesthetics are also gaining ground in realizing products and experiences in the online world User experience matters  The approach to cognitive game platform building in this thesis is generable to serious game platforms as a whole The issues solved relate to common real world problems when creating performance heavy online implementations and provide ways to overcome obstacles in the creation of good user experience as well as system performance The project approach emphasizes a focus on user experience and solid software engineering  The reference project is a complete cognitive gaming product a feature rich user experience oriented platform for measuring attention and working memory It is based on a data driven architecture with proper separation of concerns for rendering game logic and user interface elements The modular approach allows good controllability and modifiability without changing the front end platform code  The rendering engine provides organic movement and creatures with a natural feel in endless combinations based on mathematical formulas It is optimized on the algorithmic and micro level to provide pleasing visuals and fast enough rendering for attention games Due to the separation of game logic and rendering as well as the heavy emphasis on mathematics in the rendering the engine is easy to export to different platforms in the future  The project was a success and is being used in clinical trials in Finland and Estonia as well as in the Science Changing the World Exhibit around Europe User feedback has been encouraging during the alpha and the current medical trials The future remains open but the direction is towards mobile and tablet use as well as employing 3D as a means for more immersed gameplay  This study and the reference project stand as an example of the viability of modern web technologies in creating complex serious gaming platforms It is a testament to the possibilities of cognitive end user training and serves as a guide on the approaches necessary to accomplish a successful cognitive gaming project  This postgraduate study was done as part of the Master of Engineering Degree Program in Information Technology for the Metropolia University of Applied Sciences The motivation for this thesis originated from personal interest in both the 4X strategy games and the game artificial intelligence in general having experienced this genre first time when playing Sid Meiers Civilization back in 1993 on the Macintosh LC II computer  While I have been working in the game industry for over a decade now the possibilities for exploring this particular area of interest have been limited so the possibility of using it as the topic of my thesis was a rather natural choice Although the scope of the research was very broad and the results did not reach the practical levels which I hoped for working on this project was highly rewarding and will further motivate me for years to come  I would like to thank my supervisor Ville Jääskeläinen my family and friends for their support and encouragement during the writing of this thesis  Instructors  Ville Jääskeläinen  Although computer games have been around for over half a century the gaming has matured to become a mainstream phenomenon in the past decade partly propelled by the breakthrough of mobile platforms which provide users access to a vast selection of games As the complexity and selection of games is constantly increasing there is growing pressure to provide meaningful artificial intelligence AI opponents in certain gaming genres  This masters thesis focused on finding a way to implement an AI player for a turnbased 4X computer strategy game As there was no suitable project at work to apply this research on at the time of the study the project was created as a personal venture with a theoretical game used as the source for requirements  In the research phase dozens of sources of existing literature and information about the field were analyzed which included extraction of the knowledge and technologies appropriate for this project The selected technologies were documented in the thesis and their use cases were identified and examples were created for most of them  A highlevel technical design for AI integration was created as an outcome of this thesis which describes a proposed architecture for the AI opponent and combines the technologies evaluated in the thesis into a modular framework These features can also be leveraged in the player assistance features such as micromanagement automation and advisor functions  The resulting design was supported by actual prototype development done in Unity Editor of selected key technologies These prototype implementations included a rulebased system inference engine A* pathfinding on a hexagonal grid map a spatial database for tracking map data such as player influence and tactical pathfinding leveraging the information in this database  Although a complete game was not created during the project the technical design and research done can be used as a foundation for building AI opponents in turnbased strategy games in future The practical implementations will most likely also provide feedback on the possible shortcomings of the design and possibilities for improvement which will be reflected back on the design  Keywords  Artificial Intelligence Computer Games Game Industry Prototyping Strategy Games Technical Design Unity Since the early days of first publicly available video games in 1970s the business around gaming has grown to a large and highrevenue business known as interactive entertainment or video game industry These are some key facts about the industry mentioned in Entertainment Software Associations annual report of 2015 to give perspective about the business case for this project 1 p 14  • Video game industry generated $23 billion sales in United States alone in 2014  • Gamers spent globally estimated $71 billion on games in 2014  • There are 155 million people in United States that play video games  • A total of 1641 video game companies are in United States alone spread along 1871 locations  • Education of video game industry is being offered in 496 programs in 406 schools in United States  The process of game production can be simplified as an illustration of the value chain of a production as seen below in Figure 1  Time Constraints in Production  In game industry the schedule of game production has a crucial role because it directly affects the development costs of the title Due to this there is limited time for doing specific pioneering research during game development as there is a lot of pressure to meet deadlines and to provide value for the producer Sometimes there are separate teams which will do engine and technology development but even in that case prioritization needs to be done for optimal developer resource allocation In the authors experience Artificial Intelligence AI development often ends up being one of the fields which will get less focus 3  Role of AI Programming  Because of the previously mentioned factors especially in small and mediumsized teams there usually is not a dedicated AI programmer but those features will be rather added into the game by gameplay and generalist programmers This allows better flexibility in the team resource management and usually generalist programmers do have good basic knowledge on the topic to create usable and functional implementations However when the requirements for the AI features and quality increase dedicated skills in AI programming will prove to be invaluable  Comparison between Academic AI research and Game Industry  When people talk about AI they are generally referring to the academic AI which is quite different from the game AI While academic AI aims to solve problems requiring intelligence the purpose of game AI is to give illusion of intelligence and entertain the player The goal of AI research in academic setting is usually to create publications and do original research while game AI development aims to create a game Academic AI research is often funded by grants academic institutions or sponsorships while game AI is funded by the game publishers Its said that there is strong division between the two fields but in practice both parties have the possibility to benefit from each other with help of the results of academic research game developers can add increasingly advanced AI to the games while academic AI research can benefit from game engines which they can use for their research 3  11 	Scope of Thesis and Research Design  The goal of this thesis was to produce a technical design for adding AI features into a turnbased strategy game This included  • Figuring out the requirements imposed by the game for the AI  • Determining which of the existing solutions in the field of AI knowledge were appropriate to satisfy those goals  • Designing the best methods for integrating them in the game  • Choosing proper balance between AI programmers and game designers workload by necessary tools  A full working game where the technical design would have been implemented was out of the scope of this thesis but limited amount of prototyping was set as a secondary goal during the writing process to provide some practical analysis on the feasibility of theoretical choices made in the thesis The research design is illustrated in Figure 2  The research design Figure 2 shows the phases of the project and the respective thesis and prototype work involved in each of them Most of them match the chapters in this thesis in the following order  1 Introduction chapter outlines the goals and motivations behind the thesis work and gives introduction to the game business to which this work relates to The initial prototype work was also started at this point and the requirements it imposes for project were gathered  2 Background chapter contains brief history of AI in games and the related gaming genre and describes the prototype and lists the previously gathered requirements During this phase the relevant technologies were picked  3 Technology chapter was created through iteration of selected technologies during which the purpose of each one of them was described in the context of this thesis work Also some of them were integrated in this phase to the prototype  4 The Proposed Solution was developed initially during the technology iteration phase and finalized in the review phase This included outlining the highlevel system during which prototype was also refactored based on the results of earlier integration  5 In Evaluation chapter the feasibility of proposed solution is evaluated and this evaluation is partially backed by the data that was extracted from the prototype at this stage  6 The Discussion and Conclusions chapter analyses the results of the thesis evaluates the outcome of the research and contains suggestions for further improvements  In addition the project also produced a partially working prototype for which the source code was included in the appendices of this thesis 	  This section gives a brief overview of the history of game AI and the 4X strategy gaming genre which the project belongs to It also describes the technical starting point of the prototype and outlines its requirements for the AI  21 	History of Artificial Intelligence in Games  The concept of artificial intelligence itself is nothing new even in ancient times mankind has been interested in the concept of artificial life and intelligence This shows in the various fictional stories and attempts to build automatons even centuries ago However the major breakthroughs in electronics and computer technology in past century have allowed unprecedented advancement in research of artificial intelligence 4 pp 45  After the first general purpose programmable computers were invented it did not take long time until the first AI programs were developed One of the first published ones was Alan Turings chess program although at the time the computers were not advanced enough to completely run it 4 p 6  Although there has been constant academic interest in AI research breakthrough in game AI started in the 1970s when first video arcade games were developed They featured primitive computer opponents such as the aliens in Space Invaders and ghosts in Pacman Although the AI in those games operated on simple deterministic algorithms they gave the player impression of intelligent behavior Since the early video games the game AI development has slowly advanced and different gaming genres have their own specialized requirements ranging from tactical realtime NonPlayer Characters NPCs in firstperson shooters to complex strategic planning in strategy games and even artificial life simulation 5  Current State and Future Development  In the past decades there has been a lot of progress especially in terms of graphics quality and storytelling aspects of games With the increased complexity and depth of games turning into interactive entertainment there is increased desire for creating advanced artificial intelligence functionality to improve the gameplay experience and immersion for the players Thanks to the constantly advancing computing capabilities of modern computers there are now better possibilities to implement advanced AI than ever before such as increased focus on selflearning AIs that will adapt and learn from players 6 pp 35  22 	4X Strategy Games  The term “4X was created by Alan Emrich who used it in review of “Master of Orion in 1993 for the first time  “it features the essential four Xs of any good strategic conquest game EXplore EXpand EXploit and EXterminate In other words players must rise from humble beginnings finding their way around the map while building up the largest most efficient empire possible Naturally the other players will be trying to do the same therefore their extermination becomes a paramount concern 7  Although majority of 4X strategy games are turnbased there are a few examples of realtime games which are considered to belong to this genre However the classification of games in 4X genre can be difficult and sometimes a few additional criteria have been used to narrow what fits the definition such as empire economy control and diplomacy versus combatfocused gameplay in regular strategy games 8  Unlike many other game genres strategy games are highly dependent on skilled AI to provide meaningful gameplay experience for the player As the opponents tactics and strategy are foundation of the core gameplay challenge for player a poorly implemented computer AI would most likely be acceptable by casual gamers but advanced players would find it boring and unrewarding 5  Civilization  Sid Meiers Civilization series is one of the bestknown examples of turnbased 4X strategy games see Figure 3 The game has taken heavily inspiration from previous board and computer games such as Risk and Empire but also added novel features such as technology tree 9   Figure 3 The original Sid Meiers Civilization Macintosh version pictured  Originally released in 1991 the series has had six major releases and several spinoffs In Civilization the player takes role of leader of a nation which heshe will lead from stone age to modern day In the first game in the series there were only three conditions for ending the game world domination by eliminating all opponents building a spaceship to reach Alpha Centauri or running out of time in the year 2100 Later versions of game have gradually added more victory and endgame conditions along with many other features in each major release 9  Master of Orion  In Master of Orion see Figure 4 the players control number of different races which compete for control of the galaxy The game features exploration discovery of new technologies dealing with other players with diplomatic or military means and endgame conditions which are similar like to the ones in Civilization 10   Figure 4 Master of Orion II  Besides being spacethemed the game trades off the gridbased movement and exploration with a more restricted and simpler model where movement is only allowed between solar systems Other major difference is also how battles are handled instead of singleunit attacks the combat happens on a separate combat screen where multiple fleets of both participants are fighting at once Players have control over individual ships and weapons which can lead to complex tactical battles 10  The game has been influenced by some earlier games such as Reach for the Stars The original series had three releases and the franchise was recently rebooted by Wargaming on Steam 7 10  Galactic Civilizations  Galactic Civilizations see Figure 5 is a series of strategy games which was initially released for OS2 in 1993 by Stardock Corporation The game combines the turnbased grid map familiar from Civilization with space exploration theme like in Master of Orion 11   Figure 5 Galactic Civilizations Gold for OS2 12  Galactic Civilizations was one of the few games released for IBMs OS2 operating system and it received recognition even from IBM who licensed the game and included it rebranded as “Star Emperor in their FunPak software bundle However the OS2 operating system had limited user base and later lost its remaining market share to Microsoft Windows on the PC platform thus later releases were made for the Windows platform most recently on Steam 11 13  23 	Prototype of Strategy Game as Foundation  Due to time constraints the implementation part of this thesis focuses on working on a theoretical prototype of the game This allows the development to focus on areas relevant for artificial intelligence integration  The game itself is be turnbased with each player performing actions in sequential order The game world is represented as a hexagonal grid map to which players have their own views depending on exploration and espionage status The game draws inspiration from Civilization and Master of Orion series creating mix of them with resemblance of the original Galactic Civilizations  Unity 3D  For rapid prototype development an offshelf game engine is used Unity 3D see Figure 6 is one of the most popular engines today which has not only gained popularity as mobile game development platform but also has seen use in recent AAAgrade desktop titles such as the recent remake of Master of Orion and cityplanning simulator Cities Skylines 14  Figure 6 The Unity3D Editor running on macOS platform  Originally released for Mac OS X platform in 2005 the engine has had five major releases and is currently available for both Windows and Mac OS X 15 Unity features componentbased architecture advanced 3D engine that can target various graphic APIs on several platforms NVidia PhysX engine and for physics simulation and various other frameworks Gameplay logic in this project is implemented in C which is supported as default scripting language by Unity other option being the UnityScript It is a mature and highlevel language originally developed by Microsoft which is integrated into Unity through the Mono framework 16  The choice of Unity for prototyping does not limit the options for selecting different game engine or building a custom one for the final game but given Unitys track record of being the platform of choice in number of highquality titles using it for the actual production is a viable option  24 	Requirements  To evaluate the technical needs for AI integration a set of requirements was created based on the game concept which outline the expected features of the game These are highlevel nonfunctional requirements which also outline the systemlevel design  241 Common Features  There is a set of features which are not specific to AI players but are also used by human players in the game A historical challenge for 4X games has been the amount of micromanagement which increases as the game progresses The amount of micromanagement is directly proportional to the size of game world complexity of game economy and length of the game This has potential to frustrate players and when they have to spend excess amount of time dealing with lots of repetitive lowlevel tasks instead of focusing in larger scale strategies and planning  There are various ways to reduce this micromanagement which have been added to recent 4X games and there are a few ones that considered when defining the requirements The quality of artificial intelligence to which mundane tasks are delegated to is important because if a poor implementation makes players feel that automation makes worse decisions than they would themselves do it would discourage them from using automation altogether and make the attempt to reduce micromanagement void  Colony management  There are a few aspects in managing colonies which can be delegated to automatic colony governor AI  • Production management  • Work force distribution  • Tax rates  • Import and export balancing  • Security level  It should be noted that having the support for automatic control of these properties of colony does not prevent the human player from altering or disabling any of them if heshe feels like it Also depending on how much configuration will be exposed to the player any of these could be parametrized with userspecified customization  Automated units  Another good opportunity to reduce mundane tasks for the human player is the automation of certain unit actions  • Worker automation build new improvements and adjust existing ones  • Automatic exploration reveal unexplored space and patrol previously explored areas  The above tasks are good fit to be implemented with AI automation as the scope of strategic choices made in them are focused on certain isolated parts of the game and thus do not depend on the highlevel strategic plans player may have The worker automation has the possibility to determine best possible improvement actions based on the economic status of the players empire and colonies and can adapt also previously made improvements to match the most recent situation Exploration is also by itself an isolated function where different input patterns can be fed to the exploration AI to prioritize areas to explore for example based on evaluation of military threat on influence map  242 Computer Player Specific Features  The rest of AI features are specific for the computer player and they are used to simulate the actions a human player would be performing in the game  Longterm Planning  The computer player needs to be able to choose and strive for various longterm goals most important one being desired victory condition The AI should be able to reach this goal by dividing the plan into smaller subplans and adjusting them to match the constantly changing conditions which are affected also by other players during the game  AI State Persistency  All the data used by the AI should be serializable so that the game state can be saved and loaded at any time without disrupting the functionality of the computer players decision making  Diplomacy  The AI should have ability to make diplomatic decisions including declaring war and creating alliances It should have possibility to do those choices in informed manner with knowledge about the other players economic and military power past trustworthiness and any other game parameters ie common ideologies personalities etc that might make the other player more or less favorable  243 Optional Features  There is a set of features which should also be evaluated though they are not required by the core gameplay and thus can be considered optional If implemented they do though have possibility to enhance playability value for more hardcore players  Tactical combat  When battles are initiated between fleets they are by default be resolved using simplified simulation which considers only the ship statistics numbers combat bonuses and other predetermined factors affecting the battle This can be enhanced by introducing tactical combat in which entire battle is fought as turnbased minigame and moves of computer players are handled by tactical AI Human player can either choose himselfherself all moves against the opponent or activate an automatic battle mode in which the same AI features will fight the battle also for himher  Ground combat  The fight over control of colonies on planets is handled through ground combat By default the results of these battles will be determined by the ground troop technology level number of troops and other possible combat bonuses There is possibility to further expand this battle into more finegrained ground combat simulation similar to the tactical combat in space where players would have more opportunities to control individual platoons of troops Practically this would be similar to the previously detailed tactical mode and could leverage some of the technologies but would happen on planet surface instead of space 	  3 	Technology  Since the early applications of AI in computer games the number and complexity of technologies involved have gradually grown partially with help from advancements in processing performance but also due to research done in the field of artificial intelligence research done for academic purposes The technologies introduced in this section are chosen by their potential usability in the game being created allowing the scope of thesis to focus on the most relevant ones  General Architecture  When creating AI for strategy game attention should be given to the design of overall architecture technologies involved how they are bound together and how they impact the gameplay According Ian Millington turnbased strategy games share many aspects with realtime strategy games with most important ones being Pathfinding Decision Making Tactical and Strategic AI and Group Movement 17 pp 809815 as shown below in Figure 7  Figure 7 An example of AI architecture for turnbased strategy games 17 p 815  Figure 7 shows a possible architecture for turnbased strategy games although exact model has variations depending on the gameplay elements involved in the design The technologies presented in this thesis use that model as a starting point with adjustments done to suit the exact requirements of this project  31 	Introduction to Traditional Board Game Techniques  As previously briefly mentioned in Chapter 21 one of the earliest applications for computer game AI was the game of chess Since those early days a large amount of time and effort has been spent on research and studying AI for several board games which have provided both academic and practical challenge for the researchers During the past decades a set of algorithms has gained foothold to become the foundation shared by many of the board game AIs and a few key concepts are introduced in this chapter 17 p 647  Game Tree  The most important concept for the majority of board games is the game tree which represents the game states as nodes in the tree and all possible moves as branches leading from nodes to new possible states With this data structure the goal for the AI is to choose one of the branches as a move it should make and needs to use various algorithms to find out which move is the best one it can make 18 pp 1629  Minimax  When evaluating the game tree the basic idea is to use a heuristic to give each possible move a score which indicates how good the move would be for the player in singleplayer games the heuristic could for example be the number of moves to finish a game The score of each move bubbles back up in the search tree and after scoring all possible moves the AI just needs to choose the move that has the best score However when two or more players are involved the evaluation algorithm should not only try to find the best score for the player but also acknowledge that the opponent will try to choose a move that yields the least score for the player Thus when bubbling up the scores the minimum score should be picked for enemy moves and player should choose move which give the largest of the minimum scores hence the name “Minimax of the algorithm 18 pp 3039 An example of the basic principle in minimax algorithm can be seen below in Figure 8  In this case the best move with 2ply search would be p2 because it would end up with score of 5 for the player Other moves end up with lower score because move p1 would allow opponent to do move e11 resulting with score of 2 and p3 would allow move e31 with score of 4 both of which are lower than the smallest score of 5 given by move e23  The challenge with game trees is the balance between ply depth and processing power requirements the deeper the tree is the number of possible moves usually increases exponentially and so does the time required to process it On other hand if the search depth is too short the AI cannot predict the game events far enough in advance and might not be able to predict possible “killer moves which might decide the winner of the game in long run There are various methods which have been developed to help and speed up searching the game trees including AlphaBeta Pruning Killer Heuristic AlphaBeta Windows and Transposition Tables 18 pp 4060 17 p 651  Applications in turnbased strategy games  Although there are similarities between board games and strategy games the complexity and number of possible moves during each turn in strategy games causes the size of game tree to grow so large that using traditional board game AI algorithms such as minimax becomes unfeasible in most situations There are however certain cases in which using some of the techniques are beneficial such as using cost estimation similar to the game tree scoring approach with task planning which can be used for example in decision making for research construction troop movement and military actions 17 pp  688670  32 	Virtual Player  Traditionally the AI has been implemented in games through the concept of AI agents which are usually divided in two types Characters and Virtual Players Characters appear as visible entities in the game that player can usually interact with they be as simple as ghosts in Pacman more advanced enemies like demons in Doom or other NPC opponents They can either be directly involved in the gameplay or just exist to add to the ambience and general immersion of the game 4 pp 1112  Virtual Players on other hand do not usually have physical representation in the game world but instead replace other human opponents in game and assume the tasks and responsibilities of that player The classic example for this kind of AI is used in board games such as Chess where the Virtual Player decides which moves it should make in the same way as a human player would This model applies to turnbased strategy games where the individual units in the game do not usually contain any intelligence but all decisions are made by the Virtual Player Exception to this are realtime strategy games where the behavior of individual units has important role in the game but that subtype of strategy games is out of the scope of this thesis 4 pp 1617  For this thesis the design of the Virtual Player is the central outcome of the research project as it is an umbrella concept that encapsulate all the supporting technologies studied in this project  321 MultiTier AI Framework  Building on the principle of the strategy game AI architecture shown earlier in Figure 7 the multitiered AI framework approach is based on separating responsibilities of the AI to individual levels The structure framework resembles the military hierarchy in which highlevel AI sets the general strategy and goals which translate to commands that are given to the next level which set their own goals to be able to fulfill those orders This continues until the individual unit level is reached where the commands are turned into actions performed by the units 17 p 544 An example of this topdown command hierarchy is shown below in Figure 9  Figure 9 Example of multitier AI for military strategy game 19  In the example some possible situational projects are shown on their source level with destination visualized The roles of these individual levels used in the model are described below in Table 1  Table 1 Roles of levels in the multitiered AI model 19  Level  Role  Strategic Intelligence  Knowledge of the entire empire management of grand strategy goals global resource levels research and diplomacy  Operational Intelligence  Divided into activity groups can track also noncombat activities such as economic and diplomatic operations  Tactical Intelligence  Information about encountered opponents geography resources  Individual Units  Pathfinding unit movement combat  There are also other possible ways of building the hierarchy in the framework including bottomup approach where individual units are autonomous and higher levels of the hierarchy just provide intelligence and general information about the game world It should be noted that the framework allows information anyway to pass in both directions in the model depending on how the gameplay requirements imposed on the AI 17 p  544 19  33 	Pathfinding  One of the most important features in nearly any game dealing with a map with entities that should be able to navigate on it is pathfinding There are various maps and grids that can be considered to be node graphs as seen in Figure 10 to which graph search algorithms can be applied to Of the examples shown hexagonal grid is used for presenting the game world in this project  Figure 10 Examples of node graphs created from maps for pathfinding  The basic case is the ability to find shortest route from point A to point B To do this there are several graph and tree search algorithms the most common ones used in games are summarized in Table 2  Table 2 Comparison of pathfinding algorithms 4 p 171  Algorithm  Description  Benefits  Drawbacks  Breadth first search  Simple algorithm but does not consider movement cost  Most simple pathfinding algorithm  Not optimized  Does not always return shortest path  Dijkstras algorithm  Improvement on the breadth first search which adds path cost  Guaranteed to find shortest path  Not optimized  Astar A*  Combines Dijkstras algorithm with a casespecific heuristic value to the cost estimation function  Guaranteed to find shortest path  Heuristic helps optimize the search  No major drawbacks  There are numerous other algorithms but due to simplicity this study only considers the commonly used ones shown in the above table Because the maps in the game are mostly generated at runtime have a large number of cells and are highly dynamic the possibility to precompute navigation data is limited  331 A* Algorithm  The Astar A* algorithm is one of the most used pathfinding algorithms in games as it is relatively simple to implement and has good performance Because of this it was chosen as the default pathfinding algorithm for the game An example of A* pathfinding case is shown below in Figure 11  Figure 11 A* Pathfinding on hexagonal grid with Manhattandistance heuristic  The pathfinding starts from the hex on lefthand side of figure with black dot and target cell is indicated with checkmark on righthand side of the grid Red color indicates cells with higher movement cost of 5 while other cells only cost 1 to move through The search begins by putting hex coordinate of starting cell into the priority queue During each iteration the first item is removed from the priority queue one with highest priority priority is calculated for each neighbor cell which has not yet been processed and each of them are added into the queue To calculate the priority the A* algorithm uses priority formula shown in Equation 1 20 below   	 		     	1  In which fn is the resulting priority gn is movement cost for this specific cell and hn is the additional A* heuristic value In this example the Manhattan distance to the target cell is used After all neighbors have been inserted to the queue the iteration proceeds to next step The search ends when either the target position has been found as one of the neighbors or when the priority queue runs out of items ie when there is no solution In the above figure green cells outlined show the resulting optimal path to target position It should be noted that if the heuristic function hn  0 then the search behaves equally to Dijkstras algorithm which A* was extended from 20  332 Optimizations  In some cases the number of search nodes from which pathfinding lookup is queried may end up being too sparse causing an excessive amount of time being spent on performing the query In this case optimization is needed and there are a few approaches that may be beneficial depending on the use case  Hierarchical Pathfinding  To speed up searching paths in large sets of nodes it may be possible to combine physically adjacent nodes as groups so that pathfinding operates initially on the higherlevel group nodes from which it progresses to lower levels after highlevel path is found Depending on the type and size of original node tree for example in large openspace maps this combining of node clusters can be extended further to higher levels creating a hierarchy of search trees which is base idea in hierarchical pathfinding 17 p 265  Zone Mapping  In some cases there may be search trees which have completely disjoint start and goal nodes When this happens a A* search would end up having to look through all connected nodes in the tree just to find out that there is no solution for the path query In zone mapping a special floodfill algorithm is used to identify isolated regions in the search tree with result of this process stored in a zone map With this cached data it is possible to know in advance whether there is any solution before having to attempt doing the path query 4 p 197  34 	Decision Making  One of the key requirements for the AI is the ability to make decisions which translate to actions executed by the computer player The relationship between input data and action request effects is visualized below in Figure 12  Figure 12 Decision making schematic by Ian Millington 17 p 303  There are several algorithms and techniques for this purpose which share the core idea of having internal or external knowledge sources as input which are processed to action requests as output which affect the internal or external state of the game 17 pp 301302 This section introduces the most common decisionmaking methods which have applications on multiple levels on the MultiTier AI model  341 Finite State Machines  Finite State Machines FSMs are one of the key concepts used in computer games A state machine is composed of a set of states and rules defining transitions between those states In a FSM only one state is active at a time and switching to other states only happens when one of the transitions out of current state is requested 6 pp 165166 A simplified FSM for scout unit is shown below Figure 13  Figure 13 A possible FSM for controlling scout behavior  There are three states in the FSM to either idle explore or return to nearest colony and four transitions controlled by two external flags which indicate whether there are areas to explore and whether the unit is at colony  In the ideal FSM model transitions are handled internally using the data provided to the FSM and are thus selfcontained However in some cases using the input data as sole trigger to state changes might not be enough for example when UI triggers user input which needs to immediately alter the state of a unit In this situation an external transition can be triggered procedurally which is used to set the state of FSM directly without use of a predefined transition condition 4 pp 5052  Hierarchical State Machines  The Hierarchical State Machines HFSMs extend the basic principle of FSMs by adding the possibility of using subFSMs which are basically state machines nested inside of a parent FSM They add flexibility to the state transition options through the ability to continue parent FSM flow after finishing execution of the subFSM Another benefit is the possibility to split more complex states into substates 17 pp 327330 Figure 14 below shows a simple example case for Hierarchical FSM  Figure 14 Example of hierarchical FSM for worker unit  In this case a worker unit would by default toggle between idle state and build and repair tasks but at the appearance of enemy unit would interrupt any task it was doing and seek cover After the threat of enemy would be cleared the Hierarchical FSM would resume the worker automation SubFSM and its active state would automatically be the one which was interrupted earlier  342 Decision and Behavior Trees  There are a couple of common techniques which have the advantage of both being simple to implement and to use Decision Trees and Behavior Trees They are usually used to control NPC actions in games but they have also potential use cases for decision making in military units in strategy games They are both treelike structures which have the benefit of being able to be shown as a visual presentation of the decisionmaking process to the AI designer 17 pp 303309 21  Decision Trees  The Decision Trees DTs help making choices based on the world state at a specific time through the use of tree built of decision branches which lead to actions Usually the decision nodes are binary and have only two branches but it is possible to make selections with more than two options The decisionmaking process starts from the root and simply just moves down to the next branch depending on the outcome of each decision node This flow is very similar to traditional ifthenelse control flow in highlevel programming languages but the nodes as usually expressed explicitly as data structures which can be defined either in code or using data model which the AI designer can modify 17 pp 303309 The above Figure 15 below shows how the previously proposed FSM logic in Figure 14 might be converted into a DT  Figure 15 The decision logic from Figure 14 converted into a decision tree  At root decision the presence of enemy nearby would be checked first in which case the worker unit would move to cover If no enemy were nearby the tree evaluation would continue to next decisions to check if either type of tasks would be available for it if not the idle action would finally be picked as the last option  Decision Tree Learning  One interesting aspect of DTs is that they can be generated using machine learning from input of observation and action sets which represent the desired outcomes for each world state observed There are various possible methods for accomplishing this but the commonly used ones are based on the Iterative Dichotomiser 3 ID3 algorithm It uses the measurement of entropy to calculate information gain from available attributes for selecting the most relevant decision factor as the next node in the decision tree and continues this process until all action nodes have been created 17 pp 593597 For example the following observations could be used as input for the ID3 algorithm  Table 3 Input observations for ID3 algorithm  Build Task Available  Repair Task Available  Enemy Near  Action  Yes  Yes  Yes  Cover  Yes  Yes  No  Build  Yes  No  Yes  Cover  Yes  No  No  Build  No  Yes  Yes  Cover  No  Yes  No  Repair  No  No  Yes  Cover  No  No  No  Idle  In each iteration the algorithm first uses Equation 2 to calculate the entropy of the entire set of actions 17 pp 593597   	 		** 	2  02 In first iteration the formula gives the following entropy values for the entire set and available subsets  Es  175  	Ehavebuildtask  1 	 	Enobuildtask  15  	Ehaverepairtask  15 	 	Enorepairtask  15  	Eenemyneary  0 	 	Eenemynear  15  Those results can now be used in Equation 3 to calculate the information gain from the subsets 17 pp 593597   	3 	4	89	3  02 Which results in the following information gain values for the attributes  Ghavebuildtask  05  Ghaverepairtask  025  Genemynear  1  With the above results the algorithm chooses the attribute with highest information gain value as input for decision node which in this case would be the presence of a nearby enemy After this the algorithm repeats the same process for the both subsets of the observation data adding new child nodes until all relevant branches have been created The DT which was created by ID3 algorithm from the sample observations is shown below in Figure 16  Figure 16 Decision tree generated by ID3 algorithm  The nodes outlined with red circles were the ones where calculation of information gain was performed Note that no decision node was added for the “Yes branch of “Enemy near check because neither of the two remaining attributes contributed any information gain to the decision and thus were not required at all  Some software frameworks even include Decision Tree Learning as part of their feature set for example Apple offers builtin support generating decision trees using machine learning in their GameplayKit framework on iOS macOS and tvos platforms as part of their basic Decision Tree implementation 22  Behavior Trees  Another graphstyle decisionmaking method is Behavior Tree BT which generalizes the previously introduced Decision Tree concept Any DT can be represented as BT but the modularity and extensibility combined with their simplicity is what makes them powerful 17 pp 5253 23 The difference between DT and BT trees is shown below in Figure 17  Figure 17 A Decision tree expressed as behavior tree with identical logic 23  Unlike DTs which are solely composed out of decision and action nodes BTs have a multitude of possible node types which are generally divided into interior nodes also known as composite nodes which have one or more children and leaf nodes which have no child nodes The child nodes in BT are ordered in priority order usually visualized from left to right which dictates the evaluation order of the nodes The processing starts from root node and progresses down to child nodes in the tree depending on the node types in the tree All nodes have a precondition which can have three possible return values success failure and running The most commonly used nodes in BTs and their return values are listed below in Table 4 23  Table 4 Most common behavior tree node types 23  Node  Type  Precondition return values  Success  Failure  Running  Action  Leaf Node  Upon comple tion  When cannot complete  During comple tion  Condition  Leaf Node  If true  If false  Never  Sequence  Interior Node  If all children  succeed  If one child fails  If one child returns Running  Selector  Interior Node  If all children  succeed  If all children fail  If one child returns Running  Parallel  Interior Node  If  M children succeed  If > N – M children fail  If neither Success or Failure condition is met  Each time the BT is ticked the tree is traversed down and the node preconditions checked until the active task node is reached which performs the action in during this particular tick The return value is backpropagated up in the hierarchy to the parent nodes which depending on their behavior decides what to do ie possibly evaluate next child in interior nodes and return the appropriate value back to their parent node This progress continues until the return value reaches the root of tree which is returned to the original caller 17 pp 5253 23  Thanks to the flexible structure any type of nodes can be added to the BT for example UtilityBased Systems can be leveraged to create a utility selector node which can use internally utility scoring to choose the appropriate child to execute 24 Also reusable parts of BTs can be shared as subtrees reducing amount of work needed to create duplicate behaviors  343 Fuzzy Logic  The traditional computer logic is based on Boolean algebra which infers absolute values of either true or false as the only possible conditional states There are however certain cases in which a more finegrained evaluation is required for example to assess the threat of an enemy fleet and choose appropriate actions based on the analysis of the situation For this one fuzzy logic can be used in which the absolute true and false states are replaced by a membership degree which is expressed as normalized value between  00 and 10 17 pp 344345 The overview of how fuzzy logic is used is shown below in Figure 18  Figure 18 Fuzzy process overview 6 p 192 17 pp 344354  The process starts with fuzzification of input data after which fuzzy rules can be applied to the fuzzy sets and results can be obtained through defuzzification which converts the data back into crisp values 6 p 192  Fuzzification  The Fuzzification is done through of membership functions which convert predefined ranges of values into fuzzy set membership degrees Commonly used membership functions include grade reverse grade triangular and trapezoid functions but other functions can also be used if necessary The number of membership degrees is not bound by the number of inputs as same input values can be assigned to multiple membership sets at the same time 6 pp 193198  Fuzzy Rules  After conversion to Fuzzy Sets the membership degrees can be combined using Fuzzy Rules which are built using Fuzzy Axioms which resemble the operators used in the traditional Boolean logic 6 pp 200201 The most commonly used operators are listed below in Table 5 Comparison between Boolean and Fuzzy Logic operators  Table 5 Comparison between Boolean and Fuzzy Logic operators 6 p 200  Defuzzification  After combining the values using Fuzzy Rules the membership degrees need to be extracted from the Fuzzy System to be usable in the game There are few commonly used methods for doing this  • Highest Membership  • Membershipbased Blending  • Center of Gravity  The best method to use depends on how the data is used For a simple Boolean decision the Highest Membership method should be enough but if there is need to aggregate output strength other methods are needed Although the Center of Gravity is often favored it comes with increased overhead due to the need to integrate surface areas of the membership regions The blending approach usually is good enough and is much quicker to use 17 pp 347351  Use case Threat Assessment  One good application of Fuzzy Logic in strategy games is using it for threat assessment and classification purposes A simple example case for this is shown below in Figure 19  Figure 19 Threat assessment example case for Fuzzy Logic  In the above case the AI wants to evaluate the presence of enemy fleets around its colony to adjust its defensive stance if needed As input data it uses the proportional ratio between its defensive strength and strength of enemy fleets in Equation 4   	*< 	log	|DG0||DAE0|9AB9 F 	4  Where Ae is the list of enemy fleet attack strengths Ao is list of own fleet attack strengths and do is the defensive strength of the colony With the use of log2n in formula the exponential change in ratio between own and enemy strength can be converted into a linear value which the threat assessment can more practically be applied to When considering all units within 2 hexes distance from the own territory borders applying the situation in Figure 19 to the previously introduced Equation 4 results in the following values in Equation 5   		 	5  To use this input value two membership functions with three fuzzy sets in each are defined as shown below in Figure 20  	Weaker   Equal   Stronger	Bad  Average    Good 	15	1	05	0	05	1	15	40	20	0	20	40	60	80 	pratio	reputation  Figure 20 Membership functions for force strength ratio and diplomatic reputation  The three fuzzy sets in the pratio membership function evaluate the threat which is considered to be minimal when the enemy fleet strength is less than 50 pratio  1 of defensive strength and very high when it is over 200 pratio  1 At equal strengths pratio  0 the threat is considered to be medium The three fuzzy sets in the reputation membership function are defined to represent the diplomatic reputation of enemy so their trustworthiness can be included in the evaluation of probability for their aggression Performing the fuzzification of values pratio  066 and reputation  5 with the membership functions in Figure 20 results with the following membership degrees  Table 6 Output membership degrees of the fuzzification  Fuzzy Set  Degree of Membership  Weaker  0  Equal  034  Stronger  066  Bad  025  Average  075  Good  0  The get the threat level from these values the following fuzzy rule matrix is used to map the values to Low Medium and High threat levels  Table 7 Fuzzy rule matrix for combining the force size ratio and reputation  Weaker  Equal  Stronger  Bad  Medium  High  High  Average  Low  Medium  High  Good  Low  Low  Medium  These above rules can be written as the following logic expressions  mLow  mWeaker mMedium  mWeaker mHigh  mEqual   mAverage   mBad mBad  mWeaker   mEqual mStronger   mGood  mAverage    mEqual   mStronger  Stronger   mGood   mGood mAverage    mBad m Populating the above expressions with the previously calculated membership degrees for the fuzzy sets results in the following values for the threat membership  mLow  maxmin0 075 min0 0 min034 0  0 mMedium  maxmin0 025 min034 075 min066 0  034 mHigh  maxmin034 025 min066 025 min066 075  066  When the highest membership selection is used to determine the threat level it can be concluded that mHigh has the highest membership value of 066 meaning that the threat level is high and decision making can act on strategic and diplomatic level accordingly Another option would be using the membership blending method to calculate a numeric threat level value out of the membership values if needed to for example adjust internal diplomatic stance level  Other applications  Besides the threat assessment other applications for Fuzzy Logic in strategy games also include Bayesian Network probability reasoning 6 pp 253254 and decision making in RuleBased Systems 17 p 354 Fuzzy State Machines can use Fuzzy Logic to do blending between states allowing smooth transitions based on the Fuzzy transition conditions 17 pp 364369 This technique is sometimes used for example to do animation state blending like in Unitys Mecanim Other gaming genres such as racing can also benefit from the way Fuzzy Logic can be used to control vehicle steering but that is out of the scope of this thesis 6 pp 205207  344 RuleBased AI  Rulebased Systems sometimes also known as Expert Systems have existed in the AI field since 1970s They are sometimes considered a doubleedged sword as although they allow the experts to share their knowledge of the situation and reasoning about how to handle it there is the downside that rule sets to define this knowledge must be created by those experts Although rulebased systems have many applications outside gaming industry such as in financial medical and industrial software there is also use for this approach also in games 4 p 134 Their strengths include the ability to use extensive rule sets to capture highlevel knowledge of various complex problems 4 pp 139140 and the capability to make decisions in unexpected situations which cannot be easily handled with more simple approaches such as decision trees 17 p 403 The Figure 21 below shows the basic structure of rulebased system  Figure 21 Overview of RuleBased System 4 p 138 17 p 404  As pictured this system consists of main components called the Rule Set Inference Engine and Working Memory Each of these parts are described below in more detail  Rule Set  The actual knowledge of a problem is encoded in various rules which are kept in the Rule Set also known as the Knowledge Base Each rule has two parts the condition which must be satisfied for the rule to be fired and action which defines what happens when the rule is triggered The condition can evaluate facts in Working Memory in various ways and rules can also be enabled and disabled when needed The action can alter facts in Working Memory but can also control which rules are active and even stop the processing completely 4 pp 134135  Working Memory  All facts known by the RuleBased System are kept in the Working Memory which works as a database for the Inference Engine Although the format of facts is not limited they are usually stored as Boolean numeric string and enumeration values 4 pp 134135  Inference Engine  The actual processing of rules happens in the Inference Engine which checks the rule conditions of the rule set and either selects the first match or uses the arbiter to choose which action to trigger The processing takes place in iterations which continue until either no more facts are changed in the database or a stop action is encountered Usually forward chaining rule matching is used but sometimes backward chaining can be used which matches the rules based on their outcome effect of actions on facts instead of conditions trying to find a starting state that can derive the expected result 17 pp 407408 RuleBased Systems are however notorious for suffering performance issues when large rule sets are used which need considerable processing time There are some optimizations for this process the Rete Algorithm being one of the best known of them 4 pp 134135 17 pp 422423  Arbiter  Sometimes the system contains a separate arbiter component which decides which rule is triggered if multiple rules are matched simultaneously during one iteration Possible common approaches include using first applicable rule least recently used rule random rule most specific conditions and dynamic priority arbitration 17 pp 418419  Use case Inferring tech tree state through rulebased reasoning  As rulebased systems allow inferring new facts from existing ones through the rules one possible use for them in 4X strategy game is the capability for AI to use knowledge about enemys possession of a single technology to deduce the state of other technologies in the tech tree This use case has been adapted and refined from the example provided by Bourg and Seemann 6 pp 214218 The Figure 22 below shows a possible subset of tech tree  Figure 22 Inferring state of tech tree from knowledge of a single technology  In the tree technologies are connected by arrows leading from prerequisite technologies on higher levels down to the subsequent technologies on the next level The following rules can be generated from this tech tree  IF defensetech2 THEN colonytech1Yes AND defensetech1Yes  IF weapontech2 THEN weapontech1Yes  IF shiptech2 THEN weapontech1Yes AND shiptech1Yes  IF colonytech2 THEN defensetech2Yes  IF defensetech2 THEN defensetech2Yes  IF weapontech3 THEN defensetech2Yes AND weapontech2Yes  IF shiptech3 THEN weapontech2Yes AND shiptech2Yes  As the AI has learned that enemy has a ship equipped with Missiles 2 upgrade shown green in Figure 22 and thus has researched the Weapon Tech 3 technology it can use the above rules to infer which other technologies are consequently possessed by the player as prerequisites This starts by putting the fact weapontech3 into the Working Memory and running the first iteration in Inference Engine which finds a match for the following rule  IF weapontech3 THEN defensetech2Yes AND weapontech2Yes  This rule adds the facts defensetech2 and weapontech2 into the Working Memory The inference engine runs next iteration to evaluate the rules and continue the process until no more new facts are added to the Knowledge Base at which point the processing has finished In Figure 22 the technologies set to “Yes during this process are indicated in red  Other potential uses in strategy games could be predefining certain events for scenarios triggers and possibility to control unit and strategic behavior with AI scripts implemented by AI programmer andor designer  345 Utility Theory  The idea behind the Utility theory has existed for a long time and has a history predating even computers in the economics field where it is used to study consumer behavior and choices The core concept in Utility Theory is scoring each action or state in the utility model with a uniform value which represents the usefulness of each choice in the given context To allow scores of multiple sources to be comparable the utility values are normalized using methods appropriate to the given input data and the scores can be combined from multiple sources to end up with final utility score which can be used to select the appropriate action A simplified overview of the information flow inside a Utility System is shown below in Figure 23 25  Figure 23 Simplified flow of information in a Utility System  This flow of information inside Utility System can be roughly divided into the following phases  Phase 1 Converting Game Values into Utility Factors  There is no harddefined way of converting data into Utility Factors the only rule is that all factors must have the same scale so that they can be combined together and be comparable with each other There are certain generally used methods for converting arbitrary game values shown below in Figure 24 Other methods may also be used depending on what is required by the usecases of the utility factors 25  	Simple Cutoff	Linear	Quadratic	Logistic 1111 08080808 06060606 04040404 02020202 0000 00015 Figure 24 Commonly used formulas for calculating utility factors 25  Phase 2 Combining Utility Factors  Usually there are more than one factor affecting the desirability of actions and to get final utility scores for each of them they are combined Commonly used methods include calculating average of utility factors multiplying them together picking the smaller or larger of them or reversing the factor by inverting it These operations can also be chained after each other and exposing them as a visual graph editable by designers can be a very powerful tool 25  Phase 3 Picking the Best Action  After each one of the actions have been given a final utility score the AI selects which of them it should execute The most straightforward way is to just pick the one with greatest utility but it might in some cases lead to repeatable or predictable AI behavior This can be overcome in some cases by using weighed random approach where a random selection is made from possible actions with the utility score used as weight to give the actions with higher utility score a better chance of getting picked This can also be combined with bucketing also known as Dual Utility AI in which the actions are categorized and assigned a bucket based on the effect they have For example when choosing production goal in a colony the military units could be assigned in one bucket and colony improvements in another one With this approach when building army has highest utility it guarantees that a military unit is produced but the type of unit can be randomized 25  The utility scoring has a strong resemblance to fuzzification in fuzzy logic and they share some principles especially in the way game values are converted into the internal representation in both approaches They are also both good for promotion emergent behavior in AI when used properly by the AI designer 25  Use Case Diplomatic Decision Making  In the 4X strategy games one possible use case for utility reasoning might be choosing an action during a diplomatic negotiation with another player A simplified case for this is outlined below in Figure 25  Figure 25 A possible Utility System for diplomatic decision making  This situation assumes that the players have currently signed a peace treaty which offers three possible actions proposing alliance signing a research agreement or declaring war There are also certain game values exposed to the Utility System opponent reputation score for scientific benefit of research agreement and military strengths of own and opponent armies Also the AI personality goals are exposed as diplomatic victory science victory and military victory priority values There values are then converted to Utility Factors using operators defined by the designer or AI programmer which end up as utility values of the possible output actions as shown in Figure 25 This gives each of the actions a utility score and if picking the action based on highest utility the choice in this case would be declaring war against the opponent  It should be noted that in an actual production implementation various other factors should be considered such as how much the player likes or dislikes the opponent when declaring war and what impact it would have on the players own reputation how likely the opponent is to enter an alliance or research agreement before offering them and many others  Using utility scoring for decision making is especially fit for the type of strategy games that this project represents as due to the nearly infinite number of possible moves per turn there is no way to score individual actions in a purely deterministic way In this situation the reasoning of utility of actions allows the AI to make educated “bestguess choices based on the available data of the game state 25 The utilitybased approach is also highly versatile thanks to its simple concept which helps it combined with a number of other techniques such as implementing utility selector in behavior trees 24 and applying utilitybased costs in colony production goal trees 26  35 	Influence Maps  To allow strategic analysis of map and game world various types of influence maps can be utilized to give the AI environmental awareness Some examples of use cases for this data are listed below 27  • Pathfinding can include influence as part of heuristic to avoid or favor certain areas  • Weak spots in enemy influence can be used to target attacks in planning of higherlevel military operations or to prioritize reinforcing own territory  The basic structure and function of influence maps has similarities to cellular automata in which uniform grids of values are modified by certain rules as a function of time usually based on the values of surrounding tiles One classic example is Conways “Game of Life which uses a very simple set of rules although cellular automata has many other higherlevel uses such as city simulation in SimCity 17 pp 536537 The data in influence maps can be composed of multiple layers of data including for example 17 pp  499512  • Tactical Analysis o Friendly and enemy unit and pointofinterest threat generation  • Terrain Analysis o Defensive andor movement bonuses from terrain  o Map visibility which can be used to either increase the “threat of unknown or to prioritize exploration  • Learning o Past events recorded on map such as unit kills ie “frag map  Some of the data such as terrain analysis is by default spread on the influence map layer uniformly and can be used as input as such Some other data though like tactical positions such as unit threat are localized to single spots in the map and their influence needs to be distributed on the layer to be usable To do this there are a couple of common options available shown in Table 8  Table 8 List of common influence calculation methods 17 pp 502505  Method  Description  Limited Radius of Effect  Influence is applied on map as a function of distance to the unit with fixed falloff  Convolution Filters  The unit influence is applied on map using twoor threedimensional filter matrix for example using Gaussian blur  Map Flooding  The unit influence is propagated on map using Dijkstra or A* algorithm  It is also possible to use variations of the above methods depending on the source data and how the influence map is used in the game This involves usually finetuning by the AI programmers and designers to find a good balance for the influence which benefits the AI in decision making For example if certain areas of map are not visible to the player it is good idea to take the factor of unknown into account when calculating influence this however means that each AI player needs to run its own analysis of the influence map in contrast to a game state where all players have the same knowledge of unit positions and strengths in which case the data could be shared 17 pp 505507  An example of a simple influence map is visualized in Figure 26  Figure 26 An example of map of unit threat influence on hexagonal grid  This example shows a hexagonal grid with three units belonging to each player A and B of equal influence value In this case the unit influence values were propagated on the map using normalized Gaussian blur convolution filter applied through a rank 3 tensor on cubic hex projection plane q  r  s  0  Spatial Database  One possible way to represent the different sources of data affecting influence is the use of spatial database as suggested by Paul Tozour In his approach the data is applied to distinct layers in a generalized way with some possible examples of data layers listed below 28  • Openness layer  • Cover layer  • Area searching layer  • Lineoffire layer  • Light level layer  The layers can be combined using various algorithms at runtime for example using a formula like the one in Equation 6 to calculate dynamic desirability layer from other source layers 28   	FQRSTAUSSVW  *QQRR	×	XXY*AXW	×	RVAVSXXQT 	6  One of the possible benefits of using this layering of data is the increased tendency of emergent behavior in AI unit coordination through the use of shared data structures 28  Strategic Dispositions  When units are being categorized in order to identify strategic dispositions the information in spatial database can be used to aid this purpose The knowledge can be used in tactical analysis and decision making for example to identify weak spots which can be engaged in enemy territory or areas in own defences that need to be reinforced 29  An example of evaluation of strategic dispositions is shown below in Figure 27  Figure 27 A possible grouping of units for analyzing strategic dispositions  Figure 27 shows a case where a simplified map of fleet and colony influences has been propagated on the map using limited radius with fixed falloff and clusters of unit have been grouped using a simple algorithm which selects the strongest units and units in their immediate vicinity to be part of their group The total strengths of each group of units is known and thus their threat level can be estimated using fuzzy logic methods similar to the ones demonstrated earlier in Chapter 343 This data can be combined with the influence map for example by calculating the gradient of influence level between nearby grouped units In this case a higher gradient would indicate higher tension between units which can be used as input data to the tactical analysis algorithm which directs the units in groups to either engage enemy unit groups or to reinforce the defenses on local territory  The actual implementation of selection of actions depends on the iterative experimentation by designers and the AI programmer but could for example use utilitybased scoring based on the input factors gained from the analysis  Tactical Pathfinding  The influence maps can also be used in pathfinding to allow the units to consider possible threats when planning the route to the target position The tactical pathfinding can be implemented easily by adjusting the heuristic function of the A* pathfinding algorithm for example by adding penalty based on enemy threat level on the influence map which makes the units evade dangerous areas of the map giving the AI movement choices are stronger impression of intelligence One challenge in this approach however is the care needed when applying changes to the scoring heuristic function to avoid increasing the cost of pathfinding processing time too much 30  A possible use case for this approach in a 4X strategy game might be for example the need to plan route for worker unit across unclaimed space with recently observed enemy movement In this case the pathfinding should avoid areas which would most likely to lead to encounter with enemy  36 	GoalOriented Behaviors  With the previously described methods it is possible to build an AI that can evaluate the current game state and choose appropriate actions which appears sufficiently intelligent in casual gameplay However it is especially important in strategy games for the AI to be have longterm strategy and goals which makes AIs actions and decisions more meaningful and thus giving more challenging and meaningful gameplay experience for the players To accomplish this various forms of GoalOriented Behaviors GOBs can be implemented which can give the AI not only immediate internal needs which it aims to fulfill but also the capability of chaining multiple actions together in order to reach more complex goals 17 pp 376377  This section introduces a few key technologies that can be used to implement this kind of behavior which is useful in the higher layers of the multitier AI mode including production and research planning which involves also coordination between different higherlevel agents  361 GoalOriented Action Planning  The idea behind GoalOriented Action Planning GOAP has long history having roots in the Stanford Research Institute Problem Solver STRIPS which was created already as early as in the 1970s 21 GOAP planning uses backwardchaining search which means that it uses the desired goal state as starting point and traces the action sequence which leads to the starting state There are a few basic building blocks in this approach 31  Goal  A goal represents the desired final state which the planner should attempt to reach Each goal has a set of conditions which must be satisfied for the goal to be reachable  Action  There is a predefined set of actions each of which represent what the AI can do Each action has set of preconditions and effects the preconditions define what the world state should be for the action be doable and the effects define how the world state is changed by this action  Plan  The final plan is a sequence of actions leading from the current world state to the desired goal state  World State  The GOAP planner uses symbolic representation of world to perform search in statespace This abstraction allows both preconditions to be matched against the world state and effects can also be used to apply changes to the simulated states  Planning process The simplistic approach  When running plan formulation the GOAP planner is given the desired goal state a list of possible actions and the current world state which is abstracted from the concrete game world into the symbolic presentation The Figure 28 below shows a simplified overview of the planning process in statespace during the plan formulation  Figure 28 Abstract illustration of GOAP planning process  The planner starts from the desired goal state adding its conditions into the list of unsatisfied world properties During each iteration the planner searches for actions which have effects that match the unsatisfied world properties Each of the possible actions is picked as a possible node in the search graph and evaluated recursively by applying the effects to world state and adding the preconditions of the action to the list of unsatisfied world properties When the planner reaches a state where all the world properties are satisfied it has reached the initial state and thus has found a valid plan or if no more actions can be matched in which case there is no solution After a valid plan has been found it is made active and the AI attempts to follow it However if any alterations are made to the world state during the plan execution replanning is required and thus the planning process is run again 31  Adding action costs to the plan formulation  Sometimes just knowing a possible sequence of actions for reaching the goal does not suffice as there might be other lowercost paths leading to it The types of cost factors depend on the use case for example time money or health  When cost is added to the actions the search space can be considered as a weighed graph which can be evaluated using A* algorithm with the expected cost used as heuristic for the search formula Figure 29 below shows part of a possible search tree which might be formed during GOAP planning  Figure 29 A partial statespace search tree for GOAP  A benefit of the statespace presentation is that as it is practically a game tree structure certain traditional board game AI techniques can be applied to it For example to prevent unnecessary time spent on evaluating duplicate subtrees the evaluated states can be stored in a transposition table with the hash of symbolic world state used as cache key Other benefits include the possibility of using AlphaBeta Pruning and the Killer Heuristic  Iterative Deepening A* IDA*  When A* is used for pathfinding each graph node is only evaluated once and there is limited number of nodes to explore However with GOAP planning there is no limit on how many times a single action may be performed leading to infinitely long action sequences To avoid this Iterative Deepening A* IDA* which is a variant of Iterative Deepening Search IDS algorithm can be used for traversing the state graph 17 pp  376401 The progress of IDA* search is shown below in Figure 30  Figure 30 Iterative Deepening A* search example  The IDA* search works by defining a cutoff value which is the maximum cost until which the search iteration terminates On each iteration the regular depthfirst search is run until the current cutoff limit is reached If the target node was not found the cutoff value is increased and search is run again thus iteratively progressing further in the search tree each time 17 pp 376401 The above Figure 30 shows roughly how this iterative progress works  362 Hierarchical Task Networks  Although sharing some concepts with STRIPS planning the Hierarchical Task Network HTN approach assigns the current world state as starting point and uses available tasks to construct the plan through forwardchaining task decomposition This is opposite to previously introduced GOAP which uses backwardchaining search in the statespace graph to find plan leading from goal state to the initial world state 32 The Figure 31 below shows an overview of a simple HTN planning system adapted for games  This HTN planning system is divided into the following components  HTN Domain  The essential part of HTN planning system is the HTN domain which contains all tasks available for solving the particular problem There are two main types of tasks  • Primitive Tasks which are the basic building blocks of the plan They contain an operator which defines the actual lowlevel task for the game condition which uses the world state properties to evaluate whether the task can be executed and effects which alter the planner world state  • Compound Tasks which contain multiple methods of executing a particular task Each of these methods have set of preconditions that dictate which of the methods if any gets chosen to be decomposed into the plan based of the current world state  The tasks available in the domain form a hierarchy hence giving the name for this planning approach  World State and Sensors  Like in the GOAP approach the planner uses an internal world state to simulate effects of tasks in the game using the resulting state in primitive task conditions and compound task preconditions to control the planning process Sensors work as adapters providing the simulated world state from actual game state  Planner  The planner does the actual planning work which starts from the root task in the HTN domain which gets inserted into the list of tasks to process in beginning of this process after which the iterative planning process is started On each iteration the first item in the list of tasks to progress is dequeued and processed If the item in list is a primitive task its condition gets run and if satisfied the task gets appended to the final plan If the task is a compound task the preconditions of the methods get run to select the appropriate method to decompose This decomposition enqueues the tasks in the method in front of the list of tasks to progress The iterations continue until the list of tasks to progress is empty An example planning case is shown below in Figure 32  Figure 32 Illustration of a simplified HTN planning example  After the planning process is completed the planner has the final plan which can be passed to the plan runner Depending on the structure of the HTN domain it is possible that the planning may in some cases fail to provide any valid plan at all  Plan Runner  After the planner has created a plan the plan runner starts executing the plan during gameplay keeping track of currently active task in the plan and checking the task conditions during this process If the world state gets changed unexpectedly during plan execution ie when the state does not match the conditions of task executed next in the plan the HTN is forced to do a replan to run the planning process again  363 Composite Tasks  One approach to handling goaloriented behavior is the use of the Composite Task architecture Originally implemented in 1995 for a realtime strategy game it has since found use in CSXII Tactical Combat Simulator used by the US Army 33 The Figure 33 below shows the basic structure of Composite Tasks  Figure 33 Structure of Composite Tasks  The main concept in Composite Task model is the ability to split a highlevel main goal into smaller subgoals creating a hierarchy of tasks This flexibility allows expressing even very complex goals and how to satisfy them using combination of lowlevel actions  The Composite Tasks consist of the following components  • Composite Tasks which can contain either other Composite Tasks or Simple Tasks  • Simple Tasks which contain one or more Actions  • Actions which are individual atomic operations that the AI can perform  The execution of tasks starts from the root task which evaluates its child components in priority usually lefttoright order until the entire hierarchy has been walked through The benefits of Composite Tasks include design simplicity datadriven content and generalized evaluation process 33  364 MultiUnit Planning with Hierarchical PlanSpaces  The traditional planning methods are well suited for planning actions for a single actor when the number of possible actions stays in reasonable amount However when planning actions for multiple units at once for example for military incursions the statespace searching suffers from combinatory explosion This means that the number of possible combinations of actions grows exponentially exceeding the available processing power and thus becoming unusable To solve this problem the planning can be done in planspace instead of statespace 34 The Figure 34 below shows abstract illustration of the difference between statespace and planspace planning  Figure 34 Comparison of statespace and planspace planning 34  With this approach the planning is started from highlevel task which is further refined into lowerlevel tasks and eventually individual unit actions 34  The planner  Instead of keeping track of possible states the planner uses list of possible plans and scores them based on their expected cost using the same A* algorithm like in other graphbased planners On each iteration planner dequeues the most promising plan ie the one with lowest estimated cost select the appropriate planner methods and their alternative approaches to get a list of new possible plans to branch off from this plan Each of these new plans get refined by the planner method after which their estimated cost is calculated by the task cost estimation function and they are queued into the list of possible open plans This iterative process runs until either a planner succeeds by finding a complete plan in the queue or fails by running out of plans to refine 34  The tasks  The plan is composed of a hierarchy of tasks that can either be compound tasks which can be further refined to other tasks or primitive tasks which represent individual unit actions The planning domain is defined by list of possible tasks which belong to specific scope in the domain based on their position in the task hierarchy Some possible tasks are listed below in Table 9  Table 9 Some possible tasks for the planspace planning adapted from 34  Scope 	  Task examples  Mission  Highlevel mission task  Objective  Capture Colony Defend Colony  Group  Form Up Eliminate Colony Defenses Attack Invaders  Tactic  Bombing Run  Units  Attack Fleet  Individual Unit  Move Attack Wait Bomb Defend Deploy Troops  The individual tasks set of inputs and outputs which are used to link unit states between sequential tasks usually providing the output state of previous task as input of the next task When tasks are chained sequentially the preceding tasks are required to be completed before the next task in sequence can be activated This allows the planner methods to control which tasks can be executed in parallel and sequential order  Planner methods  The actual refining of tasks is done by planner methods which take the current plan and task to be refined as input and provide a refined plan for the planner The planner methods only apply to specific tasks and their complexity ranges from simple single task output to complex combination of tasks They work by creating a number of subtasks for the task being refined thus expanding the current plan to lower level The planner methods matching the tasks in Table 9 are listed below in Table 10  Table 10 Some possible planner methods adapted from 34  Scope  Planner method examples and responsibilities  Mission  Allocate units  Objective  Define activities assign units to groups  Group  Execute tasks as groups  Tactic  Synchronize tactical activity  Units  Arrange cooperation between units  Individual Unit  Define the actions  The planspace graph  As mentioned earlier the planner maintains a list of all possible complete and noncomplete plans as it searches through the planspace A part of this planspace graph is illustrated below in Figure 35  Figure 35 Illustration of the planspace graph adapted from 34  In the illustration each plan is shown as a branch in the planspace graph with their associated cost estimate The green plans are in the closedlist of plans that have been refined and red plans are in the queue of open plans The tasks inside each plan show how deep the particular plan has been refined green indicates tasks that has been refined white shows the task being refined now and red tasks are unrefined tasks  Use case Planning attack on enemy colony  The hierarchical planspace planning has various uses in a 4X strategy game and this example focuses on a simple case of attack on enemy colony The player has four fleets available to be allocated for this mission a battleship fleet a destroyer fleet a bomber fleet and group of troop transports The resulting plan that can be generated using the example tasks listed previously in Table 9 is shown below in Figure 36  Figure 36 An example plan for invasion of enemy colony  The planning starts by creating the capture colony task on objective layer which contains information about the target colony its defenses and the AI players available fleets for the mission After this the objective gets further refined into set of grouplevel tasks  Forming up the fleets eliminating defenders and invading the colony  The form up task can take advantage of influence map and tactical pathfinding to pick the best positions for the fleets and use this data to create the tasks for fleet movement to those positions It can also consider the vulnerability of certain unit types such as troop transports when it assigns these positions  The task for eliminating defenders can have various alternatives depending on the type and number of defenders in the colony presence of enemy fleet creates the need for attacking enemy fleet and existence of any planetary defenses requires using bombers to eliminate them when other fleets are protecting the bombing run These tasks are further refined down into actions for the appropriate fleet types available  The last task in capturing the colony is planetary invasion which in straightforward way creates the troop deployment actions for troop transports and assigns the other fleets to defend the transports  37 	Diplomatic Reasoning  A game featuring players with the ability to engage in diplomatic relationships with each other imposes a certain set of requirements for the AI  Forming the opinion of other players  The most important part of diplomatic interaction is the ability to evaluate opinions about other players how they have behaved in the past what they are expected to do what things the players agree and disagree about how the military scientific economic and social status are evaluated etc This includes the mechanism how the actions of players affect these opinions and how these opinions end up shaping diplomatic relationships into friendships alliances enmity or animosity A method for handling opinions is presented later in Chapter 371  The ability to estimate percussions of actions  When it comes to making diplomatic decisions the AI player needs to be able to understand effects of its actions With the opinion system in place the AI can use the expected opinion changes in goal tree search with combination of utility scoring to evaluate its actions and choose the one which yields the highest score With this approach the AI can utilize knowledge of army sizes the players opinions about each other and other factors such as personality weights in making educated guess for results of the actions  Illusions of a character with personality behind the AI player  If all AI players would make decisions based on the same goals and using the same scoring methods there would be little variation between the different types of players in the game rendering the AI behavior more predictable and boring and removing any differences in behavior among the opponents With certain preset weights given to each of the AI players their decisions can be influenced to be focused on unique goals giving each of them a more distinct personality This also allows a human player who wants to play the game with a certain strategy to seek alliances with AI players which have goals and priorities matching hisher own goals  Longterm goals and persistence  The AI needs to have logical goals and the ability to make longterm decisions to help forming alliances and other agreements with other players The combination of playerspecific weights and opinion system create a natural foundation for this process  371 Opinion Systems  The original approach to Opinion Systems as used by Adam Russell is focused on allowing individual NPC agents in game world to shape their opinion about other players based on their actions but the core mechanism he proposed can be adapted for controlling the opinions of virtual players in AI about other players 35 The Figure 37 below shows adaption of Russells Opinion System for a 4X strategy game diplomacy  Figure 37 Opinion System adapted for 4X strategy games  The main differences from his model are the replacement of NPC characters with virtual players and replacement of global opinion with visibility and weights used to select audience of the deeds and other minor adjustments to handle local opinion transformations  Opinion State  The central piece of data in the Opinion System is an opinion state which contains the opinion in either discrete or numeric format This could for example be one players trustworthiness opinion about another player ranging from 10 to 10 These opinion states can be either simple singletrack values or multidimensional opinions which are composed of more than one value affecting the opinion state 35  There are both positive and negative aspects of the multidimensional approach the positive features include orthogonality greater variety of effects better match to natural language and having more information The downsides however include being more brittle at design changes increased confusion difficulty in visualization and challenges in quantization 35 Table 11 lists a subset of the possible opinion values that can be used in 4X strategy game diplomacy with multidimensional approach involving four different opinion values  Table 11 Examples of some potential opinion values in diplomacy  Opinion Value  Meaning of 10  Meaning of 10  Scariness  Unthreatening  Terrifying  Trustworthiness  Deceitful  Honest  Sentiment  Loathed  Admirable  Aggression  Pacifist  Warmonger  This approach has the benefit of giving diplomacy more depth for example if a player has strong army and thus high scariness score but low sentiment score for past offenses another player might be unwilling to enter into a trade pact even if they would fear the opponents army 35  Actions and Deeds  The deeds originate from either direct or indirect actions made by the player Direct actions might include declaration of war or using spy to perform a sabotage mission while massing troops could be induced as indirect action measured using threat analysis and influence maps It is possible that one action causes multiple deeds such as declaring war during peace treaty would also raise the break treaty deed Some examples of possible deeds are listed in Table 12 below with their associated audiences and weights  Table 12 Some possible deeds and their weights  Deed  Affected Opinion  Audience  Weights  Target  Allies  All  Declare War  Aggression  Public  05  05  01  Break Treaty  Trustworthiness  Public  05  04  02  Demand Tribute  Sentiment  Public  02  02  02  Sabotage  Sentiment  Allies  02  01  Mass Troops  Scariness  Private  01  Trespass Territory  Scariness  Visibility  01  Mass Genocide  Sentiment  Public  05  05  05  There are four different audience types in this model  • Private Only target player receives the deed notification  • Allies Target player and its allies receive the deed notification  • Public All players receive the deed notification  • Visibility Players who have currently visibility of the affected map square get notified  The deed audience type is specific to each deed type and posting a deed requires either a target player or target location depending on the type  Deed Log  The deeds are posted into Deed Log through filtering and logging pipeline which is used to allow for example to temporarily suspend delivery of certain deeds and to track statistics about the deed posts The Deed Log not forwards the deeds to the subscribers of deed events but also keeps track of past deeds and has a list of persistent deeds for example trespassing enemy territory could be handled as a persistent deed which posts the deed notification every turn until it gets deactivated when the units leave enemy territory 35  Audience Selection  The original audience type and the target passed with deed are used to pick the destination of the deed and each of the recipients get notified  Local Transformation  Before the deed is used to affect a players opinion it goes through local transformation which is specific to each deed type In this modified 4X strategy game model this transformation process is used to apply in certain cases multiplier to the deed weight based on the notification recipients existing opinion about the target player For example lets assume that player A demands tribute from player B incurring sentiment penalty for player Bs opinion about player A If there exists player C which has negative opinion about player B then player C would use negative multiplier for local transformation about the deed weight leading to positive sentiment offset for player Cs opinion about player A 35  Deed Effects  When the deed notification eventually receives the player it has to have effect on the receiving players opinion This change in opinion is invoked as transient offset through various possible offset functions one of which is specific to each deed type Example of a transient offset function is shown below in Figure 38  Figure 38 Opinion transient offset function example by Adam Russell 35 p 544  The function has a runin time during which the deed offset increases until it reaches the peak offset and highest effect on the opinion After this it decreases during the runout time and when the transient offset function ends the final offset is left as the permanent change in opinion This allows for example a genocide deed to have strong permanent impact on opinion but a trespassing of territory yields only a temporary change which dissipates gradually 35  To prevent excess accumulation of transient offset effects on opinion by repeated deeds the effect frequencies can be regulated by adding a minimum time between repeated deed effects for a single player 35  38 	Customizing AI  Although it is possible to implement AI completely by hardcoding it within the game code there are motivations for making AI customizable which are twofold For the first the development of AI is collaboration between programmers and designers and to support this process the designers should be able to project their visions in the game with as little friction and delay as possible And for the second by providing flexible methods of modifying and creating additional content to the game user community and players can create custom mods and other expansions which can provide additional gameplay value for other players of the game 4 pp 99107 In this section some common approaches for providing AI customizability are considered from this projects point of view  Black Box and White Box Approaches  The AI system implementation philosophies can be generally characterized into two groups White Box and Black Box systems The White Box systems offer more flexibility are good for team of multiple people collaborating and they allow designers to work more independently Black box systems on other hand are good for singleperson implementation but force designers to have greater dependency on programmers for providing implementations for the blackboxed behaviors Neural Networks are one example of Black Box systems which take discrete input and provide output through trained processing in the hidden layer 4 pp 100107  Sometimes mixing the two is possible for example reusable components in White Box systems requiring less customization are usually better suited as black box components such as Pathfinding logic The choice of better approach often depends on the project needs for example implementing AI components as Black Box systems might be good for a team with many programmers but in a team with many designers a White Box System would be more preferable 4 pp 100107  381 DataDriven Design  The key idea in DataDriven Design is detaching the AI behavior and logic from the game code into a separate data model which can be independently modified by the designer without need for programmer intervention There are various ways of accomplishing this for example with FSM state masks custom parameter configurations for individual AI agents external definition of rules for RuleBased Systems and Scripting Languages 4 pp 109111  Custom Tools  One way to increase the designers power in AI development is through the development of custom tools for creation debugging and visualization of AI in the DataDriven Design model The benefits of this are the increased flexibility easier maintenance and balancing and increased usability through UIs tailored specifically for the designers needs Drawbacks however include the upkeep required from the tool programmers and extra care needed for version control handling 36  These tools can either be completely specific to the data required for a single usecase or they can be more general such as tools for visualizing and editing Decision Trees Behavior Trees or rules in RuleBased Systems The tools can either be external applications or they can be built into the game allowing adjusting of the AI data in realtime without need of exporting data and restarting the game 4 pp 104111 The drawbacks of scripting however include the need for script development tools for designers and the possible performance overhead due to script interpretation at runtime Both of these problems can be alleviated by using existing scripting tools which have already good tools for scripting and are already optimized to be embedded as part of game 37 Although there is no limit in which language can be used for scripting there are a few most commonly used options in game development Some game engines come with a builtin scripting system such as UnrealScript in the Unreal Engine and Torgue Script in the Torque Engine The use of opensource scripting engines such as Lua or Python is also very popular among game developers Visual Scripting in which the logic is visualized graphically to the designer instead of a textbased presentation is also one possibility and commercial libraries such as PlayMaker Visual Scripting for Unity3D exist to support this approach And if none of these options is suitable for a particular game developers can opt to create a completely custom scripting language and engine tailored for the particular needs of their use case 4 pp 112130 38  For this project the choice of customization was narrowed to a combination of casespecific custom data models and behavior scripting using an opensource scripting engine A custom script engine was out of the scope of this thesis so the choice of scripting language was further narrowed down to selection between Lua and Python with motivations explained in more detail below Both Lua and Python are scripting engines with a good reputation of being well suited for embedding into games due to their free license easy integration and stable language specifications which are backed by strong existing developer communities 4 pp 114130 As this project uses Unity3D and C for implementation of the prototype there are two popular frameworks which provide support for adding scripting languages on top of it IronPython for Python scripting 39 and Moonsharp for Lua scripting 40 In Lua the language itself has been designed to be flexible and extensible so although by default there is no support for ObjectOriented OO paradigm such as classes inheritance and encapsulation they can be added through the use of metatables The interpreter itself does not support multithreading so care has to be taken by either running the interpreter only in a single thread using mutual exclusion for access control or having multiple interpreters on separate threads The language syntax itself is very similar to Clike languages with shallow learning curve and the standard library is very small and easy to learn 17 pp 449450 Python has native support for OO programming and it excels when it comes to mixing the script language with native languages The language syntax depends heavily on indentation but is generally considered one of the easiest languages to read and learn There is a very large number of libraries available for Python but in runtime the language suffers from size and speed issues 17 pp 451452 Based on the above considerations Lua integration using MoonSharp was chosen for this project main reasons being the easy integration and lightweight runtime which were key for the nature and scope of the prototype As the majority of how the AI works is hidden from the player there is often temptation to cut corners short either to artificially increase the level of difficulty provided by the AI or perhaps just to save time in development Some games are notorious for having cheating AI and it may be a big spoiler for the gameplay experience and source of frustration if the player feels that his opponent is exploiting an advantage heshe cannot match 6 pp 34  Some examples of cheating might be the ability of AI to ignore visibility status of map thus having always full knowledge of the location and arrangement of all enemy units or adding a certain resource production multiplier to the computer players production which would be tied to the AI difficulty level chosen by the player 41 For this project the use of features giving the AI player unfair advantage over human player is prohibited and instead the difficulty variation is done through combination of regulating AI aggression on easier levels adding randomness to the decision scoring and otherwise tweaking the way AI makes choices based on the same information that is available to the player Unlike many other realtime game genres with strict performance requirements the 4X strategy games have the benefit of being more relaxed when giving the AI processing time thanks to their turnbased gameplay model However care needs to be taken to balance between how much computing time is given to the AI to not cause too big slowdown in the gameplay especially in lategame situations in larger game worlds when many AI agents owned by multiple AI players might be operating This section introduces certain techniques to optimize and balance the processing time given to the AI features A key feature in managing the time consumed by AI is division of the AI logic into manageable tasks and using some method to control the execution of them In a singlethreaded execution model there is a fixed maximum amount of time that can be spent for performing other tasks as rendering of the game usually happens on the same thread A generalpurpose execution management system can be used to not only run the AI code but to also control the time given to many other background tasks in the game such as asset downloading audio and physics processing 17 pp 693725 A basic scheduling approach is to assign tasks to be executed on certain frames using a simple algorithm such as execution frequency Figure 39 below shows how tasks A B and C might be executed when scheduled on relatively prime frequencies In a loadbalancing system the scheduler keeps track of time spent for each task ensuring that processing time on each tick does not exceed the maximum limit In addition the expected running time of tasks can be estimated which can be used to predict how long the task takes and thus help scheduling it for execution Scheduling Groups can also be used to divide tasks into groups which share a certain scheduling algorithm such as One way to control the time consumed by pathfinding and to prevent CPU spikes caused by excessively large pathfinding queries is to divide a single path query operation to multiple steps a technique known as timesliced pathfinding With this approach the pathfinder is given fixed amount of time to process during each tick and when the time is exhausted the state of pathfinder is saved in such way that the query can be resumed when the pathfinder is again given processing time One challenge with this approach is that if state of map affecting the result changes during the query the results may not be guaranteed to be valid How much this shortcoming affects the feasibility of the approach is however dependent on the use case of the pathfinding 43 In the simple manual scheduling scenario all tasks are run on a single thread in cooperative fashion This requires both the AI tasks to behave well in terms of how long they take to execute on a single frame and the scheduler has responsibility on how to divide the workflow during each tick Another approach for allocating CPU time not only for AI tasks but potentially to other components in the game engine too is the use of multithreading Traditionally this approach has had the downsides of requiring synchronization of data structures and performance penalty caused by context switching but in the past decade the advent of multicore CPUs even in the lowestend mobile devices has created a situation where the use of multithreading to leverage the new hardware features gives performance gains that outweigh the disadvantages 17 pp 693725  Although the performance of Central Processing Units CPUs has increased constantly during the history of computers the power of graphics processing units GPUs has exploded in the past decades to be a very viable option for performing largescale computation With the introduction of GeneralPurpose computation on the GPU GPGPU APIs such as OpenCL and DirectCompute this power has become easily accessible to developers including AI programmers This allows great chance to speed up the AI by offloading applicable parts of it to the GPGPU processing 44 As pathfinding is one of the core methods used in gridbased strategy games seeking to optimize it through GPU offloading is one of the most promising applications for this technology Recent research on has demonstrated that A*type pathfinding can be implemented with GPGPU with a much higher performance compared to traditional CPUbased search 45 The GPU offloading in Unity projects can be achieved with the use of Compute shaders which provides an API for running GPGPU programs within Unity applications The downside of Compute shaders is they require using the latest versions of graphics programming APIs and graphics hardware supporting them 46 There are also a couple of methods which could be useful in the development of AI for strategy games but they were considered not to fit in the scope of this project due to a combination of practical and schedule challenges They do however have potential to be explored in future research and are listed here to explain why they were rejected at this point Explaining away in which a known result fact is used to reason the probability of presence of source facts leading to it including characteristics of independence and conditional dependence The challenge in Bayesian networks is that for them to be useful the probabilities used for inferring states need to be acquired by either gathering the information during simulated gameplay or through training of the network This means that the problems they are used to solve need to be fairly uniform as alteration of gameplay and rules affecting the probabilities causes the network to produce invalid decisions One solution for this could be allowing the network to adapt to the users gameplay by training it during actual gameplay but this might make the AI show too much emergent behavior and thus lead to undesirable decisions which might not be expected by the AI programmer or programmer causing other parts of the AI to behave unexpectedly and appear broken to the player The concept of Neural Networks is one of the machine learning techniques which has gained a lot of popularity in the AI research in the recent years and there are even some commercial games which have utilized this approach The Neural Networks try to simulate the function of neurons in actual human brain with a simulated mathematical model In this model the Neural Network is divided into three layers Input layer Hidden layer and the Output layer with each of those layers containing sets of neurons The number on each layer dependent on the use case of the network The source data is fed into the input layer from which a feedforward process passes the information through the hidden layer all the way until it reaches the output layer During training the output data is evaluated and backpropagated to the weights in the hidden layer and repeated until the Neural Network outputs the expected values After this the trained network can either be used in the final AI implementation or the training process can be infinitely continued to allow the AI to be able to adapt to player behavior during the gameplay 6 pp 269315 The major challenge in the Neural Networks is like previously outlined in the evaluation of Bayesian Networks the need for training and high risk of unexpected emergent behavior and these features make especially debugging testing and balancing gameplay difficult 6 p 271 It should be noted that it is possible to leverage these techniques in games successfully but due to the time and effort required for these approaches compared to the potential gain they were not included in the scope of this thesis In this section the produced high level technical design is presented in addition to outlining the best practices for implementing it Due to the highly iterative process in game development this solution should be considered as a starting point for prototyping and most likely goes through various alterations and fine tuning as the development proceeds It should however provide an understanding of what kind of options are being considered as realistic goals for the project On high level the entire AI is encapsulated as the virtual player which interfaces with the game by taking the world state as input and providing set of actions as output Figure 41 below shows an abstract overview of this information flow To give a better understanding of this process below is a brief explanation of the purpose of each step in this illustration This abstraction of the AIs understanding of the game state and its operations allows better modularity and helps preventing cheating by allowing the AI to use only the same information and actions which a human player would be able to utilize in the game Furthermore with this architecture it is possible to use certain parts of AI features for human players to allow worker automation colony governors and other features that might be automated to reduce the dreaded micromanagement which might be present in largescale games When implementing the AI a good approach for controlling the complexity is the division of the AI to dedicated components The central part of the virtual player is the multitier AI model which follows the principles set earlier in Chapter 321 The responsibilities of each tier level are capsulated to managers each of which has a dedicated role in the decisionmaking hierarchy These managers and individual agents in lower tiers of the model utilize various AI tools which are available for them throughout the tier levels as a separate toolbox This division to components is illustrated below in Figure 42 The HTN planner in this component utilizes a highly abstract highlevel strategic decisionmaking domain which composes to various abstract tasks which are assigned to the other managers The Diplomacy and Expansion Managers also provide data for the planner to help in the decision making The research goal selection uses in straightforward way the active goal given by the Strategy Manager to choose which field of research should be given the research priority For tracking the state of enemy research the Research Manager uses functionality of RuleBased Systems inference engine allowing it to use knowledge of enemy fleet types colony improvements and existing knowledge of research The Diplomacy Manager is responsible for handling any diplomatic actions requested by the Strategy Manager and it also provides necessary data for the highlevel HTN planner such as the opponent reputation based on opinion values The overview of the Diplomacy Manager is shown below in Figure 45 One central component of the Colony Manager is the goal pool it contains all possible goals and at any given time each colony is assigned one of these goals which it needs to fulfill As the Colony Manager gets the highlevel goals from the Strategy Manager it makes sure that the currently assigned goals have the highest utility for reaching this highlevel goal As the Colony Manager is also responsible of tracking global resource production and allocation this allows it to not only directly respond to specific production goals but also to balance the resources between individual colonies to avoid potential production or growth bottlenecks The mission planning relies strongly on the Tactical Analysis Server to find both weak and strong areas in the friendly or enemy territories which allows it to create missions not only for incursions to enemy space but also for reinforcing own defenses This planning process also includes allocation of available military units to specific missions When a potential mission is formulated the allocated units and the mission goal is forwarded to the military coordinator which handles planning of the actual mission and decomposition to individual unit actions The Expansion Manager is responsible for handling both the expansion of players empire with colony ships and using worker fleets to improve the existing structures owned by it The overview of the Expansion Manager is shown below in Figure 48 The goals given to colony ships and worker units are directly assigned to the individual units which use their own decisionmaking model to fulfill them The core of Military Coordinator is the Hierarchical PlanSpace HPS Planner which uses the planner methods and tasks provided by the coordinators data model Functionality of this HPS planning process was detailed earlier in Chapter 364 If the planning succeeds the resulting plan containing tasks for each individual fleet is executed by the coordinator which provides goals for the individual units during the plan execution until mission is either finished or failed The decisions made at this level are mapped directly to the commands given to fleets during the AI players unit movement turn during the gameplay The worker automation can be also used by the human player to toggle partial Expansion Manager and AI fleet behavior on and off when needed Each colony owned by the AI player is managed by the Individual Colonies AI module which takes the goals given by the Colony Manager and translates them into actions that can be performed by the AI for that particular colony Overview of the Individual Colonies module is shown below in Figure 51 The AI toolbox contains various generic AI algorithms and tools introduced earlier in the chapter 3 which are utilized by the various other modules throughout the multitier levels of the AI decisionmaking model The most important principle of the toolbox is that each of the technologies used by the AI is encapsulated into a reusable component which allows integration with other AI code through predefined interfaces This not only allows keeping the componentspecific code separate from other AI logic but also allows better possibility for scripting language integration explained later in Chapter 44 Unlike in realtime strategy games which have strict limits on the CPU budget for AI execution time per frame the nature of turnbased games allows certain level of flexibility in the implementation of AI processing model Although not optimal in the proposed prototype all AI actions can be isolated to the AI players turn thus reducing the pressure to interleaving AI processing with other players turns However even when executing all AI code of a particular player in one batch certain steps must be taken to avoid undesirable behavior of the game such as stalling the game interface during the AI processing In Unity the default method of implementing asynchronous behavior is through the use of “Coroutines which resemble cooperative multitasking Each Coroutine gets executed on the Unitys main thread and they are responsible for handing over the execution voluntarily by yielding Although this approach is easy to implement it has the drawback of needing careful planning of when to yield the execution to avoid either clogging too much CPU time on a single Coroutine pass or wasting CPU time by yielding excessively often Other challenge of this approach is the care needed when designing the AI model to allow Coroutine yielding throughout different components Another way of implementing the asynchronous execution is through the use of C threads Although the use of threading allows independent execution from the main thread and possibility of benefiting from multicore CPUs they have the drawback of needing synchronization between any shared data structures and as Unitys API is not threadsafe would be limited to executing only the parts of C code which are not using Unity features In this prototype the Coroutine method was chosen to be used When balancing the AI behavior and features it would be unnecessarily time consuming to have QC play multiple games against an AI after each modification done to it by the programmers or designers To alleviate this problem automated testing can speed up a lot of this process by allowing simulation of entire games played by variable number of AI players A set of parameters can be configured for this process Also different variations of AI using their own versions of data models could be potentially matched against each other to directly evaluate the effect of modifications of the model data on AI performance with minimal iteration time Each automated game produces a transcript of all actions performed during the game and the resulting outcome of the game This recorded journal of the game can be replayed in either highor lowlevel detail which allows designers to pinpoint which actions by the AI were undesired and give them ideas which features would need to be improved In the proposed AI model the individual manager modules in the multitier have the potential to benefit from being abstracted from the other AI code through use of LUA scripting engine The AI toolbox which have very little need if any for specialization can be easily utilized by scripts as standalone components exposed to the scripting environment removing the need to write any lowlevel AI code in the scripting language As explained in Chapter 382 the MoonSharp library was chosen to be used in the prototype but due to schedule challenges the experimentation of scripting integration was not finalized in time for this thesis and remains on theoretical level Following the principles of the Data Driven Design introduced in Chapter 381 many parts of the AI model are isolated into data models which can be customized either by custom tools or direct manipulation of the data in question These datamodels are listed below in Table 13 All of the above data models can be stored in either JSON or XML presentation which can be further exposed to the designers with an UI through custom tools where needed This allows building a foundation for the AI behavior before devoting time to building the custom tools It should be noted that some data such as the rules used for technology inference can be derived from other game data This chapter explains what was developed during the project and evaluates how well the project output matched the initial goals The biggest challenge for the implementation of the complete AI player was the lack of a 4X game engine and creating one singlehandedly for the purpose of this thesis would have exceeded the feasible time limits available to finish the project and risked generating excessive workload beyond the scope of the project The outcome of this thesis can be categorized in two distinct parts The most important outcome of the project was the highlevel design of the AI player which should be used as a starting point and guideline for the implementation of the actual AI code in the actual game as presented earlier in Chapter 4 as the proposed solution This technical design contributed to the selection of parts suitable for independent prototyping which are detailed later in Chapter 52  It should be noted that the greatest value of the highlevel design comes from this profound research of AI field done for the theory part of this thesis as the time spent on this process will be saved in any possible future implementation of actual games  As the evaluation of this technical design was limited to general assessment of its usefulness in the scope of this thesis due to lack of a complete game prototype the results of the prototyping were used to reinforce and assess the feasibility of this highlevel design Although the technical design represents the virtual player feature of the strategy game all necessary parts of the design can be applied for playerassisting “governor features by considering the human player as a complete AI player with certain decisionmaking modules disabled This allows the full influence mapping threat evaluation and other features to provide necessary data for the assisted features such as production planning advisor recommendations scientific research goal suggestion etc As per the original game design the map prototype was based on the hexagonal grid This grid type provides better movement and distance handling by eliminating the different between diagonal and axial distances but in contrast requires more complicated handling of hexagonal coordinates A hex coordinate utility was developed during the project to help managing this challenge This first version provided a good foundation for the map and pathfinder code although some bugs were found and fixed in the later version Building upon the map prototype used for the pathfinding testing previously the functionality was extended to include support for multilevel spatial database to map player influence and the combined effects for visualization on the map Additionally the pathfinding engine was adapted to leverage the spatial database information to allow experimenting with tactical pathfinding Appendix 2 shows screenshots of the various spatial database layers being visualized This version of prototype was equipped with a primitive representation of military units which appear as circles on the map Each of the units has the following properties The aforementioned spatial database layers were used just for testing purposes and actual game could use any number and any combination of layers in whichever way the game designers might feel useful Tactical pathfinding was created as a simple extension to the original pathfinder using the influence data in the spatial database to show benefits of this approach The screenshots in Appendix 3 show three different pathfinding modes with equal start and goal locations  Simple Each node is considered to have fixed cost of 1 which makes the pathfinder attempt finding the route with lowest number of map cells from start to the goal node The screenshot also shows that this approach makes the pathfinder visit very low number of map cells thanks to the A* algorithm which prioritizes cells closer to the target Terrain movement cost The pathfinder now considers movement cost of terrain when doing the path query showing a different resulting route Also a much higher number of map nodes was visited during this query Enemy threat avoidance Again the same start and goal nodes are queried but this time the scoring of path is done using data from the enemy influence layer although boosted 10 times to allow including terrain movement cost as a secondary scoring method which creates a completely different path which nicely skirts around the areas around enemy influence The actual formulas for combining influence data from spatial database for map cells are highly customizable and may need a lot finetuning depending on the use case for the game For example it may need careful consideration on how much effort the AI should put into avoiding enemy if resulting path leads to excessively long detour However for the prototype purpose the combinations used seem to work quite well A simple inference engine was created to demonstrate the basic concepts of rulebased system functionality This implementation uses heavily Unity features such as storing rulesets in ScriptableObjects and editor extensibility to allow running the inference engine in either edit or runtime mode Screenshots of the inference engine test are shown in Appendix 4 which were taken from the Unity Editor mode The first screenshot shows the ruleset asset and the associated test rules used in the prototype The other screenshots show state of the inference engine after each user action starting from resetting the inference engine followed by singlestepping it one step at a time until the inference process was finished The debug output of the Unity console is also included in the screenshots which shows the effects of inference process during each step Although the basic inference process works and demonstrates the functionality of associated algorithms a more generic implementation supporting dynamic rule generation would probably be most useful for the actual productionquality game especially if used for technology inference like proposed in the highlevel design The motivation for this thesis originated from the interest in game artificial intelligence and the 4X strategy gaming genre This combined with the ongoing trend of constantly increasing mainstream popularity of gaming gave further incentive for this research as a way to create a design which could benefit future development projects requiring this type of knowledge As possibilities for exploring this field of technology were not available at the workplace for the timeframe of this thesis the project was implemented as a personal undertaking focusing strongly on theoretical research The main goal of this thesis was to research various AI technologies in order to create a plan for integrating AI for a turnbased 4X strategy game and a secondary goal was to do prototype implementation of it The theoretical part of the study was finished but the implementation was limited to prototyping only certain areas of the researched technologies The technology research phase proved highly informative and a lot of knowledge was gained for creating a feasible highlevel design for the AI implementation During the prototyping phase the technologies that were experimented with also showed a lot of their potential also in practical setting thus reinforcing the credibility of the parts of design which they were associated with Although there were certain shortcomings in the practical output of the thesis the theoretical knowledge gathered has high value for not only the author personally as game developer but hopefully also for other people facing similar challenges who might be reading this One major improvement in future would be the full implementation of AI in the actual game As the AI design is theoretical and highlevel there are most likely many challenges in the practical implementation which will be reflected back on the design of the AI model This includes for example considering the actual gameplay design of the game and the processing performance of the target platform Also the further work should not be limited by the research done in this thesis but should also be open to exploring possibilities of both future technologies and reevaluating the existing technologies such as Bayesian and Neural Networks which were ruled out of the scope for this thesis The purpose of this thesis was to design and develop a piece of software to improve Google Drive shared folders in organizational use The main reason why the topic was chosen is that organizations that use Google Drive as a centralized data storage are especially vulnerable to a potentially vast amount of work in case of an accidental deletion of items I immediately started to figure out a topic for my upcoming Masters thesis As Google Apps was one of the main tools we used in my workplace I figured it could provide an interesting topic in some form As all software Google Apps also proved to include design flaws partially addressed by at least one third party vendor Because the vendor did not include a solution which would have been feasible to our company and most of our customers I decided to develop a software solution which would be feasible for reselling and provide a good topic for my thesis The design and development was more challenging than anticipated but so was the actual report in a thesis form as most of the development was done before the written report All figures tables and appendices were made completely by the author All elements in this report were made with Google Docs Google Sheets and Google Drawing excluding screenshots Due to limitations in Google Docs the final document was rebuilt with Microsoft Word Last but not least Id like to give a loving thank you for my wife Tiina for taking care of our daughter while allowing me to work on this project besides my regular day job This thesis focuses on the design and development of a piece of software which improves the functionality of Google Drive shared folders for organizational use The goal of the project was to design and develop a software solution which monitors and sets file ownerships within a shared folder in a way that accidentally deleted files are easy to restore The software is to be sold as a service The finished product was named HSWDrive The study discusses the use of shared folders in Google Drive Google Drive is an internetbased computer file storage service also known as cloud storage developed by Google Inc It is part of a wider suite of webbased applications called Google Apps which is reasonably priced for companies Google Apps for Work and available free for consumers with a free Gmail account and education facilities Google Apps for Education The current service model was launched on 2012 although the same base has existed since 2010 when it was still known as Google Docs Google Drive is currently used by roughly 240 million active users 1 The basic idea of a cloud storage is that a user has hisher own password protected account which the user uses to add modify or delete hisher personal files With the use of cloud storage the enduser does not have to carry any removable mass media CD  pendrive  hard drive with himher as long as heshe has access to the internet Because internet connectivity has spread so wide in todays world it is usually unnecessary to carry the files with you physically Due to the fact that a user can share a file or folder to other users or user groups in Google Drive it is also possible to use Google Drive as a centralized enterprise data storage To establish a centralized data storage a user must create a folder and share it to all other users which require access to the same files When a user adds files to a shared folder the ownership of the files remain to the user who added the files Otherwise any user could fill up the disk space of another user When a file is deleted only the file owner can restore it from the trash bin Because of this logic a problem arises when someone accidentally deletes a file from the shared folder The original owner has to be found and the owner must restore the file back to the shared folder This could potentially cause a lot of manual work in case a whole file structure has been deleted This challenge can be bypassed by creating a software solution which automatically monitors and logs all the changes done within a Google Drive shared folder By changing the ownership of each file to a single dedicated user account the files are easy to restore from one user interface in case of an accidental deletion Although Google Drive provides a platform to share files within a company there is one major problem in the concept when a file is deleted from a shared folder by someone who is not the original owner of the file it disappears from the shared folder and is left orphaned An orphaned file is a file which has an owner but does not have a location so it can only be found by the owner of the file by conducting a search It cannot be found in any particular folder not even in the trash In case of an accidental deletion of large folder structures this causes a significant problem in restoring the deleted files and folders This has happened to the case company and its customers thus a solution to avoid it in the future is required 2 As to this date the case company has found only one widely recognised commercial service to address this problem AODocs Although AODocs is much more than just a system to prevent accidental deletions the case companies required functionality of AODocs is to convert any Google Drive folder to a controllable file server with advanced options The problem with AODocs is that since they released a new version in the last quarter of 2014 it has not been possible for users to delete or add files to a shared folder thru the Google Drive interface thus forcing the users to use the AODocs interface or a separate browser plugin for adding and deleting files The case company used AODocs for a trial period of 30 days at the end of 2014 and was almost ready to purchase it but after noticing some properties which would have required the endusers to change working habits a questionnaire was conducted within the company to decide whether AODocs was suitable or not for this environment The questionnaire included the following questions Based on the results of the survey 8 users out of 9 answered yes to the first three questions All users preferred Google Chrome as the initial application to access Google Drive but some users also used the Google Drive app on mobile devices and Google Drive synchronization client to have a local copy of the files At this point it was clear that the employees did not like the idea that each and every user in the organization should either install a browser plugin or learn a new system to add modify or delete files in a shared Google Drive folder According to the free comments section of the questionnaire AODocs was taking too much control over the user experience which led to the conclusion that AODocs was not a feasible solution for the case company The software must be easy to implement on other folders and organisations thus making it easy to sell as a service Because this type of functionality requires continuous monitoring of the folders and files the finished software must run on a dedicated server The aim of this project was to design and develop a software solution which monitors and sets file ownerships in a way that accidentally deleted files are easy to restore After the initial requirements development server and Google Developer credentials were obtained the software was designed and developed using the agile software development method and objectoriented PHP language Most of the programming was done with the combination of TextMate and Transmit TextMate was used to write the code and Transmit was used to upload changes to the server Programming was done in various locations mostly in the developers home or workplace due to the fact that software design The Google Developer Console credentials were used to gain access to the Google Drive API Google Drive API was used to send commands and perform queries to Google Drive The actual software was be developed using the PHP language PHP was chosen due to the developers vast experience in it and because Google provides PHP client libraries for the Google Drive API This project was limited to Google Apps for Work and Google Apps for Education environments so it does not work with free Gmail accounts due to limitations of Google Drive API Unlike in AODocs a separate interface will not be created for handling files All file and folder handling and restoration of deleted files can be done directly in Google Drive although a separate file restoration interface was made for easier restoration of large file structures All files saved in the shared folder must be owned by someone within the company of the shared folder Files which are owned by external collaborators were not included in the scope of the software although this might be possible in a future release Section 2 explains more about the technical aspects of Google Drive Google Drive API and the programming language of the developed software Section 3 describes the requirements for the developed software to function Section 4 describes the program logic and includes illustrating tables for the logic and database structure Section 5 describes the evaluation for the finished software Section 6 lists all encountered problems while developing the software Section 7 expresses the current and future possibilities for licensing and distribution of the software and Section 8 includes measurement results and analysis of the finished product and discusses of the possible future insights To better understand the technical aspects of Google Drive and the development of the software solution the used technologies and software logic are described more thoroughly in the following sections Google Drive was introduced in April 2012 even though the core service did exist before this as a file storage system for the Googles webbased office suite The early webbased office suite was called Google Docs and it included functionality for word processing spreadsheets and presentations After the launch of Google Drive these were separated into three individual apps Google Docs Google Sheets and Google Slides respectively Google Drive is used to store and share files and folders in the cloud It is directly integrated to Gmail Googles email system which enables file sharing directly via the email interface The use of Google Drive for file sharing addresses the widely known problem of large email attachments filling mailboxes By only sending a link to the file the recipients mailbox does not get filled by large attachments and the recipient is able to download the attachments on demand The downside of this method is that the file can only be downloaded as long as the sender does not delete the original file By having files in a cloud storage there is no need to carry any physical storage media All changes made to the files are usually viewable by all privileged users and devices within a few seconds delay either from the web interface or in a locally synchronized folder when using a separate synchronization client software A noticeable difference between Google Drive and many other Cloud Storage services is that Google Drive uses tags instead of folders Although files appear to be inside folders they are actually just tagged with the folder names By using tags it is possible to have a file located in various locations at the same time This method is very similar to using hard links in a Unixbased operating system such as Linux The only practical difference between a hard link and tag is that hard links are done in the operating systems file system level while tags are done in a separate database By utilizing tags instead of traditional folders it is possible to save a considerable amount of disk space because there is only one copy of any given file even if it is located in various locations of various users 3 Although having a separate backup system is important it is not that crucial in such as vast system as Google Drive because of the various safety and security implementations All data is replicated between various data centers around the globe and each data center is equipped with emergency power generators which enable data integrity in case of a natural catastrophe The data centers operate on custom built servers which are exclusively built by Google and run a stripped version of the Linux operating system 4 All overwritten files are automatically added to a separate revision history which allows file restoration in case of an accidental change in a file Deleted files are added to the trash bin before final deletion which allows users to restore them in case of an accidental deletion Some files are even recoverable by Google Support and the organization administrator for a few days after emptying the trash bin For extended data integrity it is possible to take data backups from Google Drive to a local computer or a third party cloud storage such as Backupify 5 6 Google Drive was originally intended for web browser use due to its integration with Docs Sheets and Slides As the number of users increased Google released the synchronization client for Windows and Mac OS X The idea of the synchronization client is to have a local copy of all or selected files within Google Drive This way users are able to access files without opening the web browser According to the experience of the case company many users face the problem of not understanding that deleting or moving items from a locally synchronized folder also deletes the items from Google Drive This functionality is one of the main drivers for this project As of today there is no official version of the synchronization client for Linux All files and folders in Google Drive are always owned by someone Even though teams and companies use Google Drive to collaborate within shared folders the files within shared folders are by default always owned by the person who creates or uploads the file Each file consumes disk space only from the owner regardless of the file location The file ownership can be transferred to another user within the same organization by the current owner or programmatically by using Google Drive REST API After a file has been transferred to a new user it will free the consumed disk space from the previous user and consume it from the new userHaving files with different owners in a shared folder can create a problem when a user account is deleted Because the files are owned by the user deleting a user will also delete the files owned by the user within a shared folder Another possible problem arises when a file is deleted from such a shared folder Because the file is owned by some user it can only be found from the owners recycle bin which makes it difficult to restore in a multiuser environment Each Google Drive user has a “My Drive folder which is also called the root or root folder Google Drive allows files and folders to be shared and to exist in multiple locations at the same time These locations are called parents in the Google Drive REST API Having files and folders in multiple locations simultaneously is a handy feature but in theory allows a recursive loop A recursive loop occurs when a parent folder is added into its child folder The problem of a recursive loop is that a folder structure is infinite because the child folder includes its parent folder In case a folder structure with a recursive loop would be synchronized as a local copy to the end users computer it would cause an infinite number of items which would rapidly fill the computers hard drive Besides filling the end users hard drive it would also make deleting the folder structure troublesome because most file systems start the deletion process by calculating the number of items to be deleted If the number of items is infinite the calculation process also takes an infinite amount of time to advance thus never completes or causes an error message A graphical representation of a recursive loop can be seen in Figure 1 Each user can also add shared files and folders to the users own root folder for easier access The root folder of each Google Drive is called “My Drive Even though a shared folder is owned by someone else each collaborator can add it into any folder under My Drive as long as the user has write access and the location does not cause a recursive loop In case the user does not have write access or tries to create a recursive loop the operation fails and an error message is shown to the user When adding files to a shared folder the file access permissions are inherited from the parent Thus when adding items to a shared folder it is not required to explicitly share them to other users unless the targeted user does not have access to the shared folder Google Drive automatically increments access permissions to any object within a shared folder to match the access permissions of the parent folder Manually added permissions are not removed in this process  In case a user with write access deletes files or folders from a shared location the items will only be removed from the current parent location but not from the owners My Drive which is considered as the root folder If the shared location was the only location for the deleted item it becomes orphaned An orphaned file or folder does not exist in any folder because it does not have any parent locations An orphaned item is not considered to be deleted or trashed it just no longer exists in any location The only way to find an orphaned file or folder is to conduct a search in the Google Drives web interface Finding an orphaned file can be challenging but it becomes even more burdening in case the files owner is unknown The visibility of files and folders in My Drive is illustrated in Figure 2 2 As illustrated in Figure 2 files which reside in an orphaned folder such as File A in Folder A are also potentially hard to find due to the parent folder being orphaned Even though File B1 is situated in the orphaned Folder A it can still be easily found due to its existence in Folder B which is situated in My Drive Although having files directly in the root of My Drive is not considered as best practice for file organization it is still directly visible to the user When someone deletes a singleparented item from a shared folder one of the two situations occurs If the deleting user is the owner of the item the folder will be added to the owners trash from where it is no longer accessible to collaborators in any form before the user restores the item If the deleting user is not the owner of the item the item is deleted from the current location but is still accessible to the owner and other users who are explicitly granted access via manually sharing the item Even though access is still possible it is still as an orphaned file and needs to located by searching via the web interface of Google Drive Google Drive REST API is an essential part of Google Drive SDK which includes all the required commands protocols and tools to interact with Google Drive By having an API developers around the world are able to extend the functionalities of Google Drive The API officially supports the following languages Go Java JavaScript NET Nodejs PHP Python  Ruby Besides these languages it is also possible to use the API with any other programming language as long as the language supports REST calls over HTTPS The downside of using an unsupported language is that there is very little documentation to support development for such languages 7  Google provides up to one billion 1 000 000 000 free Google Drive API requests per project on a daily basis and more can be requested The number of API requests is limited to 50 requests per second per user Most operations are done as the file storage user of an organization Depending on the requirements additional API requests might be prone to financial expenses Google Drive API consists of 13 different resources to access Google Drive functionalities A resource can be thought as an object in terms of programming The available resources are Files About Changes Children Parents Permissions Revisions Apps Comments Replies Properties Channels and Realtime The Files resource includes methods to list view follow create modify and delete files and folders When using the listing method this resource cannot be used directly to list files within a folder The listing method works in a very similar way than using the search bar in the web based Google Drive interface with the addition of having more options such as the ability to limit a search only to Google Photos or reducing the number of returned objects The About resource is very simple in terms of methods because it only includes one method to get information about a user such as the used amount of data in Gmail Google Drive and Google Photos The About resource also includes a vast amount of Google Drive related information such as sharing policies some Google Drive API settings and the file ID for the users root folder also known as My Drive The Changes resource includes methods to get list and watch changes done to files and folders Every time a change has been done in the users Google Drive a new change item is created which includes basic information about the file which has been changed The use of a change log for monitoring file modifications is considerably faster than recursively scanning for each file in a folder structure because only changed files are listed The Children resource includes methods to list remove and add children objects to and from a folder To use the Children resource a file ID of a parent folder must be given before being able to use this resource Also folders are considered as files in the Google Drive API thus folders also have a file ID This is the most used resource in this project as it is required to list files within folders while checking for user ownerships This resource cannot be used to delete or trash files or folders When deleting a child object it only deletes the object from the selected parent To actually delete a file or folder the Files resource must be used The Parents resource provides methods to list add and remove parents locations of any item Like the Children resource this resource cannot be used to actually delete files or folders It is only used to add or remove a parent from any child object This resource is utilized as part of checking if a file is situated within a shared folder The Permissions resource includes methods to list modify and remove permissions from items Each permission setting for every item has its own permission ID The permission resource defines the permission role of an item In case of modifying existing permissions for a file or folder the current permission ID for the given item and user combination must be obtained by listing the permission objects for a given file In case of changing the ownership of an item the user needs to have a permission object assigned to the given item If the user does not have any permission objects assigned to a given item one needs to be created before the user can be converted as the owner of the given file This resource is one of the most used resource of this project as the primary focus is to centralize the ownership of files to one storage user The Revisions resource includes methods to list modify and delete revisions of a file Revisions are stored automatically for each file by Google Drive when a file has been updated Although the possibility to restore file contents may be an option in the future this project does not currently focus in restoring file contents thus this resource is not utilized at all The Apps resource includes methods to list and view apps which have been enabled in the users Google Drive As this project is not focused in listing for existing external Google Apps it is not used at all The Comments resource includes methods to list modify add and delete comments to file created in Google Docs Google Sheets or Google Slides Comments are not currently supported in other types of files As this project does not include functionalities to view or alter comments in separate files it is not used at all The Replies resource includes methods to list modify add and delete replies to comments in file created with Google Docs Google Sheets or Google Slides As this project does not include functionalities to view or alter comments in separate files it is not used at all The Properties resource includes methods to list create modify and delete custom properties for drives stored in Google Drive The properties can be public to all apps or private to just one app As this project does not focus in file properties of separate files it is not used at all The Channels resource is a resource which is created when a File resource is set to be followed with the Watch method The Channels resource can be stopped with the Stop method thus ending the watching or following of a file Due to requirement of following all items within a given structure this resource does not have any practical use in the current project The Realtime resource includes methods to get and update realtime API models which are associated to a selected file It can be used to monitor changes to a file in realtime thus allowing external applications to reflect changes The Google Drive REST API uses mainly OAuth 20 for authorization of the software against Googles servers OAuth 20 is the successor of Oauth 10 and an open standard for allowing secure delegation of data resources The main idea of Oauth is to allow delegated access to only some part of resources not to everything 8 It is used for access authorization by many notable cloud service providers such as Google Microsoft Amazon and Dropbox 9 To establish a successful connection between the software and Google Apps instance a client ID and client secret must be generated within the Google Development console The client ID must be granted appropriate permissions for Google Drive on each Google Apps instance which will be hosting a shared folder The permissions are granted from the Google Apps administration console by an administrator of the Google Apps instance Once the client ID has been created and permissions are granted the software is able to authenticate and gain authorization to Googles servers by using Oauth 20 The actual authentication and authorization and communication between the software and Googles servers is usually done in the following steps First the software requests an access token from Google The access token is received upon a successful authentication A successful authentication is achieved by using user consent In this example user consent is achieved with the client ID and client secret which were generated in the Google Development Console Assuming the authentication was successful the software receives a session object and an access token which is valid for a limited time The session object is used as a proof of authentication by the software when sending queries and commands to Googles servers The access token is used to request a new token before the original token expires 10 PHP is a widely used serverside scripting language which was officially introduced in 1995 by Mr Rasmus Lerdorf PHP started as a simple procedural language but it has been further developed to an advanced objectoriented language 11 The latest version branch of PHP is 56 12  Even though PHP is an objectoriented language it is still not categorised as a programming language due to the fact that PHP applications are not usually compiled to bytecode PHP applications are typically processed directly on a server by a PHP interpreter  MySQL is a relational database management system RDBMS owned by Oracle Corporation MySQL was initially released in 1995 by MySQL AB MySQL is the worlds second most used relational database management system and the most used opensource relational database management system The latest stable version as of September 30th 2015 is 5627 13  MySQL is widely used in website development but it can also be used for other applications such as serverside applications and even as a local database for individual applications Since Oracle bought MySQL in 2008 a new opensource fork of MySQL was created known as MariaDB to address concerns about keeping MySQL free and under the GNUGPL license The intent of MariaDB is to maintain a high compatibility with MySQL for easier transition in case the development of a free MySQL is seized MariaDBs current lead developer is Michael Widenius who was one of the founders of MySQL AB Based on the findings of the inquiry done in the case company at the end of 2014 most users preferred Google Chrome as the application to access the web interface of Google Drive Some users also used mobile apps and the Google Drive synchronization client to have a local copy of all files within a shared Google Drive folder Based on this information the developed software needs to work discreetly without affecting current working habits or applications File restoration should be easy enough for regular office workers although restoration should be done by an experienced administrator for best results The requirements for the development environment are not high because the software is initially used only to monitor a very limited number of shared folders within Google Drive The official requirements of the selected operating system Debian GNULinux are also so minor that any modern PC is sufficient for development purposes 13 While the development environment does not need to be powerful the production environment on the other hand needs to be powerful enough to provide a good user experience Initial benchmarks Appendix 1 indicate that the monitoring of one shared folder consumes an average of 3 to 6  of CPU resources on a typical virtual machine with 1 vCPU and 4 GBs of RAM This result was measured by recursively listing all files within a shared folder After changing the softwares logic from recursively listing items directories into a changebased scanning the CPU consumption was reduced to a stable 3  of CPU usage Appendix 2 The changebased scanning lists only files which have been changed since the last scan With this change the number of required files to list was reduced exponentially thus reducing the amount of consumed resources Based on these measurements one relatively slow virtual machine is able to withstand the monitoring of at least 32 concurrent shared folders within Google Drive This assumption was made on the basis that each new shared folder to monitor would cause an average of 3 percentage points increase in CPU usage By multiplying 3 percentage points by 32 a 96  CPU usage is achieved The actual consumption should be considerably lower due to the fact that the servers operating system itself could consume up to 2  of CPU while on standby without any shared folders to monitor due to automatic updates indexing and connectivity checks A more specific measurement can be achieved with a larger number of folders to monitor The CPU consumption is directly linked to the number of API calls API requests in Appendix 1 and API Response in Appendix 2 done via Google Drive API as can be conducted by comparing the CPU usage to the number of API calls per second The software requirements to run the developed software can be divided roughly in two sections serverside requirements and clientside requirements Serverside requirements must be met for the software to be able to work in the server Clientside requirements are to be met for the user interface to work for the enduser who intends to restore file structures to an earlier state As for the serverside requirements Google Drive API requires Google Developer Console authorization credentials to function which can be obtained from Google Developers Console The Google Drive API PHP client library requires at least PHP version 53 to function and the user interface requires at least Apache 20 to function The targeted development and production environment is Debian GNULinux wheezy which already includes Apache 22 and PHP version 54 so these requirements are automatically fulfilled just by choosing this operating system 14 The clientside requirements are exactly same as using Google Drive itself because all file controlling is made within Google Drive or with a separate file restoration interface which originates partly from Google Drive Google Drive requirements include any of the following web browsers Chrome Firefox Internet Explorer Safari only on a Mac as long as the browser is the newest or second newest version release 15 All interaction between the software and Google Drive REST API is done via regular HTTPS requests over port 443 so there is no major requirements for the internet connection speed or firewall settings Due to the potentially vast number of requests per shared folder instance the connection needs to be stable and able to withstand multiple requests within a small timeframe Initial measurements Appendix 1 indicated that the monitoring of one shared folder handles about 2 API requests per second and requires about 012 Mbs of download and about 0035 Mbs of upload speed to interact seamlessly with the Google Drive REST API After changing the software logic from a full recursive scan to a changebased scan the number of API requests was significantly reduced to about 02 requests per second on average A full recursive scan was named as full synchronization and a changebased scan was named as delta synchronization Appendix 2 The requirements for this project consist of three main categories First the hardware requirements for the server which runs the software application must be fulfilled Second software requirements must be fulfilled to provide the necessary platform for programming and using the file restoration interface Third running the software and connectivity requirements must be fulfilled to provide a consistent quality of service for the software Debian GNULinux was chosen as the operating system for the server platform According to the official system requirements guide of Debian GNULinux any modern computer would satisfy the basic hardware requirements Software requirements are divided in two sections serverside requirements and clientside requirements Serverside requirements are automatically met by having the latest version of Debian GNULinux and client side requirements are automatically met if the endusers are able to access Google Drive via a web browser since the restoration interface is only plain HTML with the exception of the instance creation interface which uses Google Drive file picker for selecting a folder to be monitored Because the measured network usage was very low connectivity requirements are quite modest in terms of speed Due to this any modern broadband connection should suffice More important than the connection speed would be the reliability of the connection Long connection outages might lead into a situation where a file has been created and deleted before it has been noticed by the software Because the software is intended to run quickly and serve multiple customers reliably 365 days a year all the minimum requirements must be exceeded with significantly better hardware and network connections While considering data security aspects it is also a best practice to use the latest stable release of required software services and tools This section begins by explaining the methodology used to design and develop the software After the methodology section the technical details are explained more thoroughly The project started with the knowledge of a problem with Google Drives shared folders in organizational use The first step was to find out of any already existing commercial products which could resolve the current problem According to a conducted questionnaire in the case company the only commercial product was not considered feasible The questionnaire provided a good starting point for the design of the requirements for a new software to overcome the encountered problem The software was designed and developed in various locations and with various devices in a timeframe of nine months As the agile development method was chosen the initial design did not include too strict guidelines on how the software should act in different situations While developing the software design plans were constantly changed to reflect the encountered problems As a starting point the project required a dedicated server with a PHP runtime environment a MySQL database a Google Apps for Work account and Google Developer credentials A virtual server with the product name of n1standard1 was bought as a Google Cloud Computing service The n1standard1 was the cheapest standard typed virtual server from Google which was still considered to be more than enough for the software to function in a pilot phase The required runtimes and databases were installed on the server separately The required Google Apps for Work account and Google Developer credentials were obtained from Google before the project started Due to the developers many years of experience with the PHP language it was chosen as the language to develop the software Although the developer had a considerable amount of knowledge in PHP some additional information had to be studied via the official PHP website during the development As PHP is categorized as a scripting language it did not require any code compilation and all changes were updated in realtime The developer did not have any previous experience with the Google Drive REST API so information regarding this had to be gathered from the Google Drive REST API reference page The software was developed to run on the server as scheduled task so it does not need any user interaction The evaluation of this project was done by simulating reallife situations of file and folder deletions while trying to restore folder structures after a deliberate deletion The ease and speed of file restorations were the primary metrics of the evaluation for this project Both metrics were measured by assigning endusers of pilot companies the task of restoring files which were deleted from a shared folder The evaluation methods of the finished product are described in the Solution Evaluation section User authentication and authorization process was developed by utilizing Googles Users PHP API This means that for a user to access the system the user has to be in the super administrators group of the respective organization The authentication process was developed to function in the following way First the user fills a registration form to sign up for the service After registering the user receives a followup email with instructions on how to permit the software access to the users Google Apps domain After the user has permitted access to the software the user is able to login to the softwares user interface The user interfaces login page does not process any credentials but redirects the user to a Googles login page When the user logs in to hishers Google Account the user will be redirected back to the softwares user interface as an authorized user All traffic between the endusers web browser and the developed software is processed via a secure HTTPS HTTP over SSL connection with the TLS 12 protocol which is the same technology as used in most commercial online banking systems The user interface was designed to allow Google Apps for Work administrators a oneclick restoration of a file structure to a given time The interface was programmed to allow access only to registered users with the Super Administrator role for their Google Apps for Work domain Once the user has registered to HSWDrive the user must login to the system via Googles authentication system Googles authentication system provides HSWDrive the users administrative role which is used to verify access privileges Authorized users are given the following options Create a new shared folder instance modify the organizations current instances and restore an existing instance to a certain point of time The date restoration interface only restores item locations not the actual data In case the content of a file needs to be restored to a certain date Google Drives builtin revision control can be used Items are also never removed even though a restoration would be done to a time when a certain item did not exist Table 1 illustrates the previous statements in a more understandable way As designed the restoration interface did not include anything else than a item listing for the selected date and current date a date selector and a restoration button The restoration button was implemented with a confirmation alert to prevent accidental restorations Figure 3 illustrates the file restoration interface in practice As can be seen in Figure 3 the logout link includes the email address of the currently logged user The email address is directly linked to the currently logged Google Apps user and in case the user wishes to login again a new authentication must be done against Googles authentication system The lefthand side column displays the current status of the selected folder while the righthand side column displays the status on the given date The main purpose of the designed software is to change the ownership of all files within a shared folder to one centralized user for easier file restoration Once the files are owned by one centralized user the files cannot be permanently deleted by other users as explained in the technology behind Google Drive and the developed software section The centralized user is called as the data storage user Due to the nature of Google Drive API various steps had to be taken in order to change the owner of an item within a shared folder Because the ownership of an item cannot be changed centrally by one administrative user the software has to act on behalf of the previous owner to grant ownership to a new user Figure 4 illustrates how the owner changing process was achieved in the developed software Based on the flowchart in Figure 4 the first step is to list all permission objects for the given item A permission object is an instance of the Permission resource which was explained in the Google Drive REST API section Once the list of permission objects has been obtained the software looks for the permission object with the owner role Based on the found object the software is able to detect the username of the files owner If the file is owned by an external organization or the storage user is already the current owner the file is bypassed After obtaining the owners username the software checks if the storage user already has a permission object the ID for that object is stored for future use In case the storage user does not have a permission object the software creates a permission object for the storage user and stores the ID for the newly created object Once the software has the permission object for the current and future user the software creates a new Google Drive service as the previous owner and transfers the ownership to the storage user Most of the programming was done with the PHP language in combination with a MySQL database for file status recording According to the original plan a PHP based programming framework called Symfony2 was intended to be used for the project As the software logic advanced it seemed Symfony2 would have been too resource consuming for such software Thus plain PHP was finally selected to be the programming language for this project Some sections of the user interface required the use of JavaScript As the project advanced it was clear that the original plan of using at least some sort of PHP framework would have been better in terms of manageability All functions were achieved but as a result of using plain PHP the software became more complex to update in the future If the software becomes a success in terms of sales a next logical step would be to rewrite most of the code in some PHP framework to support easier updates in the future MySQL was selected as the database engine The database consists of four separate tables The first table includes a list of all instances An instance is basically just a row of information consisting of the basic information of the centralized data user company information file ID for the shared folder and file IDs for the lost files folders The second table consists of the status of files in the shared folder This table includes the basic information of each file such as the file name instance ID and last update time The third table is structurally identical to the file table but only includes the history of old files Every time a file is being updated the third table shows the status history of each file as can be deducted from the table below The fourth table consists of registered users Only registered users are allowed to add modify and restore instances to an earlier stage Only registered users are allowed to add modify and restore instances to an earlier stage Each row in the instances table represents a shared folder for a single organization The shared folder may have numerous subfolders and files The instances table also defines the location for the files with the status of “Escaped “Released or “Orphaned which are all moved under respectively named folders in the “Lost and found folder for that instance The file tables and instance table are connected to each other with the instance ID parameter by using the builtin relation model of MySQL InnoDB This relation is indicated with an asterisk character in Table 2 The main idea of this database scheme is to have all the up to date file information in one table and another table is used only for logging purposes By having a link to the instance ID if is easy to query for files which belong to a single instance Each row in the users table includes basic information about the user The isAdmin column of the user table describes whether the user has full administrative privileges for the whole system Users with full administrative privileges are automatically granted full access to all organizations instances Unlike most database systems there is no password column in the users table The reason for this is that authentication is done via Googles Users PHP API Login passwords are never stored or handled within HSWDrive Each item file or folder in the database has a status Due to the fact that one item can reside in multiple locations within Google Drive one item might be listed multiple times in the database if the item is in multiple shared folders that are monitored by HSWDrive The items are separated by an instance ID which indicates to the folder that is being monitored Possible item statuses are ok deleted orphaned escaped released trashed or external Table 3 explains thoroughly what each  The item is accessible by the centralized data user but ownership has been given away and the item is no longer in the shared folder This type of item is automatically added to the released folder As stated previously all changes to items are scanned once every minute Each scan checks the following for all items Is the item the root folder of the shared folder has the item been deleted does the item exist in the database what are the current parents are the parents currently within the shared folder is the item owned by the storage user is the file owned by the same Google Apps organization is the file trashed have the parents changed and has the items name changed To decide what to do to a file in case of a detected change the table of Appendix 3 is used to determine the next action and file status The software was named as HSWDrive It was developed to function in the following steps First a company authorizes the software to have access to its Google Drive Next the company decides and provides the software a shared folder and a centralized data user account which will perform as the central data storage owner In the third step the software performs a recursive scan for the whole shared folder The software lists all files and folders stored in the shared folder and adds or updates the files in a database table While scanning the files the software changes the ownership of each file to match the centralized data user account Only files owned by someone in the same organization will be modified After these two steps are done the software starts to monitor all changes done within the shared folder From this point on it will perform a full scan only once daily but individual changes are monitored every minute In a perfect world the monitoring alone would be enough but due to possible errors in file ownership updates or network connectivity issues a daily scan was scheduled to overcome possible update errors The endproduct is a fullyfunctional serverside software which runs as scheduled tasks via crontab The software is programmed with the PHP language and it uses the Google Drive API for interaction with Google Drive The stages of development are roughly in the following steps First the prerequisites software hardware credentials must be met second follows the initial development and deployment third comes piloting and feedback and last comes the final release The finished product is sold as a service so the customer never gets any executable files or code The software consists of two separate main components to handle files Delta synchronization and full sync The delta synchronization component only checks for changes done to files in the given folder since the last change The full synchronization component runs a recursive scan of all files and folders within the shared folder A delta synchronization is run every minute while a full synchronization is run only once daily The reason for this division is that neither of the components alone are effective alone A delta synchronization is very quick to run because it detects only the last changes On the other hand in case of a slight network failure in the delta sync some files might not be processed The full synchronization is less prone to miss files in case of a slight network error but it might take a very long time to scan a large file structure Another downside of a full synchronization is that it consumes a lot more network and processor resources due to scanning each and every single file in the folder structure Due to the increasing number of shared folder instances two minor components were made to run all folder instance synchronizations simultaneously The main benefit of running a synchronization for all folders simultaneously is the noticeable increase of speed compared to running the synchronization on all folders consecutively Besides the finished product a deployment guide was made for administrators to easily setup HSWDrive on their organization This was done to make deployment as easy as possible considering not being able to use Google Apps Marketplace which would have been the easiest solution Due to time constraints the deployment guide was considered to be more worth the effort than submitting a beta staged software publicly to Google Apps Marketplace The product quality was evaluated in terms of functionality ease of deployment ease of use and speed of item restoration in case of an accidental file or file structure deletion The functionality evaluation was considered as successful if the software is able to keep track of all files and file changes within a shared folder Besides being able to keep track of files and changes the software must also be able to function quickly to provide a fluent user experience A delay of 5 minutes was chosen to be the maximum accepted time for a change to be recorded within normal use although the expected average delay is less than one minute These could be tested by making changes to items in a shared folder and confirming that the changes are registered within the softwares database Ease of deployment was measured by providing the customer administrators a deployment guide and observing if the customer is able to deploy the software to their organization with minimal help Ease of use and speed of item restoration was measured by asking regular users to deliberately delete files and attempt restoration via the HSWDrive file restoration interface This type of evaluation was considered to be the only way to concretely see if the software is reliable fast and easy enough for production use with future customers Besides evaluating the basic requirements of the solution practical testing was done in the following categories resource consumption speed of use and ease of use with a total of 11 shared folders from various organizations The solution was stress tested for resource usage by deploying various shared folders from various organizations for pilot use After the deployment resource usage was monitored via the servers own reporting tools and Google Developer Console The detailed measurements can be found in the Results and Analysis section Speed was monitored by adding items in a shared folder and calculating the time that the software requires to process the item The file processing speed was observed afterwards by using Google Drives activity pane for the selected items This method provided a reliable way to measure speed without having to compare update time between Ease of use was tested in two sections Implementation and usage The implementation section was tested by providing the administrators of customer organizations a deployment guide and testing if the customers were able to deploy the software by themselves The usage section was tested by giving the pilot customers a tasks to recover files by using the restoration interface If the customer was able to implement the software with minimal help the requirement for ease of deployment was considered as a success If a user was able to restore files to any earlier location with minimal help the requirement for ease of use was considered a success A separate quick guide Appendix 4 was given to the endusers to help in the deployment and recovery process As most software development projects also this project had problems which were not anticipated in the design phase This section describes the encountered problems and how they were resolved While testing the functionality of the first working version of the software a high amount of system resource usage was measured particularly in CPU network and disk usage Appendix 1 While this did not cause a major problem in the speed or reliability of the software with one case company it would have had a greater impact in an environment of multiple companies and shared folders Because the software is aimed to be sold as a service it is expected to handle various instances within one physical server without having noticeable latency or reliability issues After a logic survey of the software the cause of high resource usage was pinpointed to the logic of recursive file scanning As the initial version of the software used to scan through all files within a shared folder every hour it caused a major resource usage peak every hour Due to the large number of files within the case companies shared folder this scan took almost an hour for each scan To overcome this problem a change in program logic was implemented in the following way Instead of scanning through all the files every hour only the changes made to files within a shared folder were monitored after the initial recursive scan This was possible by using the Changes resource provided by the Google Drive REST API With this logic all changes could be detected each minute and still the average resource usage would was measured to be only 10  compared to the original resource usage Appendix 2 Because the Google Drive API PHP Client library is updated very frequently the documentation provided by Google did not always comply with the actual functions of the client library This caused some significant problems in making the software work correctly These problems were exceeded by investigating the source code of the client library Requests for correlation of the documentation were sent to Google The final decision of choosing not to use Symfony2 framework unlike originally planned caused a significant increase in programming time because many functions had to be rewritten in the event of a change to the database structure When this deficiency was noticed the software was already developed so far that moving it to a framework was not considered to be worth the effort The software was finished without a framework but if time and budget allows it will be rewritten within Symfony2 framework in the future While starting the project it was not obvious that even seemingly simple operations such as changing the owner of a file required multiple phases to achieve Even though the software can be granted full administrative privileges to an organizations Google Drive each operation is done subjectively with a user account This means that instead of changing the owner of a file needs to be done as the current owner As an example It was not possible to just define a new owner to a file with one simple command To change the owner of a file the first operation was to list all permission objects for the file After obtaining a list of permission objects each object had to be scanned for the username and permission level While scanning the permission objects the current owner had to be selected to confirm if the file was already owned by the storage user In case it was not the second phase was to check if the storage user had any permission object at all In case the storage user did not have any permission object one needed to be created Once the current owner was found and the storage user had a permission object the ownership could finally be transferred from the owners permission object to the storage users permission object The transfer had to be done as the previous owner Another complexity aspect of Google Drive API was that all items are considered as files Understanding that within Google Drive folders are actually just empty files with children objects was a new revelation For simplicity “item was selected as the word to represent any file or folder in this thesis Due to the fact that one item can coexist in multiple folders it was required to understand the concept of parent objects and children objects Parent objects can be thought as the locations for the item while child objects can be thought as files within a folder Naturally only folder objects can have children objects After having the software online for a few days some pilot companies reported on lost files which could only be found by some users in the respective organization These reports led to an investigation which indicated that although the lost items were in the corresponding folder they would not be visible to some users The users who experienced these problems had sufficient privileges to the respective folder but still some files would appear have vanished The underlying reason for this has been unknown up to this date but a workaround has been implemented to overcome this problem The workaround to overcome the issue was to reshare the folder to the users who are experiencing problems After the folder was reshared no further complaints were received by the users This section covers basic information about possible licensing terms and distribution methods for the developed software solution Even though licensing terms and distribution methods were not the focus of this project they are relevant or future reference As mentioned in the previous chapters the software is made to be sold as a service so a separate license agreement is not made between the buyer and the seller On the other hand the customer is required to accept a service contract which describes the content and price of the subscription The pricing and contract terms are not the focus of this thesis so they will not be discussed any further The initial target group consists of companies which have bought Google Apps for Work from the case company Due to this target group most agreements are initially done via telephone or email while the distribution is done remotely as a service After the software has been distributed to the initial target group it will be targeted to other companies in the future Although the commercial use of the developed software is not the main focus of this thesis it is still a focus point of the future Once the software has been successfully tested for at least 6 months on various test companies it can be considered to be a ready for commercial use To truly make it commercial it has to be published in Google Apps Marketplace for easier distribution The following procedure must be done in order to publish the app First a payment system needs to be integrated to the registration form This can be done with PayPal or another similar payment system After that it is required to register as a developer to Google Apps Marketplace Registering requires a 5 USD payment to Google After registering the second phase is to take screenshots of the software and create a manifest file The manifest file should include basic information about the software such as the name description and icons After a manifest file has been created and screenshots are taken the third phase is to submit the manifest file screenshots and application URLs to Google Developer Dashboard After submitting the app the fourth phase is to send a request for listing to Google Apps Marketplace by filling the Google Apps Marketplace Listing Review Request form Once the form has been submitted Google reviews the software and decides whether or not it is acceptable for publishing If the software is considered as acceptable the fifth phase is to test the installation of the software via Google Apps Marketplace If everything works as intended the final phase is to start marketing the software via various internetbased channels 14 Although the software itself is intended to be sold as a service for monetary gain the source code may be published with the GNU General Public License GNU GPL or similar license thus granting other developers individuals and organizations the freedom to use modify and copy the source code There are numerous reasons why this is considered as a good practice First all the developers programming knowledge has been gained from free resources including most of the API resources and code snippets Based on this it would only be fair to give something back Second publishing the source code would allow other developers to provide feedback and recommendations to improve the code Third publishing the source code could also function as public demonstration of excellence thus potentially provide work opportunities in the future Finally as the software is intended to be run on a server if is very unlikely that small business organizations would have the resources or knowledge for deploying the software while larger business organizations usually tend to avoid open source solutions without a guarantee of support If GNU GPL is chosen as the license model for the software each source code file should include a preamble indicating that the source code is under the GNU GPL license For the time being no decisions have been made regarding the licensing of the software This decision will be made in the future when the software has been further developed and implemented to a larger scale of customers This section discusses the results and outcome of the finished software solution The amount of work and resource consumption is the main focus while the summary section includes the final measurements The development of the software was done within 9 months which was within the set timeframe and did not cause any delays to other work matters The software had a total of 4194 lines of PHP code and 865 lines of CSS code for the user interface The original assumption of required PHP code was less than 1000 lines so the amount of implementation work was exceeded multiple times The main reason for the vast amount of code was clearly because of not using a PHP framework which would have minimized the required amount of manually written code If the original plan of using Symfony2 Framework had been followed most of the manually made PHP methods could have been automized and database structure changes could have been updated from one central location instead of updating all functions manually in the case of a change in the database structure The software was measured to function with very minimal resource consumption By increasing the number of shared folders to be monitored from one to eleven the only noticeable resource increase was observed in the CPU usage As indicated in Figure 5 the CPU usage with one folder was measured to be approximately 3  increasing the number of folders to eleven increased the CPU usage to approximately 17  This indicates that the server should withstand at least 32 concurrent folders while still having a 50  CPU consumption average on the current virtual machine Taking in account that the used virtual machine is the slowest standard typed virtual machine within the Google Cloud Computing Platform it is just a matter of upgrading to a faster virtual machine in case more processing power is required By observing Figure 6 it is clear that even with 11 shared folders to monitor network and disk usage activity had almost zero impact to the server with less than 50 Kbps network usage and 30 KBps disk usage on an average Some traffic spikes were observed but even those had very minimal constraint to the server The only significant increase of resource consumption was measured as the number of API requests done to the Google Drive API as can be observed in Figure 7 While the first version of the software consumed an average of 2 requests per second for one shared folder the improved version with a changebased scan only consumed an average of 02 requests per second for one shared folder The measurement of 11 shared folders consumed an average of 7 requests per second which increases that the average of one folder would be 064 requests per second This sudden increase of requests can be explained with the fact that most of the latest customers had just migrated to Google Drive thus adding new files on a daily basis Once the new customers have finished their migration the average number of requests should be considerably lower When considering that the maximum number of daily API requests is one billion 1 000 000 000 there is still capacity for 1000 shared folders even with the measured peak value of over almost one million daily requests The success rate of requests was measured as 100  which indicates in a flawless network connection and in the adequacy of quota for API requests As the requirements indicated that the software had to be easy to use various tests were performed by giving customers the task to deploy an instance and restore deleted items According to the tests all 5 customer administrators were able to restore deleted files successfully The deployment phase was considered as more difficult to most customers due to the advanced setting changes within Google Apps Administrator Console Only 2 users out of 5 users were able to deploy the software by themselves Taking in consideration that most of the customers were not technically experienced this was still considered as a success in terms of deployment easiness Once the software is deployed in Google Apps Marketplace the deployment will be considerably easier for users with a less technical background While building the solution many new aspects of Google Drive were learned including many which are not obvious to the regular user Before the project was started it was quite unclear how Google Drive handles file deletions in shared folders Even with the knowledge of Google Drives file handling logic it might still be challenging to explain the operations logic to regular users in a clear way due to its ownerbased file system In the beginning of the project Google Drive seemed like any other file system where files can be listed copied and modified by simple commands As the development advanced it was clear that the sophistication and design of Google Drive was done in a very different way since each operation to a file required multiple phases to accomplish the desired outcome Most of the functions were combined to PHP methods which made the operations easier in the future but the amount of work required to build these functions was far greater than was anticipated while starting the project One of the new discoveries was the fact that a file which is not deleted but inaccessible to a user is considered to be deleted in the Google Drives point of view A software solution was designed and developed according to the initial requirements The software was named as HSWDrive and evaluated successfully with various pilot customers The software runs a full synchronization to all HSWDrive instances on a daily basis and a delta synchronization every minute File structure restorations were tested and proven to work as designed Even though the software has been proven to work as expected with a set of customers further analysis and bug tracking is required before publishing it in Google Apps Marketplace for wide distribution The software is already in active use by the case company and many of its customers Further development for new features such as file revision restoration and support for externally owned files is scheduled for the near future The aim of this project was to design and develop a software solution to allow easy restoration of accidentally deleted files in a Google Drive shared folder The software was designed finished tested and deployed to various pilot customers within 9 months which was considered to be within an acceptable timeframe The development would have been faster with a PHP framework but on the other hand it probably would have had consumed more system resources If time allows the software will be rewritten in a PHP framework in the future The software was a success in terms of functionality and demand as it was proven to function fluently on various pilot companies Ease of use was also measured to be satisfactory as all pilot users were able to restore files and 40  were able to deploy the software independently Some technical issues were discovered during initial deployment but once these were overcome no significant deficiencies were observed The software is still in a testing phase but after a more comprehensive and successful testing period of at least 6 months the software can be submitted to Google Apps Marketplace for easier deployment and sales purposes A future release of the software will include the possibility to restore files not only to earlier locations but also earlier states Due to Google Drives automatic revision history this can be achieved without having to store file data in the softwares own database Another functionality to be developed in the future is the possibility to restore files owned by another organization Currently this is not allowed because HSWDrive only handles files owned by the same organization and file ownerships cannot be transferred between organizations due to Google Drives limitations Even though it is not possible to take ownership of such files it is still possible to monitor and log the files in case of an accidental deletion The research started in 2012 to study current business needs of the company and how the Business management tools and methods are helping to address the needs the research began with a study about the best practices and current practices in using agile methods and tools in different teams The main objective of the research was to analyse the current practices in the teams for project planning and execution and identify the ways that are needed to make the visibility of the project situation clear to all the stakeholders in the project and with a purpose to reduce delays in the project While the research moved on with the study and discussion with many individuals in the team it turned up to be a great opportunity to learn the business management methods and tools from the information shared in the discussions Also with several years of experience in the industry in different kind of roles I had an opportunity to express and relate the best ways of using the tools and methods which would help any individual in software development organisations similar to the one in the case study The research should give a fairly good view of best practices in Agile methods and tools and what is missing currently in practices correcting the missing practices could reduce delay in meeting the project deadline It brings in a clear picture of combining the latest agile methods and tools to address the current needs of the industry The solution provided will be helpful for better utilisation of tools and stay updated with the current situation of the projects at any time This research would have not been possible without many individuals whom I wish to give my sincere thanks and to mention my mother who travelled to Finland from India on time when I needed her help to go for studies and my supervisors Jukka Kainulainen and Thomas Rohweder The aim of the present thesis was to study the current ways of using agile methods and practices in the case company and find the ways to improve the current practices in order to have better visibility of the project status Accept360 is the tool used for project planning and execution in the case company hence the study concentrated on understanding the current practices in using the tool and finding out the gaps that exists in the current practices in comparison with the best practices of using the Agile methods and tools Agile approaches are used in software development to help businesses respond to unpredictability To address the uncertainties and have good visibility of the project status the agile methods provide rules and a standard ways of usage which would help the project stakeholders to stay updated with the project status In the research the best practices in agile method of software development were studied first and some of the best practices and methods that would best suit the organisation were identified Next the current practices were studied in the selected teams in the case organisation by conducting contact interviews with the key stakeholders from which the researcher collected information about the current ways of using the agile methods and tools The interviews also gave information about the opinions about the tool which is being used currently for project planning and execution how the tool addressed the current needs for project planning and execution and what were the suggestions from the key stakeholders for improving the practices in the tool and methods The data collected about the current practices was then compared with the best practices and the gaps between them were identified from which the proposals to improve the visibility of the project situation were created The study resulted in three key theme proposals The first theme is to follow the already agreed practices and guidelines which is missing currently The second theme is using the Agile Software Product Management method and the third theme is implementing Scrumban practice for project planning and execution The last two themes mainly recommended ways for having a separation between requirement and engineering management The proposals were found to be very useful input for further process improvements to have better visibility of the project status and address the current key issues in the case organisation Earlier Nokia Oy comprised following business groups Smart Devices Mobile Phones Locations and Commerce and Nokia Siemens Networks Later Nokias Smart device and Mobile Phones groups were then bought by Microsoft and merged into one to create a new group Microsoft Mobile Oy ever since the merger over the past three years the group has created incredible results awardwinning phones and amazing services that have made Nokia Windows Phones the fastestgrowing smartphones in the world In Microsoft Mobile Oy Applications Software teams which belong to Smart Devices unit are the teams which creates valuable and innovative solutions that adds value to the smartphones Projects executed by the teams in Applications Software Team require dynamic changes and updates in requirements always Projects are executed in challenging situations where the market is highly competitive and customer needs are changing rapidly in order to meet such demands and supply the best products for the customers and also to deliver products with high standard and quality it is required to make sure that the projects are planned and executed with high productivity and good utilisation of the resources available also it is required that the project situation is visible to all the audience of the project at any point of time so that the needs for executing the project successfully are meet as early as possible and there is minimum delay in delivering the project Even after having a welldefined processes and tools for project planning management and execution there are delays in some projects Hence there is a good need for evaluation of how the project planning tool and methods that are used in the projects and how efficiently the tools are used to track the progress of the project The main objective of this thesis is to study the best practices using the agile methods and tools in the company Accept360 is being used mainly for project planning and tracking the current way of usage of Accept360 tool need to be analyzed and the gaps that exist between best practices and the current practices need to be identified and solutions that could fill this gaps need to be researched Research question of this thesis is “How the visibility of the project situation could be kept updated at any point of time “What are the best ways to stay on track of the current situation of the projects and so be able to manage the issues appearing earlier and minimize the delivery slippages The outcome of the study is to propose the solutions which include guidelines for the efficient usage of best practices artefacts tools and methods sample process framework that would address the needs of the projects and that are needed to help everyone involved in the project to have a clear visibility of the project status This chapter explains research approach this includes the steps that were done to identify the research problem analyse the current state and the steps that were done to find solution for the research problem As first step the details about the case company was studied as a part of the course on Business management methods and tools it included complete study of the companys strategy mission visions goals current growth needs and KPI measurements In the study it was found that the current importance for the company is to minimise the delays in delivering the software for which the visibility of the project status plays a key role unless the needs of the project is visible to all the stack holders involved  at any point of time it is not possible to address them earlier In ASW team Accept360 tool is used as the key tool for tracking and executing the projects hence a research on how the tool is currently being used in the teams does the tools address all the needs of the projects is there any improvements needed in currents way of working in the teams needed to have better visibility of the project situation sounded a good topic of research for the thesis Hence a study about the Visibility of the project situation was decided as topic of the research A flowchart in Figure 1 was created with steps for finding the solution for research problem As second steps the research topic was discussed with the senior manager in the company and it was considered that this topic of research would yield a good review and guide for identifying the gaps that exists in the currents practices of usage of the tool and methods for Project Planning and Execution and the objective of the thesis was set Since Project planning and tracking is done using the Accept360 tool study of the tool was planned first  Initially couple of meetings were planned to discuss the available information about Accept360 tool and how the research need to be done During the first meeting the guidelines available for the Accept360 tool were discussed and previous presentations that were used to provide guidelines for the teams for using the Acccept360 tools were gone through In the second meeting a short walk through of contents in the Accept360 for some of the projects were viewed and discussed Then identification of the teams and the group of people to be contacted for current state analysis was done Next identification of a team that is using Accept360 in compliance to the guidelines was done it was found that the Team S in Sandiego has created the content in Accept360 in a best format that it could be used as benchmark for comparative study of Accept360 content As a third step a study of best practices in the software development methods and practices was done using books articles companys project guidelines and the training materials for the Accetp360 tool and practices Next access to the training website for the Accept360 tool which is an exact trial ground for the usage of the tool in the same way as real situation was requested with access to the tool studied the features supported in the tool experimented the usage of the tool to understand the complexities and merits of the tool Next a basic set of questions and topics for discussion were designed for the interview From the work experience in Nokia Research and Development centre since 2006  Nokia research and development organisation until and around 2007 had been following the waterfall model for its software development process after that time there were many trainings and changes in the way of working for moving towards Agile methods The main drawback with waterfall model is it follows the procedure in which when each stage of the project is completed the developers go on to the next stage and they do not validate the previous step when they go forward so at the end if the final product turns out to be with some flaws there is less chance to go back and fix the issues if the flaws were found at the end to go back and fix the problems means it would be possible only by scratching the whole project there is no chance for change hence extensive planning on expected outcome is done at the initial stage and then the project is executed after the planning is done the design and requirements are documented very well the documentation helps when there are very less resources available to continue in the project help to add new resources quickly In waterfall model client would know what to expect but at the same time there is a high risk that if the planning at the beginning had some faults that could end up in risking the whole project and it also does not consider the evolving needs of the customers testing is done only at the end of the project so if errors are found at this stage it could be that they will stay for rest of the project and could not be fixed Waterfall method is good only if the initial requirements are very clear and expected outcome is well defined Agile methods were introduced as solution to many problems in the waterfall methods this method allows changes to be done at any stage of the project initial requirements are kept very simple and a very simple product is made initially later changes are done as per the client needs and latest improvements in the industry The products development is done in small cycles in stages at the end of each cycle the requirements are prioritized and updated testing is done at every sprint so that the errors are identified at every stage and the prioritised errors are taken care in next development cycle this also produces good quality product over comprehensive documentation at every stage this means that the product is in quality at each stage and could be released at the end of any stage this ensure that the deadline for releasing the product is always met At the same time there is more care needed to make sure that every stages in the project are not series of coding stage and end up in a way that the initial planned requirements and product and final outcome are totally different Agile method of development is good if the product is intended for a rapidly changing market This also requires highly skilled independent and adaptive developers to be involved in the project Agile methodologies promise that the product reaches the market faster with good quality of software fulfilling customer needs There are several software development methods introduced based on the principles that are followed in different organizations For example the Lean Software Development method was introduced based on the principles followed in Toyota manufacturing unit which are based on continuous improvement and elimination of waste but these are more generic principles that this could also be applied in the agile methods The table below from the reference 1 lists different agile methods and principles mentioned in the reference 1 Scrum is a light weight software development process consisting of implementing a small number of customer requirements in two to four week sprint cycles Schwaber 1995 XP consists of collecting informal requirements from onsite customers organizing teams of pair programmers developing simple designs conducting rigorous unit testing and delivering small and simple software packages in short twoweek intervals Anderson et al 1998 Adaptive software development or ASD involves product initiation adaptive cycle planning concurrent feature development quality review and final quality assurance and release Highsmith 2000 LEAN involves eliminating waste amplifying learning deciding as late as possible delivering as fast as possible empowering the team building integrity in and seeing the whole Poppendieck  Poppendieck 2003 In general agile methods may vary in applying certain practices However the methods emphasis on producing working software in small iterations and utilising resources efficiently Main features of the agile software development could be summarized as three key things as mentioned in the reference 4 Feature orientation Reactive development and Evolving Project scope Feature orientations is to have main focus on the producing features faster with a goal to deliver the working functionality that brings most value to the customer Reactive development is reacting to a change rather than planning ahead and keeping the decision making as late as possible Examples of reactive practices are refactoring adjusting requirements priorities and release scope after each iteration Evolving project release scope is the main distinguishing features of agile development changing from a fixscope approach to a more openended approach In the traditional fixed scope approach much effort is spent on defining and planning the content of a product release of a project upfront In agile the release scope is emerging in the process of development rather than planned ahead A prioritised list of requirements serves as an initial input which is a kind of wishlist of a release scope The release scope is expected to be refined and updated at the end of each planned cycles of work in order to accommodate changes and new information that was learned in the latest stage This practice is in line with the reactive development property described above Scrums main principle is implementing a small number of requirements in a short cycle It includes mainly the following ways Selfcorrection with inspection making everything visible or known to the stakeholders for example plans schedules issues and progress more clear at every point of time as they are changing all the time Stop and Review the product and the process Scrum operates this way At the beginning of each sprint the team reviews what it should do Then the team selects what it can turn into the release of a potentially working functionality by the end of the sprint Then the team start to work independently with no interruption to make the best efforts for the rest of the sprint At the end of each sprint the Team demonstrates the release of functionality it built so that all the stakeholders can inspect and do adaptation to the project In Scrum the project is started with a vision that needs to be developed The vision initially is stated in market terms and not much in technical terms The items which are to be developed to deliver this vision are made as a list called product backlog items As scrum suggests one person is responsible for delivering the items planned to those who are funding the project and the person in scrum terms is called Product owner The product owner is responsible for delivering the vision in such a way that maximises their return of investment for that the owner creates a plan which includes the product backlog Product Backlog is a list of functional and nonfunctional requirements that when turned into functionality will deliver the vision of the project The requirements that are important to create most value for the product is at the top priority in the list of requirements The prioritized requirements is a starting point and the contents priorities and grouping of the requirements into releases this is usually expected to change the moment the project starts The updates and changes in business requirements and the performance of the team to build the requirements into functionality decides the changes in the requirement list and its content Scrum team is a selforganizing team The team plan their own work the work plan is visible as sprint backlog items it is a good practice if each functionality to be delivered is appearing as a sprint backlog item this helps the team to view the sprint backlog item as a plan and also as a reference for the other team members as they work In this way it ensure that the work to be done is thought through This would make sure that the daily scrums are more meaningful as the work plan is clearer Scrum master role is recommended in Scrum one person in the team act in this role to ensure that the scrum rules are followed and the team do not cut corners Scrum recommends it as a thumb rule that the team is given the prioritised backlog items based on which the team decide by its own on what need to be done next and it is required that the scrum master in the team ensures that the team does not skip the process of scrum and also can help remove the impediments but at the same time the person does not have the authority over the team but only acts to help shape the development processes in the team and make sure that the team brings out the good results and make sure the team is not going off track Scrum team location is recommended to be in a collocated team space which is achieved by removing the cubicles and let team members to face each other and communicate often this eliminates isolation and misunderstandings Mandatory meetings for scrum execution are the Sprint planning meeting Daily Scrum meeting Sprint review meeting and Sprint retrospective meeting these meetings are supported and encouraged by the Scrum master and these meetings are emphasised to get the maximum benefit of the scrum process framework In Scrum all work are done in small cycles called sprint which usually is a one to four weeks cycle Sprint Planning meetings usually have two parts in the first half the Product Owner presents the team with the list of requirements that are prioritised and that are expected to deliver a functionality in the release at the end of the Sprint this list is called Sprint Backlogs During the second half of the meeting the team is expected to discuss and plan their work tasks are created by the team for completing the Sprint backlog items after this meeting the sprint is started and it is time boxed to the number of days the sprint is supposed to be Daily Scrum meeting is held daily for 15 minutes in this meeting the all the team members meet at one place and each team member answers to the three questions what was done since last daily scrum What is planned to be done from now till the next daily scrum And what are the impediments that are on the way to complete the planned items in the sprint backlog The purpose of this meeting is to synchronize the work with everyone in the team and to address and schedule for more meetings if needed to go forward in completing the sprint goals Sprint review and the Retrospective meetings are held at the end of each sprint In the sprint review meeting the team presents the developed functionality or implementation to all the stakeholders of the project this is an informal meetings this helps to bring all the stakeholders together and have an understanding and view what is needed next for the project and then plan for it Sprint retrospective meeting is held after the Sprint review meeting and before the next planning meeting This meeting is to encourage the team to revise the process they adapted in the sprint and make sure it is within the scrum process framework so that the next sprint is more effective and interesting for everyone in the team User Stories Epics and Themes are the three work items which are mainly used to define the items that needs to be worked on by the scrum team Theme in scrum is the highest level in the story hierarchy and describes a view of a tangible product which can be a trading application or an abstract goal such as performance tuning A product owner breaks down a theme into one or more epics Epic represents a group of related user stories or a block of requirement that is not yet been rationalized into stories Sometimes a large user story is also called as an epic A story is a brief statement of a product requirement or a business case Typically stories are expressed in plain language to help the reader understand what the software should accomplish Product owners create stories A scrum user then divides the stories into one or more scrum tasks Scrum recommends to remove the unneeded artefact such a design documents hence the work items descriptions serve as a source of information about the project Projects are estimated based on the three key things the resources scope and time Estimation is a challenging task to do when there are uncertainties of way of execution and process followed are changing but when there is a defined process it could make estimation easier Time spend for estimation could go waste if the estimation are not useful or appropriate but the possibility to have accurate estimate is very less as there is some amount of uncertainties in the projects dependencies in projects which are driven for innovation and change of existing technologies so spending more time in estimating the project will not return a good value for investment but spending a little time to estimate even if it is less accurate would be more efficient to manage the project From the above Figure 4 indicates that the accuracy value starts to degrade when the amount of time spend in estimation is going beyond certain limit Estimate is still an estimate Spending more time in estimation is not going to make the estimate more accurate And to reach high level of accuracy in the curve which means to move away from base line only a little efforts is needed so a less amount of time spend in the estimation would increase the accuracy level As mentioned in the reference Agile teams tend to stay in the left side of the Figure 4 as they acknowledge that estimates could not be done accurately but still encourage the idea of estimation with less time and recognise those estimates that give big gain As agile process is to deliver frequently a fully working tested and integrated software they always are in a situation that they have a reliable plan Usually the user stories are not fully grained down to fine level so they are not of same order of magnitude and differ in their size Hence by aggregating some stories into themes and writing some stories as epics a team is able to reduce the effort they will spend on estimating However its important that they realize that estimates of themes and epics will be more uncertain than estimates of the more specific smaller user stories User stories that will be worked on in the near future for example in the next few iterations need to be small enough that they can be completed in a single iteration These items should be estimated within one order of magnitude It is a good practice to use the sequence 1 2 3 5 and 8 for this estimation User stories or other items that are likely to be clearer than a few iterations can be left as epics or themes These items can be estimated in units beyond the 1 to 8 range sequence recommend earlier To accommodate estimating these larger items it is good to add 13 20 40 and 100 to the preferred sequence of 1 2 3 5 and 8 In Scrum project there are four reports that need to be created by the end of each sprint The first lists the Product Backlog at the start of the previous Sprint The second lists the Product Backlog at the start of the new Sprint The third the Changes report details all of the differences between the Product Backlogs in the first two reports The fourth report is the Product Backlog Burn down report The Changes report summarizes what happened during the Sprint what was seen at the Sprint review and what adaptations have been made to the project in response to the inspection at the Sprint review Why have future Sprints been reformulated Why was the release date or content reformulated Why did the team complete fewer requirements than anticipated during the Sprint Where was the incomplete work reprioritized in the Product Backlog Why was the team less or more productive than it had anticipated All of these questions are answered in the Changes report The old and new Product Backlog reports are snapshots of the project between two Sprints The Changes report documents these differences and their causes A collection of Changes reports over a period of time documents the changes inspections and adaptations made during that period of time Burn down Report This Burn down report measures the amount of remaining Product Backlog work in the units of story points for each of them on the vertical axis and the time scale by Sprint days on the horizontal axis A Story Point is a subjective unit of estimation to estimate User Stories It represent the amount of effort required to implement a user story Using Story Points for estimation is better than estimating in hours or days as it is an estimation done using relative sizing by comparing one story with a sample set of previously sized stories Relative sizing across stories tends to be much more accurate over a larger sample than trying to estimate each individual story for the effort involved The Fibonacci series 1 2 3 5 and 8 is most commonly preferred to categorize efforts in scale of 1 2 4 8 16 points and so on The Product Owner plots remaining quantity of Product Backlog work at the start of each Sprint By drawing a line connecting the plots from all completed Sprints a trend line indicating progress in completing all work can be drawn By figuring out the average slope over the last several Sprints and velocity can be determined occurring when the trend line intersects the horizontal axis This is an important report It would graphically present to management how the factors of functionality and time were interrelated This is good to be included in the project reports but can be an appendix Kanban was initially designed and used in manufacturing units and then it had its way in to many other fields as well and in software development too the basic implementation model of Kanban described in the reference 7 gives a clear idea of steps needed to a Kanban in any field Kanban itself is a scheduling tool which replaces the traditional daily and weekly scheduling Kanban scheduling is an execution tool rather than planning tool Kanban does not replace the planning process but rather takes the information from planning and uses it to create the Kanban a Value stream which is explained later in this section Kanban is considered as a tool which helps to identify and remove the project dysfunction The word Kanban is translated as visual cards Figure 6 Kanban Implementation Model Kanban model above has the seven steps for implementing Kanban for a production organisation but the same steps would apply for any organisation like software development units The steps are 1 Conduct data collection 2 Calculate the Kanban size 3 Design the Kanban 4 Train everyone 5 Start the Kanban 6 Audit and maintain the Kanban 7 Improve the Kanban Step 1 Conduct Data Collection This Phase is to collect the data necessary to characterize the development process This is conducting value stream mapping VSM for the entire organisation which is to determine which development processes would be good candidates for implementing pilot Kanban scheduling systems Step 2 Calculate the Kanban Size This step is to calculate the size of the Kanban Initially calculate the Kanban work item size based on current conditions not based on future plans or desires The initial calculations will utilize the development requirements the productivity rate and risks involved Step 3 Design the Kanban This step is designing the stages and flow in the Kanban the value stream Once the Kanban quantities required to support development requirements based on current conditions is calculated it is time to design the Kanban The completed Kanban design will answer the question of how you will implement the Kanban The design will consider The end product of this step should be a plan for implementation of the Kanban including implementation actions action assignments and schedule milestones Step 4 Train Everyone The people involved has to be trained about how the system will work and on their role in the process the process and the visual signals has to be explained in a training Also the rules are reviewed during the training It is aimed for taking the participants through whatif scenarios to help them understand their roles and the decisionmaking process The training is focused on operating the Kanban Step 5 Start the Kanban Before Kanban scheduling is implemented all the visual management pieces are kept in place To avoid confusion and make training much easier the signals are set up control points are marked and the rules are completed and coordinated As the Kanban is deployed it is good to anticipate problems that may impact success and take action to prevent or mitigate these problems During the deployment stage develop a scheduling transition plan determining the exact point for the change and the amount of efforts required to make the change Step 6 Audit and Maintain the Kanban After the Kanban starts the next step is of the process auditing the Kanban When the Kanban is designed the person who will audit it is also identified Typically the auditor will be watching how the scheduling signals are handled and whether output stays satisfactory When the auditor finds problems then the problems need to be fixed immediately by the responsible party to maintain the integrity of the Kanban design The auditor will have to look at future requirements to make sure the Kanban quantities meet expected demand Step 7 Improve the Kanban Finally After the Kanban gets running look at how to improve the Kanban to reduce new work items waiting to enter the stream Resist the urge to just start pulling items Check how the flow is running and pull the necessary items immediately After this onetime adjustment only reduce the quantities based on improvements made to the development process Determine the amount that can be reduced by using the calculations used in sizing the Kanban to calculate the new quantities  The main idea of the Kanban is to have a flow of stages a running value stream which has different level it is the part of step to design the Kanban and is shown in the Figure 7 below  Figure 7 Kanban Value Stream  Kanban works in a way that the items are pulled into each stage of the value stream in a flow Each stage in the stream have two states Queue and Execution state when an item is moved from one stage to another it first stays in Queue and it is then moved to Execution state And there is a limit set for the total number of items in each stage this is called work in progress limit this limit is set based on the capacity of the team so this makes it a value pulling system to keep the flow steady with work in progress limits in each stage being uniform when the items in each stage exceeds the work in progress limits or if the items in a stage is empty it is a signal that the flow needs attention also makes the progress more visible to all stakeholders  342 Kanban Work Items and Terms  Most of the agile practitioners use the term iterations which is time boxed cycle in which the selected user stories are completed In Kanban which is more specifically designed for Lean practitioners the term iterations is replaced by Minimum Marketable Features MMFIn Kanban the team work on the MMF with no time limits for each MMF user stories and for each stories the scenarios are created Each Story is a card that represents functionality and Scenario represents the action related to each functionality  Figure 8 Kanban Work Items  Delivery Rate The rate at which units of work work item pass through the value stream or part of the stream  Lead Time the term is Development Delivery Rate Measured in units per dayhoursecond the two terms described above can be related as  Lead Time  Work In Progress  Delivery Rate  Delivery Rate Work In Progress  Lead Time  Valueadding Time The total time spent on valueadding activities for one unit of work  Valueadding activities exclude waiting and superfluous work  Resource Efficiency A measure of the utilisation of a given resource ie the ratio between the time working on adding value in the system to the total time available  Flow Efficiency A measure of timeutilisation on a given unit of work ie the ratio of the Valueadding Time to the System Lead Time  Backlog 	A nonWIPlimited queue containing work items awaiting service by the initial activity in a Kanban system  Work Item The item controlled in the Kanban system Effort Required Determines the approximate size of work in personunits of time May be a negotiated function of desired quality  Cadence The rhythm of the production system Not necessarily an iteration Kanban still allows for iterations but decouples prioritization delivery and cycle time to vary naturally according to the domain and its intrinsic costs The average transit time of a work item through a Kanban system  Activity Valueadding work that can be determined as complete Includes activity queue a set of resources and a WIP Limit Represents an allocation of the effort required to complete a work item  Next Work Item Selection Function Rule for selecting the next work item from a queue when an activity has less work than its WIP limit depends on both Class of Service and Value Function and leads to specific flow behaviours  Class of Service CoS Provides a variety of handling options for work items A CoS may have a corresponding WIP limit for each activity to provide guaranteed access for work of that class A CoS WIP limit must be less than the activitys overall WIP limit Examples are expedite datecertain and normal CoS may be disruptive such as expedite and is the only way to suspend work in progress  Value Function Estimates the current value of a work item within a CoS for use in the selection algorithm Can be simple null value function would produce FIFO or a complex multiple kanbansystem multifactor method considering shared scarce resources and multiple costrisk factors The means of prioritizing work items  Activity Queue Holds work items within an Activity that are awaiting processing The sum of items in process and items in activity queue must be within the WIP limit for each CoS  WIP Limit Limit of work items allowed at one time within an activity Prefer this term to flow units in process or similar Measured in units Which term should be used for the rate at which units pass through the system or part of the system Velocity in Scrum Delivery Rate and Throughput are all used frequently probably Delivery Rate is more common in the Kanban community though I have a slight preference for Throughput It is only one word and it applies equally to a subset of the system as to the final delivery part  Visible Representation A common visual indication of work flow through the activities Often a columnar display of activities and queues May be manual or automated Shows status of all workinprogress blocked work WIP limits it is a characteristic that provides transparency enabling better management Difficult to model  Flow Metrics Includes cumulative flow charting and average transit lead time  343 Kanban Execution  In Kanban work is pulled from the back rather than pushed from the front Limiting the number of items in any one flow stage at a point of time with work in progress limit WIP is the key factor which drives the Kanban execution forcing WIP limits encourages team members to stop at one point when the WIP limit has reached and everyone looks in to the issue together until it is solved before they move on to work on the next item In this ways the impediments and roadblocks are eliminated as early as possible Kanban contains an embedded process for handling items that need to be expedited through the flow fixed delivery dates and work type splitting  Work is assumed to be broken down to a roughly similar size In the Kanban Board not only the flow of stories or scenarios are represented but the MMF itself also could be added in the flow and checked Prioritisation of the backlog is performed just in time JIT  Figure 9 Kanban Board  344 Kanban Reporting  The key means to check the progress of work in Kanban team is using Cycle Time Lead Time and Cumulative Flow chart Cycle time is the time taken for a unit in terms of a user story or a scenario to pass through all the stages in the flow Cycle time starts when the flow starts and when any one of the unit is out the flow to a completed The main difference between the cycle time and Lead time is the unit is marked in the later the time between when on particular unit it entered to the flow and the same item is out of the value stream as complete  In the Figure10 below is the view of the Cumulative Flow Diagram by Kanban Tool  Coloured areas on the diagram represent work in progress for each stage of a process  Figure 10 Kanban Cumulative Flow Chart  By looking at the vertical distance of a chart we can define how many items are currently in progress The horizontal distance shows how long it takes for a task to be completed Measuring the horizontal distance on a Cumulative Flow Diagram allows you to monitor the Cycle Time according to which you can make a prediction of when all the work in progress will be done Vertical distance helps you to set the right work in progress limits  Cumulative Flow Diagram should run smoothly Large steps and flat horizontal lines indicate impediments to flow or lack of flow Variations in the gap or bands stand for bottleneck situations which usually occur due to irrelevant work in progress limits This means that the number of tasks in each column should remain at the same level over the time In addition too many tasks in the queue mean either problems with finishing work on time or that the employee on the next stage cannot deal with work  35 Hybrid Project Management Approach  Hybrid Project Management approach is a method in which the requirements and release planning are done in waterfall method this methods combining the waterfall and agile method for the project execution as explained in the picture below is referenced from the article in reference 2 which In the first sprint the project planning and proposal is done in a traditional way of analysis design and documentation and the product backlog items are made in the second spring the agile scrum method is followed in the team to critical path prototyping is made and presented to the team and the stakeholders and also tested and bugs are found and the product backlog is groomed according to the finding in the sprint and the new backlog along with the bugs are made sprint backlog for the next sprint in which the final prototyping is made and demonstrated to all the teams involved in the organisation Application development teams uses the approach similar to this hybrid project management method  Figure 11 Hybrid Project Management Skeleton  36 Agile Maturity Model  Over the last decades the CMMI models has been used most predominantly to measure the quality and maturity level of the organization ever since recent times when the agile software development came in to existence in most of the software development companies where the customer needs are changing rapidly there has been many studies done to analyse the adaptability of CMMI for the agile method of working  Main objectives Agile software development methodology are lower cost high productivity and satisfied customer The CMM tends not to focus the software process on an organizations business objectives in their software process improvement programme 7 Also most companies small to large companies found it is too difficult to reach higher levels in the CMM 7 The study also mentioned that the CMM improvement path is not always smooth the efforts generally took longer and cost more than expected While agile software development methodology is targeted to lower cost Some of the KPAs have been found difficult to apply in small projects 7 This may be because CMM was originally structured for big enterprises 7 CMM addresses practices such as document Policies and procedure that large organizations need because of their size and management structure 7 Hence there is new innovation which were created to measure the levels of Agile software development practices which is called Agile software maturity model  The Figure 12 below from the reference 7 states the different levels in AMM in simple terms from the article the levels could be defined as below Level 1 Initial Level is where the organisation unstructured and has no process improvement goal Level 2 In this level the organisation is has project or software planning customer or stakeholders orientation practices Level 3 Defined Level is about having Customer satisfaction Software quality and development practices this level denotes a more focus on practices related to customer relationship management frequent deliveries pair programming communication coding testing and quality of software Level 4 Improved Level is to have People orientation and project management Practices Companies at this maturity level are in a position to collect detailed measure of the software development process or practices and product quality both the software development practices and products are quantitatively understood and controlled using detailed measurements examination of risk and respect to the team who is going to develop the system The AMM at level 4 maturity aims to help developers or managers to respect for the coworkers or people involved in the project identify and improve problems related to team sustainable pace and organising team by itself This is achieved by an assessment of current process and to identify where weakness lie Level 5 Mature level is when the organisation has Performance Management and Defect prevention practices in place Companies at this level continually improve their processes through quantitative feedback from the process and form testing innovative ideas and technologies  Figure 12 Agile Maturity Model Levels  37 Agile Software Product Management  The software product management SPM includes the process of managing the requirements defining the release defining the context for the products involving internal and external stakeholder and this topic also includes many other areas many process are followed as this is more driven by market driven requirements engineering and currently there is very little of scrum done in the requirements engineering The article is a case study on software product management described in the reference 3 proposes the use of an agile SPM method based on SCRUM It also prescribes the use of two different sprints one for requirements engineering and the other one for development engineering  Figure 13 Agile Software Product Management  38 Accept360  Organisation which belonged to Nokia smart phone division before the merger with Microsoft used the Accept360 tool as the key tool for requirements and release management The tool has features to support requirement engineering and also the software development engineering It is a web based tool which is supported in Internet explorer and Firefox browsers in Desktop computers  381 Accept360 Elements  Accept360 has defined elements for requirement and release management they are called Roadmaps module Requirements Module and Teams Module Roadmaps Module is used to for managing the release milestones and artefacts related to that Requirements module provides features to manage the contents of the releases Team module provides support to plan and allocate the resources for working on the planned contents of the releases  382 Accept360 Team Element Ranking Tab  Ranking tab contains user interfaces for managing team sprint backlogs  It has the team element for the planning and managing the sprint backlogs The ranking tab has two tabs the Backlog pane and Sprint Pane The Backlog pane lists the backlog items for the project which could be easily dragged to the Sprint Pane which contains the list of selected backlog items for the selected sprint  383 Accept360 Recommended Practices  For setting up the Accept360 for an application it is recommended that the contents added for any application follows a certain recommended format it is recommended that the backlogs are added in a standard hierarchy and the structure of the contents is recommended to be as in the Figure 14 Each application has a folder with the application name under which sub folders are created for each version of the application and application driver if it exists   Figure 14 Accept360 Structure in Practice  384 Accept360 Responsible Actors  As explained in the Figure 15 at every stage of the project the contents needs to be updated in the Accept360 by the program manager or the project manager at some stage or by both of them to together And it emphasis that he Project manager has the overall responsibility to ensure that the contents in the tools are up to date  Figure 15 Accept360 Actors  385 Accept360 Agile Task board for Scrum features  Agile task board provides the features that are needed for supporting the task planning by the scrum teams Accept360 tool has support for complete scrum process Key components that are used in scrumming with Accept360 are Feature Subfeatures Stories and Tasks The Product owner creates the Feature SubFeature and Story contents The scrum master has access to Stories and Tasks During the Sprint planning the stories are pulled for each sprint and the Tasks required to complete the stories are created by the scrum master after discussing with the team members And the Developers have access to update the status of the tasks  Feature in the Accept360 can be compared to the Theme in scrum terms but more equivalent in usage is an application as a whole is represented as a feature and the SubFeature as Epic which is a list of all the requirements for the application When the subfeature is drafted it is then proposed for implementation and stories are created and then the stories are assigned to the teams as sprint backlog items which then gets a story point assigned for it For each stories the tasks are created and is assigned to the team members Below table represents the lifecycle states that are available in Accept360 and only some as states are used in defining the states of the requirements in the ASW projects  SW Lifecycle  SW  Draft  Initial status when a new item is created first time not yet ready for further actions and not yet in any backlog with schedule  Proposed  NOT USED  Candidate  NOT USED  Committed  Development team has committed to deliver item by Planned Delivery Date for certain programs  Implemented  Item has been implemented and tested by the responsible team and given to the integration team  Done  Item is ready ie it has been implemented tested and integrated  On hold  NOTUSED  Rejected  Item rejected  Table 1 Requirements LifeCycle  386 Accept360 Kanban features  Requirements can be synchronized from Accept360 Team Backlog to Backlog column of the Kanban Board sorted by Rank order From there it is possible to prioritize items ranking is updated to Accept360 UI accordingly see whos responsible Track changes Balance workload and limits also it is possible to configure team development states as needed each column can be bind to Accept360 Lifecycle progress can be also followed in Accept360 it is possible to set WIP limits for each state to avoid unfinished tasks and visualize the bottlenecks emphasizes to have a work item 100 done instead of having many 80 done is has to be noted that current Accept Kanban Board version supports only Requirements management not Tasks  387 Accept360 Nzilla Support  Accept360 supports importing Open bugs from Nzilla to the corresponding team It helps the development teams to give relevant information from Nzilla about the bug to enable them to Rank the related Bug against other Stories on the Team Backlog It is a Oneway integration and Defect master always in Nzilla Nzilla enables Lifecycle mapping and synchronize between tools to improve visibility To integrate Nzilla with Accept360 a separate structure has to be created in Accept In scope Programs and Component level automatically Synchronized using Nzilla Database IDs Any name changes will be automatically updated this stop the solution breaking and make deployment to teams easier Only downside is there will be some extra Bug Bags and folders for teams that may not yet be using the solution  39 San Diego Team Practices  The teams in San Diego had been able to prove that they have standard way of usage of the Accet360 tool to visualize the needs and progress of the project in every stage of the project Here is the snapshot of the way the team has defined the subfeatures which includes a clear steps and functions that needs to be done in order to execute a project starting from the initial prototyping until the planned feature or application is available in the market  The items in the Requirements module have the prefix to each item which explain the common activity that is need to be executed in the project and also there are items that explain the main stages which a project would go through so it helps to visualise the project situation with the items in the Accept360 tool  Goals of every functionality are also clearly included in the list of subfeatures and stories The team follow the template available in Accept360 tool below is the screenshot of the template  Also the teams follow the mixed Scrum and Kanban methods The teams have sprint planning Daily scrum meeting and Retrospective meetings And meetings are helped whenever it is appropriate and needed and the team has a regular practice of keeping the Accept360 updated always  310 Making Most of Scrum and Kanban  As the saying goes No tool is complete and No tool is perfect it only depends on how it is used this applies to both scrum and Kanban The value of a tool is that it limits the options A process tool that lets user do anything is not very useful this process could be named “Do Whatever but having a “Do The Right Thing process is guaranteed to work  Scrum and Kanban have both of their own pros and cons so it is good to understand the difference between the two tools so that we could make the best use of both the tools  Difference between Scrum and Kanban  Scrum  Kanban  Time boxed iterations prescribed  Time boxed iteration optional Can have separate cadences for planning release  and process improvement Can be event driven instead of time boxed  Team commits to a specific amount of work for this iteration  Commitment optional  Uses Velocity as default metric for planning and process improvement  Uses Lead Time as default metric for planning and process improvement  Cross functional teams prescribed  Cross functional teams optional Specialist teams allowed  Items must be broken down so they can be completed within 1 sprint  No Particular item size is prescribed  Burn down chart prescribed  No Particular type of diagram is prescribed  WIP limited indirectlyper sprint  WIP limited directly per workflow state  Estimation prescribed  Estimation optional  Cannot add items to ongoing iteration  Can add new items whenever capacity is available  A sprint backlog is owned by one specific team  A Kanban board may be shared by multiple teams or individuals  Prescribes 3 roles POSMTeam  Doesnt Prescribe any roles  A scrum board is reset between each sprint  A Kanban board is persistent  Prescribes a prioritized product backlog  Prioritization is optional  Table 2 Difference between Scrum and Kanban  311 Scrumban  Scrumban is a result of the thinking to get most value of the scrum process like Kanban Scrumban is a pullbased system where the team no longer plans out the work that is committed to during the planning meeting and instead continually grooms the backlog The same Scrum meetings can and should still take place but the cadence of them can be more contextdriven The real key factors for Scrumban though is ensuring that work in progress WIP is still limited  Figure 16 Scrumban Sprint and Value Stream  Scrumban works with the Workinprogress limits not Sprints With Scrum the amount of work that is ongoing is limited by the Sprint time commitment But in Scrumban with no specific time commitment the team must limit itself through the use of WIP limits on columns within their task board The goal is always to move tickets in a flow from left to right on the board If too many issues are in progress the team is at risk of not finishing anything to high quality standards Instead there should be a maximum number of tickets allowed per column If the number of tickets in that column ever exceeds the maximum the entire team should swarm onto that column and help move tickets on This should happen no matter what functional role a team member fills  Scrumban still supports the scrum ways of having the meetings and time boxed deadlines for the release of the features planning meetings should take place as often as they are needed When the team is unable to regularly pull stories off the top of the backlog at their normal pace a planning meeting is necessary Review meetings helps to improve the way of work with feedbacks Reviewing work with clients and customers is the only way that development teams can get the feedback necessary to properly adapt what they are working on Clients tend to prefer that these are held at a regular cadence  Retrospective meetings These can vary when held but a general rule of thumb is to hold a retrospective after every review This is the most useful part of the Agile process and should be given the proper place for that  Daily standup meetings in the Scrum world follow a simple pattern The team takes 15 minutes and each person says a what heshe did yesterday b what heshe is working on today and c what is blocking any of that work In practice this boils down to redundant statuses that recount information available on the teams task board For Scrumban a more effective method is to refocus on the flow of tickets on the board That same pattern of yesterdaytodayblocked can be transferred to the tickets themselves—the group moves through each column and briefly discusses each ticket and what is necessary to move that ticket rightward on the board This provides far more context to the team and informs every one of any major architectural or design decisions  Metrics can certainly be useful it interprets the complex process that is going on in the team it is not just a single value or number to be expected to know how the team is progressing and it could help visualize the situation the team is going through on the way to achieve the target The term Velocity the amount of story points a Scrum team completes in a single Sprint is such a metric that incentivizes lower quality at the end of a Sprint as a team scrambles to finish every last story they committed to When the number fluctuates as is common with a newer team the stakeholders begin to question the outputs of the team and even the effectiveness of Agile itself In Scrumban the metric is cycle time instead of velocity This is the length of time a ticket takes to complete measured from when it is first began Over time a statistical analysis of all tickets in the project can yield a mean cycle time and standard deviation This can be a useful planning tool at a macro level as it is trivial to add up the number of stories and multiply by mean cycle time  312 Summary of Best Practices  From the study and discussion about the best practices it is found that the scrum method is used mostly in the software development teams as it is very good for planning and scheduling meetings but it also recommends that the sprint has to be pre planned and should not be interrupted in between and the backlogs need to be reset which makes it to a situation that we cannot keep more buffer for backlogs to be tried and everything need to be reset at the end  Next that Kanban is used most widely in software development organizations which works with Work in progress limits are the visual indicator for the execution and planning of activities And also recommends that that designing of the Kanban and steps for designing should be followed which if not followed would result in a situations that he project situation could not be in control  Next the thinking of making most out of the Scrum and Kanban has been received as a good approach to make use of good things in both the methods and which is best suitable for the needs in software development organizations this could well be implemented with the techniques defined in the Scrumban  Next the hybrid project management models is a good method which gives a way to have the traditional way of software development methods and also have scrum included but at that same time this does not address the needs that the planning need to be dynamic not only the execution is dynamic so to have agile mode of working for planning the requirements and developing the requirements that agile product management method is very suitable  Next the study of the Accept360 tool details that the tool has a lot of features in it but it is built in a way that the tool need to be adapted but adding more plugins to it for each feature to be able to utilise it more efficiently   From the study of usage of the Accept360 in the team in Sandiego it is found that the content what is used to describe the project requirements or the backlogs play a key role in visualizing the project status and also it make it clear that defining the contents of the backlogs and work items in a standard way it is done in the team would make it more helpful to know the needs and status of the project easily  4 Current State Analysis  Current state analysis was done in the teams in Espoo  two teams were analysed and in each team the Product Manger Project Manager  Development Team Lead and The Quality Lead were contacted and interviewed on the following topics Project Details Team Information Agile PracticesProject Planning and Execution InformationProject SituationsSuggetions for desired Improvments in Accept360 tool The interview topics and questions are listed in the Appendix 1   41 Analysis of teams in Espoo  411 Analysis in Team X  Team X is working on creating a new version of an existing application so this application is expected to replace the old application mainly with the new user interface and improved usability of features the development phase of the application would need a complete cycle from proto typing the new user interface and adding all the functionalities to the new applications This means that the efforts needed for developing the working application could be estimated to an appropriate value which would help define a process to follow  The team uses the scrum process for planning and execution of the activities Accept360 tool is used during the sprint planning and review meetings UI specification document and the story description in the Accet360 serves as the documentation needs of the projects as they are updated with appropriate information UI Design team shares the updates and synchronize their work for their sprints with the spirits of the development team sprints and meetings Also a short description of key features of the application is documented in share point documents management page  Team follows two weeks sprints and at the end of the sprint there is a demo and preplanning session for the next sprint Releases and Testing are done weekly during the project execution stage  Team uses the Agile task board in Accept360 for managing the tasks also the Nzilla is used to check the issues and bugs are also worked on as tasks There are no daily meetings in the team it is expected that each team member is located in the same location and has to contact the development lead or another team members to know what has to be done and what is expected  The discussion with the key stake holders of the project the Lead Program Manager Project Manager Lead Developer and Quality lead is available in Appendix 24  412 Analysis in Team Y  Team Y is working on developing value addition features for the existing application the Team includes many small teams every feature addition and implementation is initiated whenever the decision to have the feature in the application is accepted and is prototyped by one of the small teams and is integrated to the application  Team uses the One Note as a tool for planning and updating the backlogs The release plan requirements list and status is tracked in OneNote pages Accept360 is not used for these purposes the reason explained was the releases happened ever month and the features are released in very small time  Team uses the UI specification as the document to know the details of the projectAccept360 could not be relied on for the details of the project as it is not fully updated all the time  Sprints are mostly weekly sprints sprint meetings are held every week and Adhoc meetings are organised whenever there is need At the end of every week an open agenda meeting is held which is for having an open discussion about anything related to project and or give presentation about the progress Testing actives are done on weekly basis  The team had previously tried using the Kanban for their planning and scheduling before but it was not very helpful as they did not have control of the activates and work items all the time as it ended up in just having a board with full of task cards and nothing going in planned way and things went just out of hand but what it is important to note here is the Kanban followed was not a designed to be adopted for the overlaying process in the organization  The discussion with the key stake holders of the project the Lead Program Manager Project Manager Lead Developer and Quality lead is available in Appendix 58  42 Summary of gaps between best practices and current practices  One of the main aim of the Agile methodology is to have clear visibility of the project at any point of time so that any kind of resources are needed deliver that project as planned are addressed at the earliest this could be achieved only if the teams follow the prescribed practices of the standard methods they adapted to follow or follow a methods which would not prescribe more rituals but still is would make the situation more visible to all the stake holders But currently the teams do follow standard methods but skip some of the recommendation in the methods  Though the teams are very dedicated to deliver the products and functionalities the team members are not interested to follow the best practices and tend to work in more selfcentred way because of which certain practices which would help all the other team members to perform well are not taken in to consideration The teams have their own way of working than following the standard practices in the way of working  From the study it is found that there are some practicalities that are not followed in the teams a clear understanding of responsibilities and who is responsible for updating the tool is missing This is also obvious from the contents in the items in the Accept360 tool which is not up to date for some projects  Also there are some needs that are missing in the current processes and tools for example the Product backlogs have to be synchronized with the tasks to add the tasks for the sprints there need to be a story to add task but in practice the development teams have more tasks that need not necessarily have to be a requirement backlog item but is needed for their day to day activity The requirements engineering and the software development engineering are in this way tied more closely in the tool Even though there need to be synchronization between them the tasks in the development engineering team need to be created according to the development activities to get the requirements implemented this would need different terminologies in defining the tasks that would not fall into different the category of work items and tasks definitions  The Agile task board is updated only at the end of the sprint which explains the board is not actively used in the team the reason for this found from study is that the way of working is not the same in the flow of states in the task board also there are tasks which could go longer than the sprint time  but in scrum after every sprint they had to be reset and added to next sprint or put as impeded which would give a picture that there the things are not going well but in general there are some dependencies that the tasks need to stay between the sprints for it be completed in a better way it is because the order in which the task need to be worked on changes that some tasks need to be put on hold for some time waiting for other tasks to be complete or started As the sprint is time boxed and needs a reset of the tasks and backlogs for the sprint after every sprints it leads to a situation that the tasks are updated in a way that their status is not related to actual status and is updated for the purpose of resetting the sprint backlogs which scrum recommends So there is a need for other methods and tools like Kanban which would help the developers to keep the tasks across sprints without resetting it at the end of each sprints But it has also been experimented in the Team Y that Kanban method alone was not helping enough as there is a need to time box the release of features at some point of time and the releases are tied to the release of the devices to the market this needs some kind of iterative approach as well for planning the activities hence there is a need for using some of the features from the scrum method and Kanban tool to get the needs of the project execution addressed completely  5 Proposal  51 Theme Proposals  As summarized in the best practices and current state analysis the key areas where the gap exits in knowing the status of the project is how the contents of the project planning and execution tool are defined and used the difference in work items needed for requirements engineering and development engineering and difficulties in combing the time boxed sprint development method with the continuously improvement and integrated development mode of working These gaps could be address better by the themes that are explained in the below subheadings which would help fill the gaps that exists and help improving the visibility of the project status to all stakeholders of the project  511 Usage of Structured Contents for Work Items  Work Items as explained in the best practices are the items like epics stories tasks mmf and activities The description of these items could serve as documentation for the project and this could include details about goals features of the product being developed All the work items would fall in one of the categories of the activities that are commonly needed to create the end product like Functional requirements Non Functional requirements User interface design and development Core technology implementation and interfacing Performance validation and improvements Testing and Error correction The organisation in case company in which the research is being done has a recommendation and guide for creating and maintaining the work items The guide provides information about required structure for defining the work items naming and versioning conventions avoiding duplicates in contents this guide could be used as reference to maintain the contents in a standard way The guide also includes details about environment needs to use the tool such as browser requirements for the tool plugins needed to have a better performance of the tool and it includes a template which explains the backlog creation ethics  Initiatives to use the guidelines and keeping the work items updated would help improve the visibility of the project status so that by looking at the content of the subfeature stories and tasks it is possible to have a clear picture of the situation of the project For example the Team S in Sandiego have the contents defined in a specific way they follow different syntax for defining the work items so that when looking at the contents in the Subfeatures Stories and Tasks it is possible to understand the stage of the project So keeping the contents more meaningful in the recommended standard way could make the visibility of the project situation more clear to all the stake holders involved  For example the Figure 17 show the sample content from the best practices studied in the Sandiego team this could be used as reference for creating the items for the scrum team backlogs The template has suffixes such as Story  Demo story UI design Miscellaneous UXUI Spike Automated Testing Exploratory Reliability Compatibility Interoperability Non Functional Certification Testing and other stages the project need to carry on for finally delivering the project These suffixes would give more information about what is planned and being done at that stage of the project so this helps to have more details of the activities carried on in the project and it would help to provide needed resources for those activities based on their needs  Figure 17 Template for Work Items in Scrum team  512 Usage of Agile Sotware Product Management  The software product management SPM as explained in the reference 3 proposes the use of scrum in product management It also prescribes the use of two different sprints one for requirements engineering and the other one for development engineering  For a scrum process the backlogs are the key instruments and there are two different kind of backlog items involved in software development the Product backlog which is the prioritized list of requirements that includes all relevant needs of the product that is being planned to be developed Some of these Product backlog items are then copied by the development team as their sprint backlogs called Development Sprint backlogs which are then split in to stories and tasks  Though there is a tight dependency between the two backlog items the backlog items picked up by the development teams are reset every sprint and different tasks are created newly Hence there is a good need that these two backlogs items are used differently by the software management teams and the software development teams Currently scrum is followed only by the development teams so as explained in the reference 3 the article suggests the use of agile scrum methods in defining the Product backlogs would be more efficient to manage the change and synchronize the changes and progress in both requirements engineering and development engineering  From current state analysis interviews in the case company  it show that the existence of the difference in requirement engineering and development engineering needs for managing the backlogs even though there has to be synchronized usage backlogs between these two teams their functional needs demand the need for having two different sprints or cycles for the same product  513 Usage of Scrumban  The requirements engineering teams which involve the project management and planning requires clearly what is prescribed in scrum the sprints roles and ceremonies but at the same time the development engineering teams need a stream which is more like Kanban to keep their flow of work steady and visible so Scrumban which recommends work pull system which is a scheduling method based on demand and at the same time recommends to have the scrum way of planning and checking the situation by the aid of meetings would best suit the needs in the organisation in study  There are also many teams involved in one project and each team have their own planning and execution cadence the Figure 18 is a sample framework which explains the involvement of different teams with different work cadence defined for the same project and more relevant for scrumban method The frame work is also based on the concept explained in the reference 9 which is good for complex implementation Currently there is only one cadence which is defined for the development team and other teams like requirements management and UI design teams which are very tightly working with their deliverables as input to the development engineering team do not have a designed cadence for delivering their deliverables and they are provided in a dynamic and not planned way so defining a cadence and process method for all the key teams that are involved in the project would make the interaction and situations much clearer and easier for planning and executing the activities of the development team and for the whole project as well  As an example there are three teams in the sample framework first team is the team which is the key team that creates and defines the requirements of the features of the product being developed the product needs several other teams also to be involved very closely to get the product developed fully the development engineering team and the UI design teams are the other key teams which work on the creating the product as in the Figure 18 the Requirements team follow the scrum methods they define and provide the backlogs for the other teams and also schedule the planning and status meetings which could for example happen in a three weeks cadence and the development team have a cadence of two weeks for releasing the working software product and the UI design team works in flexible cadence of delivering the UI design document whenever there is a need for new method of defining the user interface all these three teams has to work independently but still depend on very closely on each other so having a framework designing like in the sample framework in the Figure 18 would help planning and coordinating the teams in better way also this would the help visualize the dependencies on deliverables between teams and define them in a structured ways and this would help all the stakeholders involved in the project to visualise the status of the projects more clearly  Figure 18 Sample Framework with Scrumban Method  In Scrumban the story and tasks card need to be built and maintained appropriately otherwise it would not yield any value as in the Figure 19 below the story cards or the subfeatures in green are the items which are worked on by the requirement management team which mostly involve the project managers and the development team lead the stories are selected at the beginning of each sprint and story board is reset at the beginning of each sprint  Figure 19 Sample Scrumban Board  Scrumban for the development team works with the Workinprogress limits not Sprints With Scrum the amount of work that is ongoing is limited by the Sprint time commitment But in Scrumban with no specific time commitment the team must limit itself through the use of WIP limits on columns within their task board The goal is always to move tickets in a flow from left to right on the board If too many issues are in progress the team is at risk of not finishing anything to high quality standards Instead there should be a maximum number of tickets allowed per column If the number of tickets in that column ever exceeds the maximum the entire team should swarm onto that column and help move tickets on This should happen no matter what functional role a team member fills  For the Software product management Scrumban still supports the scrum ways of having the meetings and time boxed deadlines for the release of the features planning meetings should take place as often as they are needed When the team is unable to regularly pull stories off the top of the backlog at their normal pace a planning meeting is necessary Review meetings helps to improve the way of work with feedbacks Reviewing work with clients and customers is the only way that development teams can get the feedback necessary to properly adapt what they are working on Retrospectives meetings could be held when needed or in regular intervals so that team has an opportunity to give feedback about whole process involved and give input about what changes they want to have to in order to improve the Efficiency of the team  Scrumban can restore working time to the development team and avoids unnecessary meetings And most importantly it can limit the teams work in progress so that they can finish what they start to a high standard Scrumban can remove overhead stress for the development team increase efficiency and increase the overall satisfaction  52 Summary of Proposal  From the study it is understood that the usage of Accept360 tool for day to day activities of the project execution is not preferred due to the fact that the tools is very slow for accessing and updating the content The tool is more suitable for planning the releases and maintaining features for devices globally but it does not address the need for the software development projects that has quick and dynamic life cycle But as long as the tools is used and since the tool supports many features of the agile development methods the guides and training documents available for the tool are created to address the process followed in the organisation in case study over years following the guidelines in the same way it is recommended would help the teams to use the tool more efficiently for planning and execution also would help all the stakeholders involved in the project to have the clear visibility of the project situation at any point of time   From the current state analysis it shows that there is interests to use other methods and tools in addition to methods and features supported in the Accept360 tool and Accept360 is considered to be helpful more for requirement management teams rather than for software development teams so it is good if the teams could make use of other tools suggested by the stake holders interviewed for current state analysis tools like scrum works pro which already has synchronization with the Accept360 or the teams could try the tools like Jira and Version One for their activities and synchronize their sprint backlogs with the Product backlogs in the Acept360 but it is important to mention here that for synchronizing backlogs with other tools the efforts and resources needed would be more Hence it is good to further research on what tool could serve more good for the organisations need  Since in the organisation in case study  there is more clear work type splitting of requirements engineering and software development engineering the Agile methods in requirements engineering would provide more visibility in project situations and requirements creation and grooming would be carried on in more structured and dynamic way  Also from reference 8 and other online research papers recommend the usage of both Scrum and Kanban together for having more visibility of the project situation and progress It would increases the productivity and efficiency of the team  6 Conclusions  61 Summary  This chapter gives the general overview of the thesis it is a look through of all the steps done in the research process Validate the research by comparing the objective with the results how applicable and generalized are the results in another context And also check how reliable the research process is if the methods and materials used in data collection and analysis are appropriate and whether this research could be carried on further by another person in the same way it is done currently  62 Practical Implications  The thesis was aimed to study the gaps that exists in current practices in software development methods and tools in the organisation in case company in comparison with the best practices in agile software development methods and tools that are available from different sources and summarised in the literature part of the thesis and to propose the solutions would fill the existing gaps and would provide the ways to improve the visibility of the situations The current state analysis was done by discussions with many individuals in different kind of roles in three different teams These discussion turned out to be a great source of knowledge and insight into the organisational process mainly it gave an insight and good understanding of project planning and execution methods Everyone involved were in highly responsible roles hence the discussions were more clearly depicting the actual responsibilities and needs in a project which helped to frame the proposals that would address the needs to fulfil their responsibilities in an improved way and help all the key stakeholders in a project to have better visibility of the project status  The research was mainly based on the current ways of working current issues and needs that were desired by the team members The proposals are improvements that are needed in three different area of project planning and execution methods and tools and proposals are framed based on the research articles in the references The feedback about the proposal was very positive that people involved in the requirement management showed interests in the proposals and they wanted to have similar practices proposed in this research  The current research focus only in finding the gaps that exists in the methods and tools and proposes the key ways that could be used to improve the visibility of the project status and the research proposes the sample approaches that could be tried to have better visibility of the project situations which also help in reducing the delays in the project deliverables The proposed sample approaches need to be designed further and piloted according to the needs in the teams in the organisation under case study  63 Evaluation  631 Objective Vs Outcome  The main objective of the thesis was to find the ways that would improve the visibility of the project situation to all the stakeholders involved in the project In the study three main proposals were created to improve the visibility of the situation First proposal is to improve the contents in the requirements management tool so that the contents have the description of what is being done and what is the goal and outcome of the work item for each requirement Second is to have a separate scrum process for requirement engineering which is called Agile Scrum Product Management which lets the backlogs to be engineered with a process rather than continuously so that they are created with better clarity rather than in an Adhoc way which definitely would give clearer picture and estimate of the project situations Third proposal is using the Scrumban which is having a scrum process for main process of requirements engineering and the other engineering teams like development engineering teams would work in accordance with their requirements from scrum team with a key control agent as Work in progress limits The development engineering teams would have a Kanban board designed in a way that their work flow is checked by work in progress limits and is not reset every time the scrum sprint ends in this way the actual situation of their work progress could be seen in the work item flow and is more realistic The outcome of all the above three proposals would create a situation where the process and the all the stake holders in the project are more closely involved which would eventually improve the visibility of the project situations Reliability checking for the thesis is done mainly based on the validating how well the methods and the materials used and well the outcome addresses the objective of the thesis The Methods and materials used in this thesis is mainly from the referenced articles and books and the teams and people involved are from the organisation in which the case study is done The objective and the outcome of the study are closely related to the real situation and more relevant to the study topic In the literature part contents are designed in a way that the agile methods and practices are highlighted and explained in detail that this report would serve the purpose of referencing as a guideline to design and improve them Also current state analysis were conducted in a more generalised way with more generic question about project planning and execution so that the inputs from those interviews would give a generic overview of the process and the needs for improvements in the project in this way this study has initiated a research which would further be continued in an elaborate mode for example by clearly designing the scrum team in accordance with the recommended scrum ceremonies and designing the Kanban as per the needs of the project and apply them in scrumban method and validate the same with experiments in more teams piloting the proposals are some of the key research areas for future studies  The thesis is validated using following methods the Triangulation Prolonged engagement in the field and Member Checking further in this section Also the answer to the research question “How the visibility of the project situation could be kept updated at any point of time “What are the best ways to stay on track of the current situation of the projects and so be able to manage the issues appearing earlier and minimize the delivery slippages Is answered in the proposal section and is addressed completely Hence this states that the research has a consistent and logical approach in providing what is promised in as objective  Triangulation is a validity procedure where the researcher search for convergence among multiple and different sources of information to form the themes and categories in the study As a validity procedure in this method the researcher uses a systematic process of sorting the data to find themes by eliminating the overlapping areas In this methods the researcher rely on multiple evidences rather than single incidents or data point in the study The data collected include three different teams and ten individual discussion also the best practices were studied from several articles company guides and books  Next method that is used for validating the study is Prolonged Engagement in the field since the thesis was carried out in the same organization where the researcher is working since many years and one of the teams which were involved in the study is where the researcher worked at the time of study for more than seven months  as Fetterman 1989 contends that working with people day in and day out for long periods of time is what gives ethnographic research its validity and vitality p46 the study has more vitality  Member Checking method of validation involves getting feedbacks from the participants The feedback from the participants who were key stake holders in the organization in the case study and the Supervisor of the thesis who is in the role of the Operations Manager of the organization under case study were collected The participants feedback states that the study was very useful in giving details about the agile methodologies and best practices in more elaborative way The feedback from the supervisor was that the discussion and contents in the thesis were useful in understanding the current practices and also the proposals in the thesis are closer to the changes that were expected by many key stack holders in the organization and was helpful to align and improve the current practices for addressing current key issues Hence in the thesis the qualitative method used for data collection has provided with more valid information of the current state the data from the discussions are valid and used to build the proposal The research does not contain any personal opinions of the researcher but is fully based on the data collected from the key stake holders in the organisation and the articles and book in the references The researcher has used the experience in the organisation to be selective in collecting the data so that the data is more relevant for the research which has brought very good result in proposing the solution framework 
