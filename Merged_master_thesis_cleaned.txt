In today’s world, organizations in different branches are using more and more agile ways of working. As the operational environment is constantly changing and organizations are forced to keep up the pace to stay alive, they might not be able to survive by following only the old inflexible methods. However, thorough consideration and preparation needs to be done before changing into agile. In many cases, organizations are so used to follow traditional models, such as waterfall, that they do not realize that the organization itself needs to be changed as well, not just the method they are following. The number of agile pitfalls organizations are facing is endless, but there are a lot of same mistakes many organizations are doing one after another. These common issues are the most interesting ones and therefore highlighted in this thesis. In this thesis, the most common pitfalls of agile software development are investigated and suggestions how to avoid them are introduced. The thesis is not related to any specific organization or technology but common issues identified by having some informal interview discussions. First, a preliminary literature was written in order to have a hunch on common issues, before starting interview discussions and preparing current state analysis. Based on current state analysis conclusion, topics for the literature review were identified. After literature review, initial proposal for tackling the most common agile pitfalls in advance was prepared and validated by agile professionals. These agile professionals were partly representing same persons that were interviewed for the current state analysis. Finally, after initial proposal was validated, the final proposal was written. The topic for the thesis was decided based on author’s own passion and interest. The author has been working as a scrum master and wanted to gain more knowledge in order to develop the use of agile methods in her own job. She had experienced a lot of positive implications because of agile way of working instead of traditional methods. However, she had faced also some severe issues and wanted to drill down to learn whether other people are having same experience and how these could be avoided. This thesis is not built around any case organization and therefore people interviewed are representing couple of different organizations. Interviewed people were chosen based on suitable background and their willingness to participate and they are all having agile experience. Though the thesis is not done to any specific organization, the outcome of it can be considered as a checklist for any person or organization that are either planning to go agile or already are using agile but facing issues and would like to improve way of working. The business challenge of this thesis is that managers in software development adopt agile as some sort of cure-all without consideration to the challenges that are likely to be encountered for this particular field of work. The business challenge is not related to a single organization but common issues. The objective of the thesis is to develop a checklist, how to overcome issues in agile software development. Target audience for the checklist are people like the author; individuals who are using agile methods in their job and would like to improve the way of working to embrace agile benefits. However, the checklist could be useful also to persons and organizations that are only planning to go agile. The output of the thesis is a validated proposal in a form of a checklist, answering to a question how to overcome some of the most common issues in agile software development. By taking the checklist into a consideration when planning to go agile, organizations can avoid the most common agile pitfalls. As the use of agile methods has been a rising trend in many organizations in all branches and not least in the software development, agile pitfalls is very actual topic. Despite the popularity of agile, surprisingly many organizations do not familiarize themselves with careful preparations but are getting an illusion that agile simply means lightening or even skipping the planning and project management tasks. Software development is demanding and there any many possible stumbling blocks that are not fading away by just saying that traditional methods will be replaced with agile. Agile methods are not curing all the problems and not leading to a successful end without seriously going into it. The output of this thesis should help organizations to understand the pre-conditions of agile and things to consider before going agile software development. In the next chapters, first the research method and material used is explained. Then, the summary of the preliminary literature is written, following by the current state analysis. After and based on the current state analysis, the conceptual framework is introduced. Last, an initial proposal and its validation is described ending to a final proposal in addition to conclusions. This chapter describes the research design and data collection methods. Qualitative research method is used due to its suitability to the thesis. In addition to the current state analysis and literature review, also preliminary literature review is done to gain a hunch of the current issues. The design of the research process is illustrated in below figure. First preliminary literature review is carried out in order to get a hunch of the most common issues in agile software development. Though the issues that are collected from the literature are not exactly similar to the ones identified based on interview discussions, they are still directional and a good starting point. In the literature, issues are introduced from all over the world, from different kind of organizations and different technologies. Most of all, the issues in the literature are mainly more generic compared to the ones identified by discussions with individuals. After the preliminary literature review, the current state analysis is drawn up based on informal interview discussions with people involved in agile software development. Current state analysis is introducing the current strengths and weaknesses of agile software development. Interviewed people are representing scrum masters and developers from different organizations. In the next phase of the thesis, a literature review is done; the main concepts related to the summary of the current state analysis are explained, such as agile software development, scrum, traditional software development, waterfall method, differences between agile and waterfall, change management and agile transformation. The literature review is targeting to conceptual framework that will be a base for the initial proposal, a checklist how to overcome most common issues in agile software development. Initial proposal is validated by couple of the interviewed persons; the initial proposal is fine-tuned based on their comments and the outcome is the final proposal. When considering the validity of the research process it can be stated that above mentioned was valid for this case because there was no case company involved. Also, the subject is so new and broad that discussions instead of a questionnaire were more suitable. Data collection for data stage 1 was done via informal face-to-face discussions with people involved in agile software development. With some of the people, discussions were not just one-time but continued couple of times. Originally the purpose was to have few more discussions, but it became obvious rather soon that the answers were started to repeat themselves. Hence it did not make sense to continue discussions. There were total five people discussed with, representing both scrum masters and developers. As illustrated in below picture, four scrum masters and a developer were interviewed, from couple of different organizations. Discussions were done informally and incognito in order to get honest and independent opinions from people. Field notes were done by the author to record the discussions. Data was analysed by picking-up the main points from the answers and to coming back to those in cases where it was not clear enough what the interviewee was trying to say. All the interviewees were having their own point of view, a very unique way to express things and hence it required some analysis and re-discussions to be able to crystallize the main points. After the main points from the answers were picked-up, they were categorized under few topics to be able to identify the areas of issues. This was helping to understand the big picture and the areas where the biggest issues were lying. Also, the identification of the literature topics was much easier after the categorization. As shown in below table, data stage 2 was done by introducing the thesis as a whole and especially the initial proposal to two of the interviewees participating to data stage 1. Informal discussions with two individuals were done and the author prepared field notes. Their comments and suggestions were taken into account when the final proposal was prepared. Comments and suggestions were compared to the theory of the thesis and the initial proposal to figure out how they could be put into practice and fine-tune the initial proposal. In this chapter, findings from the preliminary literature are introduced. The purpose of this chapter is to gain preliminary information before starting the interviews and current state analysis, to have a hunch of the most common agile issues. In the study of Gandomani, Ghani, Ziaei and Zulzalil, (2013), the obstacles and issues in agile software development are categorized under four themes; organizational and management related challenges, people related challenges, process related challenges and technology and tools related challenges. Many of the current challenges are stem from the culture and structure of the organization which is serving needs of traditional methods. Organizational culture is affecting to agile transform. Organizational culture is a vague term covering numerous things such as prevailing attitudes, norms and values (Iivari & Iivari 2010). Gandomani, T. et al. (2013) are using a term “The agile transformation process” when discussing about organizations moving from traditional methodologies into agile. Organizations are often making a mistake by underestimating the difficulty of the agile transformation process and not investing it; this is making challenges even more difficult. Organizational issues in agile software development are coming from too narrow thinking of the meaning of agility. Organizations are often stating they are agile though it usually means only software development. The software development is failing in agility in cases where the organization around it is not agile enough. The software development projects and teams cannot fully use their agile potential unless the organization is not supporting them and getting rid of traditional thinking and old habits. When the agile software development team is lacking agile support from their organization, it tends to lead situations where people are not feeling safe to share identified issues and mistakes; this is reducing agility and impacting to end results (Gothelf, J. 2014). According to Moczar (2013), agile is promising too much when stating that it would be a solution to problems faced with traditional methods; Moczar (2013) has identified several times that agile is partly falling to same issues than with other methods. Organizations are counting too much on pure agile method and forgetting the importance of agile thinking. In cases where only the agile method has been followed without changing the mindset as well, it has sometimes leaded even to bigger catastrophes than by using traditional methods and changed the good intentions totally upside down. One of the common issues is that organizations are not considering carefully whether the use of agile is worthwhile (Moczar, L. 2013).  Since agile is all about people, people related challenges are playing a significant role especially in cases where the organizations have earlier been using traditional software development methods. One of the common people related weaknesses is the difficulty for people to change their mindset and behaviour into agile mode. During agile transformation, there is not always enough training and coaching from agile expertise though it would be needed. People related issues are concerning both customers and vendors and both can have overwhelming impacts (Gandomani, T. et al. 2013). For instance, the agile principle of early and continuous delivery is sometimes leading too hasty outcome in detriment of quality. This principle is allowing developers to neglect to bugs. The consequence of too fast delivery might be the growth of defect backlog, ending up to excessive work (Moczar, L. 2013). The manifesto for agile software development is encouraging to “development over planning”. This has been often an issue though the original idea has been to make things easier. There are often issues because the size of the changes is varying from a tiny to huge ones. Though agile is welcoming changes even late in the development, it is still commonly causing problems because the development is constantly ongoing and there might be unsolved defects making it even harder to success in agile (Moczar, L. 2013). The plan to have a totally self-organized team without a project manager who would be responsible for the whole project is not always working as desired. What happens often is that the scrum master is forced to act as a project manager to keep things going on, but without a project manager mandate. For instance, the prioritization of the tasks to be done is an issue faced in the real world; often time-pressure is so high that an additional prioritization is needed. In practise, it is difficult for developers to manage all the priorities and dependencies by themselves (Moczar, L. 2013). The outcome of the preliminary literature review are some the most common weaknesses of the agile software development on a high-level. The weaknesses of agile software development are for instance: organizations are not agile enough and therefore not able to provide support for the agile software development teams people with experience on traditional software development are not able to get rid of their old habits and mindsets and preventing the successful use of agile agile processes are not properly used due to lack of agile knowledge When reading the results of the preliminary literature review, it needs to keep in mind that though the issues mentioned are partly similar than in the current state analysis, they cannot totally match due to fact that CSA is done by interviewing Finnish IT-professionals while literature is from the wider perspective. Still, the preliminary literature is providing a hunch, a useful overview. In this chapter, the most common strengths and weaknesses of agile software development are being introduced. The current state analysis is prepared based on informal and anonymous interview discussions. Based on interview discussions, the following strengths of agile software development were identified; intense and good cooperation, easiness to plan work in small pieces, possibility to correct mistakes rather easily and quickly, allocated resources, if preconditions are in place the quality is usually good. Though above mentioned are considered as strengths, they still cannot be taken for granted but can be achieved only by treating agile method with conscious. Agile strengths can turn to weaknesses in a quick manner if agile principles are not followed actively. First, people discussed with were having positive experience on cooperation and communication between different parties such as the project team and customers. Especially when sitting at the same premises and having extended face-to-face communication, the cooperation has been much more informal and therefore better compared to traditional approaches. Communication can be done without delays and so called Chinese whispers –effect can often be avoided, also threshold to open discussion is low. One of the scrum masters highlighted the easiness of the cooperation when all project members are sitting on the same premises; he had experienced that good cooperation usually requires people locating on same premises and as soon as part of the scrum team is located for instance in another country, communication gets poor. All interviewees mentioned good cooperation and communication as the most valuable thing agile can offer. However, they all had experienced the fragility of good cooperation, meaning it can easily be spoiled. This will be elaborated more in the next subchapter. Another identified strength of agile software development is the easiness to plan work in small pieces. This is a great advantage because the changes in the schedule and error estimates are not causing as much issues as with traditional methods. The so-called snowball effect can be avoided rather easily and the possibilities to adjust the overall schedule works better. One of the scrum masters stated that it is unrealistically to even think that all the smallest details could be planned in the beginning of the project due to nature of the software development and especially regarding bigger software projects. Hence, he appreciated the possibility that agile is offering: to plan work in pieces. Third strength of the agile software development was identified to be the good possibilities to correct mistakes and bugs easily and relatively early. People were having unpleasant experience on traditional methods where mistakes are not often noticed until at the end of the project, but they considered agile way of working to enable faster issue fixing. People noticed that for example in scrumming, you are learning sprint by sprint and eventually be a master. The scrum master 1 was praising agile due to its mercifulness; in he’s experience, software development done by traditional methods is harsh and punishing people for all mistakes they are doing especially in the beginning of the project, when agile method is often offering a possibility to fix mistakes during the coming sprints. He’s opinion was that in agile software development; the learning curve of the scrum team members is much better because it is actually possible to learn by mistakes fast within the project and not only after the project is about to end or even finished. Allocated resources are also one of the agile strengths people mentioned. Allocated resources are a great benefit because they know the product that is developed but also other project members, enabling to proceed smoothly. In perfect situations resources are allocated 100% to the agile project itself, this is something that is unfortunately not always happening but when it does, it makes agile life easy. One of the interviewees, a scrum master, stated that everything is much easier by using agile because there are designated resources and they are mainly allocated to the same project. Despite all the strengths, there are also several weaknesses in agile software development, such as: agile methodology is used though there are not prerequisites lack of sufficient planning or documentation or testing too early delivery communication and cooperation issues due to resources located in different places issues due to cultural differences when projects are international resources not always able to concentrate 100% to agile work due to other responsibilities changes in staffing affecting agile projects heavier than traditional ones agile methodology and principles not known bigger risks to break existing functionalities because the big picture not always known due to constant changes done Three of the most common weaknesses are explained in detail in this chapter, though there is not much difference between the answers by the interviewees. Also, to mention, some of the weaknesses are almost overlapping. One and the most common of the weaknesses observed and discussed was that in many cases, all agile resources are not 100% allocated to agile work due to other responsibilities. This is causing delays to the development work and makes it difficult to plan schedules. Even one person with less than full-time allocation may cause tremendous issues. As the developer that was interviewed said, since things are unfortunately often depending on individuals, the non-attendance of even one person can spoil the whole thing and undercut the benefits of agile. Even too early delivery, meaning lack of sufficient planning, documentation and testing is also a big issue regarding agile software development. Some of the people interviewed stated this issue to be concerning the whole project, covering all the steps and starting from the project planning; they felt that in some cases, the project team thought that the use of agile would justify defective quality. Though agile is encouraging to iterations and welcoming changes over planning, this was sometimes misused. When using agile, there is sometimes pressure to deliver outcomes earlier than what would be wise and realistic, leading to careless development and lack of proper testing. Especially lack of planning and documentation is sometimes making bug fixing difficult and causing too much dependency on individuals. Without proper planning, there are often conflicts between the development done by other people within the same agile team or even other projects. Poor planning is often leading to quality issues and bugs as well. In cases where also the documentation is negligible, the defect fixing is even more painful and time consuming. In addition, the software around is constantly changing, making it harder to identify the root cause for issues and corrective actions. The third biggest weakness discussed was the use of agile methodology without having preconditions to adopt it. People were having bad experience of projects originally planned to be done with traditional methods but for varied reasons the method was changed to agile; these situations were often leading to confused situation where agile method was supposed to be followed but the organization around the project group was not acting agile at all. Some of the people were considering agile as a trendy concept that is rather often used without really focusing on it and the conditions it is requiring. Typically, the thought is to run a project like with waterfall method but without any specifications and with minimal testing. One of the scrum masters was even having experience on agile team developers not at all familiar with the agile method itself, leading to waist of valuable time reserved for the development work. He used a lot of time during several sprints for teaching agile principles and scrumming to other team members. Strengths and weaknesses based on interview discussions are listed in below table. Interviewees were overall satisfied with the quality of work in agile projects. They all though in many cases, agile approach works better than traditional one. Due to designated resources and emphasizing the communication and cooperation, risk to fail is less. Especially good and intense cooperation and designated resources were appreciated. However, there are several weaknesses as well, such as all resources may not be 100% allocated to agile work due to other responsibilities, misusing agile approach by working carelessly and using agile though all the preparation work was not done. As the interviewees were speculating, most of the issues are due to lack of proper preparations and underestimation of agile approach. Interesting observation was that people identified more agile issues than successes. An interesting observation is that many of the strengths and weaknesses are opposite to each other, meaning that the advantages of agile can be gained only with careful consideration and preparation, and without this they can turn into weaknesses. When rushing to agile without preconditions in place, the results are not always positive as expected. When discussing with people about what should be done differently to succeed with agile, a common denominator seems to be that better change management and learning agile deeper would be needed. In the next chapter, literature review based on findings from the current state analysis is introduced. In this chapter, a conceptual framework of the thesis is being introduced. Topics are identified based on conclusions of the current state analysis. The purpose of this chapter is to support the understanding of the thesis and to prepare the proposal. The current state analysis revealed that the most common issues are related, on a highlevel, to either agile transformation, the differences between agile and traditional methods or change management. The idea of the agile software development is to have an adaptive team which can deliver frequently and rapidly and welcome changes in the requirements. The advantages of the agile software development are “the ability to respond to the changing requirements of the project” (Balaji, S. & Murugaiyan, S. 2012) and the improved communication between the customer and the development team. Agile method is usually more profitable and suitable for smaller projects. One of the issues in agile software development is the demand for senior-level resources; agile developers should be able to do decisions and be self-imposed (Balaji, S. & Murugaiyan, S. 2012). Manifesto for agile software development: Individuals and interactions over processes and tools Working software “Scrum (n): A framework within which people can address complex adaptive problems, while productively and creatively delivering products of the highest possible value.” (Schwaber, K. & Sutherland, J. 2013). Scrum has empirical and iterative approach, aiming to control risks and highlight predictability. According to empirical approach, there are three main principles to follow; adaptation, inspection and transparency. The purpose of transparency is to keep the whole process visible to the people who are either performing or accepting the work. Inspections are referring to the idea that scrum artifacts should be inspected enough to detect the unwanted side effects but not exaggerate. Adaptation is aiming to adjustment of the artifact in case the inspection is revealing that the artifact is unacceptable (Schwaber, K. & Sutherland, J. 2013). The product owner, development team and a scrum master are formulating a self-organizing scrum team that should not be depending on outsiders. The scrum teams are having needed competencies to deliver the artifacts incrementally and iteratively. Continuous feedback is desired to develop the competence and productivity Traditional software development is approaching things from the predictive point of view. Traditional software development is based on detailed plan with a complete list of items that must be developed. All the changes are going through a change control management (Ghilic-Micu, B. et al. 2013). Traditional and one of the oldest and most popular ways of software development is the document driven, sequential waterfall method. The catch of the waterfall method is to follow the pre-defined stages and milestones and to invest on early planning. An output of a stage is an input for the for the coming stage. At first, requirements are gathered and right after that follows the design phase. After the design, the implementation i.e. coding and testing is done and the final phase is handing to maintenance (Bhuvaneswari et al., 2013: Balaji, S. & Murugaiyan, S. 2012). The advantage of the waterfall method is the easiness to understand and implement it due to its linear model. Waterfall is useful on mature products and weaker teams can benefit more from it. However, one centric pain point of the waterfall method is the unrealistic expectation that requirements in the beginning of the project could be strict and unchangeable, leading to issues in the latter phases of the projects. In this model, issues cannot usually be solved in one phase completely, leading to quality issues in the final outcome. As the final deliverable, i.e. the actual software is delivered at the end of the project, possible issues are identified late leading to expensive changes (Bhuvaneswari et al., 2013: Balaji, S. & Murugaiyan, S. 2012). According to Kotter, change management “refers to a set of basic tools or structures intended to keep any change effort under control. The goal is often to minimize the distractions and impacts of the change.” (2011). There are several alternative approaches to change and the selection should be done case by case, taking all the circumstances into account. Lockitt (2014) has roughly divided change management strategies into five different approaches; directive, expert, negotiated, educative and participative. However, these strategies are not exclusive and can be used alongside. One of the change management tasks is to make a decision what strategy or strategies to use and how and when to implement them (Lockitt, B. 2014).One of the five strategy approaches, directive strategy emphasizes the authority of the managers, even without other people involved in the decision making. This approach is allowing fast change but not taking other involved people’s opinions into account. The disadvantage of this strategy is often strong change resistance and lack of ideas from other stakeholders (Lockitt, B. 2014). Another strategy approach, expert, is looking the change management from the problem solving point of view and it is suitable especially for the technical cases such as new systems being introduced. There are often specialists leading this kind of changes which is bringing both advantage and issues as well; though this approach is enabling rather quick implementation, affected people may not share same views than experts driving the change (Lockitt, B. 2014). Negotiating strategy approach is highlighting the negotiating between the management and people affected. The management is letting stakeholders to express their views and is willing to do compromises regarding how and what is to be done. By following this approach, the change is having slower tempo and the predictability of the outcome is not complete, however people affected are more involved and there is less change resistance (Lockitt, B. 2014). Educative strategy is trying to change people’s way of thinking, leading them to support the change. Different kind of activities is used within this strategy, such as training and sweet talking by experts and consultants. Naturally, this approach is time-consuming but as an advantage, it is involving and committing people and reducing the amount of change resistance (Lockitt, B. 2014). In participative strategy, all affected people are involved and their opinions are taken into account. In case experts and consultants from the outside are used to facilitate the change management process, they are not allowed to do any decisions. This approach is offering a possibility to learn and grow up, for both individuals and the organization around them. In addition, it is committing people and making them to support the change. As a disadvantage, this kind of change process is taking a lot of time and may be expensive (Lockitt, B. 2014). When moving to agile, a strategy for the agile change management is needed. Agile transformation is socio-technical process that requires a lot of time and patient. There are three different approaches to use when moving to agile; tailoring, localization and adoption. Tailoring is aiming to fewer changes in the organization and it was popular especially in the days when agile methods were introduced. Tailoring approach may not always be the best way to implement agile but rather a way to have the disciplined process and agile side by side (Gandomani, T. et al. 2012). Instead of tailoring, localization is accepting essential changes but not all agile activities. Some parts of agile might be ignored totally and some are customized. Especially in organizations that are taking their first steps towards agile and lacking experience, some practices are still done by following traditional ways (Gandomani, T. et al. 2012). Adoption approach is emphasizing major changes to adapt organizations with agile. When using adoption approach, agile methods are tried to be used completely without any limitations. Agile adoption is considered as the best way to achieve agile method (Gandomani, T. et al. 2012). Challenges in agile transformation have been categorized as follows: management and organizational challenges, people challenges, process challenges and technology related challenges. Impacting to people’s mindset is one of the biggest challenges; it is impossible to achieve overnight and besides time, it requires mentoring as well. Individuals as members of a project team may cause severe issues because of their habits, ambitions and different cultural backgrounds. Coaching towards agile is unique comparing to other methodologies and therefore requires an experienced and professional mentor in order to succeed. When changing to agile, people must change and forget old habits and roles; for example, project managers with strong experience in traditional methods must learn new way of working and forget being a commander. Also, the role of a customer is changing radically because of the agile way of working, forcing them to contribute in a different way (Gandomani, T. et al. 2012). From the management point of view, tacit knowledge and minimal documentation are causing issues and can be treated as barriers. Still, one of the biggest management relates agile issues to be considered is the group decision making which is totally opposite when comparing to the traditional software development. Besides group decision, also letting individual project team members do self-governing decisions is part of agile but can sometimes be hard for the management to implement in practice (Gandomani, T. et al. 2012). In many organizations, changing processes from traditional life cycle model to more iterative and evolutionary agile is difficult. This change affects many levels such as strategies, people’s roles and measurement practices. In organizations where operations are spread to different locations, process related barriers towards agile transformation are playing even a bigger role and challenges regarding communication and cultural differences needs to be taken into account as well (Gandomani, T. et al. 2012). As a conclusion, transforming from the traditional software development methods to agile is never easy but a time-consuming process that needs to be treated with a conscious and understand the importance of it. Everybody involved in agile transformation needs to be aware of the challenges and sufficient training and coaching must be provided. In addition, as there are several different agile methods to choose, organizations should carefully study them to find the most suitable one for them. All in all, in order to succeed, agile transformation requires a professional change management strategy, plan and resources. Change management strategy from a wider perspective is mandatory for successful agile transformation. Purely technical point of view, concentrating on software development process is not sufficient but all aspects as illustrated in below picture should be taken into account. Agile transition is change oriented, not methodology oriented process that is touching all levels in the organization (Gandomani, T. et al. 2012). Selection of a method & selection of an approach & creating a change management strategy & creating and following the execution plan  In this chapter, initial proposal to overcome issues in agile software development is introduced. Initial proposal is prepared based on data 1 which is current state analysis and literature review.  Initial proposal is trying to take all the previously introduced aspects in to account to offer a useful checklist. Initial proposal is telling who, what and when certain actions needs to be done. The aspect “why” is not mentioned in below figure because the lack of the case company; the thesis is based on common issues and not related to a specific organization.  There are several things that organizations and individuals should be taken into account when planning to go agile. At first, a careful consideration which one, traditional or agile method would be preferable, should be done. Comparison between these two different methods should always be done case by case and understand the unique features in every project. There are cases where agile is not suitable at all despite of all the benefits it is offering. When doing the comparison, also the characteristics of the organization are crucial; some organizations are more traditional and rigid, having a lot of bureaucracy. It can be extremely challenging or even impossible to bring agility to organizations like this. After careful consideration and selection of the method, desired approach should be defined. As introduced in earlier in the literature review, there are roughly three alternatives to select from; tailoring, localization and adoption. When selecting the approach, all aspects must be considered realistically, from the project and organizational point of view. One major thing impacting to the selection of the approach is the former experience on agile or the lack of it. A change management strategy should be created by considering all known and common challenges, meaning management-, organizational-, people-, process and technology related aspects should be considered. The creation of a change management strategy must be done in the planning phase, after the method to follow and the approach has been chosen, before the actual project starts. As explained in the literature review, first the most suitable change management strategy approach to achieve the desired change needs to be defined. When defining the strategy, all aspects of the change must be taken into consideration; the organizational culture, the scale of the change, expected change resistance, schedule, budget and risks of the change. An execution plan is needed, together with the active follow-up. It is crucial to plan in detail how the actions will be executed; the plan itself is not enough but it needs to be followed-up as well. The initial proposal is validated and commented by two of the interviewees participating in current state analysis; a developer and a scrum master 2. Validation was done via email and by having informal discussion. Also comments from the thesis supervisor was received. The developer commented that the initial proposal was good and realistically. She is working in a software development industry and using agile methodology in her work currently. Her company is struggling with same issues mentioned in this thesis and hence planning to start implementing similar phase than the selection of approach -phase in the initial proposal; they came into a conclusion that a phase like this is a must in order to avoid facing same agile pitfalls over and over again. The company did the decision without knowing the initial proposal introduced in this thesis, which is a notable example of the necessity and usefulness of this kind of a checklist. The developer was thinking that the way agile methodologies are used in Finland may be different than in other countries and especially other continentals. In her experience, Finnish companies are not yet too familiar with agile software development and therefore the initial proposal would probably not be as usable in other countries but suitable in Finland. The scrum master 2 evaluated the initial proposal as simple and doable. In her experience, this kind of checklists needs to be simply enough and the correlation between commonly known issues and the checklist needs to be clear to get people interested about it. She stated that in case companies would not like to execute all phases, they could still pick-up certain phase or phases and execute them individually; this is an alternative that should be highlighted and explained. The thesis supervisor highlighted the lack of the named resources; in the initial proposal, there is only mentioned either project team or management. However, this is not sufficient but leaves it too vague and raise a question “how to make sure things will be done”. In addition, the thesis supervisor was missing a more concrete checklist with actions and their sub-tasks.Since there was not identified any major changes during the proposal validation, the final proposal is rather like the initial proposal with a comment that in case companies do not want to implement all the phases, they can also pick-up an individual phase and execute it; it is not recommended but better than ignoring the whole checklist. There is also more depth added to make sure that things will be done; there must be a responsible person pointed-out, regarding all the steps in the final proposal. In the initial proposal, instead of individuals, there were mentioned that either a project team or management should be responsible for certain steps. It was too vague definition creating a risk that things will not necessarily be done and certainly not on time. In the final proposal, it is suggested that named person can be either from the project team or management; it is depending on the project and organization which one is more preferably. A detailed check-list with all sub-tasks is also added to the final proposal. The checklist is covering all stages of the proposal and its purpose is to offer more concreteness. Selection of a method: to select between traditional and agile methods Responsible person is named individual from the project team or from the management of the organization. To succeed, the person responsible requires sufficient knowledge of the organization. Planning phase To consider what kind of change management strategy would be the most suitable. The final proposal is trying to take all the previously introduced aspects into account to offer a useful checklist. The final proposal is telling who, what, when and why certain actions needs to be done. There are several things that organizations and individuals should be taken into account when planning to go agile. At first, a careful consideration which one, traditional or agile method would be preferable, should be done. Comparison between these two different methods should always be done case by case and understand the unique features in every project. There are cases where agile is not suitable at all despite of all the benefits it is offering. When doing the comparison, also the characteristics of the organization are crucial; some organizations are more traditional and rigid, having a lot of bureaucracy. It can be extremely challenging or even impossible to bring agility to organizations like this. There must be a named individual responsible for the selection of a method; responsibility on selecting a method cannot be shared. Naturally, it is essential that responsible person is co-operating with other stakeholders and if needed, also consults subject matter experts, but he or she is responsible that the decision will be done appropriately and on time. Without a responsible individual who is having sufficient pre-conditions, there is an increased risk that this step will be done carelessly or ignored totally. Also support from the management is needed; the way the support is needed is depending on the situation, but a minimum requirement is principled support. Sometimes also financial support may be required. Selection of a method is a big decision that should not be done without a support from the management. Despite of a good plan, the first mistake is already done if responsible person with management support is not pointed out. After careful consideration and selection of the method, desired approach should be defined. As introduced in earlier in the literature review, there are roughly three alternatives to select from; tailoring, localization and adoption. When selecting the approach, all aspects must be considered realistically, from the project and organizational point of view. One major thing impacting to the selection of the approach is the former experience on agile or the lack of it. As in the first step of the proposal, selection of a method, also selection of approach requires an individual responsible with managerial support. A change management strategy should be created by considering all known and common challenges, meaning management-, organizational-, people-, process and technology related aspects should be considered. The creation of a change management strategy must be done in the planning phase, after the method to follow and the approach has been chosen, before the actual project starts. As explained in the literature review, first the most suitable change management strategy approach to achieve the desired change needs to be defined. When defining the strategy, all aspects of the change must be taken into consideration; the organizational culture, the scale of the change, expected change resistance, schedule, budget and risks of the change. The successful creation of a change management strategy requires also a named person who is in charge. Especially in this stage, the management support is crucial due to fact that changes may touch all aspects of the organization and have a significant impact on its customers as well. An execution plan is needed, together with the active follow-up. It is crucial to plan in detail how the actions will be executed; the plan itself is not enough but it needs to be followed-up as well. There must also be resources enough to execute the planned actions. As with previous step, deep and sustainable support from the management is important. The management is also needed to provide sufficient resources and finance to secure the implementation of the execution plan. The thesis is not built around a case company but done from a common point of view. Though the amount of people interviewed is not much, it was obvious that the answers and opinions were starting to be repetitive, hence there was not more interviewees involved. When considering the results of this study, it needs to keep in mind the preconditions, such as geographically location; since this study was done in a small country as Finland, it is obvious that the sizes of the projects are minor meaning that the use of agile is different than globally. In addition, the way agile methodology is used, is also depending on the organization. Some organizations are more agile-oriented than others and therefore better aware of the possible pitfalls. Out of the five interviewees, three of them were working as consultants at the time of the interview discussion; this is also a fact worth to notice since consultants may have different kind of possibilities to impact their customers’ way of work and especially the way they are adopting agile and doing all the pre-work. During the proposal validation, the developer commented that the outcome of this thesis is probably serving best Finnish people due to fact that the current state analysis was done based on interview discussions with Finnish people and the assumption that the use of agile methodologies is not yet very advanced in Finland. This is a useful view when considering the credibility of the thesis. When considering the facts mentioned above, it can be said that the study is credible enough but the pre-conditions needs to be kept in mind. If a similar study would have been done in another location or in a selected case company, the results may have been a bit different. However, the issues identified in the current state analysis are matching to the preliminary literature in a high-level. It was really educating to draw-up a study like this; the topic is near to my heart and I have been really interested on agile methodology and luckily have had the opportunity to use that in practise. I had originally a totally another topic, suggested by my employer of that time. I found this original topic to be too wide and it was difficult to seize that, hence I decided to do my thesis without a case company and select a topic that really fascinates me most. That was at the same time a really good decision but it also felt difficult to do the thesis without a case company supporting in a background, knowing there is nobody particularly ordering a study like this. Still I think the outcome of the thesis – a proposal how to overcome agile issues, in a form of a checklist, is valuable and useful for the companies planning or going agile. pment project. Chapter 5 fulfils this goal, even though the application of the theory is singular at the moment. The secondary goal of this thesis was to examine and document the practical application of the theory of test platform prioritization for functional testing in a smartwatch application development project. This goal was fulfilled by the investigation presented in Chapter 6. The findings of the investigation also support and elaborate the theory presented. Test platform prioritization as presented in this thesis has practical applications but it is not viable for every project. It can also be utilized in projects outside of the gaming field. The investigation showed no difference in testing results between the Apple Watch device and the Xcode Apple Watch simulator in functional system and regression tests. There were some functionalities of the application that could not be tested on the simulator so testing without the device would have left some gaps in the test coverage. It would be a more feasible strategy to conduct unit testing tasks, when possible, with only the simulator. Installing the application builds to the device takes considerably longer and, because unit test are conducted in a phase where more changes are still made to the project, this would lead to significant benefits to resources and work flow. The prioritization of testing platforms can be carried out on an ad hoc basis or it can be planned ahead utilizing tools such as the ones presented in this thesis. More tools can be created or discovered in the future to cover a wider range of scenarios and development frameworks. Test platform prioritization for unit testing would be an interesting topic for future study since unit tests are very different in nature to functional system tests and with unit tests there is a greater possibility of affecting the time usage through prioritization. Test automation would be another field where test platform prioritization could yield interesting results. Running automated tests on an actual device would be significantly more challenging compared to a simulator. There are frameworks for controlling mobile device functions through the desktop computer interface, such as Appium, which can be used for automation, but maintaining the test sets and the devices in working order for executing automated scheduled regression or smoke tests would certainly present complicated issues. Before setting up this kind of a system it would be important to first discover if running the automated tests on a physical device would produce greater results to justify the additional effort. The increasing focus on efficiency and optimizing the way people think and work has led to a new area of serious gaming – cognitive games. The rise of modern web rendering technologies has enabled the creation of visually interesting cognitive games on browser based technologies. The goal of this study was to assess the applicability of using modern browser technologies to create a user centric cognitive gaming platform and the use of mathematical formulas in organic rendering. The approach discusses the current market situation and the products and methods of cognitive gaming as well as the technologies involved. The user centric approach is studied through user experience design as well as graphic design and animation aspects. The reference implementation is project CCA; a user centric cognitive gaming platform built on top of Adobe Flash that uses seemingly organic movement rendering. The technical implementation is discussed from the platform client-server aspect as well as an overview of the structure of the front end architecture. The rendering engine methods go through the 2D –based rendering of mathematical formulas, the use of continuous Bezier curves in organic movement and the creative ways of using Perlin noise to generate textures as well as movement. Optimization of complex rendering and platform building is an essential part of the process. The results show the viability of using modern browser based technologies in the creation of a cognitive gaming platform. Through the use of optimization and creative mathematical solutions, as well as tending to user experience needs a successful product is built. The project platform is used in medical trials, as well as the Science Changing the World Exhibition shown in science centers around Europe. This study stands as a testament to the possibilities of cognitive end user training and a guide on the aspects of building a successful gaming platform. Humanity as a whole has seen increased focus on optimizing the way we work, the way we consume, and the way we think. This need for ever improving performance has made us consume smarter, perform more efficiently at work and waste less time. During recent years, the world of gaming and digital entertainment has seen a growth in a new area, one that was not focused solely in time consuming entertainment; the coming of so called “brain fitness”. Gaming suites and platforms offering rather simple, basic mechanics that provided the user various activities they could do to train their brains. The central tenet in these games was that through exercise powered by game related reward models one could improve the way one thinks, and track that improvement as well. The brain games became very popular, and spread throughout the entertainment ecosystem from mobile phones to modern console platforms. The games varied from simplistic memory exercises to the ones based on largely theoretical premises, and offered actual data as back up of their effectiveness. For the most part the games have been shown to increase your ability to memorize things, but the actual science proven benefit is still under much debate. Aside from the more mass market oriented products, the world of science started to take interest in the inherent interest / reward models and game environments and how they could be used in rehabilitation and analysis of different diseases and medical conditions that affect our cognitive functions. Cognitive gaming is based on the concept of neuroplasticity; the ability of the brain to physically adapt to new stimuli. At the same time, the front end tools for web development have improved to a level closer to full desktop experience. The introduction of RIA (Rich Internet Applications) technologies like Adobe Flex and Microsoft Silverlight, in addition to the rising support for the next W3C standard HTML5 and CSS3 have enabled high production value, fully fledged user experiences across platforms within the browser. This study focuses on the key issues in building a user oriented gaming platform, and the applicability of organic movement and shapes with mathematic formulas in cognitive gaming. The reference implementation is a consumer oriented platform for cognitive gaming built for Neuroware Group; a browser-ran application to measure, improve and develop cognitive abilities. The goal of this thesis is to study how to develop a rendering system for seemingly organic movement, and a fully-fledged cognitive gaming platform with a user experience focus for improving people’s lives using modern web front end technologies. The present study does not explore the medical theories of cognitive gaming, nor is it a study of software engineering methods themselves. This study focuses mostly on the front end platform implementation, where the game logic and rendering lies, and only has an overview of the backend system of project CCA. The first part of the study focuses on the basis of the reference project, the setup of the research question and key factors and challenges. The second part provides an overview of brain gaming and introduces the user experience needs and specifics in consumer market oriented platform building, as well as details on CCA platform implementation. The third part introduces the technological core of the platform, rendering engine specifics, optimization approach and implementation and how the challenges were overcome. After going through the theory and reference implementation, the final part discusses the merits, accomplishments and future of the project CCA. From current user base and use cases to possible future uses, as well as the related branch of the project currently presented at the Science Changing the World exhibit. The reference project is collaboration between the concept owner and CEO of Neuroware Group – Matias Palva, PhD, and Niilo Säämänen, the author of the present thesis. Neuroware Group is an innovative small company focusing on neuro-gaming and cognitive training games that are based on theoretical research on the fundamental workings of the human brain. The project began based on the research done by Satu and Matias Palva at the University of Helsinki, Neuroscience center. Matias Palva had started a prototype of the creature rendering system on the LabView platform to test the possibilities of making a consumer oriented cognitive game based on his research. The project CCA started in 2008, on the 18th of December with a meeting with Matias Palva. Based on the prototype idea a consensus on the viability of creating the platform with modern web technologies was found. The aim of the project was to create a consumer oriented platform for cognitive gaming based on the research done by Satu and Matias Palva on neuroplasticity and the application of organic shapes and movement in the realm of brain fitness. The target was to make a browser based solution, with a fairly large portion of the platform logic and controls coming from the back-end solution, enabling a modular and extendable solution for brain training. Figure 1 shows the approximate funding partners and their relative contributions to the project during its lifetime. The project had Tekes funding for the initial prototype and development. Development was started without certainty about future budget options. During the development additional funding was acquired as a grant from the Runar Bäckström Foundation, and closer to the final stages of the project, the trials with HermoPharma funded further development. The approximate total external funding in project CCA was 70 000€. The projects first release version was finished during the spring of 2011, after 2 years of development. Since the 1st release version, the platform has been further developed and optimized. The main goals of the project have been achieved, and the whole platform had been heavily reworked and evolved to a point of maturity. The project work was delegated as follows: The front end platform development, rendering engine development and UX-design by Niilo Säämänen, graphic design by Mikko Häkkinen and the game design, mathematical theorems and back-end solution by Matias Palva. Mostly the project was an intense collaboration between Matias Palva and Niilo Säämänen. The reference project is a cognitive gaming platform for consumer use. The tool is meant to be used by end users of all ages and trades, and to be easily approachable and trustworthy tool for measuring and improving cognitive processing efficiency. From a user’s perspective, the platform is a web-browser based game system with user authentication and personal, account based training programs. The general user flow in CCA is straightforward; users either have an account, or register for one, and log in to the game platform. The users have free choice over which order they plays their games in, and can choose from various available games to them as seen in Figure 2. The different game modes depend on the users account and targets. Each user has a specific user account based training program that allows for them to play a certain amount of games per day. Most games last around one minute and in each day a different set of games is played. The total duration of a training program is approximately 30 minutes per day. The games vary depending on the type, but contain various amounts of moving or static visual objects, called creatures. The users’ task is to perceive and/or memorize creatures or the target visual states thereof according to the instructions. The target state of a creature is a brief contrast-, color-, and/or shape-change. In some of the game modes, the target state change is relative based on a calibration round played before the actual measurement portion. Calibration changes the size of the target state so that the subject detects around 6090% of the changes in a two creature game phase. The detected amount of target states is expected to get better with training. When the target state is perceived, there is a limited window of opportunity for reaction, within which the user must react by pressing a key. (Configurable, but by default it is the space bar) The platform measures the hit rate and reaction time of the user per target state per creature on screen. The hit rate (HR) is the main measurement used in CCA to define user capability. After a successful play, the users are given a summary of their performance. The performance metrics shown are based on the hit rate and reaction time of the user. The main numerical feedback given is called capacity. Capacity is defined as, where N is the number of creatures. Capacity is given as a total value for the playthrough and as a separate value for each phase of the game. In addition to the numerical metrics, the users are also given trophies, achievements and stars based on their performance. These categories provide user friendly feedback on how well the users are doing without the need for detailed metrics. The detailed metrics are available, but not shown as the default content. The purpose behind the trophies is to empower the user and give them solid, clear and maintainable goals for their training. After the game end screen, the user returns to the main interface of the CCA platform, where he can see his statistics, review his achievements, change his information and play another game. The projects target user groups were divided into two sections: The general public, and the clinical trials. The general public users were divided into two sections: Elderly citizens whose interests would be to both test attention and working memory as well as train to improve them, and school aged children who would benefit from attention disorder testing with an automated platform. The current methods for testing and diagnosing attention disorders is work intensive and expensive, and automating the testing would yield significant savings. These users focus was on daily life, and they suffered from no known disruptions in their cognitive capabilities. Because of the scientific nature of the platform, and the precise data collected from our users, the platform could be used as a reference in clinical trials, to see how and if people improve their performance with the use of a medicine. This is important in studies of new medicine and helping people with brain trauma. The platform enables a way of measuring and making training programs specific to studying the target groups’ differences with medicine and without. Since a portion of potential users is visually challenged, significant emphasis was put on making the platform visually appealing and simple to use. The focus was on pleasant user experience, instead of pure efficiency. To make the users feel safe with the program, and to trust it, a solid user experience was necessary. The work started soon after the first meeting, and the focus was on creating a prototype of the basic rendering mechanics as a proof of concept. This was necessary as the rendering power of browser based solutions is still far behind native language based compilers, and it was needed to see if it was possible to create the rendering engine on such platforms. As seen from Figure 2, CCA was a comprehensive project spanning many years. The project was divided into separate phases, each phase consisting of a number of sprints. The first priority was in the rendering and game engine construction to make sure it was viable to build a full scale platform with the technologies chosen. After a few weeks of development, a first prototype of the rendering engine was developed and published. After some tweaking of parameters and bottleneck analyzing, it was considered to be a viable solution, and a sufficient base for building the platform. Figure 4 shows the first version of the platform rendering engine. The result contained only one stationary creature, comprised of a nominal amount of points, rendered just so one could see how it would work and what the possible performance bottlenecks were. The initial version rendered the stationary creature with barely 30 FPS, and was quite heavy on the CPU.  After the POC was finished, the full development of the platform began. Development was done in an agile way, with sprints of 2-4 weeks [1], where one aspect of the platform was tackled at a time. Since the most difficult part of the project was the rendering engine itself, the first 4-5 months were spent solely on creating, optimizing and fine tuning the first version of the core rendering engine.  Once the basic engine was built, a fast pass over visual style was done for the project, an initial GUI for test users and funders to see and test the progress. Work was started also on doing reusable UI components for various user prompts and game information needs. Beyond the GUI needs, the first game mode logic was coded, and the first actual play through of CCA was possible.  As depicted in Figure 5, the first GUI version took the metaphor of the circles quite far, and was oriented around cells that contained smaller cells. The first version of the UI was fully implemented, and contained growth animations for chosen cells and a small amount of fluid dynamics, however it was quite confusing to use, and more of a game in itself.  After the initial GUI pass, the focus turned to the game modes of the system. The final version of CCA supports 4 different game modes for cognitive gaming; from a single creature reaction observing to feeding multiple creatures. In addition to building the initial game modes, the first bug fixing and improvements for the rendering engine were implemented.  Once the game modes were done, a sprint was dedicated for properly introducing a full GUI to the platform, and further separating UI components. After the initial logic and games and UI were done, the focus was on Login and register functionality as well as user support mechanisms such as account handling and performance graphs and information graphics in general.  After the phase three of the project, the official Beta release of CCA was ready. The beta interface can be seen in Figure 6. The platform supported all the basic game modes, user accounts, a full play through experience and statistics to prove it. Once this version was done, the work for the Heureka science exhibition called Science Changing the World started. It was a separate version, completely independent from the back-end and running a local copy of the rendering engine modified for 4 player multiplayer needs.  After the Heureka version the phase 5 concentrated more on improving different parts of the platform at a time. The addition of medical imaging indicator helped automated testing of the platform, the improved target location in path finding enabled smoother movement and the GUI code had a thorough overhaul to separate it more from user logic.  Phase 6 was the 2nd major round of refactoring and improvements for the platform. The introduction of training programs changed the way the platform worked, and the new game modes added a lot of variety to the package. Engine code refactoring helped further development of game modes especially, and the target state calculation change was done in order to make it more manageable for the server to fine tune the user account based training regime.  In Phase 7 the way users were rewarded and progress communicated got a much needed overhaul, changing the direction of the feedback to a much more user friendly result. In Phase 8 the release version of the platform was finalized, the final GUI was put in place, the performance was verified and the user experience perfected.  There were a lot of features and ideas in the beginning, some of which were discarded along the way as unviable for us to implement with our schedule and needs, among those features were evolutionary algorithms in creature generation.  The technologies enabling browser based heavily mathematic rendering for the front end were evaluated before the project begun, and a choice was made in the beginning on which platform to pursue the solution on. Since much of the projects core engine is based on mathematics that are rather universally supported by the different platforms, the porting of the platform to a different technology is not considered to be an impossible choice in the future.  One of the technologies evaluated was Silverlight the RIA solution from Microsoft. It is a browser plugin similar to its main competitor Adobe Flash. An overview of the Silverlight technology stack is seen in Figure 7. The runtime is based on the popular C# language, and UI components are marked in the more human readable XAML. (Extensible Apllication Markup Language) XAML is also used in Windows presentation foundation (WPF) and .NET framework of which Silverlight is a sub sect of. Silverlight implements the same version of the Common Language Runtime (CLR) as the .NET framework  3.0.  Silverlight offered similar performance to Adobe Flash, and being a part of the MS solution package, it comes with a wide range of solid development tools and support with Visual Studio family of products. Silverlight supports the integration of multimedia, graphics, animations and interactivity in a single runtime environment. Similar to Flash, Silverlight also supports vector rendering in addition to the typical Bitmap based rendering.  The main hindrance in Silverlight and one of the reasons it was not used is that it is far behind Flash in adoption rates. Where the Flash Player 10 adoption is around 98.7% in mature markets [3], Silverlight’s adoption rate was around 22% [4] at the time of the project start. Whilst being impressive in its abilities, the Silverlight technology was rather young in 2009, and sorely lacking in features.  The technology chosen was the de facto standard for rich content online in 2009; the Adobe Flash platform. As mentioned in relation to Silverlight, it is the most widely spread browser technology in the world, and has a mature and solid development environment and support. Flash supports vector rendering as well as bitmaps, and the newest versions from 11 onwards support using OpenGL based GPU rendering as well. When the project started the player version target was 10.1, but the release version of CCA is targeted for the Flash Player 11 platform. [7]  Whilst requiring the installation of a plugin, the wide spread adoption of Flash made it a clear and easy choice for a consistent user experience across platforms and browsers. There are up–to-date versions of the player for Windows, Linux, OSX, and Android platforms.  The heralded revolution of standards based online development – HTML5 – was not around in a prominent way back in 2008. After studying the possibilities of using HTML5 and Javascript based solution as an open alternative for the plugins, it was found that the performance of Javascript VMs and especially the rendering performance was subpar when compared with the plugin based technologies.  This has changed a lot from the year 2009, and the newest JIT(Just in Time) compiler in Google Chrome is starting to surpass some of the plugin solutions in pure crunching power. However, the rendering capabilities of browsers vary a lot based on the versions, and the adoption rate of new browsers remains one of the larger obstacles in generating modern standards based rich solutions online.  Being a consumer mass market solution, we aimed at all three big players in the market on consumer platforms: Microsoft Windows family, Apple OSX, and Linux based operating systems needed to be supported. The support for our decided solution platform – Adobe Flash, is wide and envelopes all our target platforms.  The benefit of choosing a browser ran online platform was also the generic platform independent nature of the technology. With a single solution, we could reach and deliver a fully functioning solution to the whole user group. As seen in Figure 8, the project supports the main versions of Microsoft Windows from XP onwards, OSX from 10.6 onwards and Linux Red Hat Enterprise 5.6 or later, openSUSE 11.3 or later and Ubuntu 10.04 or later in both 32bit and 64 bit varieties [7].  In principle a two-person-project, the CCA was a significant undertaking. The viability of using browser based technology for creating a complex mathematical rendering engine for clinical and consumer use was something not many had done before.  The biggest challenges of the project were on the pure engine building level: The rendering of creatures in real time without pre-rendering or cheating in the rendering pipe line, and the performance of path finding logic and path wrapping. The mathematics used were performance intensive, and the rendering of several creatures with hundreds of rendering points and surfaces in addition to the path creation with continuous Bezier curves was difficult for a CPU based rendering engine.  The goal of building a medical platform for consumer use had its own challenges. The combination of a number based, mathematics oriented solution and attractive user experience provided many obstacles. Keeping the UI code separated from the engine, and ensuring a solid separation of concerns was essential in building a working platform.  On the platform front the separation of as much presentation and user logic to the back end for control and data based optimization of training posed several challenges to overcome. The parameterization of almost all UI elements and processes from game modes and their configurations to even trophy icon generation was a complex issue to solve.  Cognitive Gaming  A type of gaming and exercise that is designed to help and improve cognition. Used in the aid of recovering from brain trauma, also used as recreational activity believed to be beneficial to the mind.  RIA  Rich Internet Applications. Browser based technologies that enable creation of desktop like features in the world of internet. Example technologies include Flash and Silverlight.  Modern Browser technologies  A technology stack that contains all browser ran technologies. HTML5&JS, Flash, Silverlight, Java (?)  Neuroplasticity  The theory that the brain is capable of physical change and improvement based on outside stimuli  Creature  The main target in the project CCA. A collection of mathematical formulas that are rendered based on a path finding engine to represent an organic “creature”.  A single unit target of the platform.  Bezier curves  Smooth parametric curves based on 2 control points and a start point and an end point. Essential in project CCA.  User experience  A combination of usability, user interface, interaction design, information architecture and animation to cre- ate a complete use experience for a user.  Platform  A combination of technologies that form a coherent, reusable and deployable whole. An extensible combination of modules that works as a basis for building content on top of.  Figure 9. Key concepts of the study  Key concepts used in this study are shown in Figure 9. They cover the areas of cognitive gaming and modern browser based game development as well as the principle technological choices in organic rendering.  Cognitive gaming is an exciting new area for consumers and scientists alike. There have been various studies and trial projects about using games as platforms for advanced learning, learning through play and using virtual worlds as class rooms for learning. Cognitive gaming takes the elements of gaming such as repeatable tasks, reward models, user tracking and fun of play and combines them with medical research and recuperative methods for a game experience that also benefits and improves cognition.  The basis for all brain exercise, games and all is the concept of neuroplasticity, or brain plasticity; the ability of the brain to change physically throughout life, in response to stimuli. The times when changes happen in brains are in the beginning of life, when injury hits the brain, and whenever something new is learned and memorized. [8]  Up until recently it was believed that the connections in brains remain fixed with age, and physical changes are impossible. However, recent studies have shown that brain keeps on changing through learning and stimuli throughout our lives [9]. As an example when comparing professional musicians to amateur musicians and non-musicians, the actual physical volume of grey matter in areas involved in music, such as motor regions, anterior superior parietal areas and inferior temporal areas was larger with the professionals who practiced over one hour per day. [10] These changes were also greater when measured over time.  Another example came from a study done on extensive learning with German medical students. They used medical imaging to monitor the brains of the students before their medical exam and after, and compared the results to similar students who were not studying at that time. The students’ brains showed anatomical changes in grey matter in different areas of the brain, including the parietal cortex and posterior hippocampus, parts of the brain known to be involved in learning and memory [11].  Despite the recent studies and interest in training programs to be used, there have been very few long term studies in the effects of cognitive games and training. While there are studies that show the short term implications of training [12], especially on those suffering from early stages of cognitive impairment, there have not been sufficient enough studies to show whether the training can postpone the effects of such impairments as dementia [13].  In 2005, the size of the brain health market globally for software and biometric applications was estimated to be around 210 million dollars. The estimated value of the market in 2012 is over one billion dollars, and by 2020 it is estimated to reach six billion dollars in value. [14] This rapid growth comes in part from recent research, and in part from many medical professionals and researchers trying out the possibilities of using their research in helping people on the consumer market.  As seen in Figure 10, the biggest growth expected to happen in the growth of cognitive gaming and brain fitness is the consumer market by a large margin. The growth of self service training portals with training regimes directed for home use and selfimprovement are already growing fast, and are expected to do so in the future as well. The other big growth area is in the area of insurance and health care as well as elder living. The benefits of preventive training in age related deterioration are substantial, as well as in the rehabilitation of people with brain related injuries. The savings generated by such actions could be monumental. Aside from the main growth areas, the uses of cognitive games in school systems as well as employee care are expected to grow.  The cognitive game market is divided into 2 different parts, the pure software products and the biometric (applications that require actual hardware to measure physiological responses) applications. Examples of biometric products include products that measure hart rate variability or brain activity through EEG (Electroencephalography), the recording of the brain’s electronic activity over a short period of time through the scalp. Our reference project belongs to the software category, and does not need any hardware to function.  In the area of cognitive games the reference project lies, the focus on perception, attention and working memory, there are various competitors and products on the market. To keep the subject more valid and tied to the subject of this study, the focus is on online, browser based cognitive gaming platforms and their use cases, technology and popularity.  The biggest company in the online cognitive gaming market in 2012 was the Lumosity.com. Lumosity is partnered with researchers at Berkley, Harward and Columbia and works with numerous health care organizations to help create cognitive gaming experiences. The service has over 25 million users, and it provides comprehensive and personalized training programs based on user accounts. [15]  As seen in Figure 11, Lumosity online training program conveys facts and scientific information about what the tasks you are performing at the moment provide, and manages to create a solid user experience with enough information and play to make it interesting. The training program is fitted to your needs based on a simple questionnaire. The first steps in Lumosity are free, but after a few games you get to a point where you cannot benefit from the service without subscribing.  Technology wise the frontend base portal of Lumosity is based on standards based HTML5 and CSS3, providing the general test framework and admin functionality in the portal. The actual games themselves are made with Adobe Flash technology, similar to the reference project. The games are rather simple in function, and are very event based in nature. In addition to the web interface, Lumosity also has a mobile application available for the Apple iOS platform.  From a user experience and game design point of view, the portal comes across more as a collection of different games and a framework that ties them together, as seen from Figure 12. The different games use different visual cues and styles based on the subject matter, and are not uniformly under the same visual design aspect as the main Lumosity portal. However, they do contain repeating elements in the introductory controls, to provide similar functionality across games.  In addition to providing a cognitive gaming service, Luminosity has scientists actively working in the field of research and finding out how to best use cognitive gaming to benefit the human condition. In a recent study, it was shown that there was improvement in the working memory and visual attention of the target group when using a web based training application outside of a clinical trial setting [16].  The other competitive platform taken as an example in this study is the My Brain Solutions portal at www.mybrainsolutions.com. Similar to Lumosity, the portal provides a brain assessment in the beginning, based on which it generates a user profile and a training program for you to follow. Unlike Lumosity, the brain assessment is a quite a comprehensive test of memory, comprehension, emotion and other cognitive functionality, and lasts around 30 minutes up front. [17]  As seen from Figure 13, the My Brain Solutions contains a personalized training solution, as well as charts on how the users brain and performance range on the variety of test subjects. In addition to points which Lumosity used, in this platform the user also has badges, a reward mechanism similar to achievements to convert the arbitrary numbers and progression into more human readable terms. The platform allows you to set your own goals and encourages you to set actions for yourself to keep you busy.  Technology of the My Brain Solutions follows that of Lumosity and others, the main site is a web portal built on web standards, while the individual games are based on Adobe Flash technology. In addition to the web portal, My Brain Solution has various applications for different mobile platforms, targeting a specific feature, such as MyCalmBeat that focuses on lessening stress and increasing focus through slow breathing.  In comparison to Lumosity, My Brain Solutions has a more unified gaming platform, with all the games showing similar introduction screens, button layout and statistics / in game information when doing tasks. As seen in Figure 14, the different games all feel as if they are of similar family and go well together visually with the main visual identity of the portal. From initial testing, the games seem to have more complex interactions in them as well.  From the more entertainment oriented area of gaming recent years have shown increased popularity for titles such as Brain Age (2005) for the Nintendo DS console that sold over 18.96 million copies, as well as its sequel Brain Age 2 (2005,2007) 14.83 million copies to date. [18] While these products were very popular, and studies have been done in order to evaluate their possibilities [19], Nintendo has distanced itself from the use of scientific proof of benefit in the games. [20]  An example of the Brain Age line of games can be seen in Figure 15. In the Brain Age games the user is expected to play a small amount each day, according to a training program. This is a common approach to brain fitness, and is employed in the reference project as well. The tasks a user performs orient around simple calculations and mathematical questions, memory exercises, Stroop tests as well as Sudoku puzzles.  The cognitive gaming platforms focus on a certain set of games, each targeted for training different parts of the cognitive system. These areas are close to the ones used in clinical neuropsychology, but understandably are not as exact or as precise due to the consumer market approach. These target areas can be roughly divided into three categories: memory, attention, and executive function. Some platforms such as My Brain Solutions also contain tests related to emotion and human understanding.  Memory games test and train our ability to memorize items, words, patterns and other objects. There are different variations of memory games; a commonly used one involves working memory or short term memory. These tests are called N-back tests where a user is presented with a sequence of objects, and the task is to react when the current object matches the one shown N objects before [21]. The N-factor can be adjusted to make test more or less difficult. Short term memory tests are well suited for consumer market testing; the test gameplay does not take overly long and the results are parsed live. An example of an N-back game is seen at the top in Figure 12, as well as another type of memory exercise seen in the middle.  Attention games focus on our ability to perceive and react to our perception in a given controlled environment. The idea of attention tests is to train the cognitive processes of focus and visual search as well as long term attention. The tests often focus on pattern recognition as well as visual attention. The way to test attention implemented in the project CCA is to have multiple moving objects on screen that the user has to follow, and react whenever the object achieves target state. Target state is a change in the shape, colour, contrast or form of the object. Numerous examples of attention tests are found in the reference project platform, as well as the example game at the bottom of Figure 12.  Executive function is an umbrella term used for various cognitive processes and sub processes working together.  Executive functions are those involved in complex cognitions, such as solving novel problems, modifying behaviour in the light of new information, generating strategies or sequencing complex actions. [22]  In the reference platforms for brain gaming, the tests for executive function include games related to arithmetic, quantitative reasoning, planning, verbal fluency and task switching. An example of an executive function test can be seen in Figure 16. This task is about connecting a series of nodes with as much area as possible without ending in a dead node, a task that requires both planning and reasoning.  The reference project is mostly based on various attention games, as well a testing mode for N-back tests to improve your working memory and a continuous change mode that falls in part into the domain of executive function.  The term game engine came to be around mid-1990s in reference to 1st person shooter games such as Doom by ID Software. Game engine is a platform for game development that employs data driven architecture to create reusable software components, such as a three-dimensional graphics rendering system, collision detection system and a physics simulation. [23] The line between an engine and a game is often blurry, and to date there are few engines that can adapt to more than a few genres of games.  The engine defines how the game is rendered on screen, and influences a lot of the design around the core concept of the game. Some game engines include tools for building the actual game content on top of them, such as Unity, Unreal Engine or CryEngine [24,25,26]. This is often called scripting, since it is most often done using a scripting language. Many of the engines contain their own scripting language [26], or use a set of commonly supported high level languages; Unity includes support for C#, Javascript and Python based Boo [24]. Some engines provide only the actual rendering engine for abstracting the hardware level, such as Ogre. [27]  Game building in general has gone through a renaissance of sorts, where the middle ware is increasingly important in creating game experiences in the industry as well as in the education of game development [28]. A large portion of games today use middleware such as Bink video [29], Havok physics engine [30] or the Adobe Flash harnessing Scaleform [31] for game user interface building. These middle ware programs provide a necessary relief from the complexity of building a modern game, each solution doing its part, providing a polished and optimized way of handling one aspect of the game.  In the field of medical and cognitive gaming, some are using the same game engines as entertainment oriented gaming uses [32], while many use their own engines based on a higher level programming environment such as Flash, Silverlight or Java to create their own base engines. Many cognitive games are not yet complex enough to have the need for specific engines, but with the increase in demand, development budget and competition, it is only a matter of time.  When talking from a more conceptual level of how game interaction and gameplay is handled, there are two types of game environments; static and dynamic. In the context of this study this distinction is defined as the way game events are controlled in the game, and the way the engine handles rendering and interaction.  The traditional game engine is static, as in everything in the engine is defined by an artist or a developer, every move you make is the result of a careful calibration, iteration and concepting. Static engines allow for absolute control for the games designers, and are in many cases more optimal from rendering and calculations point of view. In the comparison products Luminosity, My Brain Solutions and the Brain Age all fall into the static game engine section. They are all also very event based in their approach, everything happening in the games is not based on real time calculations, but on specific events happening at specific times and the reaction to those events.  The downside of static game engines is that in controlling everything they lose the element of surprise in some ways. When everything is designed, there are no happy accidents, nor odd gimmicks that a player can find that cannot be reproduced, and everything in general works as it does, always the same reliable way. Many static engines also employ a basic physics model with hard-body physics. In such an engine the physical values are tweaked and set by the designers in a way it replicates some simple form of physics, but does not really allow for proper surprises.  While being more obscure, and harder to control, dynamic game engines base the world they render on a set of rules. These rules may be physics, different path algorithms and so forth, but they all have in common the lack of direct control over what happens inside. In a dynamic engine the game designer gives the objects targets, creates behavior and sets up boundaries, but how the engine executes these is left for the engines internal system to decide.  Some games use the dynamic game building in ways to create randomized environments, procedural content based on a set of rules. Games such as Diablo [33] and Minecraft [34] have used procedural generation to create whole levels, or in the case of MineCraft, a whole planet. The traditional problems with computer generated content in games have been the repetition of content and unnatural and uninteresting combinations. Some newer games use this dynamic or procedural generation in the generation of items for the player to use, such as the weapons in Borderlands and Borderlands 2  [35].  In gaming physics, a more solid way of doing physics in a dynamic way is called softbody physics, where a physical object is a collection of its sub parts physics. This is immensely heavy in calculation, and is only now emerging with the new Cryengine 3 [25] and other new game engine platforms. The project CCA is not in a world of fully fledged physics, nor is the world building in any way overly complex, but the engine and the way the creatures work is entirely dynamic.  User experience (UX) has many different definitions, depending on the subject matter it is related to. A classic example is from 1996, from the first annual ACM Interactions Design Awards:  “By “experience” we mean all the aspects of how people use an interactive product: the way it feels in their hands, how well they understand how it works, how they feel about it while they’re using it, how well it serves their and how well it fits into the entire context in which they are using it.“ [36]  The Nielsen-Norman group define user experience as:  "User experience" encompasses all aspects of the end-user's interaction with the company, its services, and its products. The first requirement for an exemplary user experience is to meet the exact needs of the customer, without fuss or bother. Next comes simplicity and elegance that produce products that are a joy to own, a joy to use. True user experience goes far beyond giving customers what they say they want, or providing checklist features. In order to achieve highquality user experience in a company's offerings there must be a seamless merging of the services of multiple disciplines, including engineering, marketing, graphical and industrial design, and interface design. [37]  In principle, UX is everything a user feels, sees, experiences when in contact with a company and/or a product. The point Nielsen-Norman make is to go beyond the needs of the user, the realization that UX is far more than providing the user with what they need, but how they experience it as well. The joy of use, aesthetics and message all combined in a thought out package.  User experience design (UXD) is an umbrella term that covers the different facets of expertise required to create a wholesome UX. The roots of UX design come from Human Centred Design (HCD). HCD can be summarized as:  Positioning the user as a central concern in the design process  Identifying the aspects of the design that are important to the target user group  Developing the design iteratively and inviting users’ participation  Collecting evidence of user-specific factors to assess a design [38]  In addition to the methodology of HCD, UXD builds on top of HCD with more complex cultural and business factors. While traditional HCD based usability factors were about performance and smooth operation, UXD brings along aspects of social interaction, the importance of aesthetics, both very culturally complex and context requiring concepts.  Since both UX and UXD are very broad subjects, in the context of this study, the focus of UX is confined to how our project platform uses UXD to create a suitable UX within the platform itself. The areas presented in relation to the reference project are interface design / aesthetics, interaction design / movement, information design / communication, and playability in the cognitive gaming context. There are also various UX concepts such as presence, immersion, flow, fun, involvement and engagement that try to describe the UX in games. [39] These theories, while important, are not discussed within the scope of this study.  What the success of iOS and mobile platforms has taught us; user experience matters. People are willing to pay for better suited and thought out solutions. The wake-up call provided by the mobile markets sudden rise has not gone unnoticed in the world of gaming. Especially in large companies there has been a surge of focus on hiring user experience designers and front end designers to focus on the neglected parts of games, the game UI, and the interactions with it.  Games have been on the forefront of human computer interaction for the past 20 years, as the playability of a game, essential to the overall experience is uniquely a quite complex endeavor from a user experience point of view. But while focusing on playability inside the engines and games themselves, they often forget the first thing a user sees when they enter a game, the user interface of the game itself. The navigation, settings and save/load; these top level controls affect a lot on how one perceives the game experience.  Often these are badly designed, riddled with flaws, inconsistencies and hiding of information. It is not uncommon in games that you are not sure what will happen when you adjust a setting, or a parameter. The inclusion of UXD methods and experts has made games more user-friendly and easy to use. In building a platform for cognitive gaming, the unified UX for the games within the platform as well as the overarching user interface was an important part.  As has happened throughout time, the advance of technology and tools for creating visual communication have changed and evolved at a rapid rate in recent decades. Despite this change, the essence of graphic design remains unchanged; to bring order to information, form to ideas, and expression and feeling to artefacts that document human experience. [40]  The international typographic style has been a significant and influential style in graphic design for the past 50 years. Its origins come from Switzerland and Germany in the 1950’s and it is also known as the Swiss Design. The visual characteristics of the style thrive for unity though a solid mathematically based grid, objective photography, sansserif typography and a solid copy that presents information in a clear and factual manner. [40] More than pure style, the importance of the international typographic style lies in the attitude and approach its early pioneers adopted. The role of graphic design was formed more towards shaping information and communication then personal expression and artistic eccentricity.  This clarity of style and information is a basis of much of modern online communication, and it is the basis for the style in CCA as well. In addition to a clear and unified visual communication, in CCA the focus was also on the visual narrative: animations, movement and flow that affect how a user perceives a product.  Typography is the art of type, the act of arranging to make language visible. In the modern day the term envelops many crafts, from the traditional typesetters and compositors to graphic designers and artists. In the context of this study, the term is used to mean everything we do with type in a digital platform; Font, sizing, characters and legibility.  Typography exists to honor content  Like oratory, music, dance, calligraphy – like anything that lends its grace to languagetypography is an art that can be deliberately misused. It is a craft by which the meanings of a text (or its absence of meaning) can be clarified, honored and shared, or knowingly disguised. [41]  In modern digital communication, the use of solid typography to create a unified and visually attractive, legible message is an essential part. It is used to both communicate efficiently, as well as to add a feel, personality and grace to the communique.  The technological choice in CCA, the Adobe Flash Platform is a good choice for working with typography, as it contains a much more advanced text rendering and font support engine than the traditional browser solutions. With the support of fully embeddable type, the platform could take use of proprietary fonts with full fidelity and control. With the recent advances in CSS type support it is even possible to use proprietary or special fonts on standards based online communication. [42]  In project CCA the main typography is provided by the fairly common Myriad Pro, originally developed by Adobe in 1992 and widely used by companies such as Apple, Walmart and Wells Fargo. [43]  Myriad Pro is a versatile sans-serif font family designed for mostly digital use. The family contains a wide variety of weights and widths to suit the needs of the CCA platform. It is a simple, elegant font with excellent readability. An overview of Myriad Pro font rendering with examples can be seen in Figure 17.  It is often a problem for digital platforms to provide similar design and typography in different languages, as the Latin based languages have their own typography, and the Chinese, Arabic and various other languages have their own typeset. Rare fonts support even the whole plethora of European language typesets, from Cyrillic to Greek. Myriad Pro is a good choice for a language versioned platform, since it provides a complete support for Greek and Cyrillic characters, which enables the use of same visual fidelity across languages.  The Flash Platform also provides the tools to embed different character sets for the use of completely different language characters, such as having Myriad Pro for Latin based languages, and a specific font for the Chinese market. Which font to use is decided during runtime when dynamic content is loaded, based on the character sets involved in the UTF-8 encoded content.  As seen in Figure 18, CCA uses Myriad Pro in two different weights, regular and bold. The main headers for each screen are 36 points big, allowing for a clear distinction from the sub headers and regular copy text. As CCA is not a very text heavy platform, the most used texts are the buttons and sub headings. All elements that indicate something that can be activated or form a separate section of information are bolded for effect. All regular text, information notices, explanations and such are in a readable 14 pt size.  In interface design the essential glue that keeps a view, a page, a dialog together is the composition, also known as the page layout. Composition can be thought of as the link between art and mathematics, the use of relations, numerical patterns such as the golden section, and geometric shapes to create a formula that organizes content in a meaningful and clear way. [44] Composition can be based on various styles, from a single visual to perhaps the most used element of composition grid theory. An effective tool is also to use Gestalt Laws of grouping, the principles of how the mind organizes visual data, in creating coherent compositions. [45]  In CCA the whole composition is based on a centred grid with a focal point always at the horizontal and vertical centre of the display area. All content containers and controls as well as all UI-elements are aligned based on the centre point. As seen in Figure 19, the centre based composition is followed by the main playground, the user menu at the bottom, and the game menu at the top, as well as the main information dialog at the start of the game. During the actual game, the main visual time indicator is at the centre point as the most obvious visual element for a user.  An important aspect of composition is the use of spatial relationships. The space can be created by content using images, texts, icons, lists, logos or just plain text – or it can be created by the space between content, called negative space or white space. The space can be actively used to create a point, or it can be passive, there just because the layout process requires it. [44] The use of negative space is essential in giving air and increasing legibility in digital design. It can be divided into two categories, macro white space – the distance between major content elements – and micro white space – the distance between elements within content elements, such as lists.  A challenge in composition in a digital medium is the dynamic nature of the viewer setups. The layout can be viewed with a screen from 1024*768px up to 2560*1440px, with varying pixel density (DPI). This creates quite unique issues for the use of white space, and the arrangement of elements. The traditional approach in digital design is to set a certain target resolution, a compromise that contains reasonable resolutions and provides a good enough result for those not matching the target resolution.  As Figure 20 shows, the most popular screen resolution of users in Europe has grown from 1024*768 to 1366*768 in the last 4 years. Beyond just the resolution, the size to resolution ratio or DPI (Dots per Inch) has also grown and diversified. The rapid change in resolution, as well as the fragmented size variance has resulted in a situation where designing for just a single base resolution is not a preferable option anymore. This is true even without taking into account the boom in mobile browsing and devices with smaller physical size and screen resolutions.  As a response to the ever growing resolution and varying DPI, the modern way of doing digital layouts is in a state of transition; an ever larger amount of platforms and front-end technologies enable the use of adaptive or responsive layouts. A responsive layout adapts to the viewing area by resizing, reorganizing and resampling the information presented to the user. [47] It is often based on set of steps within which the content scales, and when a step changes, the content arrangement, and / or organization itself changes.  A key challenge in CCA was the requirement of being completely resizable upwards from a minimum size (1024*800px). The elements must fit the screen from the minimum onwards, while maintaining a suitable space and visual feel to them. A part of the solution for this was the center-based grid design. Because the platform can be resized both vertically and horizontally, basing the user controls in the middle of the screen enabled efficient use of both axes.  In CCA the whole viewport scales based on user resolution, but the game area itself has a minimum and a maximum stop for both performance and playability reasons. The size of the game area affects how big an area the user has to visually scan in order to notice changes. This area cannot be too large, otherwise it starts affecting users performance scores.  In addition to the scaling layout size, all the main views in the UI of CCA can be dragged around the screen by the user as needed. This empowers the user to arrange the UI in efficient ways, depending on their own resolution. It also helps solve the problems of using extra space in the GUI. Each element has an active drag area everywhere where there is no control, or content presented. The dragging works with slight simulated physics, calculating a primitive velocity of the object when a user drags and releases it, and using a static coefficient for friction, decreases the elements velocity until it comes to a stop.  An important part of understandable GUI-design is the use of repeatable, recognizable visual controls and containers, also known as elements. The use of repeatable controls is also of benefit from the development point of view, as it is substantially less time consuming to create base classes for repeatable elements such as a content container or a button and button sub type. Repeating elements are also essential in creating a coherent user experience, where the user can predict how a certain view behaves, and how to interact with controls.  CCA contains a set of containers and controls for the users to use that is largely based on drag able containers that automatically centre on screen when created, and later on allow the user to place them wherever they please. As seen in Figure 21, the main container is the base of every content element. It is vector based, stretchable and contains the drag-enhanced functionality.  Other common elements for most of project CCAs layouts are the main header, the general button element and the close button. In prompts, the close and the general button provide the same functionality, with different messages. From these base elements most of project CCAs views have been built.  Aside from the floating elements, a typical CCA UI contains the user control menu. This element attaches itself to the bottom of the game screen and displays user specific information such as the account name and the amount of points the user has, as well as providing logout functionality. During a game play the UI also has a game related menu, with information about the game you are playing, as well as controls to get back to the main UI.  For proper interaction, every element needs three basic states for a solid user experience; normal, hover / active and clicked / reaction. Examples of project CCA button elements three states can be seen from Figure 22. They all follow the same logic, having a subtle highlight specific for the type of the button, and a full fill, recognizable action state that is clearly indicated. Providing these states allows the user to always be in control, to have a predictable response to every interaction.  CCA contains also elements that are not meant for interaction by the user, but for mere show of information. During the game there is a large clock displaying how much time is left in the current game, and how many sections are included in it. Another element that a user can view is the indicator for performance in the current game. The symbols stars and crosses show the user how well he is doing during the exercise, whilst being inconspicuous enough not to distract the exercise. Examples of the clock and the star/cross performance indicator can be found in Figure 53.  Colors contain a lot of meaning. The selection of colors for a user interface, and especially for branding of a platform is an important step. Colors, while being culturally dependent and highly subjective, offer a lot of meaning and interpretation and can greatly enhance the user experience and aesthetics of a system. Whilst there is a plethora of articles about color in physics, psychology and other disciplines, in the context of this study color is used to discuss the merits of the use of color in the reference project from a user and artistic point of view.  Figure 23 shows the main color themes in CCA. The main color is a green shade (R: 184, G: 227, B: 115), normally used in a radial gradient with multiple stops between the end and start points. The gradient form is to create a feeling of enlightenment, a subtle focus on the content in the middle. The green background and color scheme is used throughout the buttons and modal dialogs wherever something related to the main game is encountered. The blue (R: 91, G: 166, B: 218) theme is used in information and statistics, where there is a need for more contrast with the elements and a distinct visual appearance.  Other than the dominant background and highlight colors, the main color for text in CCA is always white, with a subtle drop shadow to bring it forward from low contrast backgrounds. In buttons and other interact able controls, the active color is always dark grey or black, to highlight the change and alert the user to an action.  In addition to the use of color in the platform interface, the games themselves have a specific use of color. Each creature in the game has its own color scheme, containing various hues and shades to create an organic “creature”.  The use of shapes and colors in the game relies on feature integration theory; a theory of attention suggesting that perceiving stimuli can be divided into two separate tasks: features and objects. Features can be registered fast and in parallel, whereas objects are slower and separately identified. Visual searches regarding these two tasks are called feature search and conjunction search. Feature searches are fast sweeps targeting only one feature, such as color or shape, while conjunction searches work with a combination of features. [48]  In CCA, there are game modes targeting both types of visual search. The game types focusing on multiple creatures on a neutral light grey background focus on the use of feature search where the target mode of the creature is a change in contrast, color and / or shape. The game platform also has a mode for rendering a specific Perlin noise background matching the color set of the active creature, aimed at the use of conjunction search. The use of a background where the creature easily blends in forces the user to focus on finding the shape by combining color and shape information. The task of separating the object from the background layer is called figure-ground separation. [49]  Motion is a powerful tool in the world of modern digital communication when creating a unified user experience, especially for the consumer market. By creating a specific visual narrative, a repeating pattern of interactions and movement that builds upon the visual style and reinforces it with suitable movement one can build a memorable and effective user experience.  In the realm of traditional animation, there are 12 principles for believable movement [50], of which the following are relevant to transitions in digital media and the CCA platform:  • Anticipation o To prepare the audience for the action and to make the action feel more realistic  • Slow in and slow out o To simulate most movement in the real world. Most of human movement and gravity based movement have an in-out easing curve, it builds up and it builds down.  • Arcs  o Trajectories are followed by most natural motion, most living creatures have structures that enable them to move in certain ways, follow certain paths. Emulating these trajectories creates organic looking movement.  • Secondary action o A complementary animation that adds to the main animation enhances its effect.  • Timing o Correct timing makes objects more real, like they were following the laws of physics.  • Appeal  o Anything the user sees and finds likeable, pleasant design, a quality of charm, simplicity and communication. Originally about drawn characters but applies to graphics as well.  The sections of movement and animation discussed in this study can be divided into three categories: Timing and flow, movement patterns and easing and nonlinear motion.  Timing is the part of animation that gives meaning to movement. [51]  The key in efficient visual narrative is twofold; timing and delays. Timing is the essence of transactions, interactions and transitions within the platform. How long does an element transition, how it appears, how it behaves. Delay is essential in combining different views and states. With the combination of these two, the base structure of visual narrative is achieved.  As Figure 24 shows, most transitions in the project platform last between 300ms and 600ms. In views, the time in is always 500ms, and the time out varies based on view, but the change trigger is always 750ms; when a transition from a content state is initiated the out animation call stack is started, and the 750ms delay timer is initiated at the same time. Regardless of the time the view takes to animate out, in 750ms, the animate in of the new state is called and started. This overlap makes it possible for a user to interact during transition, and the loose coupling between view animations makes the user always be in control.  On average, for a transition to be within acceptable limits it has to be between 100 and 1000ms. Immediate response for actions like clicking requires some indication of reaction within 100ms of the action. If you go over the 1000ms limit, people will start to think the system is sluggish and unresponsive. [37] In CCA, the times are based on iterations of visual aesthetics, feeling of motion, and professional opinion. Motion is a rather tender art, and the feel of a transition is based on the shapes, colours, graphics, elements, and the surrounding elements of the target. There is no unified rule that can be quantified for every solution; it depends greatly on the context and surrounding platform.  The buttons have the same click animation time always, to make the reaction based on a user intent unified. The difference in animation in and out time is essential since the shapes and means are different, in a menu button, the line, colour and shadow are animated, in the close button the shape and colour are animated and in the general button, the gradient, shadow and text colour are animated. All the buttons have different hover state animations; however they are visually coherent and unified to represent the same indication for the user.  Tweening, or inbetweening is a method of interpolating the change in value between points A and B. A basic tween is linear, consisting of a predictable amount of points between A and B. Dynamic tweening is adding acceleration and / or deceleration effects to animation by using easing algorithms [52]. The problem with static / linear tweening is that the movement looks fabricated and clunky. Dynamic tweening solves this problem by adding natural feeling movement patterns such as the slow in slow out principle of animation.  In programmatic animation, easing algorithms enable the feeling people perceive in physical movement. This effect is done by applying an algorithm to the interpolation of movement of the object. In principle the ease affects how the steps of movement are shown, where there are more steps and where less along the path of the animation. Figure 25 shows the most common easing algorithms used in tweening libraries on various platforms. Most are based on Robert Penners’ work on easing and tweening  [52].  In CCA, the use of easing algorithms is subtle, and a mix of the following:  Content boxes o Back.easeOut for background o Quad.easeOut for fades  Menu elements o Quad.easeOut for fades and color  o Strong.easeOut for the line  Countdowns o Strong.easeOut for scaling  Drag o Based on inertia, a custom easeOut calculated based on mouse movement and velocity  Most of the transitions are elegant, simple, based on the subtle curve of Quad based movement. The parts where there is a need for highlighting movement, as in dialog boxes and the pop out of elements, the Back based movement is used. For fast movement, and to give the feeling of control, of pace, the Strong movement is used.  For user reactions, the easing type of easeOut is almost exclusively used in CCA. The easeOut means the movement starts fast, and slows down as it decelerates in the end. It works well for user reactions as the start of the movement is the user initiated action, the response needs to be fast and clear, and easeIn algorithms tend to make the transition seem sluggish. EaseIn is only used when animating a background out with scaling of the height of the element. Since the Back easing algorithm has the bounce effect, and the background is closing, it is visually appealing and logical to have it do the bounce in the beginning.  After timing and easing are taken into account, what is left to create a solid visual narrative is the actual movement of objects. There needs to be a recognizable pattern of movement that strengthens the visual branding and solidifies the feeling of the user experience. In CCA different containers have different movement, but the general movement patterns are very similar, and create a solid feeling of a unified platform.  The button animations all contain the same click effect animation and timing. When a user clicks a button, the text or main shape of the button turn black in colour over 300ms. The menu button is a good example of the secondary action principle of animation. The main action is the colour animation to black; the secondary supporting animation is the line growing from the left to right underneath the text. The line animation does not dominate the visual, but builds on the effect, and supports the feeling of highlighting. Examples of button visual states can be seen in Figure 23.  Dialogs and content views all appear in a similar fashion, the background animates in first by scaling the height to 100% from 0, while animating the opacity of the element to a full 1.0 form 0. These two combined make the background appear smoothly from thin air. The content inside dialogs is animated in with a sequenced animation, fading in elements and in the case of lists, also animating the x coordinates of the element from –N to 0. (Where N is subjective to the size of the element) The sequence timings can be found from the Figure 24.  The basic pattern of animation in user interface elements for project CCA is the fade in / fade out; the simple vanishing and appearance of objects. It is supported by secondary actions such as colour animations, movement, scaling of horizontal and vertical size, depending on the object animated.  From development point of view, creating a visual narrative requires a specific way of creating your user interface classes. When transitioning between states, many platforms and systems make the mistake of queuing the transitions in a way that makes the user wait for interactions between states. This breaks the narrative flow and lengthens the response time of the system to the users’ frustration.  When creating a solid platform with UX in mind, it is necessary to plan in advance, and to create methods for handling view states concurrently. In project CCA when states are transitioned, every control taking part in the transition executes their hide / show mechanic. There is a set delay between calling the next state when current state is transitioning out and there is no queue. As soon as a control starts to form on the screen, a user can interact with it. The user can stop the current transition and use another control while transitions are in place as well.  Creating modern user experiences requires the developer to take this into account when creating their solution. It saves time and effort to solve the issue of state handling and transitioning between states already in the beginning or planning phase of the project. When building individual controls it is beneficial to have a set template to start from that contains all standard functionality needed for state handling. Whether to implement this via inheritance or other methods is up to the developer.  In CCA every visual element from single controls such as a button or content views such as the main menu to modal dialogs and user menus, all of them contain basic functionality for handling their own view state. The repeated functionality in all visual controls of CCA are show, hide and dispose, as seen in Figure 26.  A necessary requirement and in creating modern interfaces with programmatic animation is the ability to override on-going animations. Overriding enables the stopping transitions and interacting with controls mid transition to keep the user in control. Modern UI technologies either contain suitable Tweening libraries in-built or one can use one of the many open source alternatives out there. In CCA the library in use is an extremely robust and extensive tweening platform called Greensock Tweening Platform [54] by Jack Doyle (www.greensock.com). GSAP is available for Flash and HTML / Javascript based platforms.  GSAP abstracts many of the verbose methods needed in creating programmatic animation, such as the setting of filters, timelines and sequencing by offering a simple, but powerful syntax that allows for the tweening of any numeric property, alongside platform specific properties such as CSS transformations. It is also quite robust, efficient and contains a full open documentation. The platform is used by many of the top sites and digital experiences in the world, such as big movie productions, big brands and art projects. [54]  In CCA there are two different systems for showing performance and ability. The user has access to pure numbers and graphical visualizations of performance and numeric data. The platform provides statistics on all game modes, as well as training program based data for the user to analyze. The idea is to enable the user to transparently go through how well he has done and how he has improved and draw his own correlations about the program. Example of the histogram approach is shown in Figure 27.  The second way the user is communicated the importance and performance of the data is via a very UX oriented method – achievements. The user is provided different rewards in three different categories of achievements: achievements, stars and trophies. The different rewards are all catering to a certain part of the measured data and user response. Every achievement has their own icon and a badge the user can see once they log in.  The achievements provide a very human understandable way of communicating complex data oriented events. The user can for example get a trophy from doing better than previous runs, or for having a perfect run where there were no missed reactions. The rewards vary between the different game modes. In comparison to the raw data shown by the histograms that do provide a valuable way of seeing visually where you improve, the achievements allow the user to gain specific feedback for specific parts of their game performance. Example view of achievements in CCA is seen in Figure 28.  CCA is a user experience driven cognitive gaming platform harnessing the power of the Adobe Flash Platform in the front end implementation, and the LabVIEW system design software as the server side solution.  The general architecture of the platform is based on typical client-server architecture. Overview of the architecture is seen in Figure 29. The client connects to the server and authenticates the user account, after which the server sends a packet of data that determines what options are available for the user with the account. The user interacts with the options available, and a new request is sent to the server, to which the server responds by providing wanted data, be it user account details or a list of games.  As is common for Flash based web applications, there are no page loads or page requests during the application run. The state of the page is managed by the application platform itself, and all interactions and reactions by the user are parsed, processed and handled without the browser. As such the platform itself can be ran with a flash player outside of browser environments as well.  Labview (short for Laboratory Virtual Instrumentation Engineering Workbench) is a programming and system design platform built by National Instruments. It uses a dataflow-based programming language, which enables users to program logic with the use of graphical block diagrams. [55] The platform provides a very visual way of creating functional systems.  The platform front end is based on Actionscript 3.0 (AS3). The Ecmascript based 3rd iteration of the Actionscript language is an object oriented programming language running on the Actionscript virtual machine. The version of the virtual machine that supports AS3 was built from ground up to support the new OOP nature of the language. In combination with the Flash development tools like Flash Builder and Flash CS5, the AS3 is a powerful tool for interactive media.  The front end is built to be modular, and much of the control comes from the backend. This data-driven architecture helps keep the platform modifiable for different purposes and user programs without making changes in the front end implementation. [23] The server side provides lists of menu items and games available, as well as statistics and user account data. The front end has different tracks of interaction and progression through the solution that are triggered by the server messages.  Figure 30 gives an overview of what packages, views and controls the CCA front end platform contains. In AS3 projects, there is always a root stub that initiates the solution when started. The main solution initiates the application model and main UI elements and queries the server for language versioned UI definition XML. After that the navigation service handles user interactions and along with the UI state handler keeps the visual and logical state of the application in sync.  The model package contains main application logic and data related functionality. The application model handles application states with the navigation service, and holds references to the different parts of the application. The navigation is integrated tightly with the UI state handler that handles the UI state changes and controls what the user sees. Data services handle all server communication and data parsing.  The creatures package is the home of the main rendering engine. All the heavy lifting mathematics with path generation and creature drawing is done by the package. The pathfinder handles the generation of new path points and general route handling. The creature handles the algorithms, creature specific calibration and state information. The brain is responsible for combining the input from path finder and the creature and to pass off the necessary values to the renderer, which then does the actual plotting and drawing of the creatures. A detailed description of how the rendering core works is in Chapter 6 of this study.  The game handling package handles all game related logic. Once the game data has loaded and parsed, the game handler initiates the correct game mode. The game mode handles the states within the game; everything from info prompts to countdown to the different phases of the game, data tracking, user performance and the end and fail conditions. Each game mode has their own conditions for game complete and their own goals.  The UI package contains all UI elements, views and controls. Essentially the GUI is constructed via the UI package. The main UI class acts as an interface for the UI elements: initiating them based on UI states and disposing them as necessary. The game related UI elements are contained in their own package, and are used and called from the game handling classes. All controls are separated as usable entities, any view can use any control as needed.  The utilities package contains all extra functionality and helper classes that the views, game handling and data services need. It has sections for animation, data loaders and handlers, algorithm parsing, various physics and inertia handlers as well as the core Bezier classes. Utilities are used through the application model by any section of the solution that needs them. Most utilities are initiated only once, and kept in memory to be used as a static instance.  The CCA platform was built to be highly manageable from the back end solution. Even though the rendering engine and game logic resides in the rendering and game engine in the front end platform, as much as possible of the configuration of the games and rendering options were separated from the engine and interaction logic.  The communication between the front and the back end services is done using a fairly standard REST API. Every interaction is based on HTTP POST calls with structured XML Data. XML is an application profile or restricted form of SGML, the Standard Generalized Markup Language [56]. XML is a standards-based and well documented human readable format for communication and configuration, and a suitable choice for the platform. Due to the nature of the game rendering engine, XML was also suitable for conveying the mathematical formulas and sets of functions needed to create the creature renders. The Flash platform also contains an in built support for E4X [57], the extension for AS3 that makes XML a native primitive in the programming environment, making the parsing and use of XML efficient.  When the user enters the platform, the front end queries the server for the latest language information. After the user inputs his credentials, the server is queried with the user information and a request for the main user interface details for said user. As seen in Figure 31, the server parses the request, matches the user login information and returns information regarding the user account. The front end platform constructs the personalized user interface of the user based on said instructions.  The platform supports a user based exercise program that monitors how well the user is doing and calibrates it to match the performance and improve it. When a user logs in, they are greeted by a personalized message based on their previous performance.  The user has specific games based on his / her exercise program and a set amount of exercised per game per day or a set time period he can perform. The program can be managed and calibrated to each user’s needs, based on progression and scores from the game.  Most of the data is served to front end only when needed. For example when the user wants to see a certain performance graph it is loaded only on users request and rendered with the latest data. Each view of the chart is requested per interaction to limit the amount of data sent and to keep response times low. In principle everything in the front end is based on the data transmitted from the back end services, except for the rendering and game logic, and user interaction.  The front end game logic is highly parameterized and the games are generated based on the instructions sent by the server in a game XML.  Figure 32 provides an overview of a single game. In the initialization phase, the front end fetches the gameand object parameters and initializes the correct game logic engine, instantiates the creature renderers per creature in the configuration file and sets the initial failure, success and timing of the game. During the game the front end game logic handles all user interaction, game events and data gathering. In the event of a game over, be it via failure or success, the game termination is triggered. The performance data is sent to the server, and in return an analysis of the performance of the users in relation to their performance program is dispatched and shown in the front end platform.  The game is initialized by using a set of configuration flags for the game logic and the rendering engine via the game XML. The game information is parsed from the game XML to a game value object. This VO is essentially the core of what a single game contains, and it is used in the rendering and game logic engine.  The game XML has 2 main parts: the game configuration, and the creature rendering. The creature rendering is covered in detail in chapter 6. A simplified hierarchy of configuration details in the game XML is shown in Figure 33. In principle the configuration construes from 4 main sections: The user settings, game settings, optional configuration and game object data. The game object is a simple set of variables that define the user id, game name, game id and login id for the user session and game management with the server.  User settings are related to the user account of the logged in user. The player information is for UI notifications; mode contains game related data for the user. The mode determines what kind of a game logic instance is created, and also contains information about the delay and timing of the users abilities in reacting to stimuli. The mode related user information also contains an optional calibration property that determines the time and requirements of a calibration round before the actual game begins. Calibration is done to ensure proper user difficulty in training.  Game settings define (alongside the failure condition) the structure of the game. The periods contain, depending on the game mode, the different sections of the game. Each section has a target time, performance and creatures that are used during that period. The target timing contains an array of timestamps on when the rendering engine displays a target mode effect that prompts the user to react. The target timing also contains information on which creature said target timing affects.  The target timing was originally done in the game logic engine with only minimum and maximum times for the frequency of the target event given by the server. But during development and testing there arose a need for a more fine grained control over target states, especially with the integration of the user training programs. With the server in control of the target times, various iterations of the same game can be made without affecting the front end platform codebase.  The optional settings include modules that are used in different game modes, and a medical imaging and measurement helper. The foto detector is an indicator that shows whenever a creature reaches a target state. Its size and color can be configured depending on the use case. It is used when recording user activity via medical imaging and other instruments as a synchronizing signal. The background generation defines the needed values for the background Perlin noise generation, from the actual noise to the color treshholding. More information about the background Perlin noise generation is found in chapter 6.4.3.  5.4 Game rendering event  The system in game engines that controls the ongoing simulation is the main loop. It is the representation of time in your engine, and the layer responsible for the game life cycle [58]. In CCA the front end game engine is based on asynchronous events between different modules. The core of the rendering engine is the renderer class. Any visual, interaction or module that requires a time based rendering pass such as the path, creature and target rendering and calculations subscribe to the main rendering class render event and base their rendering passes on it. The event is a custom event with a unique rendering time key that is then employed in the various time based calculations in the rendering. Figure 34 shows the most important renderer event related dependencies.  The separation of direct calls gives the platform a manageable and simple way to add new requirements on the render pipeline. During the development of the platform the rendering core started from a simple mode of having creatures running free on the screen, and slowly evolved to support dynamically moving bubbles for the background, time rendering visualizations and calibration events all subscribing to the main rendering thread.  The separation also allows for an important feature for any gaming platform, the complete control over the flow of time. With this control all forms of prompts, countdowns, user feedback and result screens can be efficiently and smoothly incorporated to the game designs at any time.  5.5 User reaction tracking  In order to get a complete account of how a user performs during a game in the CCA platform, all user reactions are tracked, even those that don’t end up showing any visual signs in the user interface. Every reaction, be it valid or invalid is time stamped and logged, alongside all creature target renders and game phase changes. The data is held in memory during the game period, and after the game is over, it is sent to the server for analysis.  The game data tracked during a play is shown in Figure 35. The data contains events divided into the game periods they belong to for precise analysis. Each event contains the timestamp in the relative time of the game rendering process, type of the event handled and possible tags related to the event. In addition, every reaction or target state handled contains a list of all visible creature positions for further analysis. The data also contains information about the actual rendering resolution of the game area.Tracking enables the use of more detailed information about how people play, and the strategies they employ in reacting optimally within the platform games. For example the difference between a user trying to optimize hit percentage by reacting at a regular interval, and a honest try at seeing the reactions can be taken into consideration with the analysis of the reaction data, especially with comparisons to previous user account performance.The core of project CCA is the rendering engine. It is a 2-dimensional engine, based on a standard XY-coordinate system. The coordinate system starts from the top left corner, and expands to the bottom right corner. The whole rendering area is flexible and user scalable, and the platform adapts to the user resolution by adjusting the speed and size of the rendered creatures for optimal playability. For the sake of rendering performance and playability, there are set minimum and maximum width and height values for the scaling.  The engine is a so called black box system, where the game is generated based on set rules and the engines own logic. This means that while being very controllable, the engine does not mindlessly repeat control orders, nor does it follow a linear human made path. The engine draws and moves the creatures based on their paths and targets, trying to find an optimum route, but does so within its own limits and algorithms.  Games often are made based on strict artistic control and very man made worlds, as it allows for a more fine grained control and precisely deterministic outcomes, but it lacks the surprising elegance of simulation based engines. A good example of some simulation engines are the modern physics engines in games, especially the ones based on soft body physics, instead of rigid body physics.  The basic principles of the rendering engine revolve around the concept of a creature. Each creature is an entity with its own path finding and rendering logic, and all game types work with changes in the creatures. The core separation of the creature object is the shape and path, as seen in Figure 36. The shape is the actual form of the creature, the visual end result that the user sees on screen, calculated based on a series of mathematical formulas and a set of parameters. The shape consists of the actual calculations for solving the formulas as well as the rendering of said formulas on screen with a set of colors and transparencies and possible dynamic movement. The shape also takes care of wrapping the shape on the actual path, generated by the path part.  The path is the core movement of the creature itself. Each creature has its own path generation, the role of which is to find out target locations for the creature and calculate optimum movement paths between them. The path is based on continuous cubic beziers, and recalculates itself every time the creature reaches the end of an interpolated target path and embarks on the next segment.  The rendering of the creatures on screen is based on two main parts; the plotting of the stationary creature on its own and the path wrapping calculation and plot. The creatures stationary calculation defines how the creature looks, is colored and how it performs its target state. The path wrapping takes the end result of the stationary phase, and integrates the creatures form into the shape of the path it is travelling on. The creature can also be rendered without the path as a stationary object, in its prime state.  During the project lifecycle, there were a couple of different ideas on how to render the actual creatures. With the path generation separated, it was possible to use hand drawn pieces of creatures and animate them on top of the path points to make them look more artistic and use less rendering mathematics as well. But for pure organic feeling and keeping full creative control in the hands of back end generation, it was deemed better to go with the option of drawing the creatures completely with mathematical formulas for full customizability.  The creatures are rendered based on a server served creature XML that contains settings and configuration as well as the actual formulas for creature generation. Figure 37 shows an overview of the creature generation. The settings define features that the creature has as a whole, such as the target states and length of the creature. The calculations contain the essence of the creature. Each formula has a set of configuration attributes that explain how the particular set of formulas is rendered.  The formula settings define the look and feel of both the creature surfaces as well as the creature main lines. In addition the values related to actual plotting of the formulas are introduced; the amount of points to be rendered, the start value of the plot and the distance the plotting is done on. Each formula is a set of two X and Y coordinate plots. The main plot is the first XY pair, and the return plot is the XY2 pair; when these are combined they form a surface with a fill that is a part of the creature.  The actual creature rendering is a stacked plotting approach to rendering mathematical expressions, spreading them over a uniform scale, wrapping them up with return functions to create surfaces and mirroring the end result to optimize rendering. The creature form is unified on both sides, so only one half of the creature form is calculated, and then mirrored to create the actual creature.  Figure 38 shows an overview of how a creature is rendered from the set of algorithms provided by the creature XML. The first line shows the first formula being plotted, first the initial plot of points without lines drawn is shown, then the fill with lines and shape. Then the return formula (XY2) is plotted, and it forms a complex shape. Then the return and the first formula are filled to create the end result. This is done for each formula and drawn on top of each other on the creatures’ canvas, and the formula 4 line contains the end result creature, built from the 4 mathematical expressions.  The creatures are based on two types of formulas. The various formulas are plotted on the screen and the 2d coordinate system with the time seed value from the rendering event time loop.  The base of the creature and most of the formulas are static formulas. Static formulas are plotted once per creature per formula. After the initial plotting of the formula, the plotted points are stored in vectors (typed arrays of AS3) and used for path wrapping and physics calculations, before finally being turned into visual renderings.  The amount of plot points is defined per formula, as are the values the formula is plotted on. The definition of the target state is given as a separate formula to be rendered when triggered. All the times of trigger and densities and colors and opacity levels are configurable per formula.  As seen in in Figure 39, a single expression of a creature consists of a set of parameters, followed by the initial and the return formula of the single shape to be rendered. Each static formula has a variable that is plotted with the given attributes. This variable is named (t) and it is parsed in the rendering engine into AS3 native value, along with the text representations of the math functions.  In the example expression, the (t) is plotted from valStart to the distance provided by valDist, over the course of the amount of points defined in valNum. After these have been plotted, the points are stretched with the length factor of the creature XML. The length factor is the length of the creature in ideal circumstances, and is adjusted as needed based on the resolution of the game area during play.  Some creatures based on rendering type and game mode also contain dynamic formulas. Dynamic formulas are plotted per render loop every time anew. This creates a relatively significant overhead for calculations for the processor. Dynamic formulas are used sparsely and mostly to add some flair or to make it harder to recognize the target states of the creatures. Figure 40 is an example of a dynamic expression, the definition in CCA for a dynamic variable is (c). Typically dynamic formulas are circular, so they create repeating smooth movement.  The third type of algorithm every creature contains is the target state calculation formula (Figure 41). This formula decides the look of the creature when it reaches target state, the state when a user is supposed to react to the change in the creature. The target state is parameterized in size and effect based on the user accounts previous score and calibration results. It is pre-calculated at the start of a game, and contains only static formulas. The difference between a normal drawing function and the state one is that the state is drawn for a set amount of time, as defined in the creature head part of the creature XML.  Every formula the creature is based on was originally a one way formula, forming a line but not a shape. After some development time there arose a need to make shapes, fills and surfaces in the creatures to add visual fidelity and concretize the creatures some more. To solve this need the ability to render return formulas for the various algorithms was introduced.  In principle a return formula is plotted on the same time values as the initial formula, and in the same scale. With the main formula it creates a complete shape or surface, which then can be filled with color and opacity to make it more vivid. Adding the return formula to the shapes enabled the creation of complex shapes, interloping with opacity and colors for unique creature designs. An example of how the return formula forms the body of the creature can be seen in Figure 38, and an example of the XML can be seen in Figure 42.  The creature formulas plotted on their various paths form the natural state of the creature. To create life and movement it needs to be integrated with the path provided by the path creator. The integration of the creature to the path is called path wrapping.  Figure 43 shows a visual representation of the difference between the creature in its natural state, and the path wrapped bended version of the creature. The red line at the bottom is the interpolated target path of the path calculation for the creature. The target path generation is explained in detail in chapter 6.3.2  When wrapping the formulas on the path, the formulas are plotted as usual to create the natural stationary creature, but after that the whole plot is bent around the segment path so that length-wise the center of the creature is on the current point of the path rendering progression. Then both halves are calculated from the center onwards, up to the point where the length of the creature ends. The interpolated target path is the size of the creature, and contains a point for each plot point in the stationary creature plot.  After the target path is generated, the creature is wrapped around the path point by point, calculating the angle at each step. This calculation is done per formula per creature; each formula is bent individually.  A simplified principle of the creature bending can be seen in Figure 44. First the integral of the segment, as well as the angle of the segment at any given point is solved, while making sure that the angle doesn’t go over – or + pi. Then the position of the creature is interpolated on the path, and the point is rotated based on the angle on the frame path at the same point. The creature is wrapped, point by point on the ongoing path, allowing it to react to every small change in the angle of the path individually, enabling the organic feel.  The movement of the creature is based on the path generation created by the path creator class. The path creator calculates the targets where the creature needs to go and the path itself and then attaches the creature on the path. The path is based on continuous cubic Bezier curves, to create a harmonic organic movement of the creatures while maintaining randomized movement paths.  A rendering of path creation in CCA can be seen in Figure 45. When the path is created it consists of 5 sections of points, each section has a minimum amount of 100 points between control points. A creature moves between the middle control points 3-4 and whenever it hits the 4rd control point, a new control point is generated on the path and the last control point is removed. When the game begins, first 2 points are generated by random, and after that based on the same rules as the new points in the game.  Because the middle and ongoing segment of the path needs to be unchanged, every decision reaction for the creature takes the time of going through the next 2 segment paths. By making sure the length of the paths per segment is never longer than 0.5 seconds, preferably around 0.3-0.4s, the creatures reaction speed is between 0.6-0.8s which is around the same as the reaction speed of an ordinary human being in a nontrivial task.  The paths are based on cubic Bezier curves. Every cubic Bezier curve has two control points and a start and an end point. When making continuous Bezier curves the start point of the next curve is always the end point of the previous curve. Making Bezier curves that continue from each other is relatively simple and requires no complex calculations. But when making smooth movement one needs to make continuous Bezier curves that are smooth, and that means each curve affects the curve before it and after it. An example curve can be seen in Figure 46.  For the path finding and rendering system to work (And because calculating continuous Bezier curves with the path wrapping isn’t cheap), the active creature path at any given moment must be a static Bezier curve. The path will change only when the creature reaches the end of the current path; then the next point on the path is calculated, and rendering continues. Because of this requirement, the path needs to have the five segments, or six points at all times.  Once the point locations are generated, the path needs to be smoothed. Since the motion is continuous through the points, the movement cannot have jumpy or unsmooth sections. To achieve this, the last control point of the previous curve needs to be on a straight line to the first control point of the next curve as seen in Figure 47. (The line goes through the control point and the path point)  For creating cubic Bezier curves, Flash contains a native library called BezierSegment. It is a collection of four point objects that define a single cubic Bezier curve, and has methods for getting the value of the curve at any point on the curve itself. [59] The solution in CCA is based on the BezierSegment class, and more specifically on the work done by Andy Woodruff on the implementation of continuous cubic beziers with AS3  [60].  One issue when using continuous Bezier curves as paths for movement is that when the angles of the previous and next point collide in a straight line (on the x or y axis), the line drawn is a straight line, and by default is calculated without any steps. To overcome this in CCA the straight line situation is checked after the Bezier generation, and a slight variation is added to the target point to get a proper curve between the points for calculations.  In CCA the continuous Bezier curve forms only the foundation for path generation, on which the actual movement is built. Once the base path is generated, the active segment is resampled for smoother movement and simple physics.  In Figure 48, the different parts of the path generation are shown. The RawInterpPath is the path created by the smooth Bezier curve generation. After the RawInterpPath is calculated, the active segment of the RawInterpPath is separated as its own entity, called InterpPath. The actual path used by the creature for movement is made by calculating the velocity and displacement of the InterpPath. This path is called a FramePath.  Figure 49 opens up the complex logic of creating the path. The actual FramePath is not needed for the current segment of the creature for movement. For that a FractionalFramePath is generated from the InterpPath, which contains a fractional index that is used to find the actual movement point in the end. The FractionalFramePath index is used by interpolating a point on the actual curve based on said index. In principle, the FractionalFramePath is the creature location on the InterpPath.  The physics used in the game are fairly simple. The main use for physics as such is to enable the creature to have different speeds based on its mood and the curvature of the path it is on. This creates the feel of organic movement, as the creature slows down to curves and speeds up when going long ways along a slightly curving path.  The path segment where the creature is currently moving on during the rendering is never changed. Because of this, the physics and velocity of the creature are calculated when the path is calculated. In general the path is calculated and physics applied to the movement only at points where the path is recalculated – when the creature reaches a new target location.  Aside from the mathematics of actual path generation, CCA also contains methods to create path finding for the creatures and boundary methods to keep the creatures rendered inside the actual game area.  There are two modes to the path finding; hunting and foraging. When foraging, the next path point is generated without a specific goal, by using pseudo random logic. In foraging mode, the creature finds its own way around the game area without specific incentives. When in hunting mode, the creature has a target array of points where it aims to move to, the closest target point is evaluated and an optimal path is generated towards said point. With the closest one calculated, the path finder finds out if it is close enough to nab it in this path, if not it’ll create one path segment towards it first and then redo the calculation. If the target is gone, the creature falls back to normal foraging mode.  Figure 50 shows a rough overview of how path finding logic works in CCA. The next target location is generated by combining the orientation (between previous two target points) with a randomized target angle and a speed based on creature length and the angle of movement. The core idea is to have a similar direction of movement to prevent too big, drastic changes in the creature direction.  If the next natural point in a creature’s path generation is outside the boundaries of the game area, a bounce method is applied. The bounce calculates how far outside the boundaries of the stage the creature was going for, and changes the angle of movement so that the new point is inside the stage. This is done to ensure the creature is always within the visible target area, and does not wander off the sides of the screen.  After the path generation and bounce factor have been taken into account, the distance between the previous and the new point is calculated to make sure the points are not too close to each other. In case of them being too close, a small speed fix is added to the point’s location in order to keep the length of the path relatively uniform. The length of the active path segment is set to be 1…2x the length of the creature to keep the reaction times in check.  Perlin noise is a procedural texture, or a pseudo random gradient that is used abundantly in modern computer generated art and graphics for its unique attribute of seeming organic and natural. The algorithm interpolates and combines random noise functions into a single function that generates more natural-seeming random noise. Perlin noise has been described as a “fractal sum of noise”. Developed by Ken Perlin back in 1980s for the movie Tron, Perlin noise has been used in CGI and had a huge impact on computer generated graphics ever since. [61]  Aside from using Perlin noise to draw patterns or textures, it is also very useful for creating organic, smooth movement. The movement is achieved by using the luminance values of a black and white Perlin noise and attaching it to the velocity of a particle engine. Due to the nature of the noise, one can create the noise field once and then change the offset of the x and y coordinate space of a single octave of the noise. With this the noise moves without recalculating the whole noise while maintaining the same visual similarity. Perlin noise is also quite optimized and suitable for performance use.  Figure 51 shows a Perlin noise field used in project CCA to move the background particles around. This noise field is randomized on every application run, and resize event, so the movement of the background pattern seems organic and never repeats itself. Perlin noise is rarely used to just create visuals, but rather to blend other visuals together, or to generate objects, landscape and other items that benefit from natural seeming patterns. [61]  The way Perlin noise is used in the background rendering of the CCA system is quite simple. Every resize and init event, a a 2-dimensional Perlin noise is generated to fill the game background layer (no need to make it whole screen sized). It is restricted by the same resolution restriction stops as the main game area, thus simplifying the sizing issue to a single place. This Perlin noise is not visible to a user, and is used only to generate movement.  The noise field is approximately the size of the gaming area, and it is populated with N amount of particles. The amount of particles varies based on the user’s resolution, to create an optimum look versus performance. The particles are simple, round shapes with opacity, randomized to make them look like abstractions of bubbles. In the start the particles are positioned randomly along the noise field.  As previously mentioned, the game has a single renderer that all time based actions are performed on. On every render tick, the position of each particle is changed according to a set offset that is unique to each instance of the particle.  The opacity, angle, speed and scale of the particle are calculated from the brightness of the pixel at the new position of the particle (Figure 52). The brighter the noise at the coordinates, the faster the particle moves, and the larger it becomes, and vice versa. In addition to this noise based movement, each particle has a unique wander property that affects the navigation of the particle to make the movement a little bit unique. With the smooth, coherent noise we get from Perlin noise, it provides a unique and performance efficient way to generate organic movement.  In addition to working as background organic movement and mood setting, the particles served another purpose. During gameplay, correct reactions triggered a coloring effect on the background particles closest to the creature whose reaction was targeted. There was a set radius around the active creature within which the particles had to be.  The coloring was gradual; a smooth green shade with opacity stops per reaction was introduced. An opposite effect was also introduced; when a user repeatedly triggered reaction without a cause, outside of the reaction window of the target state, the particles were gradually colored with light shades of red. One could get an overview of a game situation at a glance.  In addition to making movement with Perlin noise, Perlin noise is used in the reference project as a creator of coloured, randomly generated backgrounds that fit the colour scheme of the creatures. The colour match is important as it makes the spotting of creature target states harder when there is less contrast between the background and the creature itself.  Figure 53 shows an example of how Perlin noise is used in the backgrounds of certain game modes. It was implemented by partially replicating the Perlin noise in use in the background animator. The actual effect was developed in true to platform fashion in a parameterized way, where the server provides a possible array of colours to be mapped into the background noise with the creature XML. These colour values are then mapped to the noise based on the brightness values of the noise.  The colours are mapped to the noise by first taking the highest and lowest brightness values in the black and white noise, then mapping the colours to the noise by assigning a brightness index per colour. In CCA a vector of values was used, since it is faster to iterate through an array of bytes than to manipulate an actual bitmap object.  A small trick was used in the resizing of Perlin noise fields since they are quite heavy to instantiate and generate, and there was a need to generate one anew every time the stage was resized. In calculation or rendering heavy tasks, it is not viable to run them every time a user or a program resizes the window, but rather set a small delayed call to the refresh function, and overwrite that delay every time the resize gets called. With this small optimization, one most of the time gets only one re-rendering round, even when the user holistically drags the window around.  Optimization should always be done holistically. Look at the big picture first and then drill down until you find the specific problem that is slowing your application. When you don’t optimize holistically, you risk fruitless optimizations. [62]  The amount of computing resources available to a game is finite. Optimizing code increases the amount of work the engine can achieve per unit of computational power, and maximizes the efficiency of the system. [62] In optimization there are many ways to speed up the code, but it is essential to identify and solve the actual performance bottlenecks within the laws of diminishing returns. The main trade-off to think about when optimizing is the development time versus complexity.  In a project with strict performance requirements such as the CCA, one has to be quite strict and diligent in handling performance bottlenecks. However, untimely optimization leads to chaos and low quality code, it is imperative to find out exactly what needs to be optimized before jumping in. Performance optimization is a balancing act between readable solution code and pure performance gain.  There are many ways to approach optimization, but the fundamental basis of the optimization lifecycle is testing, or benchmarking.  Figure 54 shows the basic idea of the optimization lifecycle. When optimizing, the essential point is to measure the gains. A benchmark is a point of reference in the game that serves as a comparison against future implementations. Benchmark should be reliable, quick and it should represent an actual gaming situation. The main use of benchmarks is that they enable relative comparisons.  After the comparison points are gathered, the detection step starts. The point of detection is to find the biggest return on investment for the optimization. In detection phase it is important to start from the big picture, and analyse layer by layer to the finer problem points until a suitable issue is found. After the detection, it is a matter of solving said issue. Solving can be about fixing a bug, toggling a flag, rewriting the algorithm involved or changing the data structure. [62]  Once the solving is done, the check phase begins. When checking, the benchmark is ran again and measured to see if the solution changed anything in the performance. After checking, it is all about repeating the same progress again until the biggest performance gains are figured. The idea of this cycle is to find optimization hotspots and bottlenecks. Hotspots are the points in your program that consume a lot of processing power. Typically hotspots are small amounts of code with a big hit on performance. Optimizing hotspots leads to significant performance benefits. Bottlenecks are particular points in the system execution that clog down the performance of the whole system.  Optimization can also be approached on a broader level by dividing it into three categories (Figure 55). On system level optimization the focus is on the use of resources of the target platform. The point is to find an implementation that utilises system resources with balance and efficiency. System level optimization is about planning a platform that utilizes the available resources without overusing in a sustainable way.  Application level optimization can also be called algorithm level optimization. It is the choices made in the data structures and algorithms that make up the platform. The idea is to use a good profiler tool to find out call hierarchies and time and iteration amounts of systems most used parts. Algorithm level optimisations are the crucial backbone of optimizing code. The identifying of a key part of code that uses a lot of processing time and optimizing it, or the node that calls it with a more efficient solution is of the best return on development time.  Micro level optimizations are the most common stereotypical types of optimisation. The small hacks on as low level as possible to get a bit of a performance boost out of a specific thing. The optimization of inner loops or pixel rendering routines that are called extremely often and that benefit from the most miniscule of an improvement because of the sheer amount of times they are called during an application execution.  In project CCA the concentration on optimization was made on all three levels. A structural, planning oriented performance review was conducted in the initial phases of the engine building. The algorithmic level was benchmarked and reiterated throughout the application development cycle, and the low level micro approach with rendering and mathematics was made in the engine building phase to smooth out the heaviest of operations in the engine.  Beyond the processes of optimization one must be wary of the pitfalls along the way. It is easy to get stuck on assumptions about performance problems and optimizing prematurely without knowing if it is a big performance sink. Optimizing based on your own machine, or with debug builds gives misleading results, as debug builds are often slower and riddled with processes not found in the final release build. In CCA during development there was a point where the focus was on micro optimizations on the engine rendering loop for too long, and without the aid of a profiler the real optimization pitfalls would have remained hidden.  A common trait in game rendering engines is to render only what is needed and reduce the level of detail dynamically whenever possible. In fact many games could not function without such optimizations due to the sheer size of the game worlds and the amount of rendering. In CCA the amount of rendering is relatively small compared to many of the 3-dimensional engines, but due to the measurements and the nature of the medical side of the platform it was necessary to render everything on screen without any reduction of rendering quality or level of detail. Because of this, in the creation of the rendering engine it was necessary to try and use all the tricks possible with a managed language to make it as fast as possible.  A good approach to optimizing holistically is to focus on detecting and solving issues on the system level whenever possible, then the algorithmic level, and only if no solution is found resort to micro level optimization.  The optimization of the platform can be divided into 3 main sections seen in Figure 56. In data optimization, the focus was on the issues of latency and communications with the client-server architecture. The point of data optimization was to minimize the latency of backend calls and limit the amount of data sent between the back and front end platforms. The front end requests data for a view, be it the main user interface, a data visualization component or a single game, only when initializing the view for the first time.  When non changing data such as the main user interface localization and configuration is loaded, it is cached on the front end and not re-fetched until the user next logs in again. When a response from the system takes between 200ms and 1000ms a user is prone to lose the feeling of flow and the user experience of the system is hampered. [37] For the sake of responsiveness and continuous feedback, the user interface shows a loading indicator whenever a data request takes longer than 400ms. The indicator is not shown on shorter loading times to keep the user flow uninterrupted.  When optimizing the application level, the focus was on making the rendering engine and game logic separate, and to manage garbage collection and memory handling with specific initialization and dispose functionality. The separation of the engine and the game logic optimized not only the development time of the platform, but the way responsibilities were divided between the two.  The initialization of objects in managed languages is costly, and can easily cause variation in the FPS of the system, as well as random hangs in program execution. Also the disposing of objects for garbage collection (GC) can cause the GC to run at times when the user is performing an important part of the platform. The ways of observing smoothness in a system are somewhat immaterial, and cannot be found purely from profilers. Extensive testing and visual assessing is needed to make sure the user experience flow remains unaltered. More information about garbage collection and GUI elements can be found in the Flash specific section 7.3.  The deepest iteration of optimization was done on the micro level; thinking about the mathematics used and testing different operators and in lining function calls and other hacks that produce less readable code but provide a performance boost in a time critical section of the platform. Mostly micro optimizations were done in the core rendering engine, on the calculation and drawing of the creatures and their movement.  The way to find important performance issues in project CCA was done with using the Flash Builder profiling tool for performance profiling (Figure 57). Profilers are the most common tools for optimization, gathering information about resource usage, most often the CPU load during a session. Profilers typically provide information about the amount of calls, self-time and total time of function calls as well as memory used in them. [62]. In CCA the profiler helped in generating memory footprints and locating high resource hogging sections of the rendering code, as well as finding bugs in disposing of objects.  In CCA the creature rendering model is a very versatile implementation of a data driven architecture. The creature parameters define how they each formula in them is plotted on screen and the detail level of each creature can be adjusted per formula as well. Each mathematical formula has an amount of points that are calculated and plotted to render out its true form, and these can be adjusted and manipulated with relative ease. The lower the amount points rendered on screen, the less fidelity in the creature, but on low end machines, or situations where there are many creatures on screen, it increases performance smoothly.  Figure 58 shows the clear difference in rendering quality based on the amount of control points or dots used in the creature generation. The overall shape and visual style of the creature is the same between the fill render and the 4x fill render, but the subtle details of the creature are more smooth and round in the 4x fill render than the normal fill render. The dot render shows the inflated dots generated by the plotting of the formulas, the dots themselves are used in the rendering only as the control points for the lines that shape the creature itself.  The difference in cumulative CPU calculation time in milliseconds between the 1x and 4x dot renders is shown in Figure 59. The values benchmarked by using the Flash Builder profiling tool set show that the rendering of 4x the amount of points on the same creature increase the calculation time approximately 32%. The difference is to be expected, as the fidelity increases, so does all the major computation in the creature rendering and path bending. More difference could be obtained by adding a formula for the creature generation itself instead of adjusting the points of the formulas calculated.  Different creatures can have different dot amounts for visual fidelity. Some formulas work better with low amounts of dots, while others require much more to look and behave properly. When the creatures are moving, the tighter turns they take, the more clear the complexity of the creature; if the creature consists of a low amount of dots, the lines between start to show when doing above 90 degree turns quite visibly. There is a small implementation in the path creator that tries to optimize creature movement in a way that it never does such high degree turns.  Due to the data centered design of the rendering, the creatures can be optimized based on user accounts, and user machine performance to create the best possible outcome. For most formulas in CCA, the conversion of the formulas to native code could be done in the initialization and the calculations for the necessary values out of them only once, and then reuse the plotted model of the creature in all the path bending operations without recalculating the plot. The performance gains were significant. In addition to the fidelity of the creatures, additional features can be optimized, such as the background Perlin noise particle animation can also be toggled as needed.  The Flash platform, or more specifically the Actionscript 3.0 (AS3) execution model, is single threaded. Because of the lack of threading, all sufficiently heavy actions on the UI thread slow down or stop the rendering of the whole UI layer. While being robust for a web technology, it is a severe limitation of processor use. AS3 is also a managed language, interpreted by a runtime, which leads to system managed memory and garbage collection.  There are various issues when profiling with managed languages, the nature of the byte code leads to more native level calls than pure native languages. The way AS3 handles array actions is more involved than native languages; every array is implemented via a generic data structure, so every array load means hitting that data structure and querying it for type information. [62] Because of the cost of even basic operations can be much more varied than in native code, it is beneficial to set a good baseline for performance data to know what to avoid.  The basic approach in optimizing for AS3 is to always use strictly typed operators everywhere in your code; it provides a significant runtime performance boost. Another way of optimizing what is needed in AS3 came with the Flash Player 10 release. Flash had no history of supporting typed arrays before the introduction of Vectors in FP10. Vectors have a significant overhead in declaring and wiping them, but they are extremely robust when iterated over significant amounts of values. [63] The difference between array and vector use is significant, as seen from Figure 60. The reading speed of vectors is roughly 40% faster than with traditional arrays. The writing speed difference is even greater, roughly 170%.  In AS3 the construction of new objects is very costly. When creating the creatures and parsing the formulas from XML to AS3 native math functions, it was useful to use a technique called object pooling. In object pooling you declare your objects only once and store them in a list. Instead of creating new objects when you need them, you use the objects already declared and stored from the list (Figure 61). The benefits of object pooling are greater when using more complex primitives; the increase in performance is 5x when using a basic type such as a Point, and over 80x when using a complex Sprite type. [62] In CCA each creature is object pooled, but so is each set of formulas that form a part of the creature.  To prevent the UI from slowing down in CCA, the initialization of objects into the object pool was also optimized. In the game handling initialization the creature classes belonging to the game are instantiated; The Perlin noise backgrounds are created, as well as all the creatures. When creatures are not on screen or do not belong in the current play mode, they are hidden from view and removed from the display render loop. Since the game core is about constant perception of the target moving, it is imperative not to have high changes in frame rate, on average even if the rendering frame rate is not the full required 30fps, the users aren’t as phased by the difference as long as it maintains a steady pace.  Alongside the optimizations mentioned here, there are great many smaller point solution optimizations used in CCA and available for the Flash platform. Links to further reading on the subject can be found from the associated references in the end of this thesis.  One of the basic optimizations to do is to remove loop invariant code. When doing large quantities of iterations, the cost of declaring iterators within loops and inner loops can be quite high, especially in managed languages such as AS3. It is useful to try and minimize the amount of variables declared inside loops when it is not necessary, even pre-declaring the loop iterators, let alone variables used in the loops saves performance. [62, 64]  Another optimization that tends to be bad for code readability, but that can help optimize code that is heavily used during rendering is the inlining of functions in your code. When moving functions inline, one must be wary of the cost for code readability, it significantly hampers the future development of the code base. Inlining calculation heavy functions can amount to a performance increase of 400% in AS3. [64]  Other uses for micro optimization are the manual redo of language specific mathematical libraries. There are many ways to solve basic things; such as the absolute of a number, via casting to int or using a ternary operator that can be faster than using the platform inbuilt Math.abs(). Small optimizations in AS3 can also be gained by using the right iterators for the right loops, for example when doing while loops it is faster to loop through in reverse order than forward looping. [64]  In CCA the micro level optimizations were used solely in the core rendering and path finding engine. There was an effort to keep the unmanageable only in the areas identified by profiling to require significant tuning and optimization. Function inlining is used in some processor heavy mathematical operations and reimplementation’s of things like Point.distance() and Math.abs() to improve low level efficiency. For most of the platform code algorithm and system level optimizations were preferred.  Another way of optimizing performance heavy bottlenecks in calculations and iterations is to use bitwise operators instead of verbose solutions. Since computers operate in binary, using bitwise operators can be a powerful programming tool. Often a few quick operations can easily replace what would otherwise be complex and heavy code. In certain situations, using bitwise operators can even amount to clearer code. [62]  Common bitwise techniques in AS3 revolve around using the inherent quality of binary operations; multiplying and dividing by the power of two, integer conversions, sign conversions, modulo, absolute value, minimum and maximum seeking and color conversions. Figure 62 shows common alternatives to operations in AS3 by using bitwise operators for efficiency.  Performance benefits for using bitwise operators in AS3 can be immense. Figure 63 shows the benefits of using bitwise instead of natural logic in common logical operations. The benefit is especially clear when using the bitwise method instead of the Math.abs(), an increase of roughly 2500% can be achieved. Basic functions such as multiplication and dividing run around 300-350% faster than the basic solution. When testing the modulus of a number the performance is around 600%, similar to testing if a number is even or uneven.  When optimizing the CCA rendering engine the approach was to benchmark key rendering situations, and find out the most performance intensive mathematical operations and optimize them. The most used and heavy functions were optimized with various micro optimizations, including removing loop invariant code, declaring variables together and bitwise operations.  Flash supports both Vector (Not to be confused with the Vector of typed arrays in Flash) and bitmap rendering. Both of them have their own benefits; drawing with vectors enables crisp, smooth and scalable rendering, as well as quite handy tools for handling the graphics. Whereas drawing with bitmaps allows for easy manipulation and bitmap specific filters such as PixelBender, and especially with complex and large graphical content, performance benefits. [64]  There are many ways to optimize the drawing of content in Flash. One good tool is to use the redraw regions debug option to visibly observe what regions of the screen are “dirty” and redrawn on every frame. Even though content is opaque, if it is on the display list it can still trigger hit tests and other runtime performance hogging tests as long as its visibility is set to true. One can also set the platform level toggle on movie quality to a lower amount; this affects antialiasing, smoothing of scaled bitmaps, fonts and animations.  Other performance heavy operations in native Flash drawing are the use of filters and blend modes. Filters such as drop shadow and blur need to be applied on the whole object they affect during render time, unless specifically set to cached. Efficient use of cached effects and minimizing the amount of alpha blending can also help rendering performance, especially on lower end hardware.  Most vector content in Flash can be cached as bitmaps and reused, even changing the objects x and y coordinates does not affect the caching benefit. However caching cannot be used for any content that needs to be redrawn every frame, such as the creatures in CCA.  In CCA both bitmap and vector based drawing methods were studied. The target was to find which would be most ideal for rendering the CCA creatures. Vector based rendering gives one smooth controlled geometrics in a fully scalable way, enabling graphical fidelity and an ease of creation. As opposed to vectors, bitmap rendering is purely pixel based, and as rendering goes, it is typically significantly faster than vector based rendering. Handling bitmap rendering is somewhat trickier than doing it in vectors, especially if you go with just pixel painting, since you need to manually take into account antialiasing and corners of the creatures  Since the creatures are plotted from mathematical formulas into vector shapes in CCA as the basis of rendering, using pure vector based rendering on the Flash platform level was an obvious choice. In the initial engine tests, there were no significant increases in performance from using bitmap based rendering over vector based rendering, as most of the CPU power went into path finding and creature calculations.  8 Results and future implications  8.1 Current situation  After the past two and a half years of development project, CCA is a mature, bug free platform and currently deployed in clinical trials as well as touring the Science Changing the World Exhibit. It has proven to be a useful tool for cognitive gaming; in preclinical alpha version testing, users were generally pleased with the tool. There were no harmful effects or discomfort reported, and 20 users out of 21 found the game appealing.  Despite the fact that the project got off to a good start, it was a large piece of software for two people to handle. There were many ideas, presentations and funding rounds to try and ramp up a company built around it to make it a viable consumer market platform for cognitive gaming, but none of them have succeeded so far.  Regardless of it not being as big as originally hoped it would be at this point, it is a solid work of engineering, user experience, science and hard work. CCA has performed well under tests and clinical trials. It is currently being translated to other languages and has been developed to be fully multilingual, based on different language resource XML files that are easily configured via the server interaction. There is a single simple API for language versioning that can be triggered on any back end call, so users can change the language at any point during platform use.  After the current clinical trials, there have been rough plans and ideas about the possibilities of reworking the core engine for mobile platforms, as well as ventures into the world of 3D, but the future of the platform remains open. The online version of CCA can be found at www.mybraincapacity.com.  The current version of project CCA is being used mainly in ongoing clinical trials in conjunction with HermoPharma (www.hermopharma.com), in their amblyopia studies. There have been plans for a mass market version and an open account creation, but for now the plans for large scale deployment are on hold.  Possible uses for the project CCA platform range from helping aged people hone their cognitive abilities to helping the rehabilitation of people after experiencing brain trauma. It has proven its capability as a cognitive gaming platform, and can be used for accurate user statistics and ongoing analysis of user’s progress, as well as measuring the effectiveness of drugs, by providing detailed data about gameplay and improvement.  The on-going clinical trial at the moment of writing this Thesis is related to ambylopia, also known as the lazy eye. A fairly common disorder characterized as vision deficiency in an eye that is physically normal and able. Ambylopia is estimated to affect the lives of between 1-5% of the population [66]. Hermopharma are in the process of developing a drug based on antidepressants to prevent or heal damage caused by ambylopia (www.ambylopia.fi).  In the trials, project CCA is being used as a measurement and a training program for the test users to gain accurate information about how the medicine works, and to see if the training of the affected is efficient in improving results. Figure 64 shows the campaign for the clinical trial. In addition to being used as a measurement device for the actual clinical trials, a standalone implementation of a single game mode was made to support testing for one eye at a time. The purpose for this version was to act as a campaign game for people interested in the possibility of amblyopia affecting their lives, giving them a preview of what the trial was about and to raise interest and awareness for it.  On top of the main game platform build, we were approached by Heureka (www.heureka.fi) to do a multiplayer version of the game for a new exhibition called Science changing the world they were doing in conjunction with 3 other leading science centers. The exhibition is a testament to science; a playful series of exhibits to interact with, and learn about new things in science and the ways health, life quality, even the planet can be improved with scientific methods. The show started at Heureka from 15.4.2010 (Figure 65) up until 15.1.2011. After that it has toured the Museon, the Hague in 2011, the Cité des sciences, Paris in 2011-2012, and it is in the Pavillion of Knowledge in Lisbon until 15.7.2013. After Lisbon, the exhibition is up for rent.  What Heureka wanted was an offline version of the rendering engine, with a specific game type and about 10 different game configurations and creature combinations. It differed from the main game in a few important aspects; It was not a training schedule based system, having no server interaction or user accounts, but a pure front end implementation, and it was to be played by up to four people at the same time, competing in cognition around who achieves the highest score. The game was also in 3 different languages, and the whole user interface of the game was redone to fit the resolution and language requirements.  Due to the modular nature of the platform and engine code, it was possible to separate the rendering engine with a game logic part and implement the new multiplayer requirements without too much trouble on top of the custom version. It was mutually beneficial, as some of the improvements made in the Heureka version ended up being ported back to the main game platform.  An additional other challenge of the Heureka version was that it was run on a 46” touch screen on a full high definition television (1920*1080px). That meant that the whole game area that the game runs on was relatively bigger than the typical CCA platform game area (around 1280*800px). Because of this it required extensive optimization and calibration of the points rendering and creature styles to make it perform well enough for show use.  The multiplayer mode was a single mode of the N-object capacity test; the game featured 1 to 5 creatures at the same time on screen, and the users had to react to each target state of any of the creatures within a set time frame. Each successful reaction per player was colored with its unique shade of color glow around the creature the reaction affected to keep it clear on who did the reaction.  Example gameplay of the Heureka CCA version can be seen in Figure 66. The game was played in a customized stand, with the 46” screen facing upwards. All controls for users were abstracted from the traditional keyboard, and each player had an arcade style control button to use to react. The custom controls were connected to a keyboard input, and could be mapped into the CCA Flash application via normal keyboard events.  In addition to the changes in the game mechanics, the multiplayer mode required its own user interface. In the multiplayer version, there were between 1 to 4 players, and the game needed be playable with any number of people between that range. The UI had to work in four directions, matching the four player slots around the stand.  To enable the users to play at the same time, a separate UI based on the same logic and graphical design of project CCA was implemented. The UI was built so a separate instance of for each direction was created and shown based on user participation. Each side of the game had an initial information screen informing the users about what they are supposed to do, and the game started by anyone pressing the start button. The game allowed hot seat style multiplayer, where anyone could jump in during the game and start reacting to the creatures, albeit coming in later would affect your score in the end of the game.  Only the players who were active during the game run were shown the game end screen with detailed information about how they did. In addition to the statistics, a crown reward system was implemented; the best user was rewarded with a gold crown, the second player a silver one, 3rd player got a bronze crown and the 4th one had to settle for nothing.  As opposed to the online version of the main platform, the Heureka version had to be a standalone installed package, with everything needed inside a single installer file. To do this with traditional Flash was impossible, and third party extensions were not reliable enough. Luckily Adobe itself had come up with a solution to the problem. Adobe AIR or the Adobe Integrated Runtime is a wrapper for the Flash player that allows it to handle native functionality, from file system access to running its own instance. [69]  Adobe AIR enables the use of web standard HTML and Javascript, as well as Actionscript 3.0/Flash to create native applications for all major desktop environments, as well as Android, iOS and certain smart TVs. [69] It is widely used in creating casual games for mobile platforms, as well as a lot of entertainment and ad campaign applications. Air has full support for Stage 3D OpenGL API for accelerated graphics, as well as a lot of native integration to iOS and Android services.  Developing for AIR brought about some small changes to the CCA codebase because it handles certain things like stage events and window events as well as keyboard interaction a bit differently, but all in all around 90% of the code could be ported to AIR within days. The in-built installer, application visuals such as icons and automatic updating functionality made the AIR version completely stand-alone, and enabled a solid experience for the people keeping up the show at the various science centers.  The biggest issue to solve in the Heureka version was the requirement for the game to run on its own for a minimum of 16 hours in a row. The game was automatically started at the beginning of a day, and the whole machine was shut down at the end of a show day. The application needed to be maintenance free during uptime, and it should be trustworthy and robust throughout the whole life time of the show.  The approach to the Heureka version required a lot of profiling and optimization. Before deploying the build in the actual environment, a series of test scripts were implemented. The scripts made the game play itself for a long time, changing the game parameters and reactions to simulate a real user situation. A logging of memory footprint and performance details was also implemented. These features enabled the testing of the game before actual deployment.  But as optimizations go, it is rarely beneficial to test only in development environments, as was the case in Heureka. Using the scripts the game seemed to work fine for the required 16 hours without memory leaks or slowing down, but during the actual show run, it started to show problems. We kept getting reports that the game randomly crashes after 2+ hours, depending on how many users have been using it and the rate of use during the day.  Al lot of time was spent trying to track down what caused this elaborate bug; by the Flash logs it showed to be running fine, and that memory use was not stacking up and rendering times were normal. But from the OS point of view, the app slowly ate away at the memory space allocated and crashed in the end when it had consumed too much of the system memory.  The reason this fatal bug was not noticed in the internal tests was because the tests always ran the game to completion, to the end screen, and then began a new round. It became apparent that the crash happened when time the game was started, but abandoned mid-way through without playing it to the end numerous times. (The game had a timeout function, if it was not played for a bit, it started showing instructions on what it does and encouraging people to play).  In the end after some serious time spent with the profiler and taking memory snapshots in different benchmark situations, a memory leak was found. A creature rendering class in the core engine rendering system was being reset for garbage collection with a call to not the correct level of inheritance, but the super class of it. This left the game with a small amount of assets each game creation that weren’t being cleaned up and slowly started to eat away at the memory available.  This problem had not been found during the main CCA engine development, due to the fact that the main game was never supposed to be used for more than a maximum of about an hour at a time. A lesson was learnt here; profiling and being rigorous about testing the memory footprint is imperative and extremely valuable when creating continuous running software.  Despite the problems during the development, and the rework needed to create the Heureka version, it is up and running around Europe at the moment. The branching was a success, and even if nothing else big will come of the project CCA, at least it got to educate people around Europe about science and cognition.  At the writing of this thesis, Matias is doing the clinical trials with HermoPharma, and the language versioning system enables fast and easy porting to other language areas. The Heureka version is still circling Europe. The current version is complete, but there are a few routes that have been pondered about for where to take CCA next.  One obvious platform for brain gaming and cognitive use that appeared during the development of the CCA platform and that offers lucrative amount of users and more importantly users that might benefit from the platform is the mobile space, especially the rise of the tablets. Tablets fit perfectly with the way the project CCA core works, and could be easily a good place to start spreading out to.  Due to the fact that Flash platform is not available for mobile except for Android, it is not a viable option for creating the project CCA for mobile. Even though the Air platform would support building to mobile environments, the computing power required on top of the managed language would most likely be too high for mobile CPUs. The heavy rendering and calculations required would need a native application to be built for CCA platform. On the other hand the core of the CCA engine is reasonably easily converted since it is very mathematics based.  The other way aside from mobile platforms is to evolve the CCA forwards towards the 3D space. The same engine would not work in a similar fashion in 3 dimensions, as it is currently a very much 2 dimensional solution, but the same ideas and approach could be viable in 3D. The serious gaming around platforms such as Unity (www.unity.com) is growing at a significant pace, enabling efficient solutions for beautiful yet useful gaming.  There are some initial steps towards looking into the possibility of building a next version of CCA with the Unity engine toolkit. Unity would provide a unified platform for all native computers, as well as a way into consoles and mobile gaming. Then again the advances in web standards and the rise of WebGL could enable a remake of CCA with Javascript and HTML5 Canvas based approach. The future remains open.  Cognitive gaming is a rising area of serious gaming. The approach that a brain can be trained and moulded to perform better at tasks we perceive to be meaningful is a exciting thought. In this day and age efficiency is one of the top most factors working life focuses on, and the ability to improve not only working methods or tools, but people themselves provides valuable and genuine possibilities.  With the rising popularity of mobile platforms and the improvements in traditional web, the focus has shifted from creating features to providing good user experience and servicing user needs. Going beyond usability, the importance of brand and aesthetics are also gaining ground in realizing products and experiences in the online world. User experience matters.  The approach to cognitive game platform building in this thesis is generable to serious game platforms as a whole. The issues solved relate to common, real world problems when creating performance heavy online implementations, and provide ways to overcome obstacles in the creation of good user experience as well as system performance. The project approach emphasizes a focus on user experience and solid software engineering.  The reference project is a complete cognitive gaming product; a feature rich user experience oriented platform for measuring attention and working memory. It is based on a data driven architecture with proper separation of concerns for rendering, game logic and user interface elements. The modular approach allows good controllability and modifiability without changing the front end platform code.  The rendering engine provides organic movement and creatures with a natural feel in endless combinations based on mathematical formulas. It is optimized on the algorithmic and micro level to provide pleasing visuals and fast enough rendering for attention games. Due to the separation of game logic and rendering, as well as the heavy emphasis on mathematics in the rendering, the engine is easy to export to different platforms in the future.  The project was a success, and is being used in clinical trials in Finland and Estonia as well as in the Science Changing the World Exhibit around Europe. User feedback has been encouraging during the alpha and the current medical trials. The future remains open, but the direction is towards mobile and tablet use, as well as employing 3D as a means for more immersed gameplay.  This study and the reference project stand as an example of the viability of modern web technologies in creating complex serious gaming platforms. It is a testament to the possibilities of cognitive end user training and serves as a guide on the approaches necessary to accomplish a successful cognitive gaming project.  This postgraduate study was done as part of the Master of Engineering Degree Program in Information Technology for the Metropolia University of Applied Sciences. The motivation for this thesis originated from personal interest in both the 4X strategy games and the game artificial intelligence in general, having experienced this genre first time when playing Sid Meier’s Civilization back in 1993 on the Macintosh LC II computer.  While I have been working in the game industry for over a decade now, the possibilities for exploring this particular area of interest have been limited, so the possibility of using it as the topic of my thesis was a rather natural choice. Although the scope of the research was very broad and the results did not reach the practical levels which I hoped for, working on this project was highly rewarding and will further motivate me for years to come.  I would like to thank my supervisor Ville Jääskeläinen, my family and friends for their support and encouragement during the writing of this thesis.  Instructor(s)  Ville Jääskeläinen  Although computer games have been around for over half a century, the gaming has matured to become a mainstream phenomenon in the past decade, partly propelled by the breakthrough of mobile platforms which provide users access to a vast selection of games. As the complexity and selection of games is constantly increasing, there is growing pressure to provide meaningful artificial intelligence (AI) opponents in certain gaming genres.  This master’s thesis focused on finding a way to implement an AI player for a turn-based 4X computer strategy game. As there was no suitable project at work to apply this research on at the time of the study, the project was created as a personal venture with a theoretical game used as the source for requirements.  In the research phase, dozens of sources of existing literature and information about the field were analyzed, which included extraction of the knowledge and technologies appropriate for this project. The selected technologies were documented in the thesis, and their use cases were identified, and examples were created for most of them.  A high-level technical design for AI integration was created as an outcome of this thesis, which describes a proposed architecture for the AI opponent and combines the technologies evaluated in the thesis into a modular framework. These features can also be leveraged in the player assistance features such as micro-management automation and advisor functions.  The resulting design was supported by actual prototype development done in Unity Editor of selected key technologies. These prototype implementations included a rule-based system inference engine, A* pathfinding on a hexagonal grid map, a spatial database for tracking map data such as player influence, and tactical pathfinding leveraging the information in this database.  Although a complete game was not created during the project, the technical design and research done can be used as a foundation for building AI opponents in turn-based strategy games in future. The practical implementations will most likely also provide feedback on the possible shortcomings of the design and possibilities for improvement, which will be reflected back on the design.  Keywords  Artificial Intelligence, Computer Games, Game Industry, Prototyping, Strategy Games, Technical Design, Unity Since the early days of first publicly available video games in 1970s, the business around gaming has grown to a large and high-revenue business, known as interactive entertainment or video game industry. These are some key facts about the industry mentioned in Entertainment Software Association’s annual report of 2015 to give perspective about the business case for this project [1 p. 14]:  • Video game industry generated $23 billion sales in United States alone in 2014  • Gamers spent globally estimated $71 billion on games in 2014  • There are 155 million people in United States that play video games  • A total of 1641 video game companies are in United States alone, spread along 1871 locations  • Education of video game industry is being offered in 496 programs in 406 schools in United States  The process of game production can be simplified as an illustration of the value chain of a production, as seen below in Figure 1:  Time Constraints in Production  In game industry, the schedule of game production has a crucial role because it directly affects the development costs of the title. Due to this, there is limited time for doing specific pioneering research during game development, as there is a lot of pressure to meet deadlines and to provide value for the producer. Sometimes, there are separate teams which will do engine and technology development, but even in that case prioritization needs to be done for optimal developer resource allocation. In the author’s experience, Artificial Intelligence (AI) development often ends up being one of the fields which will get less focus [3].  Role of AI Programming  Because of the previously mentioned factors, especially in small and medium-sized teams there usually is not a dedicated AI programmer, but those features will be rather added into the game by gameplay and generalist programmers. This allows better flexibility in the team resource management, and usually generalist programmers do have good basic knowledge on the topic to create usable and functional implementations. However, when the requirements for the AI features and quality increase, dedicated skills in AI programming will prove to be invaluable.  Comparison between Academic AI research and Game Industry  When people talk about AI, they are generally referring to the academic AI which is quite different from the game AI. While academic AI aims to solve problems requiring intelligence, the purpose of game AI is to give illusion of intelligence and entertain the player. The goal of AI research in academic setting is usually to create publications and do original research, while game AI development aims to create a game. Academic AI research is often funded by grants, academic institutions or sponsorships, while game AI is funded by the game publishers. It’s said that there is strong division between the two fields, but in practice both parties have the possibility to benefit from each other; with help of the results of academic research, game developers can add increasingly advanced AI to the games, while academic AI research can benefit from game engines which they can use for their research [3].  1.1 	Scope of Thesis and Research Design  The goal of this thesis was to produce a technical design for adding AI features into a turn-based strategy game. This included:  • Figuring out the requirements imposed by the game for the AI.  • Determining which of the existing solutions in the field of AI knowledge were appropriate to satisfy those goals.  • Designing the best methods for integrating them in the game.  • Choosing proper balance between AI programmers’ and game designers’ workload by necessary tools.  A full, working game where the technical design would have been implemented was out of the scope of this thesis, but limited amount of prototyping was set as a secondary goal during the writing process to provide some practical analysis on the feasibility of theoretical choices made in the thesis. The research design is illustrated in Figure 2.  The research design (Figure 2) shows the phases of the project and the respective thesis and prototype work involved in each of them. Most of them match the chapters in this thesis, in the following order:  1. Introduction chapter outlines the goals and motivations behind the thesis work and gives introduction to the game business to which this work relates to. The initial prototype work was also started at this point, and the requirements it imposes for project were gathered  2. Background chapter contains brief history of AI in games and the related gaming genre, and describes the prototype and lists the previously gathered requirements. During this phase, the relevant technologies were picked  3. Technology chapter was created through iteration of selected technologies, during which the purpose of each one of them was described in the context of this thesis work. Also, some of them were integrated in this phase to the prototype  4. The Proposed Solution was developed initially during the technology iteration phase, and finalized in the review phase. This included outlining the high-level system, during which prototype was also refactored based on the results of earlier integration  5. In Evaluation chapter, the feasibility of proposed solution is evaluated, and this evaluation is partially backed by the data that was extracted from the prototype at this stage  6. The Discussion and Conclusions chapter analyses the results of the thesis, evaluates the outcome of the research and contains suggestions for further improvements  In addition, the project also produced a partially working prototype, for which the source code was included in the appendices of this thesis. 	  This section gives a brief overview of the history of game AI and the 4X strategy gaming genre which the project belongs to. It also describes the technical starting point of the prototype and outlines its requirements for the AI.  2.1 	History of Artificial Intelligence in Games  The concept of artificial intelligence itself is nothing new; even in ancient times, mankind has been interested in the concept of artificial life and intelligence. This shows in the various fictional stories and attempts to build automatons even centuries ago. However, the major breakthroughs in electronics and computer technology in past century have allowed unprecedented advancement in research of artificial intelligence [4 pp. 4-5].  After the first general purpose programmable computers were invented, it did not take long time until the first AI programs were developed. One of the first published ones was Alan Turing’s chess program, although at the time the computers were not advanced enough to completely run it [4 p. 6].  Although there has been constant academic interest in AI research, breakthrough in game AI started in the 1970’s, when first video arcade games were developed. They featured primitive computer opponents such as the aliens in Space Invaders and ghosts in Pacman. Although the AI in those games operated on simple deterministic algorithms, they gave the player impression of intelligent behavior. Since the early video games, the game AI development has slowly advanced, and different gaming genres have their own specialized requirements, ranging from tactical real-time Non-Player Characters (NPCs) in first-person shooters to complex strategic planning in strategy games, and even artificial life simulation [5].  Current State and Future Development  In the past decades, there has been a lot of progress especially in terms of graphics quality and storytelling aspects of games. With the increased complexity and depth of games turning into interactive entertainment, there is increased desire for creating advanced artificial intelligence functionality to improve the gameplay experience and immersion for the players. Thanks to the constantly advancing computing capabilities of modern computers there are now better possibilities to implement advanced AI than ever before, such as increased focus on self-learning AIs that will adapt and learn from players [6 pp. 3-5].  2.2 	4X Strategy Games  The term “4X” was created by Alan Emrich, who used it in review of “Master of Orion” in 1993 for the first time:  “...it features the essential four X's of any good strategic conquest game: EXplore, EXpand, EXploit and EXterminate. In other words, players must rise from humble beginnings, finding their way around the map while building up the largest, most efficient empire possible. Naturally, the other players will be trying to do the same, therefore their extermination becomes a paramount concern.” [7]  Although majority of 4X strategy games are turn-based, there are a few examples of realtime games which are considered to belong to this genre. However, the classification of games in 4X genre can be difficult, and sometimes a few additional criteria have been used to narrow what fits the definition, such as empire economy control and diplomacy versus combat-focused gameplay in regular strategy games [8].  Unlike many other game genres, strategy games are highly dependent on skilled AI to provide meaningful gameplay experience for the player. As the opponents’ tactics and strategy are foundation of the core gameplay challenge for player, a poorly implemented computer AI would most likely be acceptable by casual gamers, but advanced players would find it boring and unrewarding [5].  Civilization  Sid Meier’s Civilization series is one of the best-known examples of turn-based 4X strategy games (see Figure 3). The game has taken heavily inspiration from previous board and computer games such as Risk and Empire, but also added novel features such as technology tree [9].   Figure 3. The original Sid Meier’s Civilization (Macintosh version pictured).  Originally released in 1991, the series has had six major releases and several spin-offs. In Civilization, the player takes role of leader of a nation which he/she will lead from stone age to modern day. In the first game in the series there were only three conditions for ending the game; world domination by eliminating all opponents, building a spaceship to reach Alpha Centauri, or running out of time in the year 2100. Later versions of game have gradually added more victory and endgame conditions along with many other features in each major release [9].  Master of Orion  In Master of Orion (see Figure 4), the players control number of different races which compete for control of the galaxy. The game features exploration, discovery of new technologies, dealing with other players with diplomatic or military means, and endgame conditions which are similar like to the ones in Civilization [10].   Figure 4. Master of Orion II.  Besides being space-themed, the game trades off the grid-based movement and exploration with a more restricted and simpler model where movement is only allowed between solar systems. Other major difference is also how battles are handled; instead of single-unit attacks, the combat happens on a separate combat screen where multiple fleets of both participants are fighting at once. Players have control over individual ships and weapons, which can lead to complex tactical battles [10].  The game has been influenced by some earlier games, such as Reach for the Stars. The original series had three releases, and the franchise was recently rebooted by Wargaming on Steam [7; 10].  Galactic Civilizations  Galactic Civilizations (see Figure 5) is a series of strategy games, which was initially released for OS/2 in 1993 by Stardock Corporation. The game combines the turn-based grid map familiar from Civilization with space exploration theme like in Master of Orion [11].   Figure 5. Galactic Civilizations Gold for OS/2 [12].  Galactic Civilizations was one of the few games released for IBM’s OS/2 operating system, and it received recognition even from IBM who licensed the game and included it rebranded as “Star Emperor” in their FunPak software bundle. However, the OS/2 operating system had limited user base and later lost its remaining market share to Microsoft Windows on the PC platform, thus later releases were made for the Windows platform, most recently on Steam [11; 13].  2.3 	Prototype of Strategy Game as Foundation  Due to time constraints, the implementation part of this thesis focuses on working on a theoretical prototype of the game. This allows the development to focus on areas relevant for artificial intelligence integration.  The game itself is be turn-based, with each player performing actions in sequential order. The game world is represented as a hexagonal grid map, to which players have their own views depending on exploration and espionage status. The game draws inspiration from Civilization and Master of Orion series, creating mix of them with resemblance of the original Galactic Civilizations.  Unity 3D  For rapid prototype development, an off-shelf game engine is used. Unity 3D (see Figure 6) is one of the most popular engines today, which has not only gained popularity as mobile game development platform, but also has seen use in recent AAA-grade desktop titles such as the recent remake of Master of Orion and city-planning simulator Cities Skylines [14].  Figure 6. The Unity3D Editor running on macOS platform.  Originally released for Mac OS X platform in 2005, the engine has had five major releases and is currently available for both Windows and Mac OS X [15]. Unity features component-based architecture, advanced 3D engine that can target various graphic APIs on several platforms, NVidia PhysX engine and for physics simulation, and various other frameworks. Gameplay logic in this project is implemented in C#, which is supported as default scripting language by Unity (other option being the UnityScript). It is a mature and high-level language originally developed by Microsoft, which is integrated into Unity through the Mono framework [16].  The choice of Unity for prototyping does not limit the options for selecting different game engine (or building a custom one) for the final game, but given Unity’s track record of being the platform of choice in number of high-quality titles, using it for the actual production is a viable option.  2.4 	Requirements  To evaluate the technical needs for AI integration, a set of requirements was created based on the game concept, which outline the expected features of the game. These are high-level non-functional requirements, which also outline the system-level design.  2.4.1 Common Features  There is a set of features which are not specific to AI players, but are also used by human players in the game. A historical challenge for 4X games has been the amount of micromanagement which increases as the game progresses. The amount of micro-management is directly proportional to the size of game world, complexity of game economy, and length of the game. This has potential to frustrate players and when they have to spend excess amount of time dealing with lots of repetitive low-level tasks, instead of focusing in larger scale strategies and planning.  There are various ways to reduce this micromanagement which have been added to recent 4X games, and there are a few ones that considered when defining the requirements. The quality of artificial intelligence to which mundane tasks are delegated to is important, because if a poor implementation makes players feel that automation makes worse decisions than they would themselves do, it would discourage them from using automation altogether and make the attempt to reduce micromanagement void.  Colony management  There are a few aspects in managing colonies which can be delegated to automatic colony governor AI:  • Production management  • Work force distribution  • Tax rates  • Import and export balancing  • Security level  It should be noted that having the support for automatic control of these properties of colony does not prevent the human player from altering or disabling any of them if he/she feels like it. Also, depending on how much configuration will be exposed to the player, any of these could be parametrized with user-specified customization.  Automated units  Another good opportunity to reduce mundane tasks for the human player is the automation of certain unit actions:  • Worker automation (build new improvements and adjust existing ones)  • Automatic exploration (reveal unexplored space and patrol previously explored areas)  The above tasks are good fit to be implemented with AI automation, as the scope of strategic choices made in them are focused on certain isolated parts of the game, and thus do not depend on the high-level strategic plans player may have. The worker automation has the possibility to determine best possible improvement actions based on the economic status of the player’s empire and colonies, and can adapt also previously made improvements to match the most recent situation. Exploration is also by itself an isolated function, where different input patterns can be fed to the exploration AI to prioritize areas to explore for example based on evaluation of military threat on influence map.  2.4.2 Computer Player Specific Features  The rest of AI features are specific for the computer player, and they are used to simulate the actions a human player would be performing in the game.  Long-term Planning  The computer player needs to be able to choose and strive for various long-term goals, most important one being desired victory condition. The AI should be able to reach this goal by dividing the plan into smaller sub-plans, and adjusting them to match the constantly changing conditions which are affected also by other players during the game.  AI State Persistency  All the data used by the AI should be serializable, so that the game state can be saved and loaded at any time without disrupting the functionality of the computer player’s decision making.  Diplomacy  The AI should have ability to make diplomatic decisions, including declaring war and creating alliances. It should have possibility to do those choices in informed manner, with knowledge about the other player’s economic and military power, past trustworthiness, and any other game parameters (i.e. common ideologies, personalities, etc.) that might make the other player more or less favorable.  2.4.3 Optional Features  There is a set of features which should also be evaluated, though they are not required by the core gameplay, and thus can be considered optional. If implemented, they do though have possibility to enhance playability value for more hard-core players.  Tactical combat  When battles are initiated between fleets, they are by default be resolved using simplified simulation which considers only the ship statistics, numbers, combat bonuses and other predetermined factors affecting the battle. This can be enhanced by introducing tactical combat, in which entire battle is fought as turn-based mini-game, and moves of computer players are handled by tactical AI. Human player can either choose himself/herself all moves against the opponent, or activate an automatic battle mode in which the same AI features will fight the battle also for him/her.  Ground combat  The fight over control of colonies on planets is handled through ground combat. By default, the results of these battles will be determined by the ground troop technology level, number of troops, and other possible combat bonuses. There is possibility to further expand this battle into more fine-grained ground combat simulation similar to the tactical combat in space, where players would have more opportunities to control individual platoons of troops. Practically this would be similar to the previously detailed tactical mode and could leverage some of the technologies, but would happen on planet surface instead of space. 	  3 	Technology  Since the early applications of AI in computer games, the number and complexity of technologies involved have gradually grown, partially with help from advancements in processing performance, but also due to research done in the field of artificial intelligence research done for academic purposes. The technologies introduced in this section are chosen by their potential usability in the game being created, allowing the scope of thesis to focus on the most relevant ones.  General Architecture  When creating AI for strategy game, attention should be given to the design of overall architecture; technologies involved, how they are bound together, and how they impact the gameplay. According Ian Millington, turn-based strategy games share many aspects with real-time strategy games, with most important ones being Pathfinding, Decision Making, Tactical and Strategic AI, and Group Movement [17 pp. 809-815] as shown below in Figure 7.  Figure 7. An example of AI architecture for turn-based strategy games [17 p. 815].  Figure 7 shows a possible architecture for turn-based strategy games, although exact model has variations depending on the gameplay elements involved in the design. The technologies presented in this thesis use that model as a starting point, with adjustments done to suit the exact requirements of this project.  3.1 	Introduction to Traditional Board Game Techniques  As previously briefly mentioned in Chapter 2.1, one of the earliest applications for computer game AI was the game of chess. Since those early days, a large amount of time and effort has been spent on research and studying AI for several board games which have provided both academic and practical challenge for the researchers. During the past decades, a set of algorithms has gained foothold to become the foundation shared by many of the board game AIs, and a few key concepts are introduced in this chapter [17 p. 647].  Game Tree  The most important concept for the majority of board games is the game tree, which represents the game states as nodes in the tree, and all possible moves as branches leading from nodes to new possible states. With this data structure, the goal for the AI is to choose one of the branches as a move it should make, and needs to use various algorithms to find out which move is the best one it can make [18 pp. 16-29].  Minimax  When evaluating the game tree, the basic idea is to use a heuristic to give each possible move a score, which indicates how good the move would be for the player; in singleplayer games, the heuristic could for example be the number of moves to finish a game. The score of each move bubbles back up in the search tree, and after scoring all possible moves the AI just needs to choose the move that has the best score. However, when two or more players are involved, the evaluation algorithm should not only try to find the best score for the player, but also acknowledge that the opponent will try to choose a move that yields the least score for the player. Thus, when bubbling up the scores the minimum score should be picked for enemy moves, and player should choose move which give the largest of the minimum scores, hence the name “Minimax” of the algorithm [18 pp. 30-39]. An example of the basic principle in minimax algorithm can be seen below in Figure 8.  In this case, the best move with 2-ply search would be p2, because it would end up with score of 5 for the player. Other moves end up with lower score, because move p1 would allow opponent to do move e11 resulting with score of 2, and p3 would allow move e31 with score of 4, both of which are lower than the smallest score of 5 given by move e23.  The challenge with game trees is the balance between ply depth and processing power requirements; the deeper the tree is, the number of possible moves usually increases exponentially, and so does the time required to process it. On other hand, if the search depth is too short, the AI cannot predict the game events far enough in advance, and might not be able to predict possible “killer” moves which might decide the winner of the game in long run. There are various methods which have been developed to help and speed up searching the game trees, including Alpha-Beta Pruning, Killer Heuristic, Alpha-Beta Windows, and Transposition Tables [18 pp. 40-60; 17 p. 651]  Applications in turn-based strategy games  Although there are similarities between board games and strategy games, the complexity and number of possible moves during each turn in strategy games causes the size of game tree to grow so large, that using traditional board game AI algorithms such as minimax becomes unfeasible in most situations. There are however certain cases in which using some of the techniques are beneficial, such as using cost estimation similar to the game tree scoring approach with task planning which can be used for example in decision making for research, construction, troop movement and military actions [17 pp.  688-670].  3.2 	Virtual Player  Traditionally the AI has been implemented in games through the concept of AI agents, which are usually divided in two types: Characters and Virtual Players. Characters appear as visible entities in the game that player can usually interact with; they be as simple as ghosts in Pacman, more advanced enemies like demons in Doom, or other NPC opponents. They can either be directly involved in the gameplay, or just exist to add to the ambience and general immersion of the game [4 pp. 11-12].  Virtual Players on other hand do not usually have physical representation in the game world, but instead replace other human opponents in game and assume the tasks and responsibilities of that player. The classic example for this kind of AI is used in board games such as Chess, where the Virtual Player decides which moves it should make in the same way as a human player would. This model applies to turn-based strategy games, where the individual units in the game do not usually contain any intelligence, but all decisions are made by the Virtual Player. Exception to this are real-time strategy games, where the behavior of individual units has important role in the game, but that subtype of strategy games is out of the scope of this thesis [4 pp. 16-17].  For this thesis, the design of the Virtual Player is the central outcome of the research project, as it is an umbrella concept that encapsulate all the supporting technologies studied in this project.  3.2.1 Multi-Tier AI Framework  Building on the principle of the strategy game AI architecture shown earlier in Figure 7, the multi-tiered AI framework approach is based on separating responsibilities of the AI to individual levels. The structure framework resembles the military hierarchy, in which high-level AI sets the general strategy and goals, which translate to commands that are given to the next level, which set their own goals to be able to fulfill those orders. This continues until the individual unit level is reached, where the commands are turned into actions performed by the units [17 p. 544]. An example of this top-down command hierarchy is shown below in Figure 9.  Figure 9. Example of multi-tier AI for military strategy game [19].  In the example, some possible situational projects are shown on their source level with destination visualized. The roles of these individual levels used in the model are described below in Table 1.  Table 1. Roles of levels in the multi-tiered AI model [19].  Level  Role  Strategic Intelligence  Knowledge of the entire empire, management of grand strategy, goals, global resource levels, research and diplomacy  Operational Intelligence  Divided into activity groups, can track also non-combat activities such as economic and diplomatic operations  Tactical Intelligence  Information about encountered opponents, geography, resources  Individual Units  Pathfinding, unit movement, combat  There are also other possible ways of building the hierarchy in the framework, including bottom-up approach, where individual units are autonomous and higher levels of the hierarchy just provide intelligence and general information about the game world. It should be noted that the framework allows information anyway to pass in both directions in the model, depending on how the gameplay requirements imposed on the AI [17 p.  544; 19].  3.3 	Pathfinding  One of the most important features in nearly any game dealing with a map with entities that should be able to navigate on it is pathfinding. There are various maps and grids that can be considered to be node graphs as seen in Figure 10, to which graph search algorithms can be applied to. Of the examples shown, hexagonal grid is used for presenting the game world in this project.  Figure 10. Examples of node graphs created from maps for pathfinding.  The basic case is the ability to find shortest route from point A to point B. To do this, there are several graph and tree search algorithms, the most common ones used in games are summarized in Table 2.  Table 2. Comparison of pathfinding algorithms [4 p. 171].  Algorithm  Description  Benefits  Drawbacks  Breadth first search  Simple algorithm, but does not consider movement cost.  Most simple pathfinding algorithm  Not optimized  Does not always return shortest path  Dijkstra’s algorithm  Improvement on the breadth first search which adds path cost  Guaranteed to find shortest path  Not optimized  A-star (A*)  Combines Dijkstra’s algorithm with a case-specific heuristic value to the cost estimation function.  Guaranteed to find shortest path  Heuristic helps optimize the search  No major drawbacks  There are numerous other algorithms, but due to simplicity this study only considers the commonly used ones shown in the above table. Because the maps in the game are mostly generated at runtime, have a large number of cells and are highly dynamic, the possibility to precompute navigation data is limited.  3.3.1 A* Algorithm  The A-star (A*) algorithm is one of the most used pathfinding algorithms in games, as it is relatively simple to implement, and has good performance. Because of this, it was chosen as the default pathfinding algorithm for the game. An example of A* pathfinding case is shown below in Figure 11.  Figure 11. A* Pathfinding on hexagonal grid with Manhattan-distance heuristic.  The pathfinding starts from the hex on left-hand side of figure with black dot, and target cell is indicated with checkmark on right-hand side of the grid. Red color indicates cells with higher movement cost of 5, while other cells only cost 1 to move through. The search begins by putting hex coordinate of starting cell into the priority queue. During each iteration, the first item is removed from the priority queue (one with highest priority), priority is calculated for each neighbor cell which has not yet been processed, and each of them are added into the queue. To calculate the priority, the A* algorithm uses priority formula shown in Equation (1) [20] below:   	! "	=	% " + ? " 	(1)  In which f(n) is the resulting priority, g(n) is movement cost for this specific cell, and h(n) is the additional A* heuristic value. In this example, the Manhattan distance to the target cell is used. After all neighbors have been inserted to the queue, the iteration proceeds to next step. The search ends, when either the target position has been found as one of the neighbors, or when the priority queue runs out of items (i.e. when there is no solution). In the above figure, green cells (outlined) show the resulting optimal path to target position. It should be noted, that if the heuristic function h(n) = 0, then the search behaves equally to Dijkstra’s algorithm, which A* was extended from [20].  3.3.2 Optimizations  In some cases, the number of search nodes from which pathfinding lookup is queried may end up being too sparse, causing an excessive amount of time being spent on performing the query. In this case optimization is needed, and there are a few approaches that may be beneficial, depending on the use case.  Hierarchical Pathfinding  To speed up searching paths in large sets of nodes, it may be possible to combine physically adjacent nodes as groups, so that pathfinding operates initially on the higher-level group nodes, from which it progresses to lower levels after high-level path is found. Depending on the type and size of original node tree (for example, in large open-space maps), this combining of node clusters can be extended further to higher levels, creating a hierarchy of search trees, which is base idea in hierarchical pathfinding [17 p. 265].  Zone Mapping  In some cases, there may be search trees which have completely disjoint start and goal nodes. When this happens, a A* search would end up having to look through all connected nodes in the tree just to find out that there is no solution for the path query. In zone mapping, a special flood-fill algorithm is used to identify isolated regions in the search tree, with result of this process stored in a zone map. With this cached data, it is possible to know in advance whether there is any solution before having to attempt doing the path query [4 p. 197].  3.4 	Decision Making  One of the key requirements for the AI is the ability to make decisions, which translate to actions executed by the computer player. The relationship between input data and action request effects is visualized below in Figure 12.  Figure 12. Decision making schematic by Ian Millington [17 p. 303].  There are several algorithms and techniques for this purpose, which share the core idea of having internal or external knowledge sources as input, which are processed to action requests as output which affect the internal or external state of the game [17 pp. 301302]. This section introduces the most common decision-making methods, which have applications on multiple levels on the Multi-Tier AI model.  3.4.1 Finite State Machines  Finite State Machines (FSMs) are one of the key concepts used in computer games. A state machine is composed of a set of states, and rules defining transitions between those states. In a FSM only one state is active at a time, and switching to other states only happens when one of the transitions out of current state is requested [6 pp. 165166]. A simplified FSM for scout unit is shown below Figure 13.  Figure 13. A possible FSM for controlling scout behavior.  There are three states in the FSM to either idle, explore or return to nearest colony, and four transitions controlled by two external flags which indicate whether there are areas to explore, and whether the unit is at colony.  In the ideal FSM model, transitions are handled internally using the data provided to the FSM, and are thus self-contained. However, in some cases using the input data as sole trigger to state changes might not be enough, for example when UI triggers user input which needs to immediately alter the state of a unit. In this situation, an external transition can be triggered procedurally, which is used to set the state of FSM directly without use of a predefined transition condition [4 pp. 50-52].  Hierarchical State Machines  The Hierarchical State Machines (HFSMs) extend the basic principle of FSMs by adding the possibility of using sub-FSMs, which are basically state machines nested inside of a parent FSM. They add flexibility to the state transition options through the ability to continue parent FSM flow after finishing execution of the sub-FSM. Another benefit is the possibility to split more complex states into sub-states [17 pp. 327-330]. Figure 14 below shows a simple example case for Hierarchical FSM.  Figure 14. Example of hierarchical FSM for worker unit.  In this case, a worker unit would by default toggle between idle state and build and repair tasks, but at the appearance of enemy unit would interrupt any task it was doing, and seek cover. After the threat of enemy would be cleared, the Hierarchical FSM would resume the worker automation Sub-FSM, and its active state would automatically be the one which was interrupted earlier.  3.4.2 Decision and Behavior Trees  There are a couple of common techniques which have the advantage of both being simple to implement and to use; Decision Trees and Behavior Trees. They are usually used to control NPC actions in games, but they have also potential use cases for decision making in military units in strategy games. They are both tree-like structures, which have the benefit of being able to be shown as a visual presentation of the decision-making process to the AI designer [17 pp. 303-309; 21].  Decision Trees  The Decision Trees (DTs) help making choices based on the world state at a specific time through the use of tree built of decision branches, which lead to actions. Usually the decision nodes are binary and have only two branches, but it is possible to make selections with more than two options. The decision-making process starts from the root, and simply just moves down to the next branch depending on the outcome of each decision node. This flow is very similar to traditional if-then-else control flow in high-level programming languages, but the nodes as usually expressed explicitly as data structures, which can be defined either in code or using data model which the AI designer can modify [17 pp. 303-309]. The above Figure 15 below shows how the previously proposed FSM logic in Figure 14 might be converted into a DT.  Figure 15. The decision logic from Figure 14 converted into a decision tree.  At root decision, the presence of enemy nearby would be checked first, in which case the worker unit would move to cover. If no enemy were nearby, the tree evaluation would continue to next decisions to check if either type of tasks would be available for it; if not, the idle action would finally be picked as the last option.  Decision Tree Learning  One interesting aspect of DTs is that they can be generated using machine learning from input of observation and action sets, which represent the desired outcomes for each world state observed. There are various possible methods for accomplishing this, but the commonly used ones are based on the Iterative Dichotomiser 3 (ID3) algorithm. It uses the measurement of entropy to calculate information gain from available attributes for selecting the most relevant decision factor as the next node in the decision tree, and continues this process until all action nodes have been created [17 pp. 593-597]. For example, the following observations could be used as input for the ID3 algorithm:  Table 3. Input observations for ID3 algorithm.  Build Task Available  Repair Task Available  Enemy Near  Action  Yes  Yes  Yes  Cover  Yes  Yes  No  Build  Yes  No  Yes  Cover  Yes  No  No  Build  No  Yes  Yes  Cover  No  Yes  No  Repair  No  No  Yes  Cover  No  No  No  Idle  In each iteration, the algorithm first uses Equation (2) to calculate the entropy of the entire set of actions [17 pp. 593-597]:   	( =		*+,-%.*+ 	(2)  +/0..2 In first iteration, the formula gives the following entropy values for the entire set and available subsets:  Es = 1.75  	Ehavebuildtask = 1 	 	Enobuildtask = 1.5  	Ehaverepairtask = 1.5 	 	Enorepairtask = 1.5  	Eenemyneary = 0 	 	Eenemynear = 1.5  Those results can now be used in Equation (3) to calculate the information gain from the subsets [17 pp. 593-597]:   	3 =	(4-	89	(3)  +/0..2 Which results in the following information gain values for the attributes:  Ghavebuildtask = 0.5,  Ghaverepairtask = 0.25,  Genemynear = 1  With the above results, the algorithm chooses the attribute with highest information gain value as input for decision node, which in this case would be the presence of a nearby enemy. After this the algorithm repeats the same process for the both subsets of the observation data, adding new child nodes until all relevant branches have been created. The DT which was created by ID3 algorithm from the sample observations is shown below in Figure 16.  Figure 16. Decision tree generated by ID3 algorithm.  The nodes outlined with red circles were the ones where calculation of information gain was performed. Note that no decision node was added for the “Yes” branch of “Enemy near” check, because neither of the two remaining attributes contributed any information gain to the decision, and thus were not required at all.  Some software frameworks even include Decision Tree Learning as part of their feature set, for example Apple offers built-in support generating decision trees using machine learning in their GameplayKit framework on iOS, macOS and tvos platforms as part of their basic Decision Tree implementation [22].  Behavior Trees  Another graph-style decision-making method is Behavior Tree (BT), which generalizes the previously introduced Decision Tree concept. Any DT can be represented as BT, but the modularity and extensibility combined with their simplicity is what makes them powerful [17 pp. 52-53; 23]. The difference between DT and BT trees is shown below in Figure 17.  Figure 17. A Decision tree expressed as behavior tree with identical logic [23].  Unlike DTs which are solely composed out of decision and action nodes, BTs have a multitude of possible node types, which are generally divided into interior nodes (also known as composite nodes) which have one or more children, and leaf nodes which have no child nodes. The child nodes in BT are ordered in priority order (usually visualized from left to right), which dictates the evaluation order of the nodes. The processing starts from root node, and progresses down to child nodes in the tree depending on the node types in the tree. All nodes have a precondition which can have three possible return values; success, failure and running. The most commonly used nodes in BTs and their return values are listed below in Table 4 [23].  Table 4. Most common behavior tree node types [23].  Node  Type  Precondition return values  Success  Failure  Running  Action  Leaf Node  Upon comple- tion  When cannot complete  During comple- tion  Condition  Leaf Node  If true  If false  Never  Sequence  Interior Node  If all children  succeed  If one child fails  If one child returns Running  Selector  Interior Node  If all children  succeed  If all children fail  If one child returns Running  Parallel  Interior Node  If ? M children succeed  If > N – M children fail  If neither Success or Failure condition is met  Each time the BT is ticked, the tree is traversed down and the node preconditions checked until the active task node is reached, which performs the action in during this particular tick. The return value is back-propagated up in the hierarchy to the parent nodes, which depending on their behavior decides what to do (i.e. possibly evaluate next child in interior nodes), and return the appropriate value back to their parent node. This progress continues until the return value reaches the root of tree, which is returned to the original caller [17 pp. 52-53; 23].  Thanks to the flexible structure any type of nodes can be added to the BT for example, Utility-Based Systems can be leveraged to create a utility selector node, which can use internally utility scoring to choose the appropriate child to execute [24]. Also, reusable parts of BTs can be shared as sub-trees, reducing amount of work needed to create duplicate behaviors.  3.4.3 Fuzzy Logic  The traditional computer logic is based on Boolean algebra, which infers absolute values of either true or false as the only possible conditional states. There are however certain cases in which a more fine-grained evaluation is required, for example to assess the threat of an enemy fleet and choose appropriate actions based on the analysis of the situation. For this one fuzzy logic can be used, in which the absolute true and false states are replaced by a membership degree, which is expressed as normalized value between  0.0 and 1.0 [17 pp. 344-345]. The overview of how fuzzy logic is used is shown below in Figure 18.  Figure 18. Fuzzy process overview [6 p. 192; 17 pp. 344-354].  The process starts with fuzzification of input data, after which fuzzy rules can be applied to the fuzzy sets, and results can be obtained through defuzzification which converts the data back into crisp values [6 p. 192].  Fuzzification  The Fuzzification is done through of membership functions which convert predefined ranges of values into fuzzy set membership degrees. Commonly used membership functions include grade, reverse grade, triangular and trapezoid functions, but other functions can also be used if necessary. The number of membership degrees is not bound by the number of inputs, as same input values can be assigned to multiple membership sets at the same time [6 pp. 193-198].  Fuzzy Rules  After conversion to Fuzzy Sets, the membership degrees can be combined using Fuzzy Rules which are built using Fuzzy Axioms, which resemble the operators used in the traditional Boolean logic [6 pp. 200-201]. The most commonly used operators are listed below in Table 5. Comparison between Boolean and Fuzzy Logic operators:  Table 5. Comparison between Boolean and Fuzzy Logic operators [6 p. 200].  Defuzzification  After combining the values using Fuzzy Rules, the membership degrees need to be extracted from the Fuzzy System to be usable in the game. There are few commonly used methods for doing this:  • Highest Membership  • Membership-based Blending  • Center of Gravity  The best method to use depends on how the data is used; For a simple Boolean decision, the Highest Membership method should be enough, but if there is need to aggregate output strength, other methods are needed. Although the Center of Gravity is often favored, it comes with increased overhead due to the need to integrate surface areas of the membership regions. The blending approach usually is good enough and is much quicker to use [17 pp. 347-351].  Use case: Threat Assessment  One good application of Fuzzy Logic in strategy games is using it for threat assessment and classification purposes. A simple example case for this is shown below in Figure 19:  Figure 19. Threat assessment example case for Fuzzy Logic.  In the above case, the AI wants to evaluate the presence of enemy fleets around its colony to adjust its defensive stance if needed. As input data, it uses the proportional ratio between its defensive strength and strength of enemy fleets in Equation (4).   	*:;<+= =	log.	|+D/G0||+D/AE0=|9A+B9 F= 	(4)  Where Ae is the list of enemy fleet attack strengths, Ao is list of own fleet attack strengths, and do is the defensive strength of the colony. With the use of log2n in formula, the exponential change in ratio between own and enemy strength can be converted into a linear value which the threat assessment can more practically be applied to. When considering all units within 2 hexes distance from the own territory borders, applying the situation in Figure 19 to the previously introduced Equation (4) results in the following values in Equation (5).   		 	(5)  To use this input value, two membership functions with three fuzzy sets in each are defined as shown below in Figure 20.  	Weaker   Equal   Stronger	Bad  Average    Good 	-1.5	-1	-0.5	0	0.5	1	1.5	-40	-20	0	20	40	60	80 	pratio	reputation  Figure 20. Membership functions for force strength ratio and diplomatic reputation.  The three fuzzy sets in the pratio membership function evaluate the threat, which is considered to be minimal when the enemy fleet strength is less than 50% (pratio = -1) of defensive strength, and very high when it is over 200% (pratio = 1). At equal strengths (pratio = 0) the threat is considered to be medium. The three fuzzy sets in the reputation membership function are defined to represent the diplomatic reputation of enemy, so their trustworthiness can be included in the evaluation of probability for their aggression. Performing the fuzzification of values pratio = 0.66 and reputation = -5 with the membership functions in Figure 20 results with the following membership degrees:  Table 6. Output membership degrees of the fuzzification.  Fuzzy Set  Degree of Membership  Weaker  0  Equal  0.34  Stronger  0.66  Bad  0.25  Average  0.75  Good  0  The get the threat level from these values, the following fuzzy rule matrix is used to map the values to Low, Medium and High threat levels:  Table 7. Fuzzy rule matrix for combining the force size ratio and reputation.  Weaker  Equal  Stronger  Bad  Medium  High  High  Average  Low  Medium  High  Good  Low  Low  Medium  These above rules can be written as the following logic expressions:  mLow = (mWeaker mMedium = (mWeaker mHigh = (mEqual   mAverage)   mBad) mBad) ( (mWeaker   mEqual mStronger   mGood)  mAverage) ) (  (mEqual   (mStronger  Stronger   mGood)   mGood) mAverage)   ( mBad m Populating the above expressions with the previously calculated membership degrees for the fuzzy sets results in the following values for the threat membership:  mLow = max(min(0, 0.75), min(0, 0), min(0.34, 0)) = 0 mMedium = max(min(0, 0.25), min(0.34, 0.75), min(0.66, 0)) = 0.34 mHigh = max(min(0.34, 0.25), min(0.66, 0.25), min(0.66, 0.75)) = 0.66  When the highest membership selection is used to determine the threat level, it can be concluded that mHigh has the highest membership value of 0.66, meaning that the threat level is high and decision making can act on strategic and diplomatic level accordingly. Another option would be using the membership blending method to calculate a numeric threat level value out of the membership values if needed to for example adjust internal diplomatic stance level.  Other applications  Besides the threat assessment, other applications for Fuzzy Logic in strategy games also include Bayesian Network probability reasoning [6 pp. 253-254] and decision making in Rule-Based Systems [17 p. 354]. Fuzzy State Machines can use Fuzzy Logic to do blending between states, allowing smooth transitions based on the Fuzzy transition conditions [17 pp. 364-369]. This technique is sometimes used for example to do animation state blending like in Unity’s Mecanim. Other gaming genres, such as racing, can also benefit from the way Fuzzy Logic can be used to control vehicle steering, but that is out of the scope of this thesis [6 pp. 205-207].  3.4.4 Rule-Based AI  Rule-based Systems, sometimes also known as Expert Systems, have existed in the AI field since 1970s. They are sometimes considered a double-edged sword, as although they allow the experts to share their knowledge of the situation and reasoning about how to handle it, there is the downside that rule sets to define this knowledge must be created by those experts. Although rule-based systems have many applications outside gaming industry, such as in financial, medical and industrial software, there is also use for this approach also in games [4 p. 134]. Their strengths include the ability to use extensive rule sets to capture high-level knowledge of various complex problems [4 pp. 139-140], and the capability to make decisions in unexpected situations which cannot be easily handled with more simple approaches such as decision trees [17 p. 403]. The Figure 21 below shows the basic structure of rule-based system.  Figure 21. Overview of Rule-Based System [4 p. 138; 17 p. 404].  As pictured, this system consists of main components called the Rule Set, Inference Engine and Working Memory. Each of these parts are described below in more detail.  Rule Set  The actual knowledge of a problem is encoded in various rules, which are kept in the Rule Set, also known as the Knowledge Base. Each rule has two parts, the condition which must be satisfied for the rule to be fired, and action which defines what happens when the rule is triggered. The condition can evaluate facts in Working Memory in various ways, and rules can also be enabled and disabled when needed. The action can alter facts in Working Memory, but can also control which rules are active, and even stop the processing completely [4 pp. 134-135].  Working Memory  All facts known by the Rule-Based System are kept in the Working Memory, which works as a database for the Inference Engine. Although the format of facts is not limited, they are usually stored as Boolean, numeric, string, and enumeration values [4 pp. 134-135].  Inference Engine  The actual processing of rules happens in the Inference Engine, which checks the rule conditions of the rule set and either selects the first match or uses the arbiter to choose which action to trigger. The processing takes place in iterations which continue until either no more facts are changed in the database, or a stop action is encountered. Usually forward chaining rule matching is used, but sometimes backward chaining can be used, which matches the rules based on their outcome (effect of actions on facts) instead of conditions, trying to find a starting state that can derive the expected result. [17 pp. 407408] Rule-Based Systems are however notorious for suffering performance issues when large rule sets are used, which need considerable processing time. There are some optimizations for this process, the Rete Algorithm being one of the best known of them [4 pp. 134-135; 17 pp. 422-423].  Arbiter  Sometimes the system contains a separate arbiter component, which decides which rule is triggered if multiple rules are matched simultaneously during one iteration. Possible common approaches include using first applicable rule, least recently used rule, random rule, most specific conditions and dynamic priority arbitration [17 pp. 418-419].  Use case: Inferring tech tree state through rule-based reasoning  As rule-based systems allow inferring new facts from existing ones through the rules, one possible use for them in 4X strategy game is the capability for AI to use knowledge about enemy’s possession of a single technology to deduce the state of other technologies in the tech tree. This use case has been adapted and refined from the example provided by Bourg and Seemann [6 pp. 214-218]. The Figure 22 below shows a possible subset of tech tree.  Figure 22. Inferring state of tech tree from knowledge of a single technology.  In the tree, technologies are connected by arrows leading from prerequisite technologies on higher levels down to the subsequent technologies on the next level. The following rules can be generated from this tech tree:  IF defense_tech_2 THEN colony_tech_1=Yes AND defense_tech_1=Yes  IF weapon_tech_2 THEN weapon_tech_1=Yes  IF ship_tech_2 THEN weapon_tech_1=Yes AND ship_tech_1=Yes  IF colony_tech_2 THEN defense_tech_2=Yes  IF defense_tech_2 THEN defense_tech_2=Yes  IF weapon_tech_3 THEN defense_tech_2=Yes AND weapon_tech_2=Yes  IF ship_tech_3 THEN weapon_tech_2=Yes AND ship_tech_2=Yes  As the AI has learned that enemy has a ship equipped with Missiles #2 upgrade (shown green in Figure 22), and thus has researched the Weapon Tech #3 technology, it can use the above rules to infer which other technologies are consequently possessed by the player as prerequisites. This starts by putting the fact weapon_tech_3 into the Working Memory, and running the first iteration in Inference Engine, which finds a match for the following rule:  IF weapon_tech_3 THEN defense_tech_2=Yes AND weapon_tech_2=Yes  This rule adds the facts defense_tech_2 and weapon_tech_2 into the Working Memory. The inference engine runs next iteration to evaluate the rules, and continue the process until no more new facts are added to the Knowledge Base, at which point the processing has finished. In Figure 22, the technologies set to “Yes” during this process are indicated in red.  Other potential uses in strategy games could be predefining certain events for scenarios (triggers) and possibility to control unit and strategic behavior with AI scripts implemented by AI programmer and/or designer.  3.4.5 Utility Theory  The idea behind the Utility theory has existed for a long time, and has a history predating even computers in the economics field where it is used to study consumer behavior and choices. The core concept in Utility Theory is scoring each action or state in the utility model with a uniform value, which represents the usefulness of each choice in the given context. To allow scores of multiple sources to be comparable, the utility values are normalized using methods appropriate to the given input data, and the scores can be combined from multiple sources to end up with final utility score which can be used to select the appropriate action. A simplified overview of the information flow inside a Utility System is shown below in Figure 23 [25].  Figure 23. Simplified flow of information in a Utility System.  This flow of information inside Utility System can be roughly divided into the following phases:  Phase 1: Converting Game Values into Utility Factors  There is no hard-defined way of converting data into Utility Factors; the only rule is that all factors must have the same scale, so that they can be combined together and be comparable with each other. There are certain generally used methods for converting arbitrary game values, shown below in Figure 24. Other methods may also be used depending on what is required by the use-cases of the utility factors [25].  	Simple Cut-off	Linear	Quadratic	Logistic 1111 0.80.80.80.8 0.60.60.60.6 0.40.40.40.4 0.20.20.20.2 0000 000-15 Figure 24. Commonly used formulas for calculating utility factors [25].  Phase 2: Combining Utility Factors  Usually there are more than one factor affecting the desirability of actions, and to get final utility scores for each of them they are combined. Commonly used methods include calculating average of utility factors, multiplying them together, picking the smaller or larger of them, or reversing the factor by inverting it. These operations can also be chained after each other, and exposing them as a visual graph editable by designers can be a very powerful tool. [25]  Phase 3: Picking the Best Action  After each one of the actions have been given a final utility score the AI selects which of them it should execute. The most straightforward way is to just pick the one with greatest utility, but it might in some cases lead to repeatable or predictable AI behavior. This can be overcome in some cases by using weighed random approach, where a random selection is made from possible actions with the utility score used as weight to give the actions with higher utility score a better chance of getting picked. This can also be combined with bucketing, also known as Dual Utility AI, in which the actions are categorized and assigned a bucket based on the effect they have. For example, when choosing production goal in a colony, the military units could be assigned in one bucket and colony improvements in another one. With this approach, when building army has highest utility, it guarantees that a military unit is produced, but the type of unit can be randomized [25].  The utility scoring has a strong resemblance to fuzzification in fuzzy logic, and they share some principles especially in the way game values are converted into the internal representation in both approaches. They are also both good for promotion emergent behavior in AI when used properly by the AI designer [25].  Use Case: Diplomatic Decision Making  In the 4X strategy games, one possible use case for utility reasoning might be choosing an action during a diplomatic negotiation with another player. A simplified case for this is outlined below in Figure 25:  Figure 25. A possible Utility System for diplomatic decision making.  This situation assumes that the players have currently signed a peace treaty, which offers three possible actions: proposing alliance, signing a research agreement, or declaring war. There are also certain game values exposed to the Utility System: opponent reputation, score for scientific benefit of research agreement, and military strengths of own and opponent armies. Also, the AI personality goals are exposed as diplomatic victory, science victory and military victory priority values. There values are then converted to Utility Factors using operators defined by the designer or AI programmer, which end up as utility values of the possible output actions as shown in Figure 25. This gives each of the actions a utility score, and if picking the action based on highest utility, the choice in this case would be declaring war against the opponent.  It should be noted that in an actual production implementation various other factors should be considered, such as how much the player likes or dislikes the opponent when declaring war and what impact it would have on the player’s own reputation, how likely the opponent is to enter an alliance or research agreement before offering them, and many others.  Using utility scoring for decision making is especially fit for the type of strategy games that this project represents, as due to the nearly infinite number of possible moves per turn there is no way to score individual actions in a purely deterministic way. In this situation, the reasoning of utility of actions allows the AI to make educated “best-guess” choices based on the available data of the game state [25]. The utility-based approach is also highly versatile thanks to its simple concept, which helps it combined with a number of other techniques such as implementing utility selector in behavior trees [24] and applying utility-based costs in colony production goal trees [26].  3.5 	Influence Maps  To allow strategic analysis of map and game world, various types of influence maps can be utilized to give the AI environmental awareness. Some examples of use cases for this data are listed below [27]:  • Pathfinding can include influence as part of heuristic to avoid or favor certain areas  • Weak spots in enemy influence can be used to target attacks in planning of higher-level military operations, or to prioritize reinforcing own territory.  The basic structure and function of influence maps has similarities to cellular automata, in which uniform grids of values are modified by certain rules as a function of time, usually based on the values of surrounding tiles. One classic example is Conway’s “Game of Life” which uses a very simple set of rules, although cellular automata has many other higher-level uses such as city simulation in SimCity [17 pp. 536-537]. The data in influence maps can be composed of multiple layers of data, including for example [17 pp.  499-512]:  • Tactical Analysis o Friendly and enemy unit and point-of-interest threat generation  • Terrain Analysis o Defensive and/or movement bonuses from terrain  o Map visibility, which can be used to either increase the “threat of unknown” or to prioritize exploration  • Learning o Past events recorded on map, such as unit kills, i.e. “frag map”  Some of the data, such as terrain analysis, is by default spread on the influence map layer uniformly, and can be used as input as such. Some other data though, like tactical positions such as unit threat, are localized to single spots in the map, and their influence needs to be distributed on the layer to be usable. To do this, there are a couple of common options available shown in Table 8:  Table 8. List of common influence calculation methods [17 pp. 502-505].  Method  Description  Limited Radius of Effect  Influence is applied on map as a function of distance to the unit, with fixed falloff.  Convolution Filters  The unit influence is applied on map using twoor threedimensional filter matrix, for example using Gaussian blur.  Map Flooding  The unit influence is propagated on map using Dijkstra or A* algorithm.  It is also possible to use variations of the above methods, depending on the source data and how the influence map is used in the game. This involves usually fine-tuning by the AI programmers and designers to find a good balance for the influence which benefits the AI in decision making. For example, if certain areas of map are not visible to the player, it is good idea to take the factor of unknown into account when calculating influence; this however means that each AI player needs to run its own analysis of the influence map, in contrast to a game state where all players have the same knowledge of unit positions and strengths, in which case the data could be shared [17 pp. 505-507].  An example of a simple influence map is visualized in Figure 26.  Figure 26. An example of map of unit threat influence on hexagonal grid.  This example shows a hexagonal grid with three units belonging to each player A and B of equal influence value. In this case, the unit influence values were propagated on the map using normalized Gaussian blur convolution filter applied through a rank 3 tensor on cubic hex projection plane (q + r + s = 0).  Spatial Database  One possible way to represent the different sources of data affecting influence is the use of spatial database, as suggested by Paul Tozour. In his approach, the data is applied to distinct layers in a generalized way, with some possible examples of data layers listed below [28]:  • Openness layer  • Cover layer  • Area searching layer  • Line-of-fire layer  • Light level layer  The layers can be combined using various algorithms at runtime, for example using a formula like the one in Equation (6) to calculate dynamic desirability layer from other source layers [28]:   	FQRSTAUS,SVW = -*Q""QRR	×	-XXY*A"XW	×	RVAVSX_X-[QT 	(6)  One of the possible benefits of using this layering of data is the increased tendency of emergent behavior in AI unit coordination through the use of shared data structures [28].  Strategic Dispositions  When units are being categorized in order to identify strategic dispositions, the information in spatial database can be used to aid this purpose. The knowledge can be used in tactical analysis and decision making, for example to identify weak spots which can be engaged in enemy territory, or areas in own defences that need to be reinforced [29].  An example of evaluation of strategic dispositions is shown below in Figure 27.  Figure 27. A possible grouping of units for analyzing strategic dispositions.  Figure 27 shows a case where a simplified map of fleet and colony influences has been propagated on the map using limited radius with fixed fall-off, and clusters of unit have been grouped using a simple algorithm which selects the strongest units and units in their immediate vicinity to be part of their group. The total strengths of each group of units is known, and thus their threat level can be estimated using fuzzy logic methods similar to the ones demonstrated earlier in Chapter 3.4.3. This data can be combined with the influence map, for example by calculating the gradient of influence level between nearby grouped units. In this case, a higher gradient would indicate higher tension between units, which can be used as input data to the tactical analysis algorithm which directs the units in groups to either engage enemy unit groups, or to reinforce the defenses on local territory.  The actual implementation of selection of actions depends on the iterative experimentation by designers and the AI programmer, but could for example use utility-based scoring based on the input factors gained from the analysis.  Tactical Pathfinding  The influence maps can also be used in pathfinding to allow the units to consider possible threats when planning the route to the target position. The tactical pathfinding can be implemented easily by adjusting the heuristic function of the A* pathfinding algorithm for example by adding penalty based on enemy threat level on the influence map, which makes the units evade dangerous areas of the map, giving the AI movement choices are stronger impression of intelligence. One challenge in this approach however is the care needed when applying changes to the scoring heuristic function to avoid increasing the cost of pathfinding processing time too much [30].  A possible use case for this approach in a 4X strategy game might be for example the need to plan route for worker unit across unclaimed space with recently observed enemy movement. In this case, the pathfinding should avoid areas which would most likely to lead to encounter with enemy.  3.6 	Goal-Oriented Behaviors  With the previously described methods, it is possible to build an AI that can evaluate the current game state and choose appropriate actions which appears sufficiently intelligent in casual gameplay. However, it is especially important in strategy games for the AI to be have long-term strategy and goals which makes AI’s actions and decisions more meaningful, and thus giving more challenging and meaningful gameplay experience for the players. To accomplish this, various forms of Goal-Oriented Behaviors (GOBs) can be implemented which can give the AI not only immediate internal needs which it aims to fulfill, but also the capability of chaining multiple actions together in order to reach more complex goals [17 pp. 376-377].  This section introduces a few key technologies that can be used to implement this kind of behavior which is useful in the higher layers of the multi-tier AI mode, including production and research planning, which involves also coordination between different higher-level agents.  3.6.1 Goal-Oriented Action Planning  The idea behind Goal-Oriented Action Planning (GOAP) has long history, having roots in the Stanford Research Institute Problem Solver (STRIPS) which was created already as early as in the 1970s [21]. GOAP planning uses backward-chaining search, which means that it uses the desired goal state as starting point, and traces the action sequence which leads to the starting state. There are a few basic building blocks in this approach [31]:  Goal  A goal represents the desired final state which the planner should attempt to reach. Each goal has a set of conditions which must be satisfied for the goal to be reachable.  Action  There is a predefined set of actions, each of which represent what the AI can do. Each action has set of preconditions and effects; the preconditions define what the world state should be for the action be doable, and the effects define how the world state is changed by this action.  Plan  The final plan is a sequence of actions leading from the current world state to the desired goal state.  World State  The GOAP planner uses symbolic representation of world to perform search in statespace. This abstraction allows both preconditions to be matched against the world state, and effects can also be used to apply changes to the simulated states.  Planning process: The simplistic approach  When running plan formulation, the GOAP planner is given the desired goal state, a list of possible actions, and the current world state which is abstracted from the concrete game world into the symbolic presentation. The Figure 28 below shows a simplified overview of the planning process in state-space during the plan formulation.  Figure 28. Abstract illustration of GOAP planning process.  The planner starts from the desired goal state, adding its conditions into the list of unsatisfied world properties. During each iteration, the planner searches for actions which have effects that match the unsatisfied world properties. Each of the possible actions is picked as a possible node in the search graph, and evaluated recursively by applying the effects to world state and adding the preconditions of the action to the list of unsatisfied world properties. When the planner reaches a state where all the world properties are satisfied, it has reached the initial state and thus has found a valid plan, or if no more actions can be matched in which case there is no solution. After a valid plan has been found, it is made active and the AI attempts to follow it. However, if any alterations are made to the world state during the plan execution, replanning is required and thus the planning process is run again [31].  Adding action costs to the plan formulation  Sometimes just knowing a possible sequence of actions for reaching the goal does not suffice, as there might be other lower-cost paths leading to it. The types of cost factors depend on the use case, for example time, money or health.  When cost is added to the actions, the search space can be considered as a weighed graph which can be evaluated using A* algorithm with the expected cost used as heuristic for the search formula. Figure 29 below shows part of a possible search tree which might be formed during GOAP planning.  Figure 29. A partial state-space search tree for GOAP.  A benefit of the state-space presentation is that as it is practically a game tree structure, certain traditional board game AI techniques can be applied to it. For example, to prevent unnecessary time spent on evaluating duplicate subtrees, the evaluated states can be stored in a transposition table, with the hash of symbolic world state used as cache key. Other benefits include the possibility of using Alpha-Beta Pruning and the Killer Heuristic.  Iterative Deepening A* (IDA*)  When A* is used for pathfinding, each graph node is only evaluated once and there is limited number of nodes to explore. However, with GOAP planning there is no limit on how many times a single action may be performed, leading to infinitely long action sequences. To avoid this, Iterative Deepening A* (IDA*), which is a variant of Iterative Deepening Search (IDS) algorithm can be used for traversing the state graph [17 pp.  376-401]. The progress of IDA* search is shown below in Figure 30.  Figure 30. Iterative Deepening A* search example.  The IDA* search works by defining a cut-off value, which is the maximum cost until which the search iteration terminates. On each iteration, the regular depth-first search is run until the current cut-off limit is reached. If the target node was not found, the cut-off value is increased and search is run again, thus iteratively progressing further in the search tree each time [17 pp. 376-401]. The above Figure 30 shows roughly how this iterative progress works.  3.6.2 Hierarchical Task Networks  Although sharing some concepts with STRIPS planning, the Hierarchical Task Network (HTN) approach assigns the current world state as starting point, and uses available tasks to construct the plan through forward-chaining task decomposition. This is opposite to previously introduced GOAP, which uses backward-chaining search in the state-space graph to find plan leading from goal state to the initial world state [32]. The Figure 31 below shows an overview of a simple HTN planning system adapted for games.  This HTN planning system is divided into the following components:  HTN Domain  The essential part of HTN planning system is the HTN domain, which contains all tasks available for solving the particular problem. There are two main types of tasks:  • Primitive Tasks, which are the basic building blocks of the plan. They contain an operator which defines the actual low-level task for the game, condition which uses the world state properties to evaluate whether the task can be executed, and effects which alter the planner world state.  • Compound Tasks, which contain multiple methods of executing a particular task. Each of these methods have set of preconditions that dictate which of the methods (if any) gets chosen to be decomposed into the plan based of the current world state.  The tasks available in the domain form a hierarchy, hence giving the name for this planning approach.  World State and Sensors  Like in the GOAP approach, the planner uses an internal world state to simulate effects of tasks in the game, using the resulting state in primitive task conditions and compound task preconditions to control the planning process. Sensors work as adapters providing the simulated world state from actual game state.  Planner  The planner does the actual planning work, which starts from the root task in the HTN domain, which gets inserted into the list of tasks to process in beginning of this process, after which the iterative planning process is started. On each iteration, the first item in the list of tasks to progress is dequeued and processed. If the item in list is a primitive task, its condition gets run, and if satisfied, the task gets appended to the final plan. If the task is a compound task, the preconditions of the methods get run to select the appropriate method to decompose. This decomposition enqueues the tasks in the method in front of the list of tasks to progress. The iterations continue until the list of tasks to progress is empty. An example planning case is shown below in Figure 32.  Figure 32. Illustration of a simplified HTN planning example.  After the planning process is completed, the planner has the final plan which can be passed to the plan runner. Depending on the structure of the HTN domain, it is possible that the planning may in some cases fail to provide any valid plan at all.  Plan Runner  After the planner has created a plan, the plan runner starts executing the plan during gameplay, keeping track of currently active task in the plan and checking the task conditions during this process. If the world state gets changed unexpectedly during plan execution, i.e. when the state does not match the conditions of task executed next in the plan, the HTN is forced to do a replan to run the planning process again.  3.6.3 Composite Tasks  One approach to handling goal-oriented behavior is the use of the Composite Task architecture. Originally implemented in 1995 for a real-time strategy game, it has since found use in CSXII Tactical Combat Simulator used by the U.S. Army [33]. The Figure 33 below shows the basic structure of Composite Tasks.  Figure 33. Structure of Composite Tasks.  The main concept in Composite Task model is the ability to split a high-level main goal into smaller subgoals, creating a hierarchy of tasks. This flexibility allows expressing even very complex goals and how to satisfy them using combination of low-level actions.  The Composite Tasks consist of the following components:  • Composite Tasks, which can contain either other Composite Tasks or Simple Tasks  • Simple Tasks, which contain one or more Actions  • Actions, which are individual atomic operations that the AI can perform  The execution of tasks starts from the root task, which evaluates its child components in priority (usually left-to-right) order, until the entire hierarchy has been walked through. The benefits of Composite Tasks include design simplicity, data-driven content and generalized evaluation process [33].  3.6.4 Multi-Unit Planning with Hierarchical Plan-Spaces  The traditional planning methods are well suited for planning actions for a single actor, when the number of possible actions stays in reasonable amount. However, when planning actions for multiple units at once, for example for military incursions, the state-space searching suffers from combinatory explosion. This means that the number of possible combinations of actions grows exponentially exceeding the available processing power and thus becoming unusable. To solve this problem, the planning can be done in planspace instead of state-space [34]. The Figure 34 below shows abstract illustration of the difference between state-space and plan-space planning.  Figure 34. Comparison of state-space and plan-space planning [34].  With this approach, the planning is started from high-level task, which is further refined into lower-level tasks, and eventually individual unit actions [34].  The planner  Instead of keeping track of possible states, the planner uses list of possible plans and scores them based on their expected cost, using the same A* algorithm like in other graph-based planners. On each iteration, planner dequeues the most promising plan (i.e. the one with lowest estimated cost), select the appropriate planner methods and their alternative approaches to get a list of new possible plans to branch off from this plan. Each of these new plans get refined by the planner method, after which their estimated cost is calculated by the task cost estimation function, and they are queued into the list of possible open plans. This iterative process runs until either a planner succeeds by finding a complete plan in the queue, or fails by running out of plans to refine [34].  The tasks  The plan is composed of a hierarchy of tasks that can either be compound tasks which can be further refined to other tasks, or primitive tasks, which represent individual unit actions. The planning domain is defined by list of possible tasks, which belong to specific scope in the domain based on their position in the task hierarchy. Some possible tasks are listed below in Table 9.  Table 9. Some possible tasks for the plan-space planning (adapted from [34]).  Scope 	  Task examples  Mission  High-level mission task  Objective  Capture Colony, Defend Colony  Group  Form Up, Eliminate Colony Defenses, Attack Invaders  Tactic  Bombing Run  Units  Attack Fleet  Individual Unit  Move, Attack, Wait, Bomb, Defend, Deploy Troops  The individual tasks set of inputs and outputs, which are used to link unit states between sequential tasks, usually providing the output state of previous task as input of the next task. When tasks are chained sequentially, the preceding tasks are required to be completed before the next task in sequence can be activated. This allows the planner methods to control which tasks can be executed in parallel and sequential order.  Planner methods  The actual refining of tasks is done by planner methods, which take the current plan and task to be refined as input, and provide a refined plan for the planner. The planner methods only apply to specific tasks, and their complexity ranges from simple single task output to complex combination of tasks. They work by creating a number of subtasks for the task being refined, thus expanding the current plan to lower level. The planner methods matching the tasks in Table 9 are listed below in Table 10.  Table 10. Some possible planner methods (adapted from [34]).  Scope  Planner method examples and responsibilities  Mission  Allocate units  Objective  Define activities, assign units to groups  Group  Execute tasks as groups  Tactic  Synchronize tactical activity  Units  Arrange cooperation between units  Individual Unit  Define the actions  The plan-space graph  As mentioned earlier, the planner maintains a list of all possible complete and non-complete plans as it searches through the plan-space. A part of this plan-space graph is illustrated below in Figure 35:  Figure 35. Illustration of the plan-space graph (adapted from [34]).  In the illustration, each plan is shown as a branch in the plan-space graph with their associated cost estimate. The green plans are in the closed-list of plans that have been refined, and red plans are in the queue of open plans. The tasks inside each plan show how deep the particular plan has been refined; green indicates tasks that has been refined, white shows the task being refined now, and red tasks are unrefined tasks.  Use case: Planning attack on enemy colony  The hierarchical plan-space planning has various uses in a 4X strategy game, and this example focuses on a simple case of attack on enemy colony. The player has four fleets available to be allocated for this mission: a battleship fleet, a destroyer fleet, a bomber fleet and group of troop transports. The resulting plan that can be generated using the example tasks listed previously in Table 9 is shown below in Figure 36:  Figure 36. An example plan for invasion of enemy colony.  The planning starts by creating the capture colony task on objective layer, which contains information about the target colony, its defenses, and the AI player’s available fleets for the mission. After this, the objective gets further refined into set of group-level tasks:  Forming up the fleets, eliminating defenders, and invading the colony.  The form up task can take advantage of influence map and tactical pathfinding to pick the best positions for the fleets, and use this data to create the tasks for fleet movement to those positions. It can also consider the vulnerability of certain unit types, such as troop transports, when it assigns these positions.  The task for eliminating defenders can have various alternatives depending on the type and number of defenders in the colony; presence of enemy fleet creates the need for attacking enemy fleet, and existence of any planetary defenses requires using bombers to eliminate them when other fleets are protecting the bombing run. These tasks are further refined down into actions for the appropriate fleet types available.  The last task in capturing the colony is planetary invasion, which in straightforward way creates the troop deployment actions for troop transports and assigns the other fleets to defend the transports.  3.7 	Diplomatic Reasoning  A game featuring players with the ability to engage in diplomatic relationships with each other imposes a certain set of requirements for the AI:  Forming the opinion of other players  The most important part of diplomatic interaction is the ability to evaluate opinions about other players; how they have behaved in the past, what they are expected to do, what things the players agree and disagree about, how the military, scientific, economic and social status are evaluated, etc. This includes the mechanism how the actions of players affect these opinions, and how these opinions end up shaping diplomatic relationships into friendships, alliances, enmity or animosity. A method for handling opinions is presented later in Chapter 3.7.1.  The ability to estimate percussions of actions  When it comes to making diplomatic decisions, the AI player needs to be able to understand effects of its actions. With the opinion system in place, the AI can use the expected opinion changes in goal tree search with combination of utility scoring to evaluate its actions, and choose the one which yields the highest score. With this approach, the AI can utilize knowledge of army sizes, the players’ opinions about each other, and other factors such as personality weights in making educated guess for results of the actions.  Illusions of a character with personality behind the AI player  If all AI players would make decisions based on the same goals and using the same scoring methods, there would be little variation between the different types of players in the game, rendering the AI behavior more predictable and boring, and removing any differences in behavior among the opponents. With certain preset weights given to each of the AI players, their decisions can be influenced to be focused on unique goals, giving each of them a more distinct personality. This also allows a human player who wants to play the game with a certain strategy to seek alliances with AI players which have goals and priorities matching his/her own goals.  Long-term goals and persistence  The AI needs to have logical goals and the ability to make long-term decisions to help forming alliances and other agreements with other players. The combination of playerspecific weights and opinion system create a natural foundation for this process.  3.7.1 Opinion Systems  The original approach to Opinion Systems as used by Adam Russell is focused on allowing individual NPC agents in game world to shape their opinion about other players based on their actions, but the core mechanism he proposed can be adapted for controlling the opinions of virtual players in AI about other players [35]. The Figure 37 below shows adaption of Russell’s Opinion System for a 4X strategy game diplomacy.  Figure 37. Opinion System adapted for 4X strategy games.  The main differences from his model are the replacement of NPC characters with virtual players, and replacement of global opinion with visibility and weights used to select audience of the deeds, and other minor adjustments to handle local opinion transformations.  Opinion State  The central piece of data in the Opinion System is an opinion state, which contains the opinion in either discrete or numeric format. This could for example be one player’s trustworthiness opinion about another player, ranging from -1.0 to 1.0. These opinion states can be either simple single-track values, or multidimensional opinions which are composed of more than one value affecting the opinion state [35].  There are both positive and negative aspects of the multidimensional approach; the positive features include orthogonality, greater variety of effects, better match to natural language and having more information. The downsides however include being more brittle at design changes, increased confusion, difficulty in visualization and challenges in quantization [35]. Table 11 lists a subset of the possible opinion values that can be used in 4X strategy game diplomacy, with multidimensional approach involving four different opinion values.  Table 11. Examples of some potential opinion values in diplomacy.  Opinion Value  Meaning of -1.0  Meaning of +1.0  Scariness  Unthreatening  Terrifying  Trustworthiness  Deceitful  Honest  Sentiment  Loathed  Admirable  Aggression  Pacifist  Warmonger  This approach has the benefit of giving diplomacy more depth, for example if a player has strong army and thus high scariness score, but low sentiment score for past offenses, another player might be unwilling to enter into a trade pact even if they would fear the opponent’s army [35].  Actions and Deeds  The deeds originate from either direct or indirect actions made by the player. Direct actions might include declaration of war or using spy to perform a sabotage mission, while massing troops could be induced as indirect action measured using threat analysis and influence maps. It is possible that one action causes multiple deeds, such as declaring war during peace treaty would also raise the break treaty deed. Some examples of possible deeds are listed in Table 12 below, with their associated audiences and weights.  Table 12. Some possible deeds and their weights.  Deed  Affected Opinion  Audience  Weights  Target  Allies  All  Declare War  Aggression  Public  +0.5  +0.5  +0.1  Break Treaty  Trustworthiness  Public  -0.5  -0.4  -0.2  Demand Tribute  Sentiment  Public  -0.2  -0.2  -0.2  Sabotage  Sentiment  Allies  -0.2  -0.1  Mass Troops  Scariness  Private  +0.1  Trespass Territory  Scariness  Visibility  +0.1  Mass Genocide  Sentiment  Public  -0.5  -0.5  -0.5  There are four different audience types in this model:  • Private: Only target player receives the deed notification  • Allies: Target player and its allies receive the deed notification  • Public: All players receive the deed notification  • Visibility: Players who have currently visibility of the affected map square get notified  The deed audience type is specific to each deed type, and posting a deed requires either a target player or target location depending on the type.  Deed Log  The deeds are posted into Deed Log through filtering and logging pipeline, which is used to allow for example to temporarily suspend delivery of certain deeds, and to track statistics about the deed posts. The Deed Log not forwards the deeds to the subscribers of deed events, but also keeps track of past deeds, and has a list of persistent deeds (for example, trespassing enemy territory could be handled as a persistent deed, which posts the deed notification every turn until it gets deactivated when the units leave enemy territory) [35].  Audience Selection  The original audience type and the target passed with deed are used to pick the destination of the deed, and each of the recipients get notified.  Local Transformation  Before the deed is used to affect a player’s opinion, it goes through local transformation which is specific to each deed type. In this modified 4X strategy game model, this transformation process is used to apply in certain cases multiplier to the deed weight based on the notification recipient’s existing opinion about the target player. For example, let’s assume that player A demands tribute from player B, incurring sentiment penalty for player B’s opinion about player A. If there exists player C, which has negative opinion about player B, then player C would use negative multiplier for local transformation about the deed weight, leading to positive sentiment offset for player C’s opinion about player A [35].  Deed Effects  When the deed notification eventually receives the player, it has to have effect on the receiving player’s opinion. This change in opinion is invoked as transient offset through various possible offset functions, one of which is specific to each deed type. Example of a transient offset function is shown below in Figure 38.  Figure 38. Opinion transient offset function example by Adam Russell [35 p. 544].  The function has a run-in time during which the deed offset increases until it reaches the peak offset and highest effect on the opinion. After this it decreases during the run-out time, and when the transient offset function ends the final offset is left as the permanent change in opinion. This allows for example a genocide deed to have strong permanent impact on opinion, but a trespassing of territory yields only a temporary change which dissipates gradually [35].  To prevent excess accumulation of transient offset effects on opinion by repeated deeds, the effect frequencies can be regulated by adding a minimum time between repeated deed effects for a single player [35].  3.8 	Customizing AI  Although it is possible to implement AI completely by hard-coding it within the game code, there are motivations for making AI customizable which are twofold. For the first, the development of AI is collaboration between programmers and designers, and to support this process the designers should be able to project their visions in the game with as little friction and delay as possible. And for the second, by providing flexible methods of modifying and creating additional content to the game, user community and players can create custom mods and other expansions which can provide additional gameplay value for other players of the game [4 pp. 99-107]. In this section, some common approaches for providing AI customizability are considered from this project’s point of view.  Black Box and White Box Approaches  The AI system implementation philosophies can be generally characterized into two groups: White Box and Black Box systems. The White Box systems offer more flexibility, are good for team of multiple people collaborating and they allow designers to work more independently. Black box systems on other hand are good for single-person implementation, but force designers to have greater dependency on programmers for providing implementations for the black-boxed behaviors. Neural Networks are one example of Black Box systems, which take discrete input and provide output through trained processing in the hidden layer [4 pp. 100-107].  Sometimes mixing the two is possible, for example reusable components in White Box systems requiring less customization are usually better suited as black box components, such as Pathfinding logic. The choice of better approach often depends on the project needs, for example implementing AI components as Black Box systems might be good for a team with many programmers, but in a team with many designers a White Box System would be more preferable [4 pp. 100-107].  3.8.1 Data-Driven Design  The key idea in Data-Driven Design is detaching the AI behavior and logic from the game code into a separate data model, which can be independently modified by the designer without need for programmer intervention. There are various ways of accomplishing this, for example with FSM state masks, custom parameter configurations for individual AI agents, external definition of rules for Rule-Based Systems and Scripting Languages [4 pp. 109-111].  Custom Tools  One way to increase the designers’ power in AI development is through the development of custom tools for creation, debugging and visualization of AI in the Data-Driven Design model. The benefits of this are the increased flexibility, easier maintenance and balancing, and increased usability through UIs tailored specifically for the designers’ needs. Drawbacks however include the upkeep required from the tool programmers and extra care needed for version control handling [36].  These tools can either be completely specific to the data required for a single use-case, or they can be more general, such as tools for visualizing and editing Decision Trees, Behavior Trees, or rules in Rule-Based Systems. The tools can either be external applications, or they can be built into the game, allowing adjusting of the AI data in real-time without need of exporting data and restarting the game [4 pp. 104-111]. The drawbacks of scripting however include the need for script development tools for designers, and the possible performance overhead due to script interpretation at runtime. Both of these problems can be alleviated by using existing scripting tools which have already good tools for scripting, and are already optimized to be embedded as part of game [37]. Although there is no limit in which language can be used for scripting, there are a few most commonly used options in game development. Some game engines come with a built-in scripting system, such as UnrealScript in the Unreal Engine, and Torgue Script in the Torque Engine. The use of open-source scripting engines, such as Lua or Python, is also very popular among game developers. Visual Scripting, in which the logic is visualized graphically to the designer instead of a text-based presentation, is also one possibility, and commercial libraries such as PlayMaker Visual Scripting for Unity3D exist to support this approach. And if none of these options is suitable for a particular game, developers can opt to create a completely custom scripting language and engine tailored for the particular needs of their use case [4 pp. 112-130; 38].  For this project, the choice of customization was narrowed to a combination of casespecific custom data models and behavior scripting using an open-source scripting engine. A custom script engine was out of the scope of this thesis, so the choice of scripting language was further narrowed down to selection between Lua and Python, with motivations explained in more detail below. Both Lua and Python are scripting engines with a good reputation of being well suited for embedding into games due to their free license, easy integration, and stable language specifications which are backed by strong existing developer communities [4 pp. 114130]. As this project uses Unity3D and C# for implementation of the prototype, there are two popular frameworks which provide support for adding scripting languages on top of it; IronPython for Python scripting [39], and Moonsharp for Lua scripting [40]. In Lua, the language itself has been designed to be flexible and extensible, so although by default there is no support for Object-Oriented (OO) paradigm such as classes, inheritance and encapsulation, they can be added through the use of meta-tables. The interpreter itself does not support multithreading, so care has to be taken by either running the interpreter only in a single thread, using mutual exclusion for access control, or having multiple interpreters on separate threads. The language syntax itself is very similar to C-like languages with shallow learning curve, and the standard library is very small and easy to learn [17 pp. 449-450]. Python has native support for OO programming and it excels when it comes to mixing the script language with native languages. The language syntax depends heavily on indentation, but is generally considered one of the easiest languages to read and learn. There is a very large number of libraries available for Python, but in runtime the language suffers from size and speed issues. [17 pp. 451-452] Based on the above considerations, Lua integration using MoonSharp was chosen for this project, main reasons being the easy integration and lightweight runtime which were key for the nature and scope of the prototype. As the majority of how the AI works is hidden from the player, there is often temptation to cut corners short either to artificially increase the level of difficulty provided by the AI, or perhaps just to save time in development. Some games are notorious for having cheating AI, and it may be a big spoiler for the gameplay experience and source of frustration, if the player feels that his opponent is exploiting an advantage he/she cannot match [6 pp. 3-4].  Some examples of cheating might be the ability of AI to ignore visibility status of map, thus having always full knowledge of the location and arrangement of all enemy units or adding a certain resource production multiplier to the computer player’s production, which would be tied to the AI difficulty level chosen by the player [41]. For this project, the use of features giving the AI player unfair advantage over human player is prohibited, and instead the difficulty variation is done through combination of regulating AI aggression on easier levels, adding randomness to the decision scoring, and otherwise tweaking the way AI makes choices based on the same information that is available to the player. Unlike many other real-time game genres with strict performance requirements, the 4X strategy games have the benefit of being more relaxed when giving the AI processing time thanks to their turn-based gameplay model. However, care needs to be taken to balance between how much computing time is given to the AI to not cause too big slowdown in the gameplay, especially in late-game situations in larger game worlds when many AI agents owned by multiple AI players might be operating. This section introduces certain techniques to optimize and balance the processing time given to the AI features. A key feature in managing the time consumed by AI is division of the AI logic into manageable tasks, and using some method to control the execution of them. In a singlethreaded execution model there is a fixed maximum amount of time that can be spent for performing other tasks, as rendering of the game usually happens on the same thread. A general-purpose execution management system can be used to not only run the AI code, but to also control the time given to many other background tasks in the game such as asset downloading, audio and physics processing [17 pp. 693-725]. A basic scheduling approach is to assign tasks to be executed on certain frames, using a simple algorithm such as execution frequency. Figure 39 below shows how tasks A, B and C might be executed when scheduled on relatively prime frequencies. In a load-balancing system, the scheduler keeps track of time spent for each task, ensuring that processing time on each tick does not exceed the maximum limit. In addition, the expected running time of tasks can be estimated, which can be used to predict how long the task takes and thus help scheduling it for execution. Scheduling Groups can also be used to divide tasks into groups which share a certain scheduling algorithm, such as: One way to control the time consumed by pathfinding, and to prevent CPU spikes caused by excessively large pathfinding queries, is to divide a single path query operation to multiple steps, a technique known as time-sliced pathfinding. With this approach, the pathfinder is given fixed amount of time to process during each tick, and when the time is exhausted the state of pathfinder is saved in such way that the query can be resumed when the pathfinder is again given processing time. One challenge with this approach is that if state of map affecting the result changes during the query, the results may not be guaranteed to be valid. How much this shortcoming affects the feasibility of the approach is however dependent on the use case of the pathfinding [43]. In the simple manual scheduling scenario, all tasks are run on a single thread in cooperative fashion. This requires both the AI tasks to behave well in terms of how long they take to execute on a single frame, and the scheduler has responsibility on how to divide the workflow during each tick. Another approach for allocating CPU time not only for AI tasks, but potentially to other components in the game engine too, is the use of multithreading. Traditionally, this approach has had the downsides of requiring synchronization of data structures and performance penalty caused by context switching, but in the past decade the advent of multicore CPUs even in the lowest-end mobile devices has created a situation, where the use of multithreading to leverage the new hardware features gives performance gains that outweigh the disadvantages [17 pp. 693-725].  Although the performance of Central Processing Units (CPUs) has increased constantly during the history of computers, the power of graphics processing units (GPUs) has exploded in the past decades to be a very viable option for performing large-scale computation. With the introduction of General-Purpose computation on the GPU (GPGPU) APIs such as OpenCL and DirectCompute, this power has become easily accessible to developers, including AI programmers. This allows great chance to speed up the AI by offloading applicable parts of it to the GPGPU processing [44]. As pathfinding is one of the core methods used in grid-based strategy games, seeking to optimize it through GPU offloading is one of the most promising applications for this technology. Recent research on has demonstrated that A*-type pathfinding can be implemented with GPGPU with a much higher performance compared to traditional CPUbased search [45]. The GPU offloading in Unity projects can be achieved with the use of Compute shaders, which provides an API for running GPGPU programs within Unity applications. The downside of Compute shaders is they require using the latest versions of graphics programming APIs and graphics hardware supporting them [46]. There are also a couple of methods which could be useful in the development of AI for strategy games, but they were considered not to fit in the scope of this project due to a combination of practical and schedule challenges. They do, however, have potential to be explored in future research, and are listed here to explain why they were rejected at this point. Explaining away, in which a known result fact is used to reason the probability of presence of source facts leading to it, including characteristics of independence and conditional dependence. The challenge in Bayesian networks is that for them to be useful, the probabilities used for inferring states need to be acquired by either gathering the information during simulated gameplay or through training of the network. This means that the problems they are used to solve need to be fairly uniform, as alteration of gameplay and rules affecting the probabilities causes the network to produce invalid decisions. One solution for this could be allowing the network to adapt to the user’s gameplay by training it during actual gameplay, but this might make the AI show too much emergent behavior, and thus lead to undesirable decisions which might not be expected by the AI programmer or programmer, causing other parts of the AI to behave unexpectedly and appear broken to the player. The concept of Neural Networks is one of the machine learning techniques which has gained a lot of popularity in the AI research in the recent years, and there are even some commercial games which have utilized this approach. The Neural Networks try to simulate the function of neurons in actual human brain with a simulated mathematical model. In this model, the Neural Network is divided into three layers: Input layer, Hidden layer and the Output layer, with each of those layers containing sets of neurons. The number on each layer dependent on the use case of the network. The source data is fed into the input layer, from which a feed-forward process passes the information through the hidden layer, all the way until it reaches the output layer. During training, the output data is evaluated and back-propagated to the weights in the hidden layer, and repeated until the Neural Network outputs the expected values. After this the trained network can either be used in the final AI implementation, or the training process can be infinitely continued to allow the AI to be able to adapt to player behavior during the gameplay [6 pp. 269-315]. The major challenge in the Neural Networks is, like previously outlined in the evaluation of Bayesian Networks, the need for training and high risk of unexpected emergent behavior, and these features make especially debugging, testing and balancing gameplay difficult [6 p. 271]. It should be noted that it is possible to leverage these techniques in games successfully, but due to the time and effort required for these approaches compared to the potential gain, they were not included in the scope of this thesis. In this section, the produced high level technical design is presented in addition to outlining the best practices for implementing it. Due to the highly iterative process in game development, this solution should be considered as a starting point for prototyping, and most likely goes through various alterations and fine tuning as the development proceeds. It should however provide an understanding of what kind of options are being considered as realistic goals for the project. On high level, the entire AI is encapsulated as the virtual player, which interfaces with the game by taking the world state as input, and providing set of actions as output. Figure 41 below shows an abstract overview of this information flow. To give a better understanding of this process, below is a brief explanation of the purpose of each step in this illustration: This abstraction of the AI’s understanding of the game state and its operations allows better modularity, and helps preventing cheating by allowing the AI to use only the same information and actions which a human player would be able to utilize in the game. Furthermore, with this architecture it is possible to use certain parts of AI features for human players to allow worker automation, colony governors and other features that might be automated to reduce the dreaded micromanagement which might be present in large-scale games. When implementing the AI, a good approach for controlling the complexity is the division of the AI to dedicated components. The central part of the virtual player is the multi-tier AI model, which follows the principles set earlier in Chapter 3.2.1. The responsibilities of each tier level are capsulated to managers, each of which has a dedicated role in the decision-making hierarchy. These managers (and individual agents in lower tiers of the model) utilize various AI tools which are available for them throughout the tier levels as a separate toolbox. This division to components is illustrated below in Figure 42. The HTN planner in this component utilizes a highly abstract, high-level strategic decision-making domain, which composes to various abstract tasks which are assigned to the other managers. The Diplomacy and Expansion Managers also provide data for the planner to help in the decision making. The research goal selection uses in straightforward way the active goal given by the Strategy Manager to choose which field of research should be given the research priority. For tracking the state of enemy research, the Research Manager uses functionality of Rule-Based System’s inference engine, allowing it to use knowledge of enemy fleet types, colony improvements, and existing knowledge of research The Diplomacy Manager is responsible for handling any diplomatic actions requested by the Strategy Manager, and it also provides necessary data for the high-level HTN planner, such as the opponent reputation based on opinion values. The overview of the Diplomacy Manager is shown below in Figure 45. One central component of the Colony Manager is the goal pool; it contains all possible goals, and at any given time each colony is assigned one of these goals which it needs to fulfill. As the Colony Manager gets the high-level goals from the Strategy Manager, it makes sure that the currently assigned goals have the highest utility for reaching this high-level goal. As the Colony Manager is also responsible of tracking global resource production and allocation, this allows it to not only directly respond to specific production goals, but also to balance the resources between individual colonies to avoid potential production or growth bottlenecks. The mission planning relies strongly on the Tactical Analysis Server to find both weak and strong areas in the friendly or enemy territories, which allows it to create missions not only for incursions to enemy space, but also for reinforcing own defenses. This planning process also includes allocation of available military units to specific missions. When a potential mission is formulated, the allocated units and the mission goal is forwarded to the military coordinator, which handles planning of the actual mission and decomposition to individual unit actions. The Expansion Manager is responsible for handling both the expansion of player’s empire with colony ships, and using worker fleets to improve the existing structures owned by it. The overview of the Expansion Manager is shown below in Figure 48. The goals given to colony ships and worker units are directly assigned to the individual units, which use their own decision-making model to fulfill them. The core of Military Coordinator is the Hierarchical Plan-Space (HPS) Planner, which uses the planner methods and tasks provided by the coordinator’s data model. Functionality of this HPS planning process was detailed earlier in Chapter 3.6.4. If the planning succeeds, the resulting plan containing tasks for each individual fleet is executed by the coordinator, which provides goals for the individual units during the plan execution until mission is either finished or failed. The decisions made at this level are mapped directly to the commands given to fleets during the AI player’s unit movement turn during the gameplay. The worker automation can be also used by the human player to toggle partial Expansion Manager and AI fleet behavior on and off when needed. Each colony owned by the AI player is managed by the Individual Colonies AI module, which takes the goals given by the Colony Manager, and translates them into actions that can be performed by the AI for that particular colony. Overview of the Individual Colonies module is shown below in Figure 51. The AI toolbox contains various generic AI algorithms and tools, introduced earlier in the chapter 3, which are utilized by the various other modules throughout the multi-tier levels of the AI decision-making model. The most important principle of the toolbox is that each of the technologies used by the AI is encapsulated into a reusable component, which allows integration with other AI code through predefined interfaces. This not only allows keeping the component-specific code separate from other AI logic, but also allows better possibility for scripting language integration, explained later in Chapter 4.4. Unlike in real-time strategy games, which have strict limits on the CPU budget for AI execution time per frame, the nature of turn-based games allows certain level of flexibility in the implementation of AI processing model. Although not optimal, in the proposed prototype, all AI actions can be isolated to the AI players’ turn, thus reducing the pressure to interleaving AI processing with other players’ turns. However, even when executing all AI code of a particular player in one batch, certain steps must be taken to avoid undesirable behavior of the game, such as stalling the game interface during the AI processing. In Unity, the default method of implementing asynchronous behavior is through the use of “Coroutines” which resemble cooperative multitasking. Each Coroutine gets executed on the Unity’s main thread, and they are responsible for handing over the execution voluntarily by yielding. Although this approach is easy to implement, it has the drawback of needing careful planning of when to yield the execution to avoid either clogging too much CPU time on a single Coroutine pass, or wasting CPU time by yielding excessively often. Other challenge of this approach is the care needed when designing the AI model to allow Coroutine yielding throughout different components. Another way of implementing the asynchronous execution is through the use of C# threads. Although the use of threading allows independent execution from the main thread, and possibility of benefiting from multi-core CPUs, they have the drawback of needing synchronization between any shared data structures, and as Unity’s API is not thread-safe, would be limited to executing only the parts of C# code which are not using Unity features. In this prototype, the Coroutine method was chosen to be used. When balancing the AI behavior and features, it would be unnecessarily time consuming to have QC play multiple games against an AI after each modification done to it by the programmers or designers. To alleviate this problem, automated testing can speed up a lot of this process by allowing simulation of entire games played by variable number of AI players. A set of parameters can be configured for this process: Also, different variations of AI using their own versions of data models could be potentially matched against each other, to directly evaluate the effect of modifications of the model data on AI performance with minimal iteration time. Each automated game produces a transcript of all actions performed during the game, and the resulting outcome of the game. This recorded journal of the game can be replayed in either highor low-level detail, which allows designers to pinpoint which actions by the AI were undesired, and give them ideas which features would need to be improved. In the proposed AI model, the individual manager modules in the multi-tier have the potential to benefit from being abstracted from the other AI code through use of LUA scripting engine. The AI toolbox, which have very little need (if any) for specialization, can be easily utilized by scripts as standalone components exposed to the scripting environment, removing the need to write any low-level AI code in the scripting language. As explained in Chapter 3.8.2, the MoonSharp library was chosen to be used in the prototype, but due to schedule challenges the experimentation of scripting integration was not finalized in time for this thesis, and remains on theoretical level. Following the principles of the Data Driven Design introduced in Chapter 3.8.1, many parts of the AI model are isolated into data models, which can be customized either by custom tools or direct manipulation of the data in question. These datamodels are listed below in Table 13. All of the above data models can be stored in either JSON or XML presentation, which can be further exposed to the designers with an UI through custom tools where needed. This allows building a foundation for the AI behavior before devoting time to building the custom tools. It should be noted that some data, such as the rules used for technology inference, can be derived from other game data. This chapter explains what was developed during the project, and evaluates how well the project output matched the initial goals. The biggest challenge for the implementation of the complete AI player was the lack of a 4X game engine, and creating one singlehandedly for the purpose of this thesis would have exceeded the feasible time limits available to finish the project, and risked generating excessive workload beyond the scope of the project. The outcome of this thesis can be categorized in two distinct parts: The most important outcome of the project was the high-level design of the AI player, which should be used as a starting point and guideline for the implementation of the actual AI code in the actual game as presented earlier in Chapter 4 as the proposed solution. This technical design contributed to the selection of parts suitable for independent prototyping, which are detailed later in Chapter 5.2.  It should be noted that the greatest value of the high-level design comes from this profound research of AI field done for the theory part of this thesis, as the time spent on this process will be saved in any possible future implementation of actual games.  As the evaluation of this technical design was limited to general assessment of its usefulness in the scope of this thesis due to lack of a complete game prototype, the results of the prototyping were used to reinforce and assess the feasibility of this high-level design. Although the technical design represents the virtual player feature of the strategy game, all necessary parts of the design can be applied for player-assisting “governor” features by considering the human player as a complete AI player with certain decision-making modules disabled. This allows the full influence mapping, threat evaluation and other features to provide necessary data for the assisted features, such as production planning, advisor recommendations, scientific research goal suggestion, etc. As per the original game design, the map prototype was based on the hexagonal grid. This grid type provides better movement and distance handling by eliminating the different between diagonal and axial distances, but in contrast requires more complicated handling of hexagonal coordinates. A hex coordinate utility was developed during the project to help managing this challenge. This first version provided a good foundation for the map and pathfinder code, although some bugs were found and fixed in the later version. Building upon the map prototype used for the pathfinding testing previously, the functionality was extended to include support for multi-level spatial database to map player influence and the combined effects for visualization on the map. Additionally, the pathfinding engine was adapted to leverage the spatial database information to allow experimenting with tactical pathfinding. Appendix 2 shows screenshots of the various spatial database layers being visualized. This version of prototype was equipped with a primitive representation of military units, which appear as circles on the map. Each of the units has the following properties: The aforementioned spatial database layers were used just for testing purposes, and actual game could use any number and any combination of layers in whichever way the game designers might feel useful. Tactical pathfinding was created as a simple extension to the original pathfinder, using the influence data in the spatial database to show benefits of this approach. The screenshots in Appendix 3 show three different pathfinding modes with equal start and goal locations:  Simple: Each node is considered to have fixed cost of 1, which makes the pathfinder attempt finding the route with lowest number of map cells from start to the goal node. The screenshot also shows, that this approach makes the pathfinder visit very low number of map cells thanks to the A* algorithm which prioritizes cells closer to the target. Terrain movement cost: The pathfinder now considers movement cost of terrain when doing the path query, showing a different resulting route. Also, a much higher number of map nodes was visited during this query. Enemy threat avoidance: Again, the same start and goal nodes are queried, but this time the scoring of path is done using data from the enemy influence layer (although boosted 10 times to allow including terrain movement cost as a secondary scoring method), which creates a completely different path, which nicely skirts around the areas around enemy influence. The actual formulas for combining influence data from spatial database for map cells are highly customizable, and may need a lot fine-tuning depending on the use case for the game. For example, it may need careful consideration on how much effort the AI should put into avoiding enemy if resulting path leads to excessively long detour. However, for the prototype purpose, the combinations used seem to work quite well. A simple inference engine was created to demonstrate the basic concepts of rulebased system functionality. This implementation uses heavily Unity features, such as storing rulesets in ScriptableObjects, and editor extensibility to allow running the inference engine in either edit or runtime mode. Screenshots of the inference engine test are shown in Appendix 4, which were taken from the Unity Editor mode. The first screenshot shows the ruleset asset, and the associated test rules used in the prototype. The other screenshots show state of the inference engine after each user action, starting from resetting the inference engine followed by single-stepping it one step at a time until the inference process was finished. The debug output of the Unity console is also included in the screenshots, which shows the effects of inference process during each step. Although the basic inference process works, and demonstrates the functionality of associated algorithms, a more generic implementation supporting dynamic rule generation would probably be most useful for the actual production-quality game, especially if used for technology inference like proposed in the high-level design. The motivation for this thesis originated from the interest in game artificial intelligence and the 4X strategy gaming genre. This, combined with the ongoing trend of constantly increasing mainstream popularity of gaming, gave further incentive for this research as a way to create a design which could benefit future development projects requiring this type of knowledge. As possibilities for exploring this field of technology were not available at the workplace for the timeframe of this thesis, the project was implemented as a personal undertaking focusing strongly on theoretical research. The main goal of this thesis was to research various AI technologies in order to create a plan for integrating AI for a turn-based 4X strategy game, and a secondary goal was to do prototype implementation of it. The theoretical part of the study was finished, but the implementation was limited to prototyping only certain areas of the researched technologies. The technology research phase proved highly informative, and a lot of knowledge was gained for creating a feasible high-level design for the AI implementation. During the prototyping phase, the technologies that were experimented with also showed a lot of their potential also in practical setting, thus reinforcing the credibility of the parts of design which they were associated with. Although there were certain shortcomings in the practical output of the thesis, the theoretical knowledge gathered has high value for not only the author personally as game developer, but hopefully also for other people facing similar challenges who might be reading this. One major improvement in future would be the full implementation of AI in the actual game. As the AI design is theoretical and high-level, there are most likely many challenges in the practical implementation which will be reflected back on the design of the AI model. This includes for example considering the actual gameplay design of the game and the processing performance of the target platform. Also, the further work should not be limited by the research done in this thesis, but should also be open to exploring possibilities of both future technologies, and re-evaluating the existing technologies, such as Bayesian and Neural Networks which were ruled out of the scope for this thesis. The purpose of this thesis was to design and develop a piece of software to improve Google Drive shared folders in organizational use. The main reason why the topic was chosen, is that organizations that use Google Drive as a centralized data storage are especially vulnerable to a potentially vast amount of work in case of an accidental deletion of items. I immediately started to figure out a topic for my upcoming Master’s thesis. As Google Apps was one of the main tools we used in my workplace, I figured it could provide an interesting topic in some form. As all software, Google Apps also proved to include design flaws partially addressed by at least one third party vendor. Because the vendor did not include a solution which would have been feasible to our company and most of our customers, I decided to develop a software solution which would be feasible for reselling and provide a good topic for my thesis. The design and development was more challenging than anticipated, but so was the actual report in a thesis form, as most of the development was done before the written report. All figures, tables and appendices were made completely by the author. All elements in this report were made with Google Docs, Google Sheets and Google Drawing, excluding screenshots. Due to limitations in Google Docs, the final document was rebuilt with Microsoft Word. Last but not least, I’d like to give a loving thank you for my wife Tiina for taking care of our daughter, while allowing me to work on this project besides my regular day job. This thesis focuses on the design and development of a piece of software, which improves the functionality of Google Drive shared folders for organizational use. The goal of the project was to design and develop a software solution which monitors and sets file ownerships within a shared folder in a way that accidentally deleted files are easy to restore. The software is to be sold as a service. The finished product was named HSWDrive. The study discusses the use of shared folders in Google Drive. Google Drive is an internet-based computer file storage service (also known as cloud storage) developed by Google Inc. It is part of a wider suite of web-based applications, called Google Apps, which is reasonably priced for companies (Google Apps for Work) and available free for consumers (with a free Gmail account) and education facilities (Google Apps for Education). The current service model was launched on 2012, although the same base has existed since 2010, when it was still known as Google Docs. Google Drive is currently used by roughly 240 million active users. (1) The basic idea of a cloud storage is that a user has his/her own password protected account, which the user uses to add, modify or delete his/her personal files. With the use of cloud storage, the end-user does not have to carry any removable mass media (CD / pendrive / hard drive) with him/her as long as he/she has access to the internet. Because internet connectivity has spread so wide in today’s world, it is usually unnecessary to carry the files with you physically. Due to the fact that a user can share a file or folder to other users or user groups in Google Drive, it is also possible to use Google Drive as a centralized enterprise data storage. To establish a centralized data storage, a user must create a folder and share it to all other users which require access to the same files. When a user adds files to a shared folder, the ownership of the files remain to the user who added the files. Otherwise any user could fill up the disk space of another user. When a file is deleted, only the file owner can restore it from the trash bin. Because of this logic, a problem arises when someone accidentally deletes a file from the shared folder. The original owner has to be found and the owner must restore the file back to the shared folder. This could potentially cause a lot of manual work in case a whole file structure has been deleted. This challenge can be bypassed by creating a software solution which automatically monitors and logs all the changes done within a Google Drive shared folder. By changing the ownership of each file to a single dedicated user account, the files are easy to restore from one user interface in case of an accidental deletion. Although Google Drive provides a platform to share files within a company, there is one major problem in the concept: when a file is deleted from a shared folder by someone who is not the original owner of the file, it disappears from the shared folder and is left orphaned. An orphaned file is a file, which has an owner, but does not have a location, so it can only be found by the owner of the file by conducting a search. It cannot be found in any particular folder, not even in the trash. In case of an accidental deletion of large folder structures, this causes a significant problem in restoring the deleted files and folders. This has happened to the case company and its customers, thus a solution to avoid it in the future is required. (2) As to this date the case company has found only one widely recognised commercial service to address this problem: AODocs. Although AODocs is much more than just a system to prevent accidental deletions, the case companies required functionality of AODocs is to convert any Google Drive folder to a controllable file server with advanced options. The problem with AODocs is that since they released a new version in the last quarter of 2014, it has not been possible for users to delete or add files to a shared folder thru the Google Drive interface, thus forcing the users to use the AODocs interface or a separate browser plugin for adding and deleting files. The case company used AODocs for a trial period of 30 days at the end of 2014 and was almost ready to purchase it, but after noticing some properties which would have required the end-users to change working habits, a questionnaire was conducted within the company to decide whether AODocs was suitable or not for this environment. The questionnaire included the following questions: Based on the results of the survey, 8 users out of 9 answered yes to the first three questions. All users preferred Google Chrome as the initial application to access Google Drive, but some users also used the Google Drive app on mobile devices and Google Drive synchronization client to have a local copy of the files. At this point it was clear that the employees did not like the idea that each and every user in the organization should either install a browser plugin or learn a new system to add, modify or delete files in a shared Google Drive folder. According to the free comments section of the questionnaire, AODocs was taking too much control over the user experience, which led to the conclusion that AODocs was not a feasible solution for the case company. The software must be easy to implement on other folders and organisations, thus making it easy to sell as a service. Because this type of functionality requires continuous monitoring of the folders and files, the finished software must run on a dedicated server. The aim of this project was to design and develop a software solution which monitors and sets file ownerships in a way that accidentally deleted files are easy to restore. After the initial requirements (development server and Google Developer credentials) were obtained, the software was designed and developed using the agile software development method and object-oriented PHP language. Most of the programming was done with the combination of TextMate and Transmit. TextMate was used to write the code and Transmit was used to upload changes to the server. Programming was done in various locations, mostly in the developer's home or workplace, due to the fact that software design The Google Developer Console credentials were used to gain access to the Google Drive API. Google Drive API was used to send commands and perform queries to Google Drive. The actual software was be developed using the PHP language. PHP was chosen due to the developers vast experience in it and because Google provides PHP client libraries for the Google Drive API. This project was limited to Google Apps for Work and Google Apps for Education environments so it does not work with free Gmail accounts due to limitations of Google Drive API. Unlike in AODocs, a separate interface will not be created for handling files. All file and folder handling and restoration of deleted files can be done directly in Google Drive, although a separate file restoration interface was made for easier restoration of large file structures. All files saved in the shared folder must be owned by someone within the company of the shared folder. Files which are owned by external collaborators, were not included in the scope of the software, although this might be possible in a future release. Section 2 explains more about the technical aspects of Google Drive, Google Drive API and the programming language of the developed software. Section 3 describes the requirements for the developed software to function. Section 4 describes the program logic and includes illustrating tables for the logic and database structure. Section 5 describes the evaluation for the finished software. Section 6 lists all encountered problems while developing the software. Section 7 expresses the current and future possibilities for licensing and distribution of the software and Section 8 includes measurement results and analysis of the finished product and discusses of the possible future insights. To better understand the technical aspects of Google Drive and the development of the software solution, the used technologies and software logic are described more thoroughly in the following sections. Google Drive was introduced in April 2012, even though the core service did exist before this as a file storage system for the Google’s web-based office suite. The early webbased office suite was called Google Docs and it included functionality for word processing, spreadsheets and presentations. After the launch of Google Drive these were separated into three individual apps: Google Docs, Google Sheets and Google Slides respectively. Google Drive is used to store and share files and folders in the cloud. It is directly integrated to Gmail (Google’s email system), which enables file sharing directly via the email interface. The use of Google Drive for file sharing addresses the widely known problem of large email attachments filling mailboxes. By only sending a link to the file, the recipient’s mailbox does not get filled by large attachments and the recipient is able to download the attachments on demand. The downside of this method is that the file can only be downloaded as long as the sender does not delete the original file. By having files in a cloud storage, there is no need to carry any physical storage media. All changes made to the files are usually viewable by all privileged users and devices within a few seconds delay either from the web interface or in a locally synchronized folder, when using a separate synchronization client software. A noticeable difference between Google Drive and many other Cloud Storage services is that Google Drive uses tags instead of folders. Although files appear to be inside folders, they are actually just tagged with the folder names. By using tags, it is possible to have a file located in various locations at the same time. This method is very similar to using hard links in a Unix-based operating system, such as Linux. The only practical difference between a hard link and tag is that hard links are done in the operating systems file system level, while tags are done in a separate database. By utilizing tags instead of traditional folders, it is possible to save a considerable amount of disk space, because there is only one copy of any given file, even if it is located in various locations of various users. (3) Although having a separate backup system is important, it is not that crucial in such as vast system as Google Drive, because of the various safety and security implementations. All data is replicated between various data centers around the globe and each data center is equipped with emergency power generators, which enable data integrity in case of a natural catastrophe. The data centers operate on custom built servers which are exclusively built by Google and run a stripped version of the Linux operating system. (4) All overwritten files are automatically added to a separate revision history, which allows file restoration in case of an accidental change in a file. Deleted files are added to the trash bin before final deletion, which allows users to restore them in case of an accidental deletion. Some files are even recoverable by Google Support and the organization administrator for a few days after emptying the trash bin. For extended data integrity, it is possible to take data backups from Google Drive to a local computer or a third party cloud storage, such as Backupify. (5) (6) Google Drive was originally intended for web browser use due to its integration with Docs, Sheets and Slides. As the number of users increased, Google released the synchronization client for Windows and Mac OS X. The idea of the synchronization client, is to have a local copy of all or selected files within Google Drive. This way users are able to access files without opening the web browser. According to the experience of the case company, many users face the problem of not understanding that deleting or moving items from a locally synchronized folder also deletes the items from Google Drive. This functionality is one of the main drivers for this project. As of today, there is no official version of the synchronization client for Linux. All files and folders in Google Drive are always owned by someone. Even though teams and companies use Google Drive to collaborate within shared folders, the files within shared folders are by default always owned by the person who creates or uploads the file. Each file consumes disk space only from the owner, regardless of the file location. The file ownership can be transferred to another user within the same organization by the current owner or programmatically by using Google Drive REST API. After a file has been transferred to a new user, it will free the consumed disk space from the previous user and consume it from the new user.Having files with different owners in a shared folder can create a problem, when a user account is deleted. Because the files are owned by the user, deleting a user will also delete the files owned by the user within a shared folder. Another possible problem arises when a file is deleted from such a shared folder. Because the file is owned by some user, it can only be found from the owners recycle bin, which makes it difficult to restore in a multi-user environment. Each Google Drive user has a “My Drive” folder, which is also called the root or root folder. Google Drive allows files and folders to be shared and to exist in multiple locations at the same time. These locations are called parents in the Google Drive REST API. Having files and folders in multiple locations simultaneously is a handy feature, but in theory allows a recursive loop. A recursive loop occurs when a parent folder is added into its child folder. The problem of a recursive loop is that a folder structure is infinite, because the child folder includes its parent folder. In case a folder structure with a recursive loop would be synchronized as a local copy to the end users computer, it would cause an infinite number of items, which would rapidly fill the computer’s hard drive. Besides filling the end users hard drive, it would also make deleting the folder structure troublesome, because most file systems start the deletion process by calculating the number of items to be deleted. If the number of items is infinite, the calculation process also takes an infinite amount of time to advance, thus never completes or causes an error message. A graphical representation of a recursive loop can be seen in Figure 1. Each user can also add shared files and folders to the user’s own root folder for easier access. The root folder of each Google Drive is called “My Drive”. Even though a shared folder is owned by someone else, each collaborator can add it into any folder under My Drive, as long as the user has write access and the location does not cause a recursive loop. In case the user does not have write access or tries to create a recursive loop, the operation fails and an error message is shown to the user. When adding files to a shared folder, the file access permissions are inherited from the parent. Thus when adding items to a shared folder, it is not required to explicitly share them to other users, unless the targeted user does not have access to the shared folder. Google Drive automatically increments access permissions to any object within a shared folder to match the access permissions of the parent folder. Manually added permissions are not removed in this process.  In case a user with write access deletes files or folders from a shared location, the items will only be removed from the current parent location, but not from the owners My Drive, which is considered as the root folder. If the shared location was the only location for the deleted item, it becomes orphaned. An orphaned file or folder does not exist in any folder, because it does not have any parent locations. An orphaned item is not considered to be deleted or trashed, it just no longer exists in any location. The only way to find an orphaned file or folder, is to conduct a search in the Google Drive’s web interface. Finding an orphaned file can be challenging, but it becomes even more burdening in case the file’s owner is unknown. The visibility of files and folders in My Drive is illustrated in Figure 2. (2) As illustrated in Figure 2, files which reside in an orphaned folder (such as File A in Folder A) are also potentially hard to find due to the parent folder being orphaned. Even though File B1 is situated in the orphaned Folder A, it can still be easily found due to its existence in Folder B, which is situated in My Drive. Although having files directly in the root of My Drive is not considered as best practice for file organization, it is still directly visible to the user. When someone deletes a single-parented item from a shared folder, one of the two situations occurs: If the deleting user is the owner of the item, the folder will be added to the owner’s trash, from where it is no longer accessible to collaborators in any form before the user restores the item. If the deleting user is not the owner of the item, the item is deleted from the current location, but is still accessible to the owner and other users who are explicitly granted access via manually sharing the item. Even though access is still possible, it is still as an orphaned file and needs to located by searching via the web interface of Google Drive. Google Drive REST API is an essential part of Google Drive SDK, which includes all the required commands, protocols and tools to interact with Google Drive. By having an API, developers around the world are able to extend the functionalities of Google Drive. The API officially supports the following languages: Go, Java, JavaScript, .NET, Node.js, PHP, Python & Ruby. Besides these languages, it is also possible to use the API with any other programming language, as long as the language supports REST calls over HTTPS. The downside of using an unsupported language is that there is very little documentation to support development for such languages. (7)  Google provides up to one billion (1 000 000 000) free Google Drive API requests per project on a daily basis and more can be requested. The number of API requests is limited to 50 requests per second per user. Most operations are done as the file storage user of an organization. Depending on the requirements, additional API requests might be prone to financial expenses. Google Drive API consists of 13 different resources to access Google Drive functionalities. A resource can be thought as an object, in terms of programming. The available resources are: Files, About, Changes, Children, Parents, Permissions, Revisions, Apps, Comments, Replies, Properties, Channels and Realtime. The Files resource includes methods to list, view, follow, create, modify and delete files and folders. When using the listing method, this resource cannot be used directly to list files within a folder. The listing method works in a very similar way than using the search bar in the web based Google Drive interface, with the addition of having more options, such as the ability to limit a search only to Google Photos or reducing the number of returned objects. The About resource is very simple in terms of methods, because it only includes one method to get information about a user, such as the used amount of data in Gmail, Google Drive and Google Photos. The About resource also includes a vast amount of Google Drive related information, such as sharing policies, some Google Drive API settings and the file ID for the user’s root folder (also known as My Drive). The Changes resource includes methods to get, list and watch changes done to files and folders. Every time a change has been done in the user’s Google Drive, a new change item is created, which includes basic information about the file which has been changed. The use of a change log for monitoring file modifications is considerably faster than recursively scanning for each file in a folder structure, because only changed files are listed. The Children resource includes methods to list, remove and add children objects to and from a folder. To use the Children resource, a file ID of a parent folder must be given before being able to use this resource. Also folders are considered as files in the Google Drive API, thus folders also have a file ID. This is the most used resource in this project, as it is required to list files within folders, while checking for user ownerships. This resource cannot be used to delete or trash files or folders. When deleting a child object, it only deletes the object from the selected parent. To actually delete a file or folder, the Files resource must be used. The Parents resource provides methods to list, add and remove parents (locations) of any item. Like the Children resource, this resource cannot be used to actually delete files or folders. It is only used to add or remove a parent from any child object. This resource is utilized as part of checking if a file is situated within a shared folder. The Permissions resource includes methods to list, modify and remove permissions from items. Each permission setting for every item has its own permission ID. The permission resource defines the permission role of an item. In case of modifying existing permissions for a file or folder, the current permission ID for the given item and user combination must be obtained by listing the permission objects for a given file. In case of changing the ownership of an item, the user needs to have a permission object assigned to the given item. If the user does not have any permission objects assigned to a given item, one needs to be created, before the user can be converted as the owner of the given file. This resource is one of the most used resource of this project, as the primary focus is to centralize the ownership of files to one storage user. The Revisions resource includes methods to list, modify and delete revisions of a file. Revisions are stored automatically for each file by Google Drive when a file has been updated. Although the possibility to restore file contents may be an option in the future, this project does not currently focus in restoring file contents, thus this resource is not utilized at all. The Apps resource includes methods to list and view apps, which have been enabled in the users Google Drive. As this project is not focused in listing for existing external Google Apps, it is not used at all. The Comments resource includes methods to list, modify, add and delete comments to file created in Google Docs, Google Sheets or Google Slides. Comments are not currently supported in other types of files. As this project does not include functionalities to view or alter comments in separate files, it is not used at all. The Replies resource includes methods to list, modify, add and delete replies to comments in file created with Google Docs, Google Sheets or Google Slides. As this project does not include functionalities to view or alter comments in separate files, it is not used at all. The Properties resource includes methods to list, create, modify and delete custom properties for drives stored in Google Drive. The properties can be public to all apps or private to just one app. As this project does not focus in file properties of separate files, it is not used at all. The Channels resource is a resource, which is created when a File resource is set to be followed with the Watch method. The Channels resource can be stopped with the Stop method, thus ending the watching or following of a file. Due to requirement of following all items within a given structure, this resource does not have any practical use in the current project. The Realtime resource includes methods to get and update realtime API models, which are associated to a selected file. It can be used to monitor changes to a file in real-time, thus allowing external applications to reflect changes. The Google Drive REST API uses mainly OAuth 2.0 for authorization of the software against Google’s servers. OAuth 2.0 is the successor of Oauth 1.0 and an open standard for allowing secure delegation of data resources. The main idea of Oauth is to allow delegated access to only some part of resources, not to everything. (8) It is used for access authorization by many notable cloud service providers, such as Google, Microsoft, Amazon and Dropbox. (9) To establish a successful connection between the software and Google Apps instance, a client ID and client secret must be generated within the Google Development console. The client ID must be granted appropriate permissions for Google Drive on each Google Apps instance, which will be hosting a shared folder. The permissions are granted from the Google Apps administration console by an administrator of the Google Apps instance. Once the client ID has been created and permissions are granted, the software is able to authenticate and gain authorization to Google’s servers by using Oauth 2.0. The actual authentication and authorization and communication between the software and Google’s servers is usually done in the following steps: First the software requests an access token from Google. The access token is received upon a successful authentication. A successful authentication is achieved by using user consent. In this example, user consent is achieved with the client ID and client secret, which were generated in the Google Development Console. Assuming the authentication was successful, the software receives a session object and an access token, which is valid for a limited time. The session object is used as a proof of authentication by the software, when sending queries and commands to Google’s servers. The access token is used to request a new token before the original token expires. (10) PHP is a widely used server-side scripting language, which was officially introduced in 1995 by Mr. Rasmus Lerdorf. PHP started as a simple procedural language, but it has been further developed to an advanced object-oriented language. (11) The latest version branch of PHP is 5.6. (12)  Even though PHP is an object-oriented language, it is still not categorised as a programming language, due to the fact that PHP applications are not usually compiled to bytecode. PHP applications are typically processed directly on a server by a PHP interpreter.  MySQL is a relational database management system (RDBMS), owned by Oracle Corporation. MySQL was initially released in 1995 by MySQL AB. MySQL is the world’s second most used relational database management system and the most used opensource relational database management system. The latest stable version as of September 30th 2015, is 5.6.27. (13)  MySQL is widely used in website development, but it can also be used for other applications, such as server-side applications and even as a local database for individual applications. Since Oracle bought MySQL in 2008, a new open-source fork of MySQL was created, known as MariaDB, to address concerns about keeping MySQL free and under the GNU/GPL license. The intent of MariaDB is to maintain a high compatibility with MySQL for easier transition, in case the development of a free MySQL is seized. MariaDB’s current lead developer is Michael Widenius, who was one of the founders of MySQL AB. Based on the findings of the inquiry done in the case company at the end of 2014, most users preferred Google Chrome as the application to access the web interface of Google Drive. Some users also used mobile apps and the Google Drive synchronization client, to have a local copy of all files within a shared Google Drive folder. Based on this information, the developed software needs to work discreetly, without affecting current working habits or applications. File restoration should be easy enough for regular office workers, although restoration should be done by an experienced administrator for best results. The requirements for the development environment are not high, because the software is initially used only to monitor a very limited number of shared folders within Google Drive. The official requirements of the selected operating system (Debian GNU/Linux) are also so minor, that any modern PC is sufficient for development purposes. (13) While the development environment does not need to be powerful, the production environment on the other hand needs to be powerful enough to provide a good user experience. Initial benchmarks (Appendix 1) indicate that the monitoring of one shared folder consumes an average of 3 to 6 % of CPU resources on a typical virtual machine with 1 vCPU and 4 GB’s of RAM. This result was measured by recursively listing all files within a shared folder. After changing the software’s logic from recursively listing items directories into a change-based scanning, the CPU consumption was reduced to a stable 3 % of CPU usage (Appendix 2). The change-based scanning lists only files which have been changed since the last scan. With this change, the number of required files to list was reduced exponentially, thus reducing the amount of consumed resources. Based on these measurements, one relatively slow virtual machine is able to withstand the monitoring of at least 32 concurrent shared folders within Google Drive. This assumption was made on the basis that each new shared folder to monitor would cause an average of 3 percentage points increase in CPU usage. By multiplying 3 percentage points by 32, a 96 % CPU usage is achieved. The actual consumption should be considerably lower due to the fact that the server’s operating system itself could consume up to 2 % of CPU while on standby, without any shared folders to monitor, due to automatic updates, indexing and connectivity checks. A more specific measurement can be achieved with a larger number of folders to monitor. The CPU consumption is directly linked to the number of API calls (API requests in Appendix 1 and API Response in Appendix 2) done via Google Drive API, as can be conducted by comparing the CPU usage to the number of API calls per second. The software requirements to run the developed software can be divided roughly in two sections: server-side requirements and client-side requirements. Server-side requirements must be met, for the software to be able to work in the server. Client-side requirements are to be met for the user interface to work for the end-user, who intends to restore file structures to an earlier state. As for the server-side requirements, Google Drive API requires Google Developer Console authorization credentials to function, which can be obtained from Google Developers Console. The Google Drive API PHP client library requires at least PHP version 5.3 to function and the user interface requires at least Apache 2.0 to function. The targeted development and production environment is Debian GNU/Linux (wheezy), which already includes Apache 2.2 and PHP version 5.4, so these requirements are automatically fulfilled just by choosing this operating system. (14) The client-side requirements are exactly same as using Google Drive itself, because all file controlling is made within Google Drive or with a separate file restoration interface, which originates partly from Google Drive. Google Drive requirements include any of the following web browsers: Chrome, Firefox, Internet Explorer, Safari (only on a Mac), as long as the browser is the newest or second newest version release. (15) All interaction between the software and Google Drive REST API is done via regular HTTPS requests over port 443, so there is no major requirements for the internet connection speed or firewall settings. Due to the potentially vast number of requests per shared folder instance, the connection needs to be stable and able to withstand multiple requests within a small timeframe. Initial measurements (Appendix 1) indicated that the monitoring of one shared folder handles about 2 API requests per second and requires about 0.12 Mb/s of download and about 0.035 Mb/s of upload speed to interact seamlessly with the Google Drive REST API. After changing the software logic from a full recursive scan to a change-based scan, the number of API requests was significantly reduced to about 0.2 requests per second on average. A full recursive scan was named as full synchronization and a change-based scan was named as delta synchronization (Appendix 2). The requirements for this project consist of three main categories: First the hardware requirements for the server, which runs the software application must be fulfilled. Second, software requirements must be fulfilled to provide the necessary platform for programming and using the file restoration interface. Third, running the software and connectivity requirements must be fulfilled to provide a consistent quality of service for the software. Debian GNU/Linux was chosen as the operating system for the server platform. According to the official system requirements guide of Debian GNU/Linux, any modern computer would satisfy the basic hardware requirements. Software requirements are divided in two sections: server-side requirements and client-side requirements. Server-side requirements are automatically met by having the latest version of Debian GNU/Linux and client side requirements are automatically met if the end-users are able to access Google Drive via a web browser, since the restoration interface is only plain HTML, with the exception of the instance creation interface, which uses Google Drive file picker for selecting a folder to be monitored. Because the measured network usage was very low, connectivity requirements are quite modest in terms of speed. Due to this, any modern broadband connection should suffice. More important than the connection speed would be the reliability of the connection. Long connection outages might lead into a situation where a file has been created and deleted before it has been noticed by the software. Because the software is intended to run quickly and serve multiple customers reliably 365 days a year, all the minimum requirements must be exceeded with significantly better hardware and network connections. While considering data security aspects, it is also a best practice to use the latest stable release of required software services and tools. This section begins by explaining the methodology used to design and develop the software. After the methodology section, the technical details are explained more thoroughly. The project started with the knowledge of a problem with Google Drive’s shared folders in organizational use. The first step was to find out of any already existing commercial products which could resolve the current problem. According to a conducted questionnaire in the case company, the only commercial product was not considered feasible. The questionnaire provided a good starting point for the design of the requirements for a new software to overcome the encountered problem. The software was designed and developed in various locations and with various devices in a timeframe of nine months. As the agile development method was chosen, the initial design did not include too strict guidelines on how the software should act in different situations. While developing the software, design plans were constantly changed to reflect the encountered problems. As a starting point the project required a dedicated server with a PHP runtime environment, a MySQL database, a Google Apps for Work account and Google Developer credentials. A virtual server with the product name of n1-standard-1 was bought as a Google Cloud Computing service. The n1-standard-1 was the cheapest standard typed virtual server from Google, which was still considered to be more than enough for the software to function in a pilot phase. The required runtimes and databases were installed on the server separately. The required Google Apps for Work account and Google Developer credentials were obtained from Google before the project started. Due to the developer’s many years of experience with the PHP language, it was chosen as the language to develop the software. Although the developer had a considerable amount of knowledge in PHP, some additional information had to be studied via the official PHP website during the development. As PHP is categorized as a scripting language, it did not require any code compilation and all changes were updated in real-time. The developer did not have any previous experience with the Google Drive REST API, so information regarding this had to be gathered from the Google Drive REST API reference page. The software was developed to run on the server as scheduled task, so it does not need any user interaction. The evaluation of this project was done by simulating real-life situations of file and folder deletions while trying to restore folder structures after a deliberate deletion. The ease and speed of file restorations were the primary metrics of the evaluation for this project. Both metrics were measured by assigning end-users of pilot companies the task of restoring files, which were deleted from a shared folder. The evaluation methods of the finished product are described in the Solution Evaluation section. User authentication and authorization process was developed by utilizing Google’s Users PHP API. This means that for a user to access the system, the user has to be in the super administrators group of the respective organization. The authentication process was developed to function in the following way: First the user fills a registration form to sign up for the service. After registering, the user receives a follow-up email with instructions on how to permit the software access to the user’s Google Apps domain. After the user has permitted access to the software, the user is able to log-in to the software’s user interface. The user interface’s login page does not process any credentials, but redirects the user to a Google’s login page. When the user logs in to his/hers Google Account, the user will be redirected back to the software’s user interface as an authorized user. All traffic between the end-users web browser and the developed software is processed via a secure HTTPS (HTTP over SSL) connection with the TLS 1.2 protocol, which is the same technology as used in most commercial online banking systems. The user interface was designed to allow Google Apps for Work administrators a oneclick restoration of a file structure to a given time. The interface was programmed to allow access only to registered users with the Super Administrator role for their Google Apps for Work domain. Once the user has registered to HSWDrive, the user must login to the system via Google’s authentication system. Google’s authentication system provides HSWDrive the user’s administrative role, which is used to verify access privileges Authorized users are given the following options: Create a new shared folder instance, modify the organization’s current instances and restore an existing instance to a certain point of time. The date restoration interface only restores item locations, not the actual data. In case the content of a file needs to be restored to a certain date, Google Drive’s built-in revision control can be used. Items are also never removed, even though a restoration would be done to a time, when a certain item did not exist. Table 1 illustrates the previous statements in a more understandable way. As designed, the restoration interface did not include anything else than a item listing for the selected date and current date, a date selector and a restoration button. The restoration button was implemented with a confirmation alert to prevent accidental restorations. Figure 3 illustrates the file restoration interface in practice. As can be seen in Figure 3, the logout link includes the email address of the currently logged user. The email address is directly linked to the currently logged Google Apps user and in case the user wishes to login again, a new authentication must be done against Google’s authentication system. The left-hand side column displays the current status of the selected folder, while the right-hand side column displays the status on the given date. The main purpose of the designed software, is to change the ownership of all files within a shared folder to one centralized user for easier file restoration. Once the files are owned by one centralized user, the files cannot be permanently deleted by other users as explained in the technology behind Google Drive and the developed software section. The centralized user is called as the data storage user. Due to the nature of Google Drive API, various steps had to be taken in order to change the owner of an item within a shared folder. Because the ownership of an item cannot be changed centrally by one administrative user, the software has to act on behalf of the previous owner to grant ownership to a new user. Figure 4 illustrates how the owner changing process was achieved in the developed software. Based on the flowchart in Figure 4, the first step is to list all permission objects for the given item. A permission object is an instance of the Permission resource, which was explained in the Google Drive REST API section. Once the list of permission objects has been obtained, the software looks for the permission object with the owner role. Based on the found object, the software is able to detect the username of the file’s owner. If the file is owned by an external organization or the storage user is already the current owner, the file is bypassed. After obtaining the owner’s username, the software checks if the storage user already has a permission object, the ID for that object is stored for future use. In case the storage user does not have a permission object, the software creates a permission object for the storage user and stores the ID for the newly created object. Once the software has the permission object for the current and future user, the software creates a new Google Drive service as the previous owner, and transfers the ownership to the storage user. Most of the programming was done with the PHP language in combination with a MySQL database for file status recording. According to the original plan, a PHP based programming framework called Symfony2 was intended to be used for the project. As the software logic advanced, it seemed Symfony2 would have been too resource consuming for such software. Thus plain PHP was finally selected to be the programming language for this project. Some sections of the user interface required the use of JavaScript. As the project advanced, it was clear that the original plan of using at least some sort of PHP framework would have been better in terms of manageability. All functions were achieved, but as a result of using plain PHP, the software became more complex to update in the future. If the software becomes a success in terms of sales, a next logical step would be to re-write most of the code in some PHP framework to support easier updates in the future. MySQL was selected as the database engine. The database consists of four separate tables. The first table includes a list of all instances. An instance is basically just a row of information, consisting of the basic information of the centralized data user, company information, file ID for the shared folder and file ID’s for the lost files folders. The second table consists of the status of files in the shared folder. This table includes the basic information of each file, such as the file name, instance ID, and last update time. The third table is structurally identical to the file table, but only includes the history of old files. Every time a file is being updated, the third table shows the status history of each file, as can be deducted from the table below. The fourth table consists of registered users. Only registered users are allowed to add, modify and restore instances to an earlier stage. Only registered users are allowed to add, modify and restore instances to an earlier stage. Each row in the instances table represents a shared folder for a single organization. The shared folder may have numerous subfolders and files. The instances table also defines the location for the files with the status of “Escaped”, “Released” or “Orphaned”, which are all moved under respectively named folders in the “Lost and found” folder for that instance. The file tables and instance table are connected to each other with the instance ID parameter, by using the built-in relation model of MySQL InnoDB. This relation is indicated with an asterisk character in Table 2. The main idea of this database scheme is to have all the up to date file information in one table and another table is used only for logging purposes. By having a link to the instance ID, if is easy to query for files which belong to a single instance. Each row in the users table includes basic information about the user. The isAdmin column of the user table describes whether the user has full administrative privileges for the whole system. Users with full administrative privileges are automatically granted full access to all organization’s instances. Unlike most database systems, there is no password column in the users table. The reason for this, is that authentication is done via Google’s Users PHP API. Login passwords are never stored or handled within HSWDrive. Each item (file or folder) in the database has a status. Due to the fact that one item can reside in multiple locations within Google Drive, one item might be listed multiple times in the database, if the item is in multiple shared folders that are monitored by HSWDrive. The items are separated by an instance ID, which indicates to the folder that is being monitored. Possible item statuses are: ok, deleted, orphaned, escaped, released, trashed or external. Table 3 explains thoroughly what each  The item is accessible by the centralized data user, but ownership has been given away and the item is no longer in the shared folder. This type of item is automatically added to the "released" folder. As stated previously, all changes to items are scanned once every minute. Each scan checks the following for all items: Is the item the root folder of the shared folder, has the item been deleted, does the item exist in the database, what are the current parents, are the parents currently within the shared folder, is the item owned by the storage user, is the file owned by the same Google Apps organization, is the file trashed, have the parents changed and has the item’s name changed. To decide what to do to a file in case of a detected change, the table of Appendix 3 is used to determine the next action and file status. The software was named as HSWDrive. It was developed to function in the following steps: First a company authorizes the software to have access to its Google Drive. Next the company decides and provides the software a shared folder and a centralized data user account, which will perform as the central data storage owner. In the third step, the software performs a recursive scan for the whole shared folder. The software lists all files and folders stored in the shared folder and adds or updates the files in a database table. While scanning the files, the software changes the ownership of each file to match the centralized data user account. Only files owned by someone in the same organization will be modified. After these two steps are done, the software starts to monitor all changes done within the shared folder. From this point on, it will perform a full scan only once daily, but individual changes are monitored every minute. In a perfect world the monitoring alone would be enough, but due to possible errors in file ownership updates or network connectivity issues, a daily scan was scheduled to overcome possible update errors. The end-product is a fully-functional server-side software, which runs as scheduled tasks via crontab. The software is programmed with the PHP language and it uses the Google Drive API for interaction with Google Drive. The stages of development are roughly in the following steps: First the prerequisites (software, hardware, credentials) must be met, second follows the initial development and deployment, third comes piloting and feedback and last comes the final release. The finished product is sold as a service, so the customer never gets any executable files or code. The software consists of two separate main components to handle files: Delta synchronization and full sync. The delta synchronization component only checks for changes done to files in the given folder since the last change. The full synchronization component runs a recursive scan of all files and folders within the shared folder. A delta synchronization is run every minute, while a full synchronization is run only once daily. The reason for this division is that neither of the components alone are effective alone. A delta synchronization is very quick to run, because it detects only the last changes. On the other hand, in case of a slight network failure in the delta sync, some files might not be processed. The full synchronization is less prone to miss files in case of a slight network error, but it might take a very long time to scan a large file structure. Another downside of a full synchronization is that it consumes a lot more network and processor resources, due to scanning each and every single file in the folder structure. Due to the increasing number of shared folder instances, two minor components were made to run all folder instance synchronizations simultaneously. The main benefit of running a synchronization for all folders simultaneously is the noticeable increase of speed, compared to running the synchronization on all folders consecutively. Besides the finished product, a deployment guide was made for administrators to easily setup HSWDrive on their organization. This was done to make deployment as easy as possible, considering not being able to use Google Apps Marketplace, which would have been the easiest solution. Due to time constraints, the deployment guide was considered to be more worth the effort than submitting a beta staged software publicly to Google Apps Marketplace. The product quality was evaluated in terms of functionality, ease of deployment, ease of use and speed of item restoration in case of an accidental file or file structure deletion. The functionality evaluation was considered as successful, if the software is able to keep track of all files and file changes within a shared folder. Besides being able to keep track of files and changes, the software must also be able to function quickly to provide a fluent user experience. A delay of 5 minutes was chosen to be the maximum accepted time for a change to be recorded within normal use, although the expected average delay is less than one minute. These could be tested by making changes to items in a shared folder and confirming that the changes are registered within the software’s database. Ease of deployment was measured by providing the customer administrators a deployment guide and observing if the customer is able to deploy the software to their organization with minimal help. Ease of use and speed of item restoration was measured by asking regular users to deliberately delete files and attempt restoration via the HSWDrive file restoration interface. This type of evaluation was considered to be the only way to concretely see if the software is reliable, fast and easy enough for production use with future customers. Besides evaluating the basic requirements of the solution, practical testing was done in the following categories: resource consumption, speed of use and ease of use with a total of 11 shared folders from various organizations. The solution was stress tested for resource usage by deploying various shared folders from various organizations for pilot use. After the deployment, resource usage was monitored via the server’s own reporting tools and Google Developer Console. The detailed measurements can be found in the Results and Analysis section. Speed was monitored by adding items in a shared folder and calculating the time that the software requires to process the item. The file processing speed was observed afterwards, by using Google Drive’s activity pane for the selected items. This method provided a reliable way to measure speed, without having to compare update time between Ease of use was tested in two sections: Implementation and usage. The implementation section was tested by providing the administrators of customer organizations a deployment guide and testing if the customers were able to deploy the software by themselves. The usage section was tested by giving the pilot customers a tasks to recover files by using the restoration interface. If the customer was able to implement the software with minimal help, the requirement for ease of deployment was considered as a success. If a user was able to restore files to any earlier location with minimal help, the requirement for ease of use was considered a success. A separate quick guide (Appendix 4) was given to the end-users to help in the deployment and recovery process. As most software development projects, also this project had problems which were not anticipated in the design phase. This section describes the encountered problems and how they were resolved. While testing the functionality of the first working version of the software, a high amount of system resource usage was measured particularly in CPU, network and disk usage (Appendix 1). While this did not cause a major problem in the speed or reliability of the software with one case company, it would have had a greater impact in an environment of multiple companies and shared folders. Because the software is aimed to be sold as a service, it is expected to handle various instances within one physical server without having noticeable latency or reliability issues. After a logic survey of the software the cause of high resource usage was pinpointed to the logic of recursive file scanning. As the initial version of the software used to scan through all files within a shared folder every hour, it caused a major resource usage peak every hour. Due to the large number of files within the case companies shared folder, this scan took almost an hour for each scan. To overcome this problem, a change in program logic was implemented in the following way: Instead of scanning through all the files every hour, only the changes made to files within a shared folder were monitored after the initial recursive scan. This was possible by using the Changes resource, provided by the Google Drive REST API. With this logic, all changes could be detected each minute, and still the average resource usage would was measured to be only 10 % compared to the original resource usage (Appendix 2). Because the Google Drive API PHP Client library is updated very frequently, the documentation provided by Google did not always comply with the actual functions of the client library. This caused some significant problems in making the software work correctly. These problems were exceeded by investigating the source code of the client library. Requests for correlation of the documentation were sent to Google. The final decision of choosing not to use Symfony2 framework, unlike originally planned, caused a significant increase in programming time, because many functions had to be re-written in the event of a change to the database structure. When this deficiency was noticed, the software was already developed so far, that moving it to a framework was not considered to be worth the effort. The software was finished without a framework, but if time and budget allows, it will be re-written within Symfony2 framework in the future. While starting the project, it was not obvious that even seemingly simple operations, such as changing the owner of a file required multiple phases to achieve. Even though the software can be granted full administrative privileges to an organization’s Google Drive, each operation is done subjectively with a user account. This means that instead of changing the owner of a file needs to be done as the current owner. As an example: It was not possible to just define a new owner to a file with one simple command. To change the owner of a file, the first operation was to list all permission objects for the file. After obtaining a list of permission objects, each object had to be scanned for the username and permission level. While scanning the permission objects, the current owner had to be selected to confirm if the file was already owned by the storage user. In case it was not, the second phase was to check if the storage user had any permission object at all. In case the storage user did not have any permission object, one needed to be created. Once the current owner was found and the storage user had a permission object, the ownership could finally be transferred from the owner’s permission object to the storage user’s permission object. The transfer had to be done as the previous owner. Another complexity aspect of Google Drive API was that all items are considered as files. Understanding that within Google Drive, folders are actually just empty files with children objects was a new revelation. For simplicity, “item” was selected as the word to represent any file or folder in this thesis. Due to the fact that one item can co-exist in multiple folders, it was required to understand the concept of parent objects and children objects. Parent objects can be thought as the locations for the item, while child objects can be thought as files within a folder. Naturally only folder objects can have children objects. After having the software online for a few days, some pilot companies reported on lost files, which could only be found by some users in the respective organization. These reports led to an investigation, which indicated that although the lost items were in the corresponding folder, they would not be visible to some users. The users who experienced these problems had sufficient privileges to the respective folder, but still some files would appear have vanished. The underlying reason for this has been unknown up to this date but a work-around has been implemented to overcome this problem. The workaround to overcome the issue was to re-share the folder to the users who are experiencing problems. After the folder was re-shared, no further complaints were received by the users. This section covers basic information about possible licensing terms and distribution methods for the developed software solution. Even though licensing terms and distribution methods were not the focus of this project, they are relevant or future reference. As mentioned in the previous chapters, the software is made to be sold as a service, so a separate license agreement is not made between the buyer and the seller. On the other hand, the customer is required to accept a service contract, which describes the content and price of the subscription. The pricing and contract terms are not the focus of this thesis, so they will not be discussed any further. The initial target group consists of companies which have bought Google Apps for Work from the case company. Due to this target group, most agreements are initially done via telephone or email, while the distribution is done remotely as a service. After the software has been distributed to the initial target group, it will be targeted to other companies in the future. Although the commercial use of the developed software is not the main focus of this thesis, it is still a focus point of the future. Once the software has been successfully tested for at least 6 months on various test companies, it can be considered to be a ready for commercial use. To truly make it commercial, it has to be published in Google Apps Marketplace for easier distribution. The following procedure must be done in order to publish the app: First a payment system needs to be integrated to the registration form. This can be done with PayPal or another similar payment system. After that, it is required to register as a developer to Google Apps Marketplace. Registering requires a 5 USD payment to Google. After registering, the second phase is to take screenshots of the software and create a manifest file. The manifest file should include basic information about the software, such as the name, description and icons. After a manifest file has been created and screenshots are taken, the third phase is to submit the manifest file, screenshots and application URLs to Google Developer Dashboard. After submitting the app, the fourth phase is to send a request for listing to Google Apps Marketplace by filling the Google Apps Marketplace Listing Review Request form. Once the form has been submitted, Google reviews the software and decides whether or not it is acceptable for publishing. If the software is considered as acceptable, the fifth phase is to test the installation of the software via Google Apps Marketplace. If everything works as intended, the final phase is to start marketing the software via various internet-based channels. (14) Although the software itself is intended to be sold as a service for monetary gain, the source code may be published with the GNU General Public License (GNU GPL) or similar license, thus granting other developers (individuals and organizations) the freedom to use, modify and copy the source code. There are numerous reasons why this is considered as a good practice. First, all the developers programming knowledge has been gained from free resources, including most of the API resources and code snippets. Based on this, it would only be fair to give something back. Second, publishing the source code would allow other developers to provide feedback and recommendations to improve the code. Third, publishing the source code could also function as public demonstration of excellence, thus potentially provide work opportunities in the future. Finally, as the software is intended to be run on a server, if is very unlikely that small business organizations would have the resources or knowledge for deploying the software, while larger business organizations usually tend to avoid open source solutions without a guarantee of support. If GNU GPL is chosen as the license model for the software, each source code file should include a preamble, indicating that the source code is under the GNU GPL license. For the time being, no decisions have been made regarding the licensing of the software. This decision will be made in the future when the software has been further developed and implemented to a larger scale of customers. This section discusses the results and outcome of the finished software solution. The amount of work and resource consumption is the main focus, while the summary section includes the final measurements. The development of the software was done within 9 months, which was within the set timeframe and did not cause any delays to other work matters. The software had a total of 4194 lines of PHP code and 865 lines of CSS code for the user interface. The original assumption of required PHP code was less than 1000 lines, so the amount of implementation work was exceeded multiple times. The main reason for the vast amount of code was clearly because of not using a PHP framework, which would have minimized the required amount of manually written code. If the original plan of using Symfony2 Framework had been followed, most of the manually made PHP methods could have been automized and database structure changes could have been updated from one central location, instead of updating all functions manually in the case of a change in the database structure. The software was measured to function with very minimal resource consumption. By increasing the number of shared folders to be monitored from one to eleven, the only noticeable resource increase was observed in the CPU usage. As indicated in Figure 5, the CPU usage with one folder was measured to be approximately 3 %, increasing the number of folders to eleven increased the CPU usage to approximately 17 %. This indicates that the server should withstand at least 32 concurrent folders, while still having a 50 % CPU consumption average on the current virtual machine. Taking in account that the used virtual machine is the slowest standard typed virtual machine within the Google Cloud Computing Platform, it is just a matter of upgrading to a faster virtual machine, in case more processing power is required. By observing Figure 6, it is clear that even with 11 shared folders to monitor, network and disk usage activity had almost zero impact to the server with less than 50 Kbps network usage and 30 KBps disk usage on an average. Some traffic spikes were observed, but even those had very minimal constraint to the server. The only significant increase of resource consumption was measured as the number of API requests done to the Google Drive API, as can be observed in Figure 7. While the first version of the software consumed an average of 2 requests per second for one shared folder, the improved version with a change-based scan only consumed an average of 0.2 requests per second for one shared folder. The measurement of 11 shared folders consumed an average of 7 requests per second, which increases that the average of one folder would be 0.64 requests per second. This sudden increase of requests can be explained with the fact that most of the latest customers had just migrated to Google Drive, thus adding new files on a daily basis. Once the new customers have finished their migration, the average number of requests should be considerably lower. When considering that the maximum number of daily API requests is one billion (1 000 000 000), there is still capacity for 1000 shared folders, even with the measured peak value of over almost one million daily requests. The success rate of requests was measured as 100 %, which indicates in a flawless network connection and in the adequacy of quota for API requests. As the requirements indicated that the software had to be easy to use, various tests were performed by giving customers the task to deploy an instance and restore deleted items. According to the tests, all 5 customer administrators were able to restore deleted files successfully. The deployment phase was considered as more difficult to most customers, due to the advanced setting changes within Google Apps Administrator Console. Only 2 users out of 5 users were able to deploy the software by themselves. Taking in consideration that most of the customers were not technically experienced, this was still considered as a success in terms of deployment easiness. Once the software is deployed in Google Apps Marketplace, the deployment will be considerably easier for users with a less technical background. While building the solution, many new aspects of Google Drive were learned, including many which are not obvious to the regular user. Before the project was started, it was quite unclear how Google Drive handles file deletions in shared folders. Even with the knowledge of Google Drive’s file handling logic, it might still be challenging to explain the operations logic to regular users in a clear way, due to its owner-based file system. In the beginning of the project, Google Drive seemed like any other file system, where files can be listed, copied and modified by simple commands. As the development advanced, it was clear that the sophistication and design of Google Drive was done in a very different way, since each operation to a file required multiple phases to accomplish the desired outcome. Most of the functions were combined to PHP methods, which made the operations easier in the future, but the amount of work required to build these functions was far greater than was anticipated while starting the project. One of the new discoveries was the fact that a file which is not deleted, but inaccessible to a user is considered to be deleted in the Google Drive’s point of view. A software solution was designed and developed according to the initial requirements. The software was named as HSWDrive and evaluated successfully with various pilot customers. The software runs a full synchronization to all HSWDrive instances on a daily basis and a delta synchronization every minute. File structure restorations were tested and proven to work as designed. Even though the software has been proven to work as expected with a set of customers, further analysis and bug tracking is required before publishing it in Google Apps Marketplace for wide distribution. The software is already in active use by the case company and many of its customers. Further development for new features, such as file revision restoration and support for externally owned files is scheduled for the near future. The aim of this project was to design and develop a software solution to allow easy restoration of accidentally deleted files in a Google Drive shared folder. The software was designed, finished, tested and deployed to various pilot customers within 9 months, which was considered to be within an acceptable timeframe. The development would have been faster with a PHP framework but on the other hand, it probably would have had consumed more system resources. If time allows, the software will be re-written in a PHP framework in the future. The software was a success in terms of functionality and demand, as it was proven to function fluently on various pilot companies. Ease of use was also measured to be satisfactory, as all pilot users were able to restore files and 40 % were able to deploy the software independently. Some technical issues were discovered during initial deployment, but once these were overcome, no significant deficiencies were observed. The software is still in a testing phase, but after a more comprehensive and successful testing period of at least 6 months, the software can be submitted to Google Apps Marketplace for easier deployment and sales purposes. A future release of the software will include the possibility to restore files not only to earlier locations, but also earlier states. Due to Google Drive’s automatic revision history, this can be achieved without having to store file data in the software’s own database. Another functionality to be developed in the future is the possibility to restore files owned by another organization. Currently this is not allowed, because HSWDrive only handles files owned by the same organization and file ownerships cannot be transferred between organizations due to Google Drive’s limitations. Even though it is not possible to take ownership of such files, it is still possible to monitor and log the files in case of an accidental deletion. The research started in 2012, to study current business needs of the company and how the Business management tools and methods are helping to address the needs, the research began with a study about the best practices and current practices in using agile methods and tools in different teams. The main objective of the research was to analyse the current practices in the teams for project planning and execution and identify the ways that are needed to make the visibility of the project situation clear to all the stakeholders in the project and with a purpose to reduce delays in the project. While the research moved on with the study and discussion with many individuals in the team, it turned up to be a great opportunity to learn the business management methods and tools from the information shared in the discussions. Also with several years of experience in the industry, in different kind of roles, I had an opportunity to express and relate the best ways of using the tools and methods, which would help any individual in software development organisations similar to the one in the case study. The research should give a fairly good view of best practices in Agile methods and tools, and what is missing currently in practices, correcting the missing practices could reduce delay in meeting the project deadline. It brings in a clear picture of combining the latest agile methods and tools, to address the current needs of the industry. The solution provided will be helpful for better utilisation of tools and stay updated with the current situation of the projects at any time. This research would have not been possible without many individuals, whom I wish to give my sincere thanks, and to mention, my mother who travelled to Finland from India on time, when I needed her help to go for studies and my supervisors Jukka Kainulainen and Thomas Rohweder. The aim of the present thesis was to study the current ways of using agile methods and practices in the case company and find the ways to improve the current practices, in order to have better visibility of the project status. Accept360 is the tool used for project planning and execution in the case company, hence the study concentrated on understanding the current practices in using the tool and finding out the gaps that exists in the current practices, in comparison with the best practices of using the Agile methods and tools. Agile approaches are used in software development to help businesses respond to unpredictability. To address the uncertainties and have good visibility of the project status, the agile methods provide rules and a standard ways of usage, which would help the project stakeholders to stay updated with the project status. In the research the best practices in agile method of software development were studied first, and some of the best practices and methods that would best suit the organisation were identified. Next, the current practices were studied in the selected teams in the case organisation, by conducting contact interviews with the key stakeholders, from which the researcher collected information about the current ways of using the agile methods and tools. The interviews also gave information about the opinions about the tool which is being used currently for project planning and execution, how the tool addressed the current needs for project planning and execution and what were the suggestions from the key stakeholders for improving the practices in the tool and methods. The data collected about the current practices was then compared with the best practices and the gaps between them were identified, from which the proposals to improve the visibility of the project situation were created. The study resulted in three key theme proposals. The first theme is to follow the already agreed practices and guidelines, which is missing currently. The second theme is using the Agile Software Product Management method, and the third theme is implementing Scrumban practice for project planning and execution. The last two themes mainly recommended ways for having a separation between requirement and engineering management. The proposals were found to be very useful input for further process improvements, to have better visibility of the project status and address the current key issues in the case organisation. Earlier Nokia Oy comprised following business groups: Smart Devices, Mobile Phones, Locations and Commerce, and Nokia Siemens Networks. Later Nokia’s Smart device and Mobile Phones groups were then bought by Microsoft and merged into one, to create a new group Microsoft Mobile Oy, ever since the merger, over the past three years the group has created incredible results: award-winning phones and amazing services that have made Nokia Windows Phones the fastest-growing smartphones in the world. In Microsoft Mobile Oy, Applications Software teams which belong to Smart Devices unit are the teams which creates valuable and innovative solutions that adds value to the smartphones. Projects executed by the teams in Applications Software Team require dynamic changes and updates in requirements always. Projects are executed in challenging situations where the market is highly competitive and customer needs are changing rapidly, in order to meet such demands and supply the best products for the customers ,and also to deliver products with high standard and quality, it is required to make sure that the projects are planned and executed with high productivity and good utilisation of the resources available, also it is required that the project situation is visible to all the audience of the project at any point of time, so that the needs for executing the project successfully are meet as early as possible and there is minimum delay in delivering the project. Even after having a well-defined processes and tools for project planning, management and execution, there are delays in some projects. Hence there is a good need for evaluation of how the project planning tool and methods that are used in the projects and how efficiently the tools are used to track the progress of the project. The main objective of this thesis is to study the best practices using the agile methods and tools, in the company Accept360 is being used mainly for project planning and tracking, the current way of usage of Accept360 tool need to be analyzed, and the gaps that exist between best practices and the current practices need to be identified, and solutions that could fill this gaps need to be researched. Research question of this thesis is “How the visibility of the project situation could be kept updated at any point of time?” “What are the best ways to stay on track of the current situation of the projects and so be able to manage the issues appearing earlier and minimize the delivery slippages?” The outcome of the study is to propose the solutions which include guidelines for the efficient usage of best practices, artefacts, tools and methods, sample process framework that would address the needs of the projects and that are needed to help everyone involved in the project to have a clear visibility of the project status. This chapter explains research approach, this includes the steps that were done to identify the research problem, analyse the current state and the steps that were done to find solution for the research problem. As first step, the details about the case company was studied as a part of the course on Business management methods and tools, it included complete study of the company’s strategy, mission, visions, goals, current growth needs and KPI measurements .In the study it was found that the current importance for the company is to minimise the delays in delivering the software, for which the visibility of the project status plays a key role, unless the needs of the project is visible to all the stack holders involved , at any point of time, it is not possible to address them earlier. In ASW team Accept360 tool is used as the key tool for tracking and executing the projects, hence a research on how the tool is currently being used in the teams, does the tools address all the needs of the projects, is there any improvements needed in currents way of working in the teams needed to have better visibility of the project situation sounded a good topic of research for the thesis. Hence a study about the Visibility of the project situation was decided as topic of the research. A flowchart in Figure 1 was created with steps for finding the solution for research problem. As second steps, the research topic was discussed with the senior manager in the company, and it was considered that this topic of research would yield a good review and guide for identifying the gaps that exists in the currents practices of usage of the tool and methods for Project Planning and Execution, and the objective of the thesis was set. Since Project planning and tracking is done using the Accept360 tool ,study of the tool was planned first , Initially couple of meetings were planned to discuss the available information about Accept360 tool and how the research need to be done. During the first meeting the guidelines available for the Accept360 tool were discussed, and previous presentations that were used to provide guidelines for the teams, for using the Acccept360 tools were gone through. In the second meeting a short walk through of contents in the Accept360, for some of the projects were viewed and discussed. Then identification of the teams and the group of people to be contacted for current state analysis was done. Next identification of a team that is using Accept360 in compliance to the guidelines was done, it was found that the Team S, in Sandiego, has created the content in Accept360 in a best format, that it could be used as benchmark for comparative study of Accept360 content. As a third step, a study of best practices in the software development methods and practices was done using books, articles, company’s project guidelines and the training materials for the Accetp360 tool and practices. Next access to the training website for the Accept360 tool, which is an exact trial ground for the usage of the tool in the same way as real situation was requested, with access to the tool, studied the features supported in the tool, experimented the usage of the tool to understand the complexities and merits of the tool. Next a basic set of questions and topics for discussion were designed for the interview. From the work experience in Nokia Research and Development centre since 2006 , Nokia research and development organisation until and around 2007 had been following the waterfall model for its software development process, after that time there were many trainings and changes in the way of working, for moving towards Agile methods. The main drawback with waterfall model is, it follows the procedure in which when each stage of the project is completed, the developers go on to the next stage, and they do not validate the previous step when they go forward, so at the end if the final product turns out to be with some flaws, there is less chance to go back and fix the issues, if the flaws were found at the end, to go back and fix the problems means it would be possible only by scratching the whole project, there is no chance for change, hence extensive planning on expected outcome is done at the initial stage and then the project is executed, after the planning is done the design and requirements are documented very well, the documentation helps when there are very less resources available to continue in the project, help to add new resources quickly. In waterfall model client would know what to expect, but at the same time there is a high risk that if the planning at the beginning had some faults, that could end up in risking the whole project and it also does not consider the evolving needs of the customers, testing is done only at the end of the project, so if errors are found at this stage, it could be that they will stay for rest of the project and could not be fixed. Waterfall method is good only if the initial requirements are very clear and expected outcome is well defined. Agile methods were introduced as solution to many problems in the waterfall methods, this method allows changes to be done at any stage of the project, initial requirements are kept very simple and a very simple product is made initially, later changes are done as per the client needs and latest improvements in the industry, The products development is done in small cycles in stages, at the end of each cycle the requirements are prioritized and updated, testing is done at every sprint, so that the errors are identified at every stage and the prioritised errors are taken care in next development cycle, this also produces good quality product, over comprehensive documentation at every stage, this means that the product is in quality at each stage and could be released at the end of any stage, this ensure that the deadline for releasing the product is always met. At the same time there is more care needed, to make sure that every stages in the project are not series of coding stage and end up in a way that the initial planned requirements and product and final outcome are totally different. Agile method of development is good if the product is intended for a rapidly changing market. This also requires highly skilled, independent and adaptive developers to be involved in the project. Agile methodologies promise that the product reaches the market faster, with good quality of software fulfilling customer needs. There are several software development methods introduced based on the principles that are followed in different organizations. For example the Lean Software Development method was introduced based on the principles followed in Toyota manufacturing unit, which are based on continuous improvement and elimination of waste, but these are more generic principles, that this could also be applied in the agile methods. The table below from the reference [1], lists different agile methods and principles mentioned in the reference [1] Scrum is a light weight software development process consisting of implementing a small number of customer requirements in two to four week sprint cycles (Schwaber, 1995). XP consists of collecting informal requirements from on-site customers, organizing teams of pair programmers, developing simple designs, conducting rigorous unit testing, and delivering small and simple software packages in short two-week intervals (Anderson et al., 1998). Adaptive software development or ASD involves product initiation, adaptive cycle planning, concurrent feature development, quality review, and final quality assurance and release (Highsmith, 2000). LEAN involves eliminating waste, amplifying learning, deciding as late as possible, delivering as fast as possible, empowering the team, building integrity in, and seeing the whole (Poppendieck & Poppendieck, 2003). In general, agile methods may vary in applying certain practices. However, the methods emphasis on producing working software in small iterations, and utilising resources efficiently. Main features of the agile software development could be summarized as three key things, as mentioned in the reference [4], Feature orientation, Reactive development and Evolving Project scope. Feature orientations is to have main focus on the producing features faster, with a goal to deliver the working functionality that brings most value to the customer. Reactive development is reacting to a change rather than planning ahead and keeping the decision making as late as possible. Examples of reactive practices are refactoring, adjusting requirements priorities, and release scope after each iteration. Evolving project (release) scope is the main distinguishing features of agile development, changing from a fix-scope approach to a more open-ended approach. In the traditional fixed scope approach much effort is spent on defining and planning the content of a product release of a project upfront. In agile the release scope is emerging in the process of development rather than planned ahead. A prioritised list of requirements serves as an initial input which is a kind of wish-list of a release scope. The release scope is expected to be refined and updated at the end of each planned cycles of work, in order to accommodate changes and new information that was learned in the latest stage. This practice is in line with the reactive development property described above. Scrums main principle is implementing a small number of requirements in a short cycle. It includes mainly the following ways, Self-correction with inspection, making everything visible or known to the stakeholders, for example, plans, schedules, issues and progress more clear at every point of time as they are changing all the time, Stop and Review the product and the process. Scrum operates this way: At the beginning of each sprint the team reviews what it should do. Then the team selects what it can turn into the release of a potentially working functionality by the end of the sprint. Then the team start to work independently with no interruption to make the best efforts for the rest of the sprint. At the end of each sprint the Team demonstrates the release of functionality it built, so that all the stakeholders can inspect and do adaptation to the project. In Scrum the project is started with a vision that needs to be developed. The vision initially is stated in market terms and not much in technical terms. The items which are to be developed to deliver this vision are made as a list called product backlog items. As scrum suggests one person is responsible for delivering the items planned to those who are funding the project and the person in scrum terms is called Product owner. The product owner is responsible for delivering the vision in such a way that maximises their return of investment, for that the owner creates a plan, which includes the product backlog. Product Backlog is a list of functional and non-functional requirements, that when turned into functionality, will deliver the vision of the project. The requirements that are important to create most value for the product is at the top priority in the list of requirements. The prioritized requirements is a starting point, and the contents, priorities, and grouping of the requirements into releases, this is usually expected to change the moment the project starts. The updates and changes in business requirements and the performance of the team to build the requirements into functionality decides the changes in the requirement list and its content. Scrum team is a self-organizing team. The team plan their own work, the work plan is visible as sprint backlog items, it is a good practice if each functionality to be delivered is appearing as a sprint backlog item, this helps the team to view the sprint backlog item as a plan and also as a reference for the other team members as they work. In this way it ensure that the work to be done is thought through. This would make sure that the daily scrums are more meaningful as the work plan is clearer. Scrum master role is recommended in Scrum, one person in the team act in this role, to ensure that the scrum rules are followed and the team do not cut corners. Scrum recommends it as a thumb rule that the team is given the prioritised backlog items, based on which the team decide by its own on what need to be done next, and it is required that the scrum master in the team ensures that the team does not skip the process of scrum, and also can help remove the impediments, but at the same time the person does not have the authority over the team, but only acts to help shape the development processes in the team and make sure that the team brings out the good results and make sure the team is not going off track. Scrum team location is recommended to be in a collocated team space, which is achieved by removing the cubicles, and let team members to face each other and communicate often, this eliminates isolation and misunderstandings. Mandatory meetings for scrum execution are, the Sprint planning meeting, Daily Scrum meeting, Sprint review meeting and Sprint retrospective meeting, these meetings are supported and encouraged by the Scrum master, and these meetings are emphasised to get the maximum benefit of the scrum process framework. In Scrum all work are done in small cycles called sprint, which usually is a one to four weeks cycle. Sprint Planning meetings usually have two parts, in the first half the Product Owner presents the team with the list of requirements that are prioritised and that are expected to deliver a functionality in the release at the end of the Sprint, this list is called Sprint Backlogs. During the second half of the meeting, the team is expected to discuss and plan their work, tasks are created by the team for completing the Sprint backlog items, after this meeting the sprint is started, and it is time boxed to the number of days the sprint is supposed to be. Daily Scrum meeting is held daily for 15 minutes, in this meeting the all the team members meet at one place and each team member answers to the three questions, what was done since last daily scrum? What is planned to be done from now till the next daily scrum? And what are the impediments that are on the way to complete the planned items in the sprint backlog. The purpose of this meeting is to synchronize the work with everyone in the team and to address and schedule for more meetings if needed to go forward in completing the sprint goals. Sprint review and the Retrospective meetings are held at the end of each sprint. In the sprint review meeting the team presents the developed functionality or implementation to all the stakeholders of the project, this is an informal meetings, this helps to bring all the stakeholders together and have an understanding and view what is needed next for the project and then plan for it. Sprint retrospective meeting is held after the Sprint review meeting and before the next planning meeting. This meeting is to encourage the team to revise the process they adapted in the sprint, and make sure it is within the scrum process framework, so that the next sprint is more effective and interesting for everyone in the team. User Stories, Epics, and Themes are the three work items which are mainly used to define the items that needs to be worked on by the scrum team. Theme in scrum is the highest level in the story hierarchy and describes a view of a tangible product which can be a trading application or an abstract goal such as performance tuning. A product owner breaks down a theme into one or more epics. Epic represents a group of related user stories or a block of requirement that is not yet been rationalized into stories. Sometimes a large user story is also called as an epic. A story is a brief statement of a product requirement or a business case. Typically, stories are expressed in plain language to help the reader understand what the software should accomplish. Product owners create stories. A scrum user then divides the stories into one or more scrum tasks. Scrum recommends to remove the unneeded artefact such a design documents, hence the work item’s descriptions serve as a source of information about the project. Projects are estimated based on the three key things, the resources, scope and time. Estimation is a challenging task to do when there are uncertainties of way of execution and process followed are changing, but when there is a defined process it could make estimation easier. Time spend for estimation could go waste if the estimation are not useful or appropriate, but the possibility to have accurate estimate is very less, as there is some amount of uncertainties in the projects dependencies, in projects which are driven for innovation and change of existing technologies, so spending more time in estimating the project will not return a good value for investment, but spending a little time to estimate even if it is less accurate would be more efficient to manage the project. From the above Figure 4 indicates that the accuracy value starts to degrade when the amount of time spend in estimation is going beyond certain limit. Estimate is still an estimate. Spending more time in estimation is not going to make the estimate more accurate. And to reach high level of accuracy in the curve, which means to move away from base line ,only a little efforts is needed, so a less amount of time spend in the estimation would increase the accuracy level. As mentioned in the reference Agile teams tend to stay in the left side of the Figure 4, as they acknowledge that estimates could not be done accurately, but still encourage the idea of estimation with less time and recognise those estimates that give big gain. As agile process is to deliver frequently a fully working, tested and integrated software, they always are in a situation that they have a reliable plan. Usually the user stories are not fully grained down to fine level, so they are not of same order of magnitude, and differ in their size, Hence by aggregating some stories into themes and writing some stories as epics, a team is able to reduce the effort they will spend on estimating. However, it’s important that they realize that estimates of themes and epics will be more uncertain than estimates of the more specific, smaller user stories. User stories that will be worked on in the near future, for example in the next few iterations, need to be small enough that they can be completed in a single iteration. These items should be estimated within one order of magnitude. It is a good practice to use the sequence 1, 2, 3, 5, and 8 for this estimation. User stories or other items that are likely to be clearer than a few iterations can be left as epics or themes. These items can be estimated in units beyond the 1 to 8 range sequence recommend earlier. To accommodate estimating these larger items it is good to add 13, 20, 40, and 100 to the preferred sequence of 1, 2, 3, 5, and 8. In Scrum project there are four reports that need to be created by the end of each sprint. The first lists the Product Backlog at the start of the previous Sprint. The second lists the Product Backlog at the start of the new Sprint. The third, the Changes report, details all of the differences between the Product Backlogs in the first two reports. The fourth report is the Product Backlog Burn down report. The Changes report summarizes what happened during the Sprint, what was seen at the Sprint review, and what adaptations have been made to the project in response to the inspection at the Sprint review. Why have future Sprints been reformulated? Why was the release date or content reformulated? Why did the team complete fewer requirements than anticipated during the Sprint? Where was the incomplete work reprioritized in the Product Backlog? Why was the team less or more productive than it had anticipated? All of these questions are answered in the Changes report. The old and new Product Backlog reports are snapshots of the project between two Sprints. The Changes report documents these differences and their causes. A collection of Changes reports over a period of time documents the changes, inspections, and adaptations made during that period of time. Burn down Report: This Burn down report measures the amount of remaining Product Backlog work in the units of story points for each of them, on the vertical axis and the time scale, by Sprint days, on the horizontal axis. A Story Point is a subjective unit of estimation to estimate User Stories. It represent the amount of effort required to implement a user story. Using Story Points for estimation is better than estimating in hours or days, as it is an estimation done using relative sizing, by comparing one story with a sample set of previously sized stories. Relative sizing across stories tends to be much more accurate over a larger sample, than trying to estimate each individual story for the effort involved. The Fibonacci series (1, 2, 3, 5, and 8) is most commonly preferred to categorize efforts in scale of 1, 2, 4, 8, 16 points and so on. The Product Owner plots remaining quantity of Product Backlog work at the start of each Sprint. By drawing a line connecting the plots from all completed Sprints, a trend line indicating progress in completing all work can be drawn. By figuring out the average slope over the last several Sprints and velocity can be determined, occurring when the trend line intersects the horizontal axis. This is an important report. It would graphically present to management how the factors of functionality and time were interrelated. This is good to be included in the project reports but can be an appendix Kanban was initially designed and used in manufacturing units and then it had its way in to many other fields as well and in software development too, the basic implementation model of Kanban described in the reference [7] gives a clear idea of steps needed to a Kanban in any field .Kanban itself is a scheduling tool, which replaces the traditional daily and weekly scheduling. Kanban scheduling is an execution tool rather than planning tool. Kanban does not replace the planning process, but rather takes the information from planning and uses it to create the Kanban, a Value stream which is explained later in this section. Kanban is considered as a tool which helps to identify and remove the project dysfunction. The word Kanban is translated as ‘visual cards’. Figure 6 Kanban Implementation Model Kanban model above has the seven steps for implementing Kanban for a production organisation, but the same steps would apply for any organisation, like software development units. The steps are 1. Conduct data collection 2. Calculate the Kanban size 3. Design the Kanban 4. Train everyone 5. Start the Kanban 6. Audit and maintain the Kanban 7. Improve the Kanban. Step 1: Conduct Data Collection: This Phase is to collect the data necessary to characterize the development process. This is conducting value stream mapping (VSM) for the entire organisation, which is to determine which development processes would be good candidates for implementing pilot Kanban scheduling systems. Step 2: Calculate the Kanban Size: This step is to calculate the size of the Kanban. Initially, calculate the Kanban work item size based on current conditions, not based on future plans or desires. The initial calculations will utilize the development requirements, the productivity rate and risks involved. Step 3: Design the Kanban: This step is designing the stages and flow in the Kanban, the value stream, Once the Kanban quantities required to support development requirements based on current conditions is calculated ,it is time to design the Kanban, The completed Kanban design will answer the question of how you will implement the Kanban. The design will consider: The end product of this step should be a plan for implementation of the Kanban, including implementation actions, action assignments, and schedule milestones. Step 4: Train Everyone: The people involved has to be trained about how the system will work and on their role in the process, the process and the visual signals has to be explained in a training. Also, the rules are reviewed during the training. It is aimed for taking the participants through what-if scenarios, to help them understand their roles and the decision-making process. The training is focused on operating the Kanban. Step 5: Start the Kanban: Before Kanban scheduling is implemented, all the visual management pieces are kept in place. To avoid confusion and make training much easier, the signals are set up, control points are marked, and the rules are completed and coordinated. As the Kanban is deployed, it is good to anticipate problems that may impact success and take action to prevent or mitigate these problems. During the deployment stage, develop a scheduling transition plan, determining the exact point for the change and the amount of efforts required to make the change. Step 6: Audit and Maintain the Kanban: After the Kanban starts, the next step is of the process, auditing the Kanban. When the Kanban is designed the person who will audit it, is also identified. Typically, the auditor will be watching how the scheduling signals are handled and whether output stays satisfactory. When the auditor finds problems, then the problems need to be fixed immediately by the responsible party to maintain the integrity of the Kanban design. The auditor will have to look at future requirements to make sure the Kanban quantities meet expected demand. Step 7: Improve the Kanban Finally: After the Kanban gets running, look at how to improve the Kanban to reduce new work items waiting to enter the stream. Resist the urge to just start pulling items. Check how the flow is running, and pull the necessary items immediately. After this one-time adjustment, only reduce the quantities based on improvements made to the development process. Determine the amount that can be reduced by using the calculations used in sizing the Kanban to calculate the new quantities.  The main idea of the Kanban is to have a flow of stages, a running value stream which has different level, it is the part of step to design the Kanban, and is shown in the Figure 7 below.  Figure 7 Kanban Value Stream  Kanban works in a way that the items are pulled into each stage of the value stream in a flow. Each stage in the stream have two states Queue and Execution state, when an item is moved from one stage to another, it first stays in Queue and it is then moved to Execution state. And there is a limit set for the total number of items in each stage, this is called work in progress limit, this limit is set based on the capacity of the team, so this makes it a value pulling system to keep the flow steady with work in progress limits in each stage being uniform, when the items in each stage exceeds the work in progress limits or if the items in a stage is empty, it is a signal that the flow needs attention ,also makes the progress more visible to all stakeholders.  3.4.2 Kanban Work Items and Terms  Most of the agile practitioners use the term iterations, which is time boxed cycle in which the selected user stories are completed. In Kanban which is more specifically designed for Lean practitioner’s the term iterations is replaced by Minimum Marketable Features (MMF).In Kanban the team work on the MMF with no time limits, for each MMF user stories and for each stories the scenarios are created. Each Story is a card that represents functionality and Scenario represents the action related to each functionality.  Figure 8 Kanban Work Items  Delivery Rate: The rate at which units of work (work item) pass through the value stream or part of the stream.  Lead Time the term is Development Delivery Rate, Measured in: "units" per day/hour/second, the two terms described above can be related as  Lead Time = (Work In Progress) / (Delivery Rate)  Delivery Rate= (Work In Progress) / (Lead Time)  Value-adding Time: The total time spent on value-adding activities for one unit of work.  Value-adding activities exclude waiting and superfluous work.  Resource Efficiency: A measure of the utilisation of a given resource, i.e. the ratio between the time working on adding value in the system to the total time available.  Flow Efficiency: A measure of time-utilisation on a given unit of work, i.e. the ratio of the Value-adding Time to the (System) Lead Time.  Backlog 	A non-WIP-limited queue containing work items awaiting service by the initial activity in a Kanban system.  Work Item The item controlled in the Kanban system; Effort Required Determines the approximate size of work in person-units of time. May be a negotiated function of desired quality.  Cadence The rhythm of the production system. Not necessarily an iteration. Kanban still allows for iterations but decouples prioritization, delivery and cycle time to vary naturally according to the domain and its intrinsic costs. The average transit time of a work item through a Kanban system.  Activity: Value-adding work that can be determined as complete. Includes: activity queue, a set of resources, and a WIP Limit. Represents an allocation of the effort required to complete a work item.  Next Work Item Selection, Function Rule for selecting the next work item from a queue when an activity has less work than its WIP limit; depends on both Class of Service and Value Function, and leads to specific flow behaviours.  Class of Service (CoS): Provides a variety of handling options for work items. A CoS may have a corresponding WIP limit for each activity to provide guaranteed access for work of that class. A CoS WIP limit must be less than the activity’s overall WIP limit. Examples are expedite, date-certain and normal. CoS may be disruptive (such as expedite) and is the only way to suspend work in progress.  Value Function: Estimates the current value of a work item within a CoS for use in the selection algorithm. Can be simple (null value function would produce FIFO) or a complex, multiple kanban-system, multi-factor method considering shared scarce resources and multiple cost/risk factors. The means of prioritizing work items.  Activity Queue: Holds work items within an Activity that are awaiting processing. The sum of items in process and items in activity queue must be within the WIP limit for each CoS.  WIP Limit: Limit of work items allowed at one time within an activity. Prefer this term to flow units in process or similar. (Measured in: "units".) Which term should be used for the rate at which units pass through the system or part of the system? Velocity (in Scrum), Delivery Rate and Throughput are all used frequently probably Delivery Rate is more common in the Kanban community, though I have a slight preference for Throughput. It is only one word, and it applies equally to a subset of the system as to the final delivery part.  Visible Representation: A common, visual indication of work flow through the activities; Often a columnar display of activities and queues. May be manual or automated. Shows status of all work-in-progress, blocked work, WIP limits it is a characteristic that provides transparency enabling better management. Difficult to model.  Flow Metrics Includes cumulative flow charting and average transit (lead) time.  3.4.3 Kanban Execution  In Kanban work is pulled from the back rather than pushed from the front. Limiting the number of items in any one flow stage at a point of time, with work in progress limit (WIP) is the key factor which drives the Kanban execution, forcing WIP limits encourages, team members to stop at one point when the WIP limit has reached and everyone looks in to the issue together until it is solved before they move on to work on the next item, In this ways the impediments and roadblocks are eliminated as early as possible. Kanban contains an embedded process for handling items that need to be expedited through the flow, fixed delivery dates, and work type splitting  Work is assumed to be broken down to a roughly similar size. In the Kanban Board not only the flow of stories or scenarios are represented, but the MMF itself also could be added in the flow and checked. Prioritisation of the backlog is performed just in time (JIT).  Figure 9 Kanban Board  3.4.4 Kanban Reporting  The key means to check the progress of work in Kanban team is using Cycle Time, Lead Time and Cumulative Flow chart. Cycle time is the time taken for a unit (in terms of a user story or a scenario) to pass through all the stages in the flow. Cycle time starts when the flow starts and when any one of the unit is out the flow to a completed. The main difference between the cycle time and Lead time is the unit is marked in the later, the time between when on particular unit it entered to the flow and the same item is out of the value stream as complete.  In the Figure10 below is the view of the Cumulative Flow Diagram by Kanban Tool.  Coloured areas on the diagram represent work in progress for each stage of a process.  Figure 10 Kanban Cumulative Flow Chart  By looking at the vertical distance of a chart we can define how many items are currently in progress. The horizontal distance shows how long it takes for a task to be completed. Measuring the horizontal distance on a Cumulative Flow Diagram allows you to monitor the Cycle Time, according to which you can make a prediction of when all the work in progress will be done. Vertical distance helps you to set the right work in progress limits.  Cumulative Flow Diagram should run smoothly. Large steps and flat horizontal lines indicate impediments to flow or lack of flow. Variations in the gap or bands stand for bottleneck situations, which usually occur due to irrelevant work in progress limits. This means that the number of tasks in each column should remain at the same level over the time. In addition, too many tasks in the queue mean either problems with finishing work on time or that the employee on the next stage cannot deal with work.  3.5 Hybrid Project Management Approach  Hybrid Project Management approach is a method in which the requirements and release planning are done in waterfall method, this methods combining the waterfall and agile method for the project execution, as explained in the picture below is referenced from the article in reference [2], which In the first sprint the project planning and proposal is done in a traditional way of analysis, design and documentation and the product backlog items are made, in the second spring the agile scrum method is followed in the team to critical path prototyping is made and presented to the team and the stakeholders and also tested and bugs are found, and the product backlog is groomed according to the finding in the sprint, and the new backlog along with the bugs are made sprint backlog for the next sprint in which the final prototyping is made, and demonstrated to all the teams involved in the organisation. Application development teams uses the approach similar to this hybrid project management method.  Figure 11 Hybrid Project Management Skeleton  3.6 Agile Maturity Model  Over the last decades the CMMI models has been used most predominantly to measure the quality and maturity level of the organization, ever since recent times when the agile software development came in to existence in most of the software development companies, where the customer needs are changing rapidly, there has been many studies done to analyse the adaptability of CMMI for the agile method of working,.  Main objectives Agile software development methodology are lower cost, high productivity and satisfied customer. The CMM tends not to focus the software process on an organization’s business objectives in their software process improvement programme [7]. Also most companies, small to large companies found it is too difficult to reach higher levels in the CMM [7]. The study also mentioned that the CMM improvement path is not always smooth, the efforts generally took longer and cost more than expected. While agile software development methodology is targeted to lower cost. Some of the KPAs have been found difficult to apply in small projects [7]. This may be because CMM was originally structured for big enterprises [7]. CMM addresses practices such as document Policies and procedure that large organizations need because of their size and management structure [7]. Hence there is new innovation which were created to measure the levels of Agile software development practices which is called Agile software maturity model  The Figure 12 below from the reference [7], states the different levels in AMM, in simple terms from the article the levels could be defined as below Level 1: Initial Level is where the organisation unstructured and has no process improvement goal. Level 2: In this level the organisation is has project or software planning, customer or stakeholders orientation practices. Level 3: Defined Level is about having Customer satisfaction, Software quality and development practices, this level denotes a more focus on practices related to customer relationship management, frequent deliveries, pair programming, communication, coding, testing and quality of software. Level 4: Improved Level is to have People orientation and project management Practices, Companies at this maturity level are in a position to collect detailed measure of the software development process or practices and product quality, both the software development practices and products are quantitatively understood and controlled using detailed measurements examination of risk and respect to the team who is going to develop the system. The AMM at level 4 maturity aims to help developers or managers to respect for the co-workers or people involved in the project, identify and improve problems related to team sustainable pace and organising team by itself. This is achieved by an assessment of current process and to identify where weakness lie. Level 5: Mature level is when the organisation has Performance Management and Defect prevention practices in place, Companies at this level continually improve their processes through quantitative feedback from the process and form testing innovative ideas and technologies.  Figure 12 Agile Maturity Model Levels  3.7 Agile Software Product Management  The software product management (SPM) includes the process of managing the requirements, defining the release, defining the context for the products involving internal and external stakeholder, and, this topic also includes many other areas, many process are followed as this is more driven by market driven requirements engineering and currently there is very little of scrum done in the requirements engineering. The article is a case study on software product management, described in the reference [3], proposes the use of an agile SPM method based on SCRUM. It also prescribes the use of two different sprints, one for requirements engineering and the other one for development engineering.  Figure 13 Agile Software Product Management  3.8 Accept360  Organisation which belonged to Nokia smart phone division, before the merger with Microsoft used the Accept360 tool as the key tool for requirements and release management. The tool has features to support requirement engineering and also the software development engineering. It is a web based tool which is supported in Internet explorer and Firefox browsers, in Desktop computers.  3.8.1 Accept360 Elements  Accept360 has defined elements for requirement and release management, they are called Roadmaps module, Requirements Module and Teams Module. Roadmaps Module is used to for managing the release milestones and artefacts related to that. Requirements module provides features to manage the contents of the releases. Team module provides support to plan and allocate the resources for working on the planned contents of the releases.  3.8.2 Accept360 Team Element Ranking Tab  Ranking tab contains user interfaces for managing team sprint backlogs.  It has the team element for the planning and managing the sprint backlogs. The ranking tab has two tabs the Backlog pane and Sprint Pane .The Backlog pane lists the backlog items for the project, which could be easily dragged to the Sprint Pane, which contains the list of selected backlog items for the selected sprint.  3.8.3 Accept360 Recommended Practices  For setting up the Accept360 for an application, it is recommended that the contents added for any application follows a certain recommended format, it is recommended that the backlogs are added in a standard hierarchy and the structure of the contents is recommended to be as in the Figure 14. Each application has a folder with the application name, under which sub folders are created for each version of the application and application driver if it exists.   Figure 14 Accept360 Structure in Practice  3.8.4 Accept360 Responsible Actors  As explained in the Figure 15, at every stage of the project the contents needs to be updated in the Accept360 by the program manager or the project manager at some stage or by both of them to together. And it emphasis that he Project manager has the overall responsibility to ensure that the contents in the tools are up to date.  Figure 15 Accept360 Actors  3.8.5 Accept360 Agile Task board for Scrum features  Agile task board provides the features that are needed for supporting the task planning by the scrum teams Accept360 tool has support for complete scrum process. Key components that are used in scrumming with Accept360 are Feature, Sub-features, Stories and Tasks. The Product owner creates the Feature, Sub-Feature and Story contents. The scrum master has access to Stories and Tasks. During the Sprint planning the stories are pulled for each sprint and the Tasks required to complete the stories are created by the scrum master after discussing with the team members. And the Developers have access to update the status of the tasks.  Feature in the Accept360 can be compared to the Theme in scrum terms, but more equivalent in usage is an application as a whole is represented as a feature, and the Sub-Feature as Epic, which is a list of all the requirements for the application. When the sub-feature is drafted, it is then proposed for implementation, and stories are created, and then the stories are assigned to the teams, as sprint backlog items, which then gets a story point assigned for it. For each stories the tasks are created and is assigned to the team members. Below table represents the lifecycle states that are available in Accept360 and only some as states are used in defining the states of the requirements in the ASW projects.  SW Lifecycle  SW  Draft  Initial status when a new item is created first time; not yet ready for further actions and not yet in any backlog (with schedule)  Proposed  NOT USED  Candidate  NOT USED  Committed  Development team has committed to deliver item by Planned Delivery Date for certain programs.  Implemented  Item has been implemented and tested by the responsible team and given to the integration team.  Done  Item is ready i.e. it has been implemented, tested and integrated.  On hold  NOTUSED  Rejected  Item rejected  Table 1 Requirements LifeCycle  3.8.6 Accept360 Kanban features  Requirements can be synchronized from Accept360 Team Backlog to ‘Backlog’ column of the Kanban Board, sorted by Rank order. From there it is possible to prioritize items (ranking is updated to Accept360 UI accordingly, see who's responsible, Track changes, Balance workload and limits, also it is possible to configure team development states as needed, each column can be bind to Accept360 Lifecycle, progress can be also followed in Accept360, it is possible to set WIP limits for each state to avoid unfinished tasks and visualize the bottlenecks, emphasizes to have a work item 100% done instead of having many 80% done, is has to be noted that current Accept Kanban Board version supports only Requirements management, not Tasks.  3.8.7 Accept360 Nzilla Support  Accept360 supports importing Open bugs from Nzilla to the corresponding team. It helps the development teams to give relevant information from Nzilla about the bug to enable them to Rank the related Bug against other Stories on the Team Backlog, It is a One-way integration and Defect master always in Nzilla. Nzilla enables Lifecycle mapping and synchronize between tools to improve visibility. To integrate Nzilla with Accept360 a separate structure has to be created in Accept. In scope Programs and Component level automatically Synchronized using Nzilla Database IDs .Any name changes will be automatically updated, this stop the solution breaking and make deployment to teams easier. Only downside is there will be some extra Bug Bags and folders for teams that may not yet be using the solution.  3.9 San Diego Team Practices  The teams in San Diego had been able to prove that they have standard way of usage of the Accet360 tool, to visualize the needs and progress of the project, in every stage of the project. Here is the snapshot of the way the team has defined the sub-features, which includes a clear steps and functions that needs to be done in order to execute a project, starting from the initial prototyping, until the planned feature or application is available in the market.  The items in the Requirements module have the prefix to each item which explain the common activity that is need to be executed in the project and also there are items that explain the main stages which a project would go through, so it helps to visualise the project situation with the items in the Accept360 tool.  Goals of every functionality are also clearly included in the list of sub-features and stories. The team follow the template available in Accept360 tool, below is the screenshot of the template.  Also the teams follow the mixed Scrum and Kanban methods. The teams have sprint planning, Daily scrum meeting and Retrospective meetings. And meetings are helped whenever it is appropriate and needed, and the team has a regular practice of keeping the Accept360 updated always.  3.10 Making Most of Scrum and Kanban  As the saying goes, No tool is complete and No tool is perfect, it only depends on how it is used, this applies to both scrum and Kanban. The value of a tool is that it limits the options. A process tool that lets user do anything is not very useful, this process could be named “Do Whatever”, but having a “Do The Right Thing” process is guaranteed to work.  Scrum and Kanban have both of their own pros and cons, so it is good to understand the difference between the two tools, so that we could make the best use of both the tools.  Difference between Scrum and Kanban  Scrum  Kanban  Time boxed iterations prescribed.  Time boxed iteration optional. Can have separate cadences for planning, release,  and process improvement. Can be event driven instead of time boxed.  Team commits to a specific amount of work for this iteration.  Commitment optional.  Uses Velocity as default metric for planning and process improvement.  Uses Lead Time as default metric for planning and process improvement.  Cross functional teams prescribed.  Cross functional teams optional Specialist teams allowed.  Items must be broken down so they can be completed within 1 sprint.  No Particular item size is prescribed.  Burn down chart prescribed.  No Particular type of diagram is prescribed.  WIP limited indirectly(per sprint)  WIP limited directly (per workflow state)  Estimation prescribed.  Estimation optional  Cannot add items to ongoing iteration.  Can add new items whenever capacity is available.  A sprint backlog is owned by one specific team  A Kanban board may be shared by multiple teams or individuals.  Prescribes 3 roles (PO/SM/Team)  Doesn't Prescribe any roles  A scrum board is reset between each sprint  A Kanban board is persistent  Prescribes a prioritized product backlog  Prioritization is optional  Table 2 Difference between Scrum and Kanban  3.11 Scrumban  Scrumban is a result of the thinking to get most value of the scrum process, like Kanban Scrumban is a pull-based system, where the team no longer plans out the work that is committed to during the planning meeting, and instead continually grooms the backlog. The same Scrum meetings can and should still take place, but the cadence of them can be more context-driven. The real key factors for Scrumban, though, is ensuring that work in progress (WIP) is still limited.  Figure 16 Scrumban Sprint and Value Stream  Scrumban works with the Work-in-progress limits, not Sprints. With Scrum, the amount of work that is ongoing is limited by the Sprint time commitment. But in Scrumban, with no specific time commitment, the team must limit itself through the use of WIP limits on columns within their task board. The goal is always to move tickets in a flow from left to right on the board. If too many issues are in progress, the team is at risk of not finishing anything to high quality standards. Instead, there should be a maximum number of tickets allowed per column. If the number of tickets in that column ever exceeds the maximum, the entire team should swarm onto that column and help move tickets on. This should happen no matter what functional role a team member fills.  Scrumban still supports the scrum ways of having the meetings and time boxed deadlines for the release of the features, planning meetings should take place as often as they are needed. When the team is unable to regularly pull stories off the top of the backlog at their normal pace, a planning meeting is necessary. Review meetings helps to improve the way of work with feedbacks. Reviewing work with clients and customers is the only way that development teams can get the feedback necessary to properly adapt what they are working on. Clients tend to prefer that these are held at a regular cadence.  Retrospective meetings. These can vary when held, but a general rule of thumb is to hold a retrospective after every review. This is the most useful part of the Agile process and should be given the proper place for that.  Daily stand-up meetings in the Scrum world follow a simple pattern. The team takes 15 minutes and each person says, a) what he/she did yesterday, b) what he/she is working on today, and c) what is blocking any of that work. In practice, this boils down to redundant statuses that recount information available on the team’s task board. For Scrumban, a more effective method is to refocus on the flow of tickets on the board. That same pattern of yesterday/today/blocked can be transferred to the tickets themselves—the group moves through each column and briefly discusses each ticket and what is necessary to move that ticket rightward on the board. This provides far more context to the team and informs every one of any major architectural or design decisions.  Metrics can certainly be useful, it interprets the complex process that is going on in the team, it is not just a single value or number to be expected to know how the team is progressing, and it could help visualize the situation the team is going through on the way to achieve the target. The term Velocity, the amount of story points a Scrum team completes in a single Sprint, is such a metric that incentivizes lower quality at the end of a Sprint as a team scrambles to finish every last story they committed to. When the number fluctuates, as is common with a newer team, the stakeholders begin to question the outputs of the team, and even the effectiveness of Agile itself. In Scrumban the metric is cycle time instead of velocity. This is the length of time a ticket takes to complete, measured from when it is first began. Over time, a statistical analysis of all tickets in the project can yield a mean cycle time and standard deviation. This can be a useful planning tool at a macro level, as it is trivial to add up the number of stories and multiply by mean cycle time.  3.12 Summary of Best Practices  From the study and discussion about the best practices, it is found that the scrum method is used mostly in the software development teams as it is very good for planning and scheduling meetings, but it also recommends that the sprint has to be pre planned and should not be interrupted in between and the backlogs need to be reset, which makes it to a situation that we cannot keep more buffer for backlogs to be tried and everything need to be reset at the end.  Next that Kanban is used most widely in software development organizations, which works with Work in progress limits are the visual indicator for the execution and planning of activities. And also recommends that that designing of the Kanban and steps for designing should be followed, which if not followed would result in a situations that he project situation could not be in control.  Next the thinking of making most out of the Scrum and Kanban has been received as a good approach to make use of good things in both the methods and which is best suitable for the needs in software development organizations, this could well be implemented with the techniques defined in the Scrumban.  Next the hybrid project management models is a good method which gives a way to have the traditional way of software development methods, and also have scrum included, but at that same time, this does not address the needs that the planning need to be dynamic, not only the execution is dynamic, so to have agile mode of working for planning the requirements and developing the requirements, that agile product management method is very suitable.  Next the study of the Accept360 tool details that, the tool has a lot of features in it, but it is built in a way that, the tool need to be adapted but adding more plugins to it ,for each feature, to be able to utilise it more efficiently .  From the study of usage of the Accept360 in the team in Sandiego, it is found that the content what is used to describe the project requirements or the backlogs play a key role in visualizing the project status, and also it make it clear that defining the contents of the backlogs and work items in a standard way it is done in the team, would make it more helpful to know the needs and status of the project easily.  4 Current State Analysis  Current state analysis was done in the teams in Espoo , two teams were analysed and in each team the Product Manger, Project Manager , Development Team Lead and The Quality Lead were contacted and interviewed on the following topics: Project Details, Team Information ,Agile Practices,Project Planning and Execution Information,Project Situations,Suggetions for desired Improvments in Accept360 tool. The interview topics and questions are listed in the Appendix 1 .  4.1 Analysis of teams in Espoo  4.1.1 Analysis in Team X  Team X is working on creating a new version of an existing application, so this application is expected to replace the old application mainly with the new user interface and improved usability of features, the development phase of the application would need a complete cycle from proto typing the new user interface and adding all the functionalities to the new applications. This means that the efforts needed for developing the working application could be estimated to an appropriate value, which would help define a process to follow.  The team uses the scrum process for planning and execution of the activities. Accept360 tool is used during the sprint planning and review meetings. UI specification document and the story description in the Accet360 serves as the documentation needs of the projects, as they are updated with appropriate information. UI Design team shares the updates and synchronize their work for their sprints with the spirits of the development team sprints and meetings. Also a short description of key features of the application is documented in share point documents management page.  Team follows two weeks sprints and at the end of the sprint there is a demo and preplanning session for the next sprint. Releases and Testing are done weekly during the project execution stage.  Team uses the Agile task board in Accept360 for managing the tasks, also the Nzilla is used to check the issues and bugs are also worked on as tasks. There are no daily meetings in the team, it is expected that each team member is located in the same location and has to contact the development lead or another team members to know what has to be done and what is expected.  The discussion with the key stake holders of the project, the Lead Program Manager, Project Manager, Lead Developer, and Quality lead is available in Appendix 2-4.  4.1.2 Analysis in Team Y  Team Y is working on developing value addition features for the existing application, the Team includes many small teams, every feature addition and implementation is initiated whenever the decision to have the feature in the application is accepted and is prototyped by one of the small teams and is integrated to the application.  Team uses the One Note as a tool for planning and updating the backlogs. The release plan, requirements list and status is tracked in OneNote pages. Accept360 is not used for these purposes, the reason explained was, the releases happened ever month and the features are released in very small time.  Team uses the UI specification as the document to know the details of the project.Accept360, could not be relied on for the details of the project, as it is not fully updated all the time.  Sprints are mostly weekly sprints, sprint meetings are held every week, and Adhoc meetings are organised whenever there is need. At the end of every week, an open agenda meeting is held, which is for having an open discussion about anything related to project and or give presentation about the progress. Testing actives are done on weekly basis.  The team had previously tried using the Kanban for their planning and scheduling before, but it was not very helpful, as they did not have control of the activates and work items all the time, as it ended up in just having a board with full of task cards and nothing going in planned way and things went just out of hand, but what it is important to note here is, the Kanban followed was not a designed to be adopted for the overlaying process in the organization.  The discussion with the key stake holders of the project, the Lead Program Manager, Project Manager, Lead Developer, and Quality lead is available in Appendix 5-8.  4.2 Summary of gaps between best practices and current practices  One of the main aim of the Agile methodology is to have clear visibility of the project at any point of time, so that any kind of resources are needed deliver that project as planned, are addressed at the earliest, this could be achieved only if the teams follow the prescribed practices of the standard methods they adapted to follow or follow a methods which would not prescribe more rituals, but still is would make the situation more visible to all the stake holders. But currently, the teams do follow standard methods, but skip some of the recommendation in the methods.  Though the teams are very dedicated to deliver the products and functionalities, the team members are not interested to follow the best practices and tend to work in more selfcentred way, because of which certain practices which would help all the other team members to perform well are not taken in to consideration. The teams have their own way of working, than following the standard practices in the way of working,  From the study it is found that there are some practicalities that are not followed in the teams, a clear understanding of responsibilities and who is responsible for updating the tool is missing. This is also obvious from the contents in the items in the Accept360 tool, which is not up to date for some projects.  Also there are some needs that are missing in the current processes and tools, for example the Product backlogs have to be synchronized with the tasks to add the tasks for the sprints, there need to be a story to add task, but in practice the development teams have more tasks, that need not necessarily have to be a requirement backlog item, but is needed for their day to day activity. The requirements engineering and the software development engineering are in this way tied more closely in the tool. Even though there need to be synchronization between them, the tasks in the development engineering team need to be created according to the development activities, to get the requirements implemented, this would need different terminologies in defining the tasks, that would not fall into different the category of work items and tasks definitions  The Agile task board is updated only at the end of the sprint, which explains the board is not actively used in the team, the reason for this found from study is that the way of working is not the same in the flow of states in the task board, also there are tasks which could go longer than the sprint time , but in scrum after every sprint they had to be reset and added to next sprint or put as impeded, which would give a picture that there the things are not going well, but in general there are some dependencies that the tasks need to stay between the sprints, for it be completed in a better way, it is because the order in which the task need to be worked on changes, that some tasks need to be put on hold for some time waiting for other tasks to be complete or started. As the sprint is time boxed and needs a reset of the tasks and backlogs for the sprint after every sprints, it leads to a situation that the tasks are updated in a way that their status is not related to actual status and is updated for the purpose of resetting the sprint backlogs which scrum recommends. So there is a need for other methods and tools like Kanban which would help the developers to keep the tasks across sprints without resetting it at the end of each sprints. But it has also been experimented in the Team Y, that Kanban method alone was not helping enough as there is a need to time box the release of features at some point of time and the releases are tied to the release of the devices to the market, this needs some kind of iterative approach as well, for planning the activities, hence there is a need for using some of the features from the scrum method and Kanban tool to get the needs of the project execution addressed completely.  5 Proposal  5.1 Theme Proposals  As summarized in the best practices and current state analysis, the key areas where the gap exits in knowing the status of the project is, how the contents of the project planning and execution tool are defined and used, the difference in work items needed for requirements engineering and development engineering, and difficulties in combing the time boxed sprint development method with the continuously improvement and integrated development mode of working. These gaps could be address better by the themes that are explained in the below subheadings ,which would help fill the gaps that exists and help improving the visibility of the project status to all stakeholders of the project.  5.1.1 Usage of Structured Contents for Work Items  Work Items as explained in the best practices are the items like epics, stories, tasks, mmf and activities. The description of these items, could serve as documentation for the project, and this could include details about goals, features of the product being developed. All the work items would fall in one of the categories of the activities that are commonly needed to create the end product, like Functional requirements, Non Functional requirements, User interface design and development, Core technology implementation and interfacing, Performance validation and improvements, Testing and Error correction. The organisation in case company in which the research is being done, has a recommendation and guide for creating and maintaining the work items. The guide provides information about required structure for defining the work items, naming and versioning conventions, avoiding duplicates in contents, this guide could be used as reference to maintain the contents in a standard way. The guide also includes details about environment needs to use the tool, such as browser requirements for the tool, plugins needed to have a better performance of the tool, and it includes a template which explains the backlog creation ethics.  Initiatives to use the guidelines and keeping the work items updated, would help improve the visibility of the project status, so that by looking at the content of the sub-feature, stories and tasks it is possible to have a clear picture of the situation of the project. For example the Team S in Sandiego have the contents defined in a specific way, they follow different syntax for defining the work items, so that when looking at the contents in the Sub-features, Stories and Tasks, it is possible to understand the stage of the project, So keeping the contents more meaningful in the recommended standard way, could make the visibility of the project situation more clear to all the stake holders involved.  For example the Figure 17, show the sample content from the best practices studied in the Sandiego team, this could be used as reference for creating the items for the scrum team backlogs. The template has suffixes such as Story , Demo story, UI design, Miscellaneous UX,UI Spike, Automated Testing, Exploratory ,Reliability ,Compatibility ,Interoperability ,Non Functional ,Certification Testing and other stages the project need to carry on for finally delivering the project. These suffixes would give more information about what is planned and being done at that stage of the project, so this helps to have more details of the activities carried on in the project and it would help to provide needed resources for those activities based on their needs.  Figure 17 Template for Work Items in Scrum team  5.1.2 Usage of Agile Sotware Product Management  The software product management (SPM) as explained in the reference [3], proposes the use of scrum in product management. It also prescribes the use of two different sprints, one for requirements engineering and the other one for development engineering.  For a scrum process the backlogs are the key instruments, and there are two different kind of backlog items involved in software development, the Product backlog, which is the prioritized list of requirements, that includes all relevant needs of the product that is being planned to be developed. Some of these Product backlog items are then copied by the development team as their sprint backlogs called Development Sprint backlogs, which are then split in to stories and tasks.  Though there is a tight dependency between the two backlog items, the backlog items picked up by the development teams are reset every sprint and different tasks are created newly. Hence there is a good need that these two backlogs items are used differently by the software management teams and the software development teams. Currently scrum is followed only by the development teams, so as explained in the reference [3] ,the article suggests the use of agile scrum methods in defining the Product backlogs would be more efficient to manage the change and synchronize the changes and progress, in both requirements engineering and development engineering.  From current state analysis interviews in the case company , it show that the existence of the difference in requirement engineering and development engineering needs for managing the backlogs, even though there has to be synchronized usage backlogs between these two teams, their functional needs demand the need for having two different sprints or cycles for the same product.  5.1.3 Usage of Scrumban  The requirements engineering teams which involve the project management and planning requires clearly what is prescribed in scrum, the sprints, roles and ceremonies, but at the same time the development engineering teams need a stream which is more like Kanban to keep their flow of work steady and visible, so Scrumban which recommends work pull system, which is a scheduling method based on demand and at the same time recommends to have the scrum way of planning and checking the situation by the aid of meetings ,would best suit the needs in the organisation in study.  There are also many teams involved in one project and each team have their own planning and execution cadence, the Figure 18 is a sample framework, which explains the involvement of different teams, with different work cadence defined, for the same project and more relevant for scrumban method. The frame work is also based on the concept explained in the reference [9], which is good for complex implementation. Currently there is only one cadence which is defined for the development team and other teams like requirements management and UI design teams ,which are very tightly working with their deliverables as input to the development engineering team, do not have a designed cadence for delivering their deliverables and they are provided in a dynamic and not planned way, so defining a cadence and process method for all the key teams, that are involved in the project, would make the interaction and situations much clearer and easier for planning and executing the activities of the development team and for the whole project as well.  As an example there are three teams in the sample framework, first team is the team which is the key team that creates and defines the requirements of the features of the product being developed, the product needs several other teams also to be involved very closely to get the product developed fully, the development engineering team and the UI design teams are the other key teams which work on the creating the product, as in the Figure 18 the Requirements team follow the scrum methods ,they define and provide the backlogs for the other teams, and also schedule the planning and status meetings, which could for example happen in a three weeks cadence, and the development team have a cadence of two weeks for releasing the working software product, and the UI design team works in flexible cadence of delivering the UI design document whenever there is a need for new method of defining the user interface, all these three teams has to work independently, but still depend on very closely on each other, so having a framework designing like in the sample framework in the Figure 18 would help planning and coordinating the teams in better way, also this would the help visualize the dependencies on deliverables between teams and define them in a structured ways and this would help all the stakeholders involved in the project to visualise the status of the projects more clearly.  Figure 18 Sample Framework with Scrumban Method  In Scrumban the story and tasks card need to be built and maintained appropriately otherwise it would not yield any value, as in the Figure 19 below, the story cards or the subfeatures in green are the items which are worked on by the requirement management team, which mostly involve the project managers and the development team lead, the stories are selected at the beginning of each sprint and story board is reset at the beginning of each sprint.  Figure 19 Sample Scrumban Board  Scrumban for the development team works with the Work-in-progress limits, not Sprints. With Scrum, the amount of work that is ongoing is limited by the Sprint time commitment. But in Scrumban, with no specific time commitment, the team must limit itself through the use of WIP limits on columns within their task board. The goal is always to move tickets in a flow from left to right on the board. If too many issues are in progress, the team is at risk of not finishing anything to high quality standards. Instead, there should be a maximum number of tickets allowed per column. If the number of tickets in that column ever exceeds the maximum, the entire team should swarm onto that column and help move tickets on. This should happen no matter what functional role a team member fills.  For the Software product management, Scrumban still supports the scrum ways of having the meetings and time boxed deadlines for the release of the features, planning meetings should take place as often as they are needed. When the team is unable to regularly pull stories off the top of the backlog at their normal pace, a planning meeting is necessary. Review meetings helps to improve the way of work with feedbacks. Reviewing work with clients and customers is the only way that development teams can get the feedback necessary to properly adapt what they are working on. Retrospectives meetings could be held when needed, or in regular intervals, so that team has an opportunity to give feedback about whole process involved and give input about what changes they want to have to, in order to improve the Efficiency of the team.  Scrumban can restore working time to the development team, and avoids unnecessary meetings. And most importantly, it can limit the team’s work in progress, so that they can finish what they start to a high standard. Scrumban can remove overhead stress for the development team, increase efficiency, and increase the overall satisfaction.  5.2 Summary of Proposal  From the study it is understood that the usage of Accept360 tool for day to day activities of the project execution is not preferred ,due to the fact that the tools is very slow for accessing and updating the content. The tool is more suitable for planning the releases and maintaining features for devices globally, but it does not address the need for the software development projects that has quick and dynamic life cycle. But as long as the tools is used and since the tool supports many features of the agile development methods, the guides and training documents available for the tool are created to address the process followed in the organisation in case study over years, following the guidelines in the same way it is recommended, would help the teams to use the tool more efficiently for planning and execution, also would help all the stakeholders involved in the project to have the clear visibility of the project situation at any point of time.   From the current state analysis, it shows that there is interests to use other methods and tools in addition to methods and features supported in the Accept360 tool, and Accept360 is considered to be helpful more for requirement management teams, rather than for software development teams, so it is good if the teams could make use of other tools suggested by the stake holders interviewed for current state analysis, tools like scrum works pro which already has synchronization with the Accept360, or the teams could try the tools like Jira and Version One for their activities and synchronize their sprint backlogs with the Product backlogs in the Acept360, but it is important to mention here that, for synchronizing backlogs with other tools the efforts and resources needed would be more. Hence it is good to further research on what tool could serve more good for the organisation’s need.  Since in the organisation in case study , there is more clear work type splitting of requirements engineering and software development engineering, the Agile methods in requirements engineering would provide more visibility in project situations and requirements creation and grooming would be carried on in more structured and dynamic way.  Also from reference [8] and other online research papers recommend the usage of both Scrum and Kanban together for having more visibility of the project situation and progress. It would increases the productivity and efficiency of the team.  6 Conclusions  6.1 Summary  This chapter gives the general overview of the thesis, it is a look through of all the steps done in the research process. Validate the research by comparing the objective with the results, how applicable and generalized are the results in another context. And also check how reliable the research process is, if the methods and materials used in data collection and analysis are appropriate and whether this research could be carried on further by another person, in the same way it is done currently.  6.2 Practical Implications  The thesis was aimed to study the gaps that exists in current practices in software development methods and tools, in the organisation in case company, in comparison with the best practices in agile software development methods and tools, that are available from different sources and summarised in the literature part of the thesis, and to propose the solutions would fill the existing gaps and would provide the ways to improve the visibility of the situations. The current state analysis was done by discussions with many individuals in different kind of roles, in three different teams. These discussion turned out to be a great source of knowledge and insight into the organisational process, mainly it gave an insight and good understanding of project planning and execution methods. Everyone involved were in highly responsible roles, hence the discussions were more clearly depicting the actual responsibilities and needs in a project, which helped to frame the proposals that would address the needs to fulfil their responsibilities in an improved way and help all the key stakeholders in a project to have better visibility of the project status.  The research was mainly based on the current ways of working, current issues and needs that were desired by the team members. The proposals are improvements that are needed in three different area of project planning and execution methods and tools, and proposals are framed based on the research articles in the references. The feedback about the proposal was very positive, that people involved in the requirement management showed interests in the proposals, and they wanted to have similar practices proposed in this research.  The current research focus only in finding the gaps that exists in the methods and tools and proposes the key ways that could be used to improve the visibility of the project status, and the research proposes the sample approaches that could be tried, to have better visibility of the project situations, which also help in reducing the delays in the project deliverables. The proposed sample approaches need to be designed further and piloted according to the needs in the teams, in the organisation under case study.  6.3 Evaluation  6.3.1 Objective Vs Outcome  The main objective of the thesis was to find the ways that would improve the visibility of the project situation to all the stakeholders involved in the project. In the study three main proposals were created to improve the visibility of the situation. First proposal is to improve the contents in the requirements management tool, so that the contents have the description of what is being done and what is the goal and outcome of the work item for each requirement. Second is to have a separate scrum process for requirement engineering, which is called Agile Scrum Product Management, which lets the backlogs to be engineered with a process rather than continuously, so that they are created with better clarity, rather than in an Adhoc way, which definitely would give clearer picture and estimate of the project situations. Third proposal is using the Scrumban, which is having a scrum process for main process of requirements engineering and the other engineering teams like development engineering teams would work in accordance with their requirements from scrum team, with a key control agent as Work in progress limits. The development engineering teams would have a Kanban board designed, in a way that their work flow is checked by work in progress limits and is not reset every time the scrum sprint ends, in this way the actual situation of their work progress, could be seen in the work item flow and is more realistic. The outcome of all the above three proposals would create a situation where the process and the all the stake holders in the project are more closely involved, which would eventually improve the visibility of the project situations. Reliability checking for the thesis is done mainly based on the validating how well the methods and the materials used and well the outcome addresses the objective of the thesis. The Methods and materials used in this thesis, is mainly from the referenced articles and books, and the teams and people involved are from the organisation in which the case study is done. The objective and the outcome of the study are closely related to the real situation and more relevant to the study topic. In the literature part contents are designed in a way that the agile methods and practices are highlighted and explained in detail, that this report would serve the purpose of referencing as a guideline to design and improve them. Also current state analysis were conducted in a more generalised way, with more generic question about project planning and execution, so that the inputs from those interviews would give a generic overview of the process and the needs for improvements in the project, in this way this study has initiated a research which would further be continued in an elaborate mode, for example by clearly designing the scrum team in accordance with the recommended scrum ceremonies and designing the Kanban as per the needs of the project, and apply them in scrumban method and validate the same with experiments in more teams, piloting the proposals, are some of the key research areas for future studies.  The thesis is validated using following methods, the Triangulation, Prolonged engagement in the field and Member Checking, further in this section. Also the answer to the research question “How the visibility of the project situation could be kept updated at any point of time?” “What are the best ways to stay on track of the current situation of the projects and so be able to manage the issues appearing earlier and minimize the delivery slippages?” Is answered in the proposal section and is addressed completely. Hence this states that the research has a consistent and logical approach in providing what is promised in as objective.  Triangulation is a validity procedure where the researcher search for convergence among multiple and different sources of information to form the themes and categories in the study. As a validity procedure in this method, the researcher uses a systematic process of sorting the data to find themes, by eliminating the overlapping areas. In this methods the researcher rely on multiple evidences rather than single incidents or data point in the study. The data collected include three different teams and ten individual discussion, also the best practices were studied from several articles, company guides and books.  Next method that is used for validating the study is, Prolonged Engagement in the field, since the thesis was carried out in the same organization where the researcher is working since many years and one of the teams which were involved in the study is where the researcher worked at the time of study for more than seven months , as Fetterman (1989) contends that "working with people day in and day out for long periods of time is what gives ethnographic research its validity and vitality. (p46).” the study has more vitality.  Member Checking method of validation, involves getting feedbacks from the participants, The feedback from the participants who were key stake holders in the organization in the case study and the Supervisor of the thesis who is in the role of the Operations Manager of the organization under case study were collected. The participant’s feedback states that the study was very useful in giving details about the agile methodologies and best practices, in more elaborative way. The feedback from the supervisor was that the discussion and contents in the thesis were useful in understanding the current practices and also the proposals in the thesis are closer to the changes that were expected by many key stack holders in the organization, and was helpful to align and improve the current practices, for addressing current key issues. Hence in the thesis, the qualitative method used for data collection, has provided with more valid information of the current state, the data from the discussions are valid and used to build the proposal. The research does not contain any personal opinions of the researcher, but is fully based on the data collected from the key stake holders in the organisation and the articles and book in the references. The researcher has used the experience in the organisation to be selective in collecting the data, so that the data is more relevant for the research, which has brought very good result in proposing the solution framework. 
