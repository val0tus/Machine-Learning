Collaboration  Software Matrix 121Conclusion 124Groundwork for a New Organization 125Introduction 125What You Need to Succeed 128A Model for Trust 129Collaborative Culture 138Goals and Rewards 146Conclusion 155 Case Publishing Company 157Mixing Software  Services 159Introduction 159The Evolution of ServiceOriented Architectures  161The Software  Services Model 168Implications of Software  Services 170Conclusion 174 Case ITAGroup 177Social Computing for Business 179Introduction 179The Emergence of Social Computing 179Collaboration and Social Computing in the Enterprise 181Web 20 185Software  Services Enabling Social Computing  192The Web as the Hub 194In Conclusion – Where Do We Go from Here 194 Case UVIT 197Fourteen Questions to Guide the Revolution 201Introduction 201The Fourteen Questions to Guide the Revolution 202 Case Toyota Material Handling Europe 215Debunking Collaboration Myths 217Introduction 217The Myths 218Conclusion 225  About the Authors 227  Index 229The New World of Business Collaborative InnovationMarket turmoil shakes consumer and business confidence diminishes the value of financial assets and creates uncertainty History however informs us that shifts in the economic landscape also offer unique opportunities for those who are able to look past the nearterm difficulties and seek out opportunities Organizations can choose to retrench or they can choose to prepare for success and leadership roles If they take the latter approach returns from hardfought costreduction battles can be turned into infrastructure improvements more rational integrated processes and fundamental changes in market presence or positioning to fill new niches or those surrendered by competitorsSeeing opportunity in times of turmoil reframes challenges in a way that projects the lessons of history onto the future Suggesting that organizations seize new opportunities during economic strife does not minimize the significant difficulties they will encounter Although picturing the future is difficult turbulent times call for balance against new factors Balance will always be essential Organizations that can balance nearterm concerns with forwardlooking expectations will be better poised to succeed as markets calm those that retreat risk becoming an anachronism while the world reinvents itselfWhether one chooses to be opportunistic or defensive in their approach to the turbulent economy software and information technology has a central role to play as the strategic enabler of success the conservator of scarce resources and the accelerator of recoveryThe transformative impact of Software  Services and a new generation of social computing technologies are profound and ongoing These innovations enable people and organizations to share information collaborate on projects and build virtual communities irrespective of time and geography In the process they have made commandandcontrol hierarchies unnecessary as mediating mechanisms for the flow of informationCollaborative software unites the blended workforce and makes the experience of working together as natural and productive as working in the same physical location Now more powerful integrated applications and services for social computing – including RSS feeds wikis blogs and social networks – are joining the arsenal of collaboration tools available to businesses as they become more secure and manageable in the enterprise Use of these tools is growing Facilities such as project workspaces document repositories team access to contact and schedule information shared project flowcharts shared task lists and automated notifications provide a foundation for virtual teamwork by keeping everyones status and work visible Team members can see shared information in the periphery of their standard work environment or they can access uptotheminute data from any portable deviceOrganizations are only beginning to come to grips with the impact of the internet and other technologies on core business functions such as product development sales customer relationship management and operations I hope that this book can help you along this journey – providing both practical insight and guidance towards realizing business value in this new world of businessRon Markezich Corporate Vice President Microsoft Online  Redmond February 20th 2009A Pragmatic RevolutionThe book that you have in front of you is an important book Not only does it discuss two of the hottest topics of the presentday IT industry collaborative software and cloud computing it also gives the contours of the New Firm that will emerge after the dust clouds of this dramatic economic recession have settled And it is these contours that will allow you to anticipate the significant change ahead and prosper more rapidly than others in the upturn that will eventually arriveThis gray and gloomy Tuesday morning saw the publication of yet another set of economic data that was again revised downward illustrating the seriousness of the recession that we are experiencing What was particularly disturbing today was the rate at which circumstances deteriorated We would have to go a very long way back to see an economy as depressed as it presently is which makes it hard to stay somewhat optimisticAlthough such shocks to the economy often lead to apathy and inertia I am totally convinced that this recession – as is always the case – is actually a sign of deep and fundamental transformation of the nature of the firm or organization Those organizations that believe that the best strategy is to lay low and wait until the storm is over are seriously mistaken To stay with a metaphor what we are dealing with here is not a storm that eventually will die down It is much more a shift of tectonic plates creating a series of violent earthquakes that will change everything forever There is no premium for laying low and for waiting in an earthquake zone When you are going through hell the main thing is to keep going as they say So the time is now to adjust to this new reality and to start creating and strengthening the competences that will determine success of the New Firm Failure to do so is risky It might jeopardize the future of your organization but it will definitely slow you down in the recoveryOrganizations that are able to resist the pressures of operational cost cutting and keep some minimum level of investment going are clearly going to profit much quicker from the upturn than those that are totally fixated on short term survival That is why the main management challenge of today is to create some kind of intelligent cost cutting which to many will sound like an oxymoron Once again the famous dilemma of management emerges keeping an eye on what is important while dealing with what is urgent However this book should help you argue for investment in collaboration in times when cash is king It could even help you save money by accelerating the shift towards delivering software as a serviceAlthough a lot still remains unclear some basics of this New Firm are emerging Hierarchical organization will give way to market Conversations become key and the capability to collaborate within and across organizational boundaries will inevitably determine success Modern software tools are creating radically new ways of collaborating between people The pervasiveness of the internet combined with new insights in software architecture is creating new possibilities for delivering this functionality from the cloud instantly widening the scope for collaboration to a global perspectiveHowever some caution is necessary since we are at risk of technological determinism Things are not as simple as they sometimes appear to be This industry has become famous for its overestimation of change in the short term It is overhyping technology breakthroughs and ignoring the difficulties that organizations will have applying these technologies in their business processes That is why two chapters in the book are dedicated to keeping this revolution pragmatic By debunking some common myths around collaborations and giving you the right questions to ask this book will help you to focus on the matters that are important and to cut through all the hypeYes this recession will cause violent change but it will not do so overnight Technology can be considered a platform for social change or a reflection of it however it is seldom the change itself It is my sincere hope that the discussions in this book will inspire you to find the true nature of this change and to determine how it will impact your firmMichiel Boreel CTO Sogeti  Amsterdam February 10th 2009Reading Guide and AcknowledgmentsThis is a book about collaboration and cloud It is about collaboration between people and between companies and about how this collaboration is changing It is also about how markets and companies themselves are changing or how they will have to change in order to confront changes in technology and society And its about software how we use it and how we are growing towards a mix of traditional software and services from the cloud This book is written for any reader interested in IT strategy innovation and trends in business and technology The book is not technical and it will show how technology can be used to create business value by improving collaboration CIOs enterprise architects and people responsible for IT direction will benefit from this book because it will advance their thinking on the topics of collaboration and cloud computing Though it is not a cookbook or how to manual this book will provide practical insights and guidance for the creation of your own strategy in these matters Specifically we will hand you a list of questions to ask when getting involved in any initiative relating to collaboration or the cloud Chapter 9 Fourteen Questions to Guide the Revolution provides a pragmatic approach to the topic Combine that with Chapter 10 where we debunk some common myths and you should be all set to move forward in this exciting field In the prelude we will examine the current crisis in the global markets and we will talk about the options companies have when faced with turbulent times  The first chapter will then introduce the concepts of collaboration and cloud computing and how they are connected We relate cloud computing to Software as a Service and show the drivers for these trends We also talk about the nature of collaboration and we sketch out several collaborative scenarios in this chapter  Chapter 2 talks about the larger shifts in society It discusses how trends in many areas are combining into large transformations and it discusses the effect of technology on people  Then in Chapter 3 we introduce the new nature of the firm where not just competition but especially collaboration is of the essence for survival We introduce the value chain 20  Chapter 4 looks at the effect inside organizations when faced with these changing times It discusses the consumeremployee the consumployee as a source of innovation and goes into how an IT department could respond  Chapter 5 shows the different ingredients of collaboration and how they are interrelated It also talks about email and the emailless organization  Chapter 6 is about the basics that need to be in place for successful collaboration It talks about trust culture and rewards Collaborative culture may be hard to create from scratch but we will give some guidance on whats involved  Chapter 7 talks about the reality of cloud or Software as a Service and shows that a mix of both traditional software and cloud services provides greatest flexibility to deliver rightsized solutions to an end user  Chapter 8 goes one step deeper into the areas where social computing for business can be of value and the chapter discusses some of the scenarios Here we also put Web 20 in a corporate context  Chapter 9 will hand you a list of questions to use to keep your feet on the ground It will serve to measure the reality of any proposal and guide you when examining cloud and collaboration further in combination with Chapter 10  In Chapter 10 we debunk some of the common myths around collaboration  Throughout the book between the chapters you will find reallife customer cases that show the reality of collaboration and/or Software as a Service AcknowledgementsThe authors thank the many clients who provided valuable input for this book through interviews discussions and of course projects Their often different views on terminology and strategy showed us the range in which both collaboration and cloud computing are used in reality today Eight of the interviews given have been transformed into the case studies that are spread throughout the book This book is the result of a close collaboration between a team of authors from Microsoft and Sogeti Supporting this team of authors were the many contributors and reviewers from both companies who have helped to create and hone the content Special thanks go to Herve Tourpe Laurent Dieterich Mike Martin Per Björkegren Albert Hoitingh William Heurdier and Michael Wagner for their input and/or reviews of early versions of the manuscript  Prelude Business RealityThe World on FireIt is September 15 2008 Meltdown Monday The worlds financial system has collapsed into a global crisis Stock market indices are dropping by double digits and shareholder value is disappearing instantaneously everywhere Seemingly healthy companies are forced to ask for assistance National governments of most countries have to jump in to prevent an even worse catastrophe Panic and uncertainty are sweeping the globeThe world has faced financial crises before Yet never before did a crisis have such a worldwide impact How can this be The day after Meltdown Monday the New York Times featured an overview of the major stock markets across the globe The surprising realization that emerges from the graph is in the pattern the markets follow These patterns are very similar The stock markets in different countries are synchronized In other crises when things were simpler the markets in different countries might have moved more or less simultaneously because for example the different currencies were all tied to the price of gold but never was the connection this close These days the markets are much more tightly connected by realtime international trade products and financial services leading us to in fact the largest synchronized downturn really in the postwar period  according to Charles Collins deputy director of the IMFs research department While we are used to thinking about local business and markets companies are increasingly operating across borders turning from national into multinational from local to global playersThe stock markets are apparently engaged in a close and intricate dance that was not obvious to the outside world before The crisis and particularly the global nature of the crisis came as a shock to all but the greatest doomsdayprophet Could the crisis have been averted Is that a rhetorical question in a time when you can find all information on the web Across the globe we see the same behavior but people are not capable of making sense out of the complexity of events beforehand Despite all transparency finding patterns and predicting the future is still impossibleFigure 01 Overview of Major Stock Markets Around the GlobeLife in a Complex WorldThe internet is now over 20 years old Never before has a new technology had such a wide impact on global society The internet has changed business life beyond recognitionDistances have shrunk or disappeared completely Technology has made the earth small and flat Information work and capital can be spread across the globe at the press of a button You can contact strangers and create new forms of collaboration in the blink of an eye At the same time the problems of the globe also find their way to everyone everybody knows about the challenges the world is facing with regard to energy the environment and clean water Wars and terrorism are global themes Everybody in the world is connected economically technologically and sociallyTime itself has changed or at least our perception of it We are living in a 24/7 economy On the World Wide Web there are no closing times or holiday closings The doors of the virtual stores are always open to anyone – or more precisely to anyone with a credit cardA transparent world makes secrets history and drowns us in data There are no more secrets Good news and bad news circles the world in an instant Information is available to anyone anytime We are continuously connected to the internet Using computers laptops cell phones and other devices we can access a mountain of data on request Moreover this mountain is still growing everything that can be digitized is being transformed into bits Maps old archives video music and statistics are added to the internet every day Every object process and service will be able to communicate and combine autonomously with someone or something else The internet is changing from a collection of pages to a database of things  The question is how do we transform this huge pile of data into intelligence And at what cost Quoting Herbert Simon What information consumes is rather obvious it consumes the attention of its recipients Hence a wealth of information creates a poverty of attention and a need to allocate that attention efficiently among the overabundance of information sources that might consume itIDC expects that by 2011 the digital universe will be ten times the size it was in 2006  How do we keep it from exploding How do we stay on topAnd language Is language the spanner in the works At this moment language remains a barrier Half of the world cannot communicate with the other half simply because they cannot understand one another or even read each others alphabet However this barrier will break down soon Numerous software vendors are busy working to allow at least some communication across linguistic boundaries The 100 automated foolproof translation service is a challenge closely tied to the holy grail of artificial intelligence but in a couple of years you might be uncertain as to what language the person on the other end is using to communicateThe above trends not only affect us humans but even more they affect the way organizations operate how society works Companies can no longer survive on their own in this dynamic world Any one player alone cannot grasp and properly respond to the complexity of the large interconnected worldTechnology and how it can be used is about to drastically change the nature of companies and how companies create value The crisis of the fourth quarter of 2008 was at least in part made possible by using technology to connect and combine markets The crisis itself shows that the disruptive nature of these new technologies is inescapable Every industry has become involved and is feeling the effectPatterns in Complex Systems The Butterfly EffectIn 1961 meteorologist and mathematician Edward Lorenz executed a simulation on his computer to create a weather forecast To his astonishment the simulation showed a completely different prediction when he rounded the number 0506127 to 0506 in a series of numbers In 1963 he published his findings in the New York Academy of Sciences He described the above result as follows One meteorologist remarked that if the theory were correct one flap of a seagulls wings could change the course of weather forever Later the seagull mentioned was replaced by a butterfly which led to the famous quote Does the flap of a butterflys wings in Brazil set off a tornado in TexasThe effect is also called the butterfly effect  and it is often used to describe chaos theory This theory states that small changes in the initial setup of a dynamic system can have a huge impact in the long run an impact that is impossible to predict Examples of such complex dynamic systems are the weather and also the global economyIn the conclusion to his book Linked writer AlbertLászló Barabási  makes a statement about how markets are defined by interaction and connections He shares the following thought The unpredictability of economic processes is rooted in the unknown interaction map behind the mythical market Therefore networks are the prerequisite for describing any complex system indicating that complexity theory must inevitably stand on the shoulders of the network theory This thought raises questions such as what is the relation between the network called The Internet and this presentday economy What is the cause and which is the effect in events that involve both How can companies and governments handle this increasingly complex world Managing in Times of ChangeA downturn does make some things easier for a manager It helps you focus on your clients and makes it easier to prioritize Managers looking to use this time to change their organization for the better will also be examining their own role in this changeMarketing guru Seth Godin wrote in his book Tribes that the real difference in todays business world is that anybody can create change Everybody has the opportunity to connect and start a new community Tribe These communities nurture the leaders of tomorrow and organizations can choose to embrace these leaders inside or outside their organizational boundariesManagement is about manipulating resources to get a known job done… leadership is about creating change you can believe in… Leaders have followers Managers have employeesThe Virginia Satir Change Model  gives insight into how organizations can change in times of chaos In this model a disruption of the old order is a golden opportunity to get things done and reach new levels of performance On his blog The Social Customer Manifesto  Christopher Carfi describes the process as followsThings are plodding along within an organization or communityThere is a foreign object eg a new thought or participant or strategy introduced into the organizationThings get chaotic while the community figures out how to deal with the newThere is a transformational thought a transforming idea and a point at which the group gets it and starts to gel in the new worldChaos declines and performance then stabilizes at a new improved levelIn the popular and related book The Black Swan  written by Nassim Nicholas Taleb more is said about events that can disrupt the old status quo He uses a story about the discovery of black swans as an example to illustrate how accidental events can change our perception and our lives Harry Potter was a black swan 911 and the Meltdown Monday of September 15 2008 were too They are unpredictable have a major impact… and afterwards we try to make them rational and predictableFigure 02 Satir Model of System ChangeTimes of Trouble or Times of ChoiceCompanies have a choice Will a company pull back in defense or use the turmoil to reach new levels of performance Focus on survival only or focus on the market of the future after the crisis Are companies able to use the economic downturn to their advantage The urge to survive drives companies to cut cost reduce operations and lay off many people Yet even survival needs direction the American car companies are not looking back for survival they are looking forward The newspaper industry is not looking at reviving print media but looking forward This is the essential insight that will shape the leading companies of the future focus to survive todays crisis and to survive the radical changes that are happening all around us Free  is heralded as one of the business models of the future Thanks to digitization globalization and opening of markets the global market is becoming more efficient Lower transaction costs are the direct result of changing markets and it forces companies to downsize and rethink their strategy New sources and markets must be opened up in order to survive and only through collaboration with others can this be achieved in time One thing that the current crisis has taught everybody is that change is needed Companies can no longer ignore the reality of a globally connected complex and volatile business world Sticking to old routines will not suffice A new era calls for new measures As for where to look for solutions Seth Godin recently shared this insight on his weblog  The dramatic leverage of the net more than overcomes the downs of the current economy The essence is this connect Connect the disconnected to each other and you create value Organizations can create value in this new economy by connecting and collaborating ppromise is a cloud fulfillment is rain – Arabian ProverbThe topics of collaboration and cloud computing have a lot in common For one thing both are concepts that touch or cross the boundaries of an organization and that are closely related to corporate innovation Both are relevant to the relation between business and IT Both play an important role in these financially unstable times And last but not least both can be broadly interpreted and can have a major impact on the efficiency of organizations and IT Microsoft and Sogeti recognized the importance of both topics and they also saw how these developments accelerate each other Accordingly these companies decided to collaborate research the topic further and write this book In it we will discuss both cloud computing and collaboration in depth We will start by exploring how these terms are commonly used 12 Defining the CloudThe term cloud originally came from diagrams where the internet itself was represented by an image of a cloud yet trying to find a narrow definition of cloud computing that everybody agrees upon is not easy The most specific definition would be that it describes a situation where some computing is taking place somewhere else using the internet But then there is also talk of something called my cloud where the somewhere else might be right in your own datacenter Clouds seem difficult to nail down Forrester Research in a recent presentation defined cloud computing more in terms of business economics Cloud computing is buying IT capacity and applications asneeded from a utility service provider While this definition does not mention the actual delivery model it does touch upon the expansive nature of cloud computing and notes that there is another party involved the utility service provider Also cloud capacity can be consumed at a cost Payment may be in money or in the case of free services payment may be made by exchanging advertising value consumer attention as currency and some providers offer services for free simply in the hope of creating lockin and selling upgrades or support Another analyst firm Gartner Research uses different phrasing and focuses on slightly different aspects but arrives at a similar definition Cloud computing is a style of computing where massively scalable ITrelated capabilities are provided as a service across the Internet to multiple external customers  It is interesting to note that Gartner specifically includes as a service in their definition Meanwhile in less formal terms cloud has been widely adopted by many parties trying to market their services as part of this new and engaging concept Companies are increasingly including internet functionality in their IT portfolios The internet is slowly but surely reaching into all areas where connecting to some functionality has become more important than owning that functionality The internet is serving up solutions for situations where its more important to get things done than to own the hardware or software that is needed to get those things done It starts with very generic solutions but increasingly solutions that are more specific become available online It is in this light that cloud has become the label to put on all things that are provisioned over the internet be it server capacity complete office solutions or a CRM system Figure 11 Cloud Computing Simply Explained Some characteristics that are commonly associated with a cloud offeringUsers and clients connect to the services using the internetThe service offered can range from technical services to a complete userfacing functional solution It can also offer part of the stack such as storage computing power technical components or partial business componentsA provider in this scenario will often aim for economy of scale by offering a multiuser or multicustomer MultiTenant environment to optimize efficiency where the fluctuations in capacity demand will even out over multiple users The services offered then only have basic configuration options but may be enhanced with standard addons The provider will generally charge for use of the service per user per day per load per call etc There are also many providers offering services for free while they are still in Beta mode or which are paid for using an advertising modelServices from different providers should be but are not always easy to combine adhoc to fit the needs of the client The term mashup defines the situation where multiple services are combined to easily create a new solutionThe cloud concept builds on the themes the IT industry has explored when introducing the Application Service Provider concept outsourcing of datacenters and the introduction of shared service centers The ASP concept introduced us to the concept of an externalparty offering functionality on an ondemand and payperuse basis Outsourcing helped us consider redrawing the boundaries of our own IT for the sake of efficiency Shared Service Centers helped us look for communality in a broader set of needs All these themes return in cloud computingThese developments were then fueled by Web 20 another one of these hardtodefine terms where simple interfacing allowed users to configure and combine the myriad of services to suit their needs while adding a social context doing this together with others The resulting explosion of startups and innovations created a rapidly evolving market where many valuable services and websites emerged Examples of these are sites that let people work together on documents or graphics wwwzohocom and sites that help people find interesting information online using social bookmarking digg com or stumbleuponcom Also providers are starting to offer all sorts of technical components online to integrate information or create new combinations online RSS readers online that allow you to integrate news feeds Yahoo Pipes and MS Popfly thereby combining feeds and processes into new feeds or servicesCloud Formerly Known as SaaSAt this point you might ask but how does cloud relate to Software as a  Service SaaS Both are definitely used in the same space Software as a Service is the term the IT industry originally used to describe the model where service providers offer a hosted solution for which clients pay based on how much capacity they actually use Software as a Service is more focused on complete solutions that are accessible over the internet In a similar vein there is also the notion of infrastructure as a service where server or storage capacity is made available Cloud computing is a broader term which also comprises Software as a Service whereas SaaS originally was more narrowly associated with complete solutions from specific Application Service Providers ASPs Figure 12 Waves of Terminology Hitting the News If you go online today you will find that many solutions can be provisioned as a service not least because it is a very attractive model for software vendors Vendors offering their products as a service have an easier time dealing with the complexities of versioning and rollouts and they can craft very lucrative contracts The focus of the larger providers is primarily to provide economies of scale offer commodity services at very competing prices cheaper than onpremise installations and earn profit by attracting and locking in large numbers of customers Large providers will be the only ones that can build for scale so they are the ones able to offer commodity services email document management messaging etc Smaller service providers cannot compete and must provide value with addon or specialized services The good thing is that the commoditized services provide a standardized groundwork on which many other services can flourish For the potential consumer of the service the attraction often lies in the lower upfront cost better financial structure fixed versus variable cost ease of deployment and easy scalability It is simple to try a new solution and scale up if it proves successful Provisioning software from the cloud is generally quicker which comes in very useful in for example a merger scenario Overall we could say that Software as a Service means that the CIO has fewer worries The CIO has to worry a lot less about Upgrading the software and technology stack with SaaS the provider takes care of most of this sometimes client software still needs to be updated to be able to consume a serviceGetting stuck using an old version of software for which support has expired using SaaS you always get the latest servicesMaking sure the software needs and infrastructure match again something the provider will take care ofMaintaining multiple staging environments testing prerelease development switching extra environments on or off is easy with SaaSBuilding technical expertise for the software all you need to know is how to use the service its contract and interfaces not its inner workingsShelfware running up a bill for unused licenses unused services may be free of charge or capable of being turned offMajor impact of software upgrades a service change will generally not affect databases platforms etcPerformance tuning in case of SaaS call the provider if the SLA isnt metVendor attitudes bad support bad quality a service contract depends upon a happy client as opposed to a onetime license sale that is final and finiteUser acceptance / adapting to new software versions fewer big releases and more small steps lead to a kind of continual software/service improvement that users can follow more easily People act as part of the viral deployment of new features/capabilitiesOn the downside new uncertainties are also part of the reality of SaaS and cloud Security and confidentiality of data are often cited as a problem see also Chapter 10 new governance models are needed to manage a multitude of external parties backups and archiving need to be approached differently how we test will change etc Also the question of integration and how to ensure a single user experience will demand some study Some of these issues are addressed by new standards some require a new way of thinking and others might simply be the cost of using SaaS Hardware ProviderdedicatedFixed dedicatedmanaged SharedElastic InternetWeb applications and resourcesresources  as a ServiceIaaS – Integration as a Figure 13 This diagram from Gartner shows the wide range of terminology used in relation to cloud and SaaSWhereas SaaS still typically involves complete applications the options available from cloud seem to be more flexible offering partial solutions components and individual services that can be used to create your own solution There are many examples where cloud can offer almost anything as a service Figure 13 is an attempt by Gartner to draw some borders around the different cloudsQuick News Highlights from the CloudTo give you some idea of what services are being offered from the cloud the following are some random highlights of recent end 2008 news announcements involving cloud computing or Software as a ServiceExpresso ExpressoCorpcom has launced an online realtime collaborative Excel solution as a service that according to their website assists in the trend toward managed online business communities  Jobsciencecom is offering new Recruitment applications as a Service  Cornerstonecom has launched a new Learning and Talent Management platform that can be used as a Service  At descartedcom you will find OnDemand fleet and transportation management services that are used by among others Home Depot  Eteloscom is offering current application vendors to pick up existing applications and start provisioning in an As A Service model  Zohocom is offering a complete range of online productivity solutions such as email document creation and spreadsheets It is now also offering SQL as a service offering to integrate data from multiple websites  Sage is offering an Online Cashbook called Sage Live Cash Having embraced the Software as a Service philosophy the Sage Live team has gone as far as it can to exploit the opportunities of the Web 20 approach integration with Google Docs and builtin link to PayPal online payment solutions  Mortgagedashboard has extended their Loan Origination System that is a SaaS solution  Themis solutions is offering a complete solution as a service for attorneys to support their practice  Demographicsnow has improved their customer and business profiling SaaS offering  Compliance 360 announced the availability of its claims audit solution that helps organizations manage a wide variety of claims audits and appeals including those conducted under the CMS Medicare Recovery Audit Contractor RAC program  Litebi has launched business intelligence delivered via Software as a Service  FiTek LLC and the Northern Trust Company announced the release of TrustPortal that is also available in an ASP model a fully integrated straightthrough solution for trust management that includes investment management with electronic trade execution compliance administration accounting operations automated account review extensive webbased report engines and with a host of third party interfaces including complete custody reconciliation  Phisme is offering a service to prevent phishing attacks  Webroot announced EMail Security SaaS that protects against spam viruses and data leakage along with additional compliance archiving and business continuity features  Winscribe has launched a Digital Dictation solution as a service It will mainly be used by healthcare professionals Physiotherapists are using Phillips devices to record their dictations which are then uploaded to a secure site that can be accessed by secretaries who work in another building  Elemica is offering a service that supports supply chains Service as a ServiceIt is important to keep in mind that whats new is not simply that computing starts to reside in different places but that for a user there is less involvement with the software part of the service The focus is shifting from the technical features of software to the use functionality and usability of the services Dont underestimate the appeal for a business user of being able to set up a portal website or dashboard without needing to call the IT department If anything online services magnify the need for good usability For a business user the important part of anything as a service is that it really can be service as a service It is no longer about software or technology it really is about the discovery and the use of a service itself If for example a business user discovers a website offering a simple tool that allows him to keep track of the competition by analyzing their rates or press releases this same business user can start to use this service and integrate it into his daily processes If he is then also supported with the knowledge and a simple framework to integrate the service into the other tools hes using for example an existing dashboard or spreadsheet the role of the IT department will begin to shift to providing an infrastructure instead of providing solutionsThere are many vendors playing a part in the current cloud each offering their own software solution as a service each providing different models of payment and delivery Blogger Matias Woloski  wrote a thesis on the topic of SaaS In his research he drew a conceptual map of the SaaS space and related topics see Figure 14 Though again using different terminology than for example Forrester Research the elements he recognized are roughly the same Matias Woloski also created a similar taxonomic diagram showing the names and logos of some companies offering services in particular spaces see Figure 15 Since this is an area that is still booming this diagram was already outdated the moment it was drawn but it will give you an idea of where different service providers are positioned Figure 14 Woloski Conceptual Map of SaaS Figure 15 Woloski Map of Service Providers Already from these diagrams and the news announcements above we see that many of the services offered in the cloud are in some way related to collaboration either by proving support for web conferencing between groups of people by creating documents online or by sharing information about clients or enterprise resources with colleagues So lets take a closer look at collaboration Organization is CollaborationBusiness is predominantly carried out by organizations and an organization is essentially an arrangement of people working together for a common goal In other words collaboration is the essence of any organization and organizations exist to better organize collaboration Still businesses have been slow to improve and support this very collaboration as something that could be of importance to the overall success of the firm Organizations try hard to hire the best people and build teams and departments with the necessary skills They try to motivate people with bonuses and benefits But when it comes to actually looking at the interaction between people and how an organization can best support it practices are thin For the most part we have used proven ie old management models introduced email and mobile phones and for the rest left people to their own devices An important aspect that also defines an organization is that it is active within an environment from which it is separated by some boundary there is an inside and an outside of the organization Where this boundary is how large the organization within this boundary is and how the organization is interacting across this boundary has all changed due to the power of the internet and other market forces And it will continue to change even more driven by competitive forces globalization and not least by new technologies The speed of change is almost realtime As Ivan Illich once said We might already be beyond the age of speed by moving into the age of realtimeNow how can you develop effective organizational strategy in times of change In times of economic turmoil there is a tendency to focus on defensive measures by trying to reduce costs and optimize efficiency Looking at productivity will surely help in that respect You can make people more effective and efficient reduce the time spent on meetings travel or searching information Help people make better and quicker decisions create better deliverables or do better knowledge management Other companies will use these times for exploring innovation trying to find a new and future market and developing a commodity that will do well in that market In times of market fluctuation its just as important to hang onto your part of the future market as it is to survive current circumstances This too is a driver for looking at collaboration trying to find new solutions innovations and opportunities that can be achieved by the people who work for you And heres the crux even people that are not part of the organization can be engaged to work with you and for you Thanks to the Web 20 tools that are widely available companies can tap into the collective intelligence  And when you succeed in engaging these people they will be your future most loyal customers On both ends it pays to examine collaboration to optimize existing processes and to create new opportunities by starting an ongoing dialogue with your customers Defining Collaboration Before moving on it will be interesting to look at your own understanding of this concept What is your definition of collaboration Which other terms do you associate with it In what context have you used the term in your job recently In talking to people from different organizations a lot of definitions of collaboration surface Some are broadly addressing business to business activities others are only using the term to talk about the implementation of specific technology But even then some use it to describe their conference calling partner others talk about their portals and intranet and still others talk about their project management approach To explore your own assumptions and understanding of the concept collaboration here are some thought provoking questions  In what context did you last talk about collaboration Do you or some of your colleagues collaborate with competitors Perhaps in an area where you dont compete or where you provide commodity products or services Is sending an email a form of collaboration Is using the phone Do you personally collaborate with some people more than with others With whom do you collaborate Who else could you collaborate with more intensively How does the culture and management of your organization influence the way you and your colleagues collaborate Are you personally trained or supported in how to collaborate Do you or some of your colleagues collaborate across boundaries With other business units With people outside your organization With friends With anonymous people outside your company Is using Facebook LinkedIn Myspace instant messaging or Twitter collaboration Can you collaborate in a virtual world such as Second Life or even World of Warcraft And what if you contribute to Wikipedia Does your organization have a strategy for collaboration If so be honest was it driven by a business need or a technological capability Is this a strategy for the short or the long term Is it even possible to create a collaboration strategy for the long term What opportunities would better collaboration inside and outside your organization bring Could you save money or create added value Since collaboration is the essence of being an organization it is a much wider concept than is traditionally discussed The word collaboration in a business context has somehow shifted towards how people work together within an organization Yet when we interviewed clients about how they would define collaboration we got a wide range of answers The one thing everybody does agree upon is that for two or more parties to collaborate you need a common goal or deliverable After that all definitions are possible it might be two people collaborating or two companies People can be part of the same company or crossing company boundaries It might be done by using paper and pencil or it might be a completely automated process where different systems interact to reach one goal In this book we will discuss the essence of present-day collaboration the importance of and the new modes of collaborating For now we will define collaboration as Interaction between multiple parties two or moreAll parties are doing work andWith a shared purpose or goal all parties will get something in return for their effortsIt can be across boundaries The shared purpose or goal does not have to be the sole thing motivating the parties to collaborate they could also have corporate or personal goals that give them the incentive to collaborate For example the shared goal for a team of people working together might be to create the most user-friendly online banking solution yet the personal goals of those individuals could be to work with a specific guru to improve the world to make promotion to learn about the topic etc As long as the personal goals are in line with the shared goal or the shared goal is part of the personal goals collaboration can be successful Perhaps we could say that collaboration needs a shared goal and/or multiple compatible goals We will discuss this topic more in depth in Chapter 6People Working with People Business with Business Are people collaborating or are companies collaborating Depending on your level of abstraction both could be happening Ultimately it is human collaboration that allows companies to work together people make the connection and set up the relationship that allows two companies to collaborate Once the connection has been made and both parties agree on the specifics of their collaboration the implementation will move into the realm of technology systems working with systems instead of people working with people The boundary between collaboration and combination begins to fade once we look at services being combined by a third party If someone combines an online mapping service with an online statistical information service are the mapping provider and the agency providing the statistical information now collaborating The two services are jointly providing a solution in which they may each perform their intended function without any contact between them Is this collaboration or merely combination And what if the person combining the services works for one or both providers As you see the introduction of services that may be used from the cloud has a direct impact on how companies collaborate and how collaboration evolves No Such Thing as CollaborationThe examples above show that collaboration like perhaps all relationships is often a trade I will do this for you if you will do this for me I will provide you with a certain skill if you help me with the skill I am missing Both parties invest and both parties get something in return The more the goal is shared between the parties the more we would rate their relationship towards the collaboration end of the scale and perhaps no money needs to change hands The less the goal is shared the more their relationship tends towards the combination end of the scale where one party might charge the other for contributing to the joint solution or both parties might achieve different goals through the combination The situation is analogous to defining another company as a partner collaboration shared goals or a supplier combination different goals The trigger for initiating collaboration or partnership in this case is the realization that together something could be achieved that one party alone cannot achieve such as providing a specific service creating a more specialized or complex solution or greatly improving service quality We generally accept that in collaboration 1  1 = 3 I need you you need me and together we reach a common goal a higher goal generate added value Basically within companies collaboration is the natural model since collaboration is the essence of organization Intercompany collaboration is a trade for the benefit of all involved There are many scenarios for collaboration In a business context some are recurring situations people within projects working together looking outside the company for innovation two companies creating proposals together or multiple companies partnering to form a supply chain Colleagues Collaborating on a Project Multiple people working together under the guidance of an appointed project manager is probably the most traditional form of explicit collaboration within an organization Teams are formed based upon skills and ideally personalities The project goals are explicit and externally defined and the roles within the project are usually well defined Technical support for the collaboration depends upon the deliverables of the project but mainly email and face-to-face meetings will be used with possibly some conference calling if the team members are working from different locations Conference calling support is commonly provisioned from the cloud using free conference calling providers or providers contracted by the organization In projects that deliver software or some other jointly composed deliverable there will be a solution that combines the contributions of the individuals in a format designed for contribution Software versioning systems fall into this category and are mostly hosted within an organization itself Other aspects relating to team function are mostly top down the project manager uses task assignment and project reporting tools to assume control over the teamIn Chapter 5 The Anatomy of Collaboration you will find extensive examples of how collaboration works best when supported by all available tools People collaborate because they must because their job requires them to work with colleagues on projects or as an ongoing process People collaborate because they are assigned tasks that they cant perform alone so they are driven to collaborate with others But there are also other reasons to collaborate In fact could it also be a matter of habit Or could it simply be for fun Think of the newer generations the GENYers the digital natives the people who grew up in a world of the internet mobile phones and TV on demand TiVo They are much more collaborative from the outset the social aspect of people gathering in an organization is becoming more important as a way to attract and bind talented people And regardless of financial turmoil attracting and motivating young people will be one of the challenges businesses face in the coming decade Motivations to collaborate can be anything from personal beliefs to a longing for status to a direct need for the deliverable that is subject of the collaboration People dont only collaborate because they need to they also collaborate just because its the natural thing to do and because its fun This social aspect of collaboration is where many organizations find a lot of the value of improved collaboration Giving people new ways to create social bonds within the organization and allowing adhoc collaborations gives people the opportunity to create friends within the company It also potentially creates the most problems how do we manage this and how do we stay productive if there is such a thing as productivity that you can measure and manageCollaboration and Cloud Come Naturally for Digital Natives Put a team of recent graduates on a project and within five minutes they will be procuring a portal for collaboration exchanging instantmessaging IDs and adding one another to their Facebook pages The team goals are important the deliverables and communication is important but the rest of the structure is fairly free and left to the individuals discretion Commitments are deliverablebased and the team members will work whenever they feel like it at night early in the morning or during regular business hours The new expectations these digital natives bring to the employment market puts pressure on organizations looking to attract the brightest students When support is not available from inside the company these digital natives will procure from the cloud When the cloud is offlimits and there is no internal alternative they are very likely to pack up and leave for a place where they CAN work the way they like A higher dislike for bureaucracy the experience of growing up during the internetera with wonderful stories of how startups treated their employees and an international orientation make this generation difficult to attract and to motivate Collaboration and networking are natural – having over 200 friends is nothing special Approaching one of these friends to get some information is the natural thing to do and vice versa Responding to the needs of the network is also characteristic of this generation Free CollaborationWhen people have different motivations to collaborate how is that for companies People can contribute to projects for many reasons how is that for commercial organizations Does it pay to freely collaborate Perhaps surprisingly on the internet free is often a viable business model As with individuals there are many different drivers for companies to offer free collaboration for example to benefit from the resulting product eg in open source to gain a marketing position or attract future clients or to build corporate image by showing you are committed to improve the world Many companies dont expect immediate returns but act on a kind of pay it forward  principle The collaborative projects the companies enter into can also be on a wide range of subjects from the creation of software to solving world problems from creating books to collaboratively finding innovative solutions to industry problemsInnovation and CreationCreating a concrete deliverable together is straightforward split up the task contribute parts and combine them to construct the complete deliverable Even this will take coordination and skill to accomplish but at least it is a concrete and often measurable task Ideation creating new ideas together or creating innovation is a lot less concrete Ideas are hard to plan measure or manage Yet continuous innovation is what makes a company profitable So how does collaboration help innovation The answer to this question lies in crowdsourcing innovation or open innovation involving a larger than usual group of individuals in company innovationThe idea is that the wider the search for ideas the greater the chance that good ideas will be found the greater the chance that good ideas may be profitably combined and the better the ultimate product or service In turn a better product may demand a higher price If I could enlist everybody in the world to help me solve a problem or even just everybody on the internet I am sure that someone out there would be able to give me a solution almost instantly However there is a catch involving many more people would only be manageable if I did not have to handle coordinating and interacting with all of them If I must personally deal with every idea I will never find the good ideas among the notsogoodCrowdsourcing innovation is a unique form of collaboration where anyone with a stake in a product or service can contribute to defining and improving that product or service Consumers work with producers to create products that best serve the consumers needs Employees can be invited to help improve the inner workings of an organization but will also be challenged to come up with ways to create better value for the end consumer Almost any creative process can be crowdsourced to benefit from the creativity of the crowd logo design chemical research challenges architecture design product development writing and othersInnovation is not the only thing that can be crowdsourced the production of deliverables can also be tendered to the crowd The most specific example of this may be in software development where on TopCodercom the crowd can build software to requirements Collaboration with the Crowd for InnovationIn a scenario where a company is looking to crowdsource parts of their research and development the aim is to allow the best ideas and solutions to surface almost automatically The company would achieve this by posting their challenge online and inviting people to post suggestions Usually this is done by providing a very freeformat platform where ideas can be posted commented on by others then cataloged and rated by the people online The creators and the people who do the rating may not be the same people but will be part of the same community Depending on the size of an organization crowdsourcing can also be initiated strictly within company boundaries not involving the general public but asking employees to take part While the principles might be generally the same the implementation will be different For one thing internet scale is different than company scale If 1 of your internet audience responds that is a large group of people responding If 1 of your employees respond it is less likely that the next greatest idea will be born The most successful crowdsourcing initiatives involve the internet invite the general public to contribute and have a clearly defined challenge The reward for contributing to this collaboration might be anything from eternal fame if your suggestion could become the name of the latest CocaCola product that might be enough motivation to take part to a monetary award or some other incentive The open source software community teaches us that financial reward is definitely not essential for an open initiative to succeed An interesting example is Talpa Creative wwwtalpaCreativecom where the community is invited to help create new television formats coming up with new ideas but also taking existing ideas to higher levels Voting pitching ideas and competition are part of the platform The rewards are financial but small and a part of the attraction is being the one who came up with the next Deal or No Deal or Big Brother showCurrent crowdsourcing initiatives use fairly basic textbased tools User identification and user profiles are important for building credentials while forums or discussion boards can be used to exchange ideas At this time the use of features like conferencing video and instant messaging for crowdsourcing is rare This might change over time given the fact that video is gaining ground as a medium of expression over the current textandimages internet14 The Cost of Crossing BoundariesThe two aspects that define an organization are collaboration and the fact that there is a boundary between the company and its environment Ever since the internet caught on there has been talk of it sounding the end of organizations The idea is that thanks to the internet individuals should be able to work together in the same way they could within an organization but without the need for corporate overhead such as management and legal structures So far this hasnt proven true and it doesnt look like it will come true anytime soonIt is worthwhile to see what is happening to this external boundary what is inside the company and what is outside What defines you and what sets you apart from the competitionIn 1937 Ronald Coase wrote a treatise called The Nature of the Firm In this book he examines the way markets operate and focuses specifically on the question whether a certain economic task will be performed by the organization itself or whether it will be left to the market For this research and other related topics Mr Coase eventually won a Nobel Prize over 50 years later The most important question Mr Coase tries to answer in his book is why organizations exist Why is it not always cheaper to let the market fill a need If an ideal free market will set the benchmark price for a commodity according to competition among suppliers why would any company choose to hire people instead of bidding their needs to this market Why cant we crowdsource every aspect of every enterprise Why does it pay to hire people and let them work within company boundaries instead of letting a collection of freelancers do the work The answer to these questions lies in the concept of transaction costs even IF a market were ideal and more and more it seems that not all markets reach the ideal state the cost of the pricing mechanism and other costs will make it too costly to tender every task to the market Pricing costs are the costs expended to find the correct service to negotiate a price and to buy and control the service All this takes effort and it makes the price of the service consumed higher than the market price of the value provided by the serviceAs an example repeatedly finding a suitable programmer who will update your website with new business functionality and reaching a new agreement with that programmer every time will in the long run probably be more expensive than hiring a programmer and doing away with the constant renegotiation The same principle is valid at all levels the action of outsourcing itself is costly thus challenging the business case for outsourcingRonald Coase also noted that the larger an organization becomes the higher the internal cost of coordination and the risk of mistakes will become shifting the balance in favor of the market again He realized that companies will expand until the case for further expansion is no longer favorable This in turn has led to the formulation of Coases inverse law which states that these days any organization will shrink as long as the cost to do something inside the company is higher than the cost of doing something on the open market If some specialized company can maintain your website at a lower cost than your own IT department there will be a push towards moving these activities outside the organizationThere is some debate as to what the impact of internet and information technology is on transaction costs IT could perhaps add to the transaction costs by overwhelming the buyer with information to sort through or by making it difficult to consume a specific service due to integration issues Generally IT is believed to decrease transaction cost making markets more efficient and making the choice to run parts of your business as a service more logical For some markets technology will bring more transparency which will decrease transaction costs almost immediately It will be easier to find the right service or product find the right price and determine the right contract In the case of cloud computing the argument can also be made that the market will start to offer more and more granular services that challenge the assumption that internal is cheaper than external on many new levelsWith changing transaction costs and new services being offered on the market any organization that needs to be competitive will be asking why are we a company and should we do this ourselves This is the reason why companies are partnering and collaborating in value chains to find the right balance between transaction costs between the parties collaborating and internal costServing Clients Together A very specific case of collaboration that occurs regularly is when two parties partner to create a joint proposal The two partners are trying to offer a solution or product that could only respond to the need with the input of both parties Responding to requests for information or bidding for contracts are situations where under great pressure people from multiple companies try to create a winning proposal The best proposals are created when both parties share a vision of the end result and there is a tight team working to combine the assets of both companies Tools used are mostly email a sad reality is that this is probably still the mostused tool in these cases portals and conference calls Usually there is a triggerandresponse system to find the best solutions Does anyone know of any solution to this problem… and a strictly coordinated effort to compose a coherent deliverable You are responsible for answering questions 1 and 5 The process tends to start in a more freeformat style solution visioning and become more practical towards the deadline For more complex proposals a project management tool will be used to track progress and dependenciesIt is interesting to note that the people working together in these joint proposal teams usually work as individuals all trying hard to create a winning deliverable Later in the process the legal department and management will take a more corporate role and look at the partnership and contract side of the collaboration is what we are offering balanced who gets what and how will we deliver togetherIf both parties are equal partners it raises the question of who will support the collaborative tools There is a good case to be made for using a third party provider in the cloud thereby allowing both parties equal accessCompetition in GovernmentBut what if you are working for the government Are you competing too Not surprisingly perhaps yes While in any government agency there might be little or no competition at the highest level at many other levels there is competition There may be competition between agencies to win execution tasks and corresponding budget allotment and there may also be competition with the open market The responsibility for national defense may rest solely with the state but supporting the HR for the employees who work in defense could be open for competition on an open market Providing social security might be the responsibility of the government but printing and mailing monthly statements could be performed by parties outside the government And at any time if the transaction cost of outsourcing becomes too high the government may decide to start competing if we can do it cheaper inhouse we willThe Reality of Crossing BoundariesCloud and crossboundary collaboration are a natural fit if information is flowing between companies using a thirdparty provider will be a logical step The old marketplaces and business hubs were precursors that led us to realize that whenever we work together we do it outside the boundaries of both organizations If we want to involve multiple parties the trust and identity issues can sometimes be solved more easily in an impartial forumThere are some thorny questions related to the crossboundary aspects of cloud and collaboration Most of these have to do with the fact that corporate data may also reside outside the corporate domain leading to questions about confidentiality corporate governance and traceability Also as the maintenance and operation is outside your control reliability and recoverability demand extra attention Evidently some data is best NOT left to the cloud and some scenarios are still best run from your own software This means that organizations will adopt a mixed model where a combination of software and services is used to create the best and most reliable support for the enduser The decisions as to what may go to the cloud and what should remain onpremises are based upon the issues described but also upon a more strategic question of an entirely different caliber namely what is your competitive advantage15 ConclusionCloud computing has become one of the scenarios for provisioning IT It is attractive on many levels but it also has some intriguing and thorny issues associated with it Collaboration is the essence of an organization and has been traditionally undersupported by technology Cloud computing and collaboration offer a combination where the actual use is paramount putting the user back in control and where boundaries are no longer obstacles In the following chapters we will examine the trends that shape contemporary business reality and we will see how collaboration on a grand scale has an impact on how organizations evolve Topics such as technological discoveries competition versus collaboration and transparency will be covered and we will discuss the new nature of the firm Case  Stimmt AGCase Stimmts Jump to SaaSPowered Collaboration Helps Consultancy to Practice What it PreachesCollaborating in the Cloud an Ideal Strategy for Small ConsultancyIn no sector has collaborative business supported by Software as a Service been embraced more enthusiastically than among small businesses A look at the SaaScollaboration strategy adopted by Stimmt AG a 15person Swiss consulting firm specializing in user interfaces shows whyThe company was struggling with a Lotus Notes platform that had to run on a mix of Windows and Mac laptops requiring almost constant maintenance And the fact that Stimmts employees often work from their homes or from customer sites requiring timeconsuming replication of Notes data made supporting the Notes infrastructure a bigger burden than a small firm like Stimmt could shoulder Moreover there was little hope of collaborating with customers remotelyWhen management finally deemed the situation untenable in 2007 it considered two options a fully managed centralized IT environment hosted by a third party or a browserbased SaaS approach Eventually Stimmt selected the SaaS path for a few reasons Not only was the pricing more attractive but Stimmt would benefit from constant and seamless updates to whatever tools it selected and SaaS applications would afford employees the most flexibility in creating their own work environmentsSuite of SaaS tools Delivers Flexibility and SimplicityWith the decision made to standardize on SaaS applications Stimmt chose a few strategic tools that have become its de facto computing platform Google Apps are used for email and calendaring an ondemand version of Confluence Wiki enables internal and external collaboration data storage and the creation of realtime executive dashboards Genius Enterprise Project supports project management and Longjump a lowercost alternative to Salesforcecom less than onesixth the monthly subscription fee per user serves not only as Stimmts CRM system but also as an ondemand application development platform that allows the company to build its own ondemand apps All of these tools are accessible to employees wherever they are through a browser and access to collaboration workspaces is easily provided to customersIf it sounds to securityobsessed technology executives like Stimmt threw caution to the wind to an extent it did – by its own design Founding Partner Lukas Karrer was confident that once the data was stored in this array of cloud applications he could trust the selected vendors to ensure the safety of Stimmts data With employees given a fixed budget to choose their own hardware and establish their own remote infrastructures he didnt feel there was any need to worry All they need is a working browser says Karrer I dont care about the rest I dont really care about the laptops or desktops our employees use I dont care about the backups of the employees laptops because everything is in the cloudThere were no setup costs for starting the various application subscriptions It took Karrer and one employee just two weeks last December to complete the whole migration process extracting the most important data from Notes and importing it into the new tools then configuring the tools to meet Stimmts basic needs Karrer estimates the company invested about € 5000 and 20 full days of employee time adjusting and customizing the tools to suit the firms business processes He continues to devote about one day a month to managing those customizationsElegant Environment Boosts Employee Morale Impresses CustomersThe results of Stimmts SaaS/collaboration strategy have been transformative The companys newfound mobility allows data access from any location The scalable business environment makes it a snap to add new people and subscriptions can be added or canceled on the fly The company is getting more work completed and its doing so more simply and efficiently And all this added functionality hasnt forced any additional technology spending I didnt reduce IT costs but I greatly enhanced functionality and I greatly enhanced usability for our employees and clients says KarrerAdoption hasnt been an issue The staff had grown so frustrated with Notes that team meetings had digressed into a stream of complaints about the companys technology Employees have embraced the new suite of tools and the complaining has ceased causing an ecstatic Karrer to proclaim Morale has skyrocketedThats not to say there havent been lessons to learn along the way Karrer says hes seen that some SaaS vendors seem reluctant to evolve from the shrinkwrapped software mentality They need to adjust their approaches to servicelevel agreements and be quicker to incorporate customer feedback given the inherent flexibility of their products But hes also learned that SaaS enables him to be much more experimental as correcting mistakes is much simplerPerhaps the best – and most unexpected – result is the feedback from Stimmts customers who are all in awe of the elegance of the firms technology environment Its an ideal imagebuilder for a company that specializes in helping clients design usable interfaces They can see that its so easy to work with these tools says Karrer Youve got to practice what you preach2 The Impact of Technological Revolutions21 IntroductionWe have seen that there is considerable pressure on companies to improve performance especially in this economic climate In this book we are looking at collaboration the very essence of organizations and cloud computing which is a model for provisioning technology So how does technology help business and how does business benefit from changes in technology In this chapter we will show what the effect is of technological revolutions and we will demonstrate that these are indeed special times that warrant a closer look at collaborationManagement guru Peter Drucker 1909–2005 was originally a writer In 1943 he was asked by the management of General Motors to report on workfloor practices However numerous employees kept a wary eye on Drucker They greatly mistrusted him They were especially worried about what he might say to management behind their backs Surely this could only have negative consequences To gain their cooperation Drucker promised them that all his observations would appear in book form The tale of the work floor would therefore not be mindlessly filed away in a report In the end the publication resulted in his bestseller The Concept of the Corporation a book in which Drucker elaborates his farreaching ideas about decentralized decisionmakingPeter Drucker was a visionary someone who was far ahead of his time and one of the few who had a clear perspective on the future of companies Many other works have come from his pen over the years The central concern of his writing remains the manner in which management has to change its decisionmaking practices by placing progressively greater trust in the observations and decisions emanating from the work floor and streaming up to the organizations higher layers Such changes in management thinking are often underpinned by technological innovation Drucker elaborated on these ideas in his 1993 The PostCapitalist Society  encapsulating them in the following statement Every few hundred years in Western history there occurs a sharp transformation Within a few short decades society rearranges itself its worldview paradigm its basic values its social and political structures its arts its key institutions Fifty years later there is a new world22 The Delicate Balance Between Technology and CommunityTechnology changes society This mostly occurs in very small ways which are nearly imperceptible but sometimes they are groundbreaking and immense turning the entire world on its head Such upheaval occurred with the introductions of the train car airplane steel industry and steam engine Each had an enormous and revolutionary impact on societyWe are now once again standing on the verge of a fundamental transformation a paradigm shift such as it is elegantly labeled that will change the world as we know it for good The internet and its underlying technology are responsible for this radical upheaval The way in which we search for information and share it with each other has changed online search engines and Wikipedia have become the accepted instruments for this activity Listening to music is now a different experience than it has been in the past No longer do we collect vinyl records or plastic CDs now we use online stores in order to place our music collection of thousands of songs on our portable mp3 players Hyves MySpace Facebook Twitter and FriendFeed are the new ways of briefly communicating with each other These social networks have replaced what has now become traditional email We no longer buy books in the bookstore but acquire them from Amazon We set up our own store with the help of eBay And some time ago we stopped watching television by sitting in front of a colored screen at a scheduled time instead we individually click on our favorite programs at YouTube use TiVo the TV stations website or download them often illegally from peertopeer networks The Convergence of KnowledgeCompared to previous technological revolutions there is one big discernible difference current technology is causing various areas of knowledge to merge The exact consequences of this fusion are still unknown but it is certain that these effects will be felt by people companies and organizations in general Our entire society is affected Figure 21 Web Science Research Initiative Map of Fields of KnowledgeFigure 21 comes from the website of the Web Science Research Initiative  set up by World Wide Web founder Tim BernersLee This organization aims to chart the ways in which the internet is changing our society and it does so by examining how various fields of knowledge are unifyingOne of the direct consequences of this evolving merger of knowledge is that we are rediscovering people members of society with whom we lost touch long ago Long before the industrial revolution the farmer and the baker knew precisely what they might expect from each other The farmer worked the land and the flour from his harvested grain ended up at the baker who then baked the farmers bread If the farmer were not satisfied with the taste of the bread he would complain directly to the baker in order to have him modify the recipe The interaction was an entirely simple form of collaboration based on direct communicationThe industrial revolutions fascination with maximum efficiency made sure that people only worried about their own tasks and never or seldom got together to deal with all the types of problems on the work floor The balance between technology and community was disturbed so that it tilted to the advantage of technology As the German thinker Karl Marx astutely states in his book Das Kapital 1867In handicrafts and manufacture the workman makes use of a tool in the factory the machine makes use of him As a direct consequence of this change people grew distant from one another This dissociation undoubtedly presented business operators with an enticing opportunity In gaining control over the new technologies they could seize power and inflict their whims and fancies on their customers Companies could impose their will on consumers by claiming to know what was good for them The first assembly line enabling Henry Ford to sell an enormously large number of cars is a perfect example of this thinking In a sense he invented the wheelInternet technology is shifting the manner in which companies and people communicate and collaborate with each other back to the more even keel that we had previously enjoyed Consumers are being taken more seriously Their voices are being heard Once again there is a dialogue between both parties between producer and consumer The prosumer – a term launched by Alvin Toffler  to describe consumers who can fill their own needs using technology – is a consumer who countsConsumers are Demanding Unique ExperiencesFor some time now consumers have no longer been fixated on mere possessions but have become concerned with total and unique experiences Consumers are demanding a say in the processes that ultimately yield goods or services The system is the product  Consumers strive to play a part in this process of collective value creation A product service or brand must be customized so that it contributes to their personal identityCall them the weapons of mass collaboration These changes among others are ushering us toward a world where knowledge power and productive capability will be more dispersed than at any time in our history – a world where value creation will be fast fluid and persistently disruptive A world where only the connected will survive A power shift is underway and a tough new business rule is emerging harness the new collaboration or perish We are discussing collaboration and technology In particular we talk about the ways in which technology can be used to facilitate collaboration not just among people but also among companies and even applicationsAs indicated above technology is causing us to enter a new phase in collective interaction changing our society for good To properly understand what this new form of collaboration looks like we will first examine the past and study the consequences that a new technology has for people commercial companies or any other kind of organization23 Technologys Poisoned ChaliceThe introduction of new technology always generates resistance In the beginning a discovery is only embraced by a small group of people When more and more people adopt the technology the tipping point  is ultimately reached and the technology becomes commonplaceFor a technology to be widely accepted it must first overcome several hurdles The introduction of a new technology renders another technology obsolete The companies that were profitable as a result of this older technology will not easily give up their market share sticking to old technology and inhibiting innovation Every innovation also has its advantages and disadvantages some of which are not always readily foreseeable and only become discernible at a later stage One of the first people to reflect on the negative consequences of new technologies was the philosopher SocratesLegend of King Thamus and the God ThothAbout 370 years before the Common Era according to the Western calendar Greek philosopher Plato 427–347 BC committed to papyrus his account of a dialogue between his teacher Socrates 470– 399 BC and a certain Phae drus In this dialogue Socrates discusses the legend of King Thamus and the god Thoth who was renowned as a great inventor According to the Egyptians Thoth was the founder of knowledge religion philosophy and magic The Greeks later added an even more impressive list of discoveries According to them he alone Figure 22 Thoth8was more or less responsible for the origins of all fields of knowledge including astronomy astrology mathematics geometry medicine theology reading and writing All these disciplines were said to have sprouted from Thoths brainThoth did not want to keep all knowledge to himself He wanted to share it with humanity In an audience with King Thamus he tried to convince the king of the virtue of his latest discoveries Thoth was especially enthusiastic about writing According to him writing would improve both the memory and the wisdom of the Egyptian people To the gods dismay the king showed no interest In fact he said to ThothMost ingenious Thoth one man has the ability to beget arts but the ability to judge of their usefulness or harmfulness to their users belongs to another and now you who are the father of letters have been led by your affection to ascribe to them a power the opposite of that which they really possess For this invention will produce forgetfulness in the minds of those who learn to use it because they will not practice their memory Their trust in writing produced by external characters which are no part of themselves will discourage the use of their own memory within them You have invented an elixir not of memory but of reminding and you offer your pupils the appearance of wisdom not true wisdom for they will read many things without instruction and will therefore seem to know many things when they are for the most part ignorant and hard to get along with since they are not wise but only appear wise The legend of King Thamus and the god Thoth does not just present the positive or negative consequences of writing It also draws attention to the possible destructive impact of technology on communities and on humanity in general A new technology can either provide a community with an enormous boost or bring about its immediate destructionTechnology is a TyrannyIn recounting this tale Socrates anticipates the ideas of French sociologist philosopher and theologian Jacques Ellul 1912–1994 who published a book called La Technique ou lEnjeu du Siècle in 1954 the English title is The Technological Society In this work Ellul explains how he regards technology as an element that disrupts society In his eyes technology is a tyranny for humanityWhat we are witnessing at the moment is a rearrangement of the world in an intermediate stage the change is not in the use of a natural force but in the application of technique to all spheres of lifeTechnology Leads to SelfAmputationThe prophet of our electronic age Marshall McLuhan 1911–1980  made a similar pronouncement in 1964 when he coined the maxim the medium is the message  According to McLuhan the content of the message is not very important Rather the underlying technology the medium has far more significant consequences for the proximate surroundings We shape our tools and thereafter our tools shape usIn McLuhans view technology is an extension of the human body For instance the car has replaced peoples feet Thanks to the car we are able to move from A to B much more quickly while also being sheltered from a heavy downpour Increased mobility and comfort are certainly two of the most evident advantages of using a carUnfortunately technology also has disadvantages When a technology is used excessively and even to the point of overuse it results in a form of selfamputation that unquestionably has negative effects In the case of the car driving has led to less walking reduced muscle strength in the legs and correspondingly augmented problems in relation to health and obesity The number of fatal and nonfatal accidents has also consequently risen due to motorvehicle use And the air we breathe is contaminated by the large quantities of exhaust spewed out by the internal combustion engine giving rise to all types of lung disease Technology does not therefore only affect us as individuals but it has consequences for the community as a whole24 Six Technological RevolutionsTechnology has a large impact on people and organizations commercial or any other kind and therefore on society at large An uneasy equilibrium exists between technology and community In the past the effects of new technologies on society have been studied by various researchersOne of the first scholars devoted to this field was the Russian economist Nikolai Dmitriyevitch Kondratiev 1892–1938  At the beginning of the twentieth century he investigated the relationship between the price of goods and investment behavior His research based on data from the period of 1789 to 1922 revealed a series of four wave movements in the economy each with its own peak and valley boom and bust Strikingly these trends displayed consistent features each of the cycles encompassing a period of fifty to sixty yearsKondratiev announced his findings to the world in The Major Economic Cycles a book published in 1925  His research clearly demonstrated that the waves were based on the accumulation of fundamental innovations each underlying a corresponding technological revolution Technological development was not evolutionary but revolutionary as it occurred in jumps associated with fundamental transformations of industry and the economy which affected all of societyHis insights which were controversial at the time did not earn him any gratitude In 1938 Josef Stalin gave the order for the scholars arrest and he was executed shortly afterwards P 1SPTQFSJUZ R 3FDFTTJPO	D FQSFTTJPO E *NQSPWFNFOUFigure 23 Kondratiev Wave A fifth wave has since been detected and there are now five identifiable historical trends The first boom period approximately encompasses the years from 1780 to 1815 an era that saw basic innovations in the textile industry the use of water power and the construction of ports canals and paved roadsThe second peak more or less coincides with the period from 1845 to 1875 and involves such basic innovations as the railway gas lighting and the telegraphThe third upswing roughly covers the years between 1890 and 1916 involving innovations in the electronics and automobile industry as well as the emergence of chemistryThe fourth surge corresponds to the postwar period of 1944 to 1985 with its rapid proliferation of longlasting household consumer goodsThe fifth wave which will likely cover the period from 1995 to 2020 is driven by the innovations involved in numerous IT applications Each Revolution takes 40 to 60 years to spread across the world and reach maturityEach begins in a core countryFigure 24 Five Technological Revolutions in 240 YearsA great more detail about these five technological revolutions is provided by Carlota Perez in her book Technological Revolutions and Financial Capital  Her work is strongly influenced by the great economist Joseph Schumpeter  who was well known for his theory about Creative Destruction the fact that the old ways of doing things are endogenously destroyed and replaced by new ways Perez even adds a sixth revolution the approaching revolution that will be brought about by bio and nanotechnology Technology Changes Ways of CollaboratingA technological innovation requires time in order to become commonplace in a society In the beginning only a limited group of people will use a given technology Only time will tell whether this technology will or will not be embraced by everyone That is why all technologies have their own adoption curves Figure 25 Adoption of Technology by Consumers When a technology catches on there is a chance that it will transform an entire society But a technology only has such an impact once every fifty to sixty years On these rare occasions it is not only society that is impacted but the manner in which people are accustomed to working together is also radically transformedIn Why the Demise of Civilization May Be Inevitable  Deborah Mackenzie gives some thought to the ways in which society is changed by technology Figure 26 displays a number of diagrams representing the ways in which collaborations evolve Figure 26 The Changing Shape of SocietyFigure 26 makes it clear that two types of collaboration are dominant The traditional hierarchical forms such as those that came into vogue with the industrial revolution and the network form which is now coming into use as a consequence of the emergence of the World Wide Web In terms of time we are currently in a transition phase in which companies are mostly adopting hybrid formsTo understand the manner in which collaboration has evolved over time and the ways in which collaboration is now being reexamined the following sections will reconsider two important economists who were responsible for the development of the very first corporate structures and the modes of collective work adopted within the walls of these organizationsThe Invisible Hand of Adam SmithTraces of an evolutionary theory based on natural selection can already be found in early works on the economy The magnum opus of economist Adam Smith 1723–1790 entitled An Inquiry into the Nature and Causes of the Wealth of Nations uses something akin to natural selection to explain the workings of the free market This book was first published in 1776 just after the start of the industrial revolutionIn the view of Adam Smith the interests of the individual do not come before the interests of the collective To his mind collaboration occurs automatically One person works as a farmer the other as a baker Together they help each other earn their livelihood Working together comes naturally It manifests the effects of the invisible handTaylorismFrederic Winslow Taylor 1856–1915 was the first person who emphatically departed from this manner of collaboration which people had known for centuries He let go of Adam Smiths invisible hand and took up the part of the rigidly organized system in which employees were given little latitude	Figure 27 Adam SmithMoney Makes the World Go Around Taylor was born in 1856 in Germantown a part of Philadelphia Pennsylvania He grew up in an affluent family and was not surprised about the fact that there were different classes of people in the world When he reached the age of twelve his father took the entire family to Europe for a trip lasting three years Such a European tour was not unusual for wealthy people at the time During this tour Taylor learned for the first time about the ways in which money can be employed as a means to influence another persons behavior In crossing the high 	Figure 28 Frederic Winslow Taylor22mountains of the Alps the family was stranded in the village of Finsterminz23 The local bridge had been largely swept away preventing access to the pass Over time the local population had made a few halfhearted attempts to reach the pass none of which went very far The family would have been unable to complete its crossing of the Alps except that failure was not a part of the vocabulary of Frederics father Pulling out his wallet he was able to motivate the residents of the village so that the entire family and their baggage was on the other side of the pass the next dayI have you for your strength and mechanical ability and we have other men paid for thinkingIn 1911 Taylor published his revolutionary ideas about management In his book The Principles of Scientific Management he explains how business processes must be managed in a scientific manner in order to promote standardization and efficiency His theories presented companies with the means of skewing the balance of power in their favor The manner in which consumers and producers had been accustomed to dealing and working with each other was consequently brought into questionThe Film Modern TimesAlthough Taylors original objectives where quite idealistic a great deal of opposition was mounted against his theories Putting them into practice required a farreaching division of labor with little concern for peoples social needs The result was increasing selfalienation the top layers of Maslows pyramid no longer being attainable and alienation from the end product One of the best known critical views of such relentless industrialization is undoubtedly the black and white movie Modern Times by Charlie Chaplin In this movie Chaplin plays Figure 29 Modern Times24 his trademark character the little tramp who this time is an assemblyline worker being subjected to the torments of the modern machine It is interesting to note that the idea for this movie was taken from a journalist who told Chaplin about the depressed assembly line workers in a factory in DetroitIndustrial RevolutionThe industrial revolution caused the balance the market equilibrium in the demand and supply model to shift in favor of producers The use of machinery enabled organizations to grow in size and scale allowing them to serve more customers simultaneously The process caused employees to lose touch with their end customers Since they were cut off from direct feedback they had to rely on others who were closer to customers to coordinate the effectiveness of their efforts and to indicate how they might better satisfy customer wantsThe previous intimate collaboration between producer and consumer began to fracture and eventually fell apart Producers no longer had their ears attuned to their customers but took control themselves From then on producers of goods and services assumed that they best knew what consumers wantedDeploying the resources of various mass media newspapers radio and television they imposed their will on consumers and were able to manipulate them into making purchases After all producers felt that they knew best They very well knew or so they thought what consumers wanted and how these wants were to be satisfiedEfficiencyWith the growing size of organizations and resultant division into departments and duties employees became increasingly further removed from the ultimate end product The entire business process was directed at the most efficient realization of this good or service As a result employees had to specialize in order to take responsibility for just one small task Employees were seemingly turned into nothing more than cogs in a machine small replaceable parts to be thrown on the scrap heap when expendedEverything and everyone was placed in service of the end product while any interests of the individual were subsequently disregarded This change on the work floor reduced the feeling of responsibility that workers had felt for the quality of the end product They also became increasingly alienated from coworkers and even from their own sense of self The overview of the entire business process was more difficult to maintain making it no longer possible to intervene when calamities occurred Likely Jacques Ellul the abovementioned French sociologist of technology had this in mind when he coined the term soulless efficiencyThe inescapable conclusion is that the effort to realize the advantages of scale and efficiency caused workers to lose contact with customers To put it simply there was no longer any time for collaboration between producer and consumer The essential input enabling the craftsmen of former times to be largely selfdirecting was missing and the corresponding expansion of the management class therefore became inevitable25 ConclusionThroughout history we encounter various forms of collaboration both inside and outside organizations Technological innovations often underlie the transition from one form to another these revolutions disrupting the balance between old and new between technology and the communityThanks to the World Wide Web we have again entered a transitional phase More and more companies are abandoning centralized hierarchical organizational forms and are switching to a model that uses decentralized network structures even extending beyond company walls Another catalyst for this change is the fact that in contrast with previous transformations diverse fields of knowledge are now undergoing a process of convergence For the first time in human history everyone has nearly unimpeded access to all information The resulting impact of the internet on society is therefore larger than anything previously experienced and consequently distinguishes the present from all previous phases ie technological revolutionsThe following chapter will further explore the effects that the new technology is having on society companies and humanity in general A distinctive feature is that all parties are virtually unable to hide anything anymore – NO MORE SECRETS We are entering an age in which transparency openness and crossboundary collaboration will be crucial for the continued existence of companies Case  REAAL VerzekeringenCase REAAL Verzekeringen Turns to Collaboration as It Contends with Seismic Changes in Its BusinessWholesale Evolution of Channels Pushes Company to Work DifferentlyAfter nearly 120 years in business REAAL Verzekeringen the €4 billionayear insurance arm of Dutch financial services group SNS REAAL has seen a profound shift in the way insurance is delivered The traditional approach of relying on brokers as the main distribution channel is giving way more and more to webbased selfservice sales through larger partners who offer complementary services and growing opportunities as a value added service sold by SNS REAALs banking unitThe facetoface relationship between brokers and consumers is being replaced by electronic channels and that means the services that are so crucial to an insurance company – such as providing quotes and processing claims – are increasingly reliant on technology for their delivery Because REAAL acquires Insurance companies to spur growth it also needs to speed up the process of integrating acquired companies into its infrastructure That means the technology has to enable the collaborative processes that support such key business activities Throw in the fact that growing numbers of employees are working from home and all these processes have to be extended to support coordination among multiple locationsThis fundamental transition has highlighted the need for REAAL Verzekeringen to look for new more efficient ways to get things done Specifically it is injecting collaborative technologies into its business processes The company needs its employees to be able to more effectively brainstorm new products coordinate crossselling efforts and process claims so the company is supporting its employees in these efforts by making it easy for them to share information and knowledge across organizational and geographic boundaries It also wants to provide an external collaborative environment where it can work with partners to efficiently pair products and servicesEnabling a New Way to WorkTo accomplish this REAAL has initiated a broad initiative to achieve the New Way to Work In this initiative there are many projects that will change and improve the way REAAL works It will change the way in which people work together and how the company will innovate The program addresses anything from the physical environment buildings physical workspace etc to the social and HR aspects A relatively small but important part of this broad initiative addresses the technology used to collaborate For this part REAAL turned to Microsoft technologies SharePoint/ Office Communications Server It embarked on this threeyear effort to establish a collaboration infrastructure six months ago when it made an early incarnation of its SharePoint environment available to a strategic project group of 250 employees The company has 1015 people working on a daily basis in conjunction with consultants and experts from Microsoft to flesh out the design of an environment that will support all of SNS REAALs 7000 employees Plans call for the first full implementations to start rolling out in the first half of 2009 with pilots centered on departmental groups of 80100 people Barring unforeseen problems REAAL intends to proceed with a largerscale rollout later in 2009 and into 2010–2011Potential Internal and External Benefits Becoming ApparentEarly indications of the impact of this technology havent done anything to slow the effort says Kees Tuijnman enterprise architect at REAAL One of the benefits Tuijnman sees is that the technology is already enabling improvements in the information flows The resulting benefits have fueled optimism for what the results of the overall program will do for the companyThe IT department which was an early adopter of some of the elements of this New Way of Work was also among the first to see what can change For instance IT relies upon a structure in which employees with similar skill sets – such as software development or design – are grouped together That makes it difficult to transfer those skills between groups The new technology and new focus on collaboration helps IT to work in virtual teams in which the various skills are clustered together making IT projects a more collaborative pursuitAs the program is introduced throughout the company the effects are expected to become visible including cost reduction productivity improvements more effective talent acquisition more efficient knowledge sharing and a boost in employee satisfaction The company is not only looking to improve the internal workings of the organization but is also extending their vision to external collaborators We have a number of large distribution partners with whom we connect selling processes and work on innovation says Tuijnman We want a better exchange of ideas and a way to implement those ideas into new products3 The New Nature of the Firm31 IntroductionIn the previous chapter the ambiguous role of technology was discussed in light of several converging trends We saw that these trends spell great changes that only happen every fifty or sixty years In this chapter we will bring the discussion closer to the organization and explore what competition and collaboration look like and how they could change We will see how collaboration might in some cases even take the place of competition changing the essentials of survival and competition foreverLife is one long struggle wherever you look Elements of competition are to be found everywhere The survival drive is not only deeply rooted in the animal kingdom around us but it also permeates our own society Of course it is most evident in the area of sport but we also find it in politics religion education the business community and language goodbetterbest And even in art Why is one painter a great master and another one notIn this environment there is always the urge to score or to exhibit that you are better than others often at the cost of others There are good reasons behind the impulse to kill or be killed One of the first persons to study this phenomenon in detail was the biologist and naturalist Charles Darwin 1809– 1882The Voyage of Discovery on Board HMS BeagleCharles Robert Darwin first became interested in science during his medical studies in the Scottish city of Edinburgh which he discontinued after two years His father then sent him to Cambridge to pursue religious studies While in Edinburgh Darwin had developed an interest in animal life One of his great passions was to collect and categorize all types of beetles At Cambridge he was given the chance to pursue his passion by taking courses in botany and geology in addition to studying theologyAt the end of 1831 Charles Darwin obtained a position as a naturalist on the British naval ship HMS Beagle His task would be conducting geological research on SouthAmerican coastal regions After convincing his father about this unique opportunity he was given permission to interrupt his study enabling him to depart on the Beagle from Plymouth harbor on December 27 1831The boat was commanded by Captain Robert Fitzroy 1805–1865 who had been given the task of surveying the southern point of South America in more detail The commission was expected to take two years However the trip lasted longer than expected The voyage included visits to New Zealand Australia and South Africa in addition to South America The ship didnt return to England until 1836 five years after its departureWhen he departed Charles Darwin was only 22 years old No one could have suspected what an enormous impact he would have on the history of humanity It was on board the Beagle that Darwin was inspired to formulate his theory of how evolution works via natural selection which is still considered one of the most important scientific works ever writtenFigure 31 Charles Darwin How a Finch Explains Human EvolutionThe Galapagos Islands an archipelago off the west coast of South America played a crucial role in the creation of Darwins theory To Darwins surprise different species of finches were living on the various islands How was it possible that such great diversity could have arisen among these finches when the islands lay so close togetherSince it could be assumed that the birds flew from island to island in their search for food distinctions among the birds should not occur Yet observa tion proved otherwise The birds on each individual island had distinctive beaks Darwins research demonstrated that the different food consumed by the birds on each island was responsible for the different beak formsA Finch Eats What Its Beak Can FetchEach different type of beak had developed into a unique instrument designed to consume the available food in the easiest manner One finch had a sharp pointed beak in order to pick seeds out of pinecones another had a short beak to facilitate the plucking of insects from branches The finches had adapted to the conditions in which they were living Over time they had evolvedThe Origin of SpeciesThree years after the Beagles return the first edition of Charles Darwins On the Origin of Species by Means of Natural Selection appeared bearing the subtitle or the Preservation of Favoured Races in the Struggle for Life  Darwin first committed his theory of evolution to paper in this book He explains how natural selection is responsible for the fact that life on earth is divided into various species In the struggle for survival the individuals best adapted to conditions around them will survive and reproduceThe publication of this book exposed Darwin to attacks from all angles In particular the Catholic Church took steady aim at his ideas regarding his book as a specific challenge to the existence of God After all Darwin was arguing that humans were not the offspring of Adam and Eve but the descendants of apes Only after his death in 1882 was the value of his views permanently recognized His fellow scientists made sure that Darwin received a state funeral in Westminster Abbey Eight years after his demise the Royal Society of London even established the prize for scientific work that bears his name Darwin Medal and is still being awarded annually Incidentally another award that bears his name the Darwin Awards3 are much more frivolous and are posthumously awarded to people who died doing stupid things thereby proving themselves unfit for reproduction and removing their genes from the gene pool at the same timeDarwins theory of evolution has radically transformed our worldview portraying a world with which not everyone could identify Captain Robert Fitzroy was literally ruined by Darwins ideas which contradicted his strong religious devotion The idea that he had played an important role in Darwins life and accordingly in the development of his theory of evolution drove him mad and ultimately to suicideThe Right of MightDarwins book popularized the notion of survival of the fittest  In fact the term was originally coined by the economist Herbert Spencer In his 1851 book Social Statics Spencer states that as long as the government does not intervene the best qualified read richest people will ultimately survive and a super civilization will be created from which the weaker groups have been eliminatedDarwin adopted this economic survival of the fittest theory in explaining how natural selection functioned He always meant that species adapt to ie fit better with conditions in order to survive If they are unable to adapt then they simply become extinctIt is not the strongest of the species that survives nor the most intelligent that survives It is the one that is the most adaptable to changeHowever the notion survival of the fittest took on its own life after publication of his book Increasingly often the emphasis was placed on the fact that survival was only a question of winning a struggle between one individual and another Might is right so to speak Darwin did not however have this meaning in mind In fact his work demonstrates that evolution does not only operate through survival by fighting but and perhaps predominantly by mutual collaboration among various speciesIn the long history of humankind and animal kind too those who learned to collaborate and improvise most effectively have prevailedNot Fighting but CollaboratingThe biologist Lynn Margulis also disputes the notion that a given species can only evolve at the cost of others Evolution does not just involve competition it also involves collaboration interaction and interdependence of organisms She reflected on forms of collaboration in nature as long ago as 1966 discussing them in her paper The Origin of Mitosing Eukaryotic Cells In it she demonstrates among other things how cells can be created from the symbiosis of various species of bacteria Since then other theories have emerged as to how multicellular organisms started as complex collaboration between singlecelled organisms Lynn Margulis subsequently married the worldrenowned astronomer Carl Sagan From this marriage she gave birth to two sons one of them being the sciencefiction author Dorion Sagan The two of them copublished her 2001 book Marvellous Microbes a work in which she refocuses on the fact that evolution is not driven by competition but by cooperation Life did not take over the globe by combat but by networkingOn Bullthorn AcaciasLiving networks not only occur at the cellular level but also at higher levels of complexity among various species A good example is the story of the bullthorn acacia Acacia cornigera a plant that grows in the tropical rainforests of South America The species is named for the large hollow thorns on its branches but is best known for the symbiotic relationship that it shares with creatures in its proximate environment In particular the hollow thorns provide ideal shelter for a species of ant Pseudomyrmex ferruginea and the material that the plant discharges is an excellent food source for these antsIn exchange for this food the ants protect their host against attacks from outside As soon as other animals attempt to feast on this plant the ants alert each other by dispersing a pheromone and join in the struggle against the outsider en masse Possessing a long and hard tongue the insects are even able to frighten off larger animals by giving them venomous bitesFittest Through CollaborationThere is a lot of collaboration in nature and in some instances species that collaborate with sometimes unlikely partners stand a better chance of survival Ants and the bullthorn tree have created an intricate interdependency for the benefit of both Fierce lions hunt together with other lions They are partnering to catch bigger game than they ever could alone but in return they will have to share the prey and obey group etiquette Zebra like to flock together with giraffes to make use of the giraffes long necks if the giraffes run so will the zebra They have outsourced the lookout function so to speak for free evenWhatever the nature of collaboration or cooperation between one or multiple species there is always a benefit to the survival or reproduction of at least one of the species Species that collaborate do so to survive a harsh world or they collaborate to compete for scarce resources Whenever the environmental pressures are high when there is great scarcity or a lot of competition collaboration is a winning approach in the game of life It all comes together in the term ecosystems where togetherness and interrelatedness are much more important than the element of competition Could this also be true for the market as an ecosystem of companies consumers and suppliers32 A Society of ConversationsThe previous chapter suggests that the industrial revolution put an end to the ways that humans had been working together for centuries The vitality was taken out of work The individual became only a small cog in the machinery of the assembly line Everyone had a job to do without having to think too much about their surroundingsWith the introduction of the internet this type of thinking has slowly begun to change Thanks to this new technology people or consumers have obtained a platform by means of which they can compel companies to take their wishes into account Companies must collaborate with consumers if they want to continue in business at allIn April 1999 four internet pioneers Rick Levine Christopher Locke Doc Searls and David Weinberger foresaw this turn of events They formulated their vision into a manifesto containing 95 avant garde propositions in which they predicted how the business world would be irreversibly transformed in the near future The manifesto was published on a website named for the manifesto http//wwwcluetraincom A transcript was quickly published as a book in 2000 under the title The Cluetrain Manifesto The End of Business as UsualA SelfEvident TruthThe very first statement reads Markets are conversations This truism should almost go without saying After all everyone has long known that discussion and dialogue underlie all markets Still they often forget about this basic fact over the course of time When we look back on the last century we immediately see that the twentieth century was dominated by only a handful of men companies and countries These were the elite so to speak This elite was responsible for transforming markets into mere monologues They determined the message that people were allowed to hear acting like a corporate dictatorshipAt the dawn of the twentyfirst century the abovementioned manifesto made it apparent that a fundamental leap forward was about to occur The book was the first to proclaim the enormous potential of the World Wide Web A new world was dawning And this world would no longer allow itself to be dominated by a small group of people using outdated techniques and strategies This new world demands a new approach a new form of management a new way of doing business a new manner of collaboratingMarkets Are ConversationsMarkets have always been conversations They are places where demand and supply come together in order to determine the prices of goods or services In an ideal market producers and consumers have equal power to affect the interplay of supply and demand In an ideal market producers and consumers collaborate That is the utopian ideal which is not always what happens in practiceTraditionally the interaction of supply and demand is the cornerstone of the valuecreation process In the preindustrial age this process of value determination functioned well because producers and consumers had access to the same information To put it more strongly because they mostly lived in small communities villages producers and consumers could meet every day for discussion and negotiation Consumers were able to provide direct and immediate commentary and the producer could take immediate action Producers and consumers had an especially intimate relationship which ultimately meant that they worked together to determine the value of a good or service A simple handshake was then sufficient to indicate that both parties were satisfied Figure 32 Market EquilibriumNaked ConversationsConsumers are in the midst of a conversation that isnt ours The race is on to grow ears to learn what they are saying – John Hayes CMO American ExpressWith the dawning of the information age a transition is underway Information technology gives consumers their voices back Using all types of communication blogs microblogs forums wikis social networks chatting etc they are again able to express their dissatisfactionIn his book Naked Conversations Robert Scoble shows how businesses are being dragged into these conversations almost against their will  After all companies cannot afford to ignore a discussion when it involves their brands Before they know it the viral effect of the web can create an enormous thunder of negative voices against which the companys soothing words go unheardRobert Scoble focuses his book specifically on the impact of weblogs on organizations The internet will not stand still and is in fact gaining momentum With the boost of Web 20 technologies a tremendous array of new communication and collaboration options became available for companies all becoming part of one continuous conversation online Figure 33 of the Conversation Prism by Brian Solis and Jesse Thomas shows what the current landscape of communication channels looks like  For a fullsize version of the diagram refer to Figure 55 Figure 33 The Conversation PrismIt is striking that conversations are now discernibly shifting toward the lifestreaming phenomenon  More and more users are employing communication tools to share stream their lives with family friends and acquaintances This burgeoning popularity further increases the complexity facing companies that are trying to participate in these discussions How can corporations keep up with this type of fast talk How can organizations make the transition from the mechanical age of speed to the digital age of real time Companies are now being forced to play with all their cards on the table Transparency and truthiness  have become the key terms for companies making contact with their customers No more secrets Early warning about the places where web discussions concerning a companys brand are occurring is now essential Such advanced detection enables companies to participate in this discussion right from the start The discussions then make their way inside company walls and collaborations are struck with outsiders especially those who are most critical of the company We are moving from a conversation economy into a conversation societyNicholas Carr comments on these naked conversations in his book The Big Switch  He notes that By shifting power from institutions to individuals information processing machines can dilute and disturb control as well as reinforce it It is therefore very possible that when companies get involved in the new social game they do so in order to recover power once again Perhaps this even occurs surreptitiously without consumers catching onFrom Speed to Real TimeWe might already be beyond the age of speed by moving into the age of realtime – Ivan Illich 1996The fact that the relationships between consumers and companies are fundamentally changing as a result of information technology means that companies have to approach their business processes differently The struggle for efficiency is no longer as important while real time is playing an everlarger role Organizations must change from unwieldy rigid bureaucratic monsters into flexible and adaptive organismsChange is the process by which the future invades our lives – Alvin TofflerWhat distinguishes our age from all those that have preceded it is the exponentially increasingly rate of change Companies can no longer be complacent They must respond directly to the changes in their environment The resulting complexity means that no company can operate on its own any longer It is impossible to take on this new world alone The various members of the business community must learn to think differently they must join hands and work together This collaboration can only succeed if each company makes the best possible effort Every company needs to concentrate on its own qualities and skills – on its own core competencies When each company is prepared to change its thinking as described above then the sum of the parts will exceed the wholeIn the mechanical age now receding many actions could be taken without too much concern Slow movement insured that the reactions were delayed for considerable periods of time Today the action and the reaction occur almost at the same time […] The restructuring of human work and association was shaped by the technique of fragmentation that is the essence of machine technology The essence of automation technology is the opposite It is integral and decentralist in depth just as the machine was fragmentary centralist and superficial in its patterning of human relationships — Marshall McLuhan33 Management 20The Future of ManagementThe emergence of information technology means that companies no longer have to operate in a landscape of continuous change on their own Collaborating with others makes it easier for them to respond to the continuous changes occurring in the world around them This world no longer stands still for any length of time A new market has been created which is open twentyfour hours a day and seven days a week Company management must therefore adjust their strategies to suit this contemporary aroundtheclock time In The Future of Management Gary Hamel examines the ways in which managers function inside organizations  He says the following on the subject Managers focus on the value chain the flow of producs and services through the activities the company controls or influences … Managers should focus on the quality and the experience of cocreation not just on the quality of the products and services of the companyHamel also examines the ways in which management has been keeping pace with change in recent decades Compared to the the enormous changes in technology lifestyle and geopolitics of the last fifty years management seems to have developed at a snails paceNew Age of InnovationThe most recent book by CK Prahalad and MS Krishnan The New Age of Innovation provides a detailed description of the reforms required of management Prahalad formulates the new ways in which companies will have to operate in the near future as followsValue is based on unique personalized experiences of consumers Firms have to learn to focus on one consumer and her experience at a time even if they serve 100 million consumers The focus is on the centrality of the individual N=1No firm is big enough in scope and size to satisfy the experiences of one consumer at a time All firms will access resources from a wide variety of other big and small firms – a global ecosystem The focus is on access to resources not ownership of resources R=GCustomers are now demanding that goods or services they purchase should be customized to their needs The individual has become the demanding key figure in the transaction No single company can service every individual customer on its own Collaborations must therefore be established around the world in order to satisfy this multitude of needsThe economy of the industrial revolution was characterized by shortages Due to the Long Tail  such scarcity no longer exists Instead there is abundance  The consequence for consumption is that the focus is no longer on possession but has increasingly shifted to experience  the focus is on the perceptions and emotions that a product evokes The modern smart phones are outstanding examples of this way of thinking Ultimately a product or service has to contribute to a definition of selfidentityIn the hypercapitalist economy – characterized by continuous innovation and dizzying speed of change – buying things in markets and owning property becomes an outdated idea while just in time access to virtually every kind of service through vast commercial networks operating in cyberspace becomes the norm We increasingly pay for the experience of using things – in the form of subscriptions memberships and leases – rather than pay for the things themselves The bottom line we are spending more and owning less – Jeremy Rifkin Unbundling Value ChainsOne of the immediate consequences of the new ways of thinking described above is the unbundling of value chains a process that is now underway The bundling of the worlds computers into a single network is ushering in what may be called the unbundled age – Daniel AkstPhenomena such as mashups and the cloud are the very first signs of this change As Michael Porter has taught us the concept of the value chain must be given free reign It is no longer sufficient to examine how an organization has structured its internal business processes A much stronger focus must be placed on the collaborations among partners in the chain – not only locally but certainly globally as wellThe mutual needs of consumers must also be clearly examined How can the consumer be best involved in the development process in order to customize the experience for this consumer And how should a company deal with large user groups with communities How can companies work together with their customers Crowdsourcing  involving customers in the innovation process is a tool used increasingly frequently by companies seeking to generate innovationPorters Value ChainsEach company is a collection of activities developed to bring a product or service to market Michael Porter identifies this series of activities as an organizations value chain He first described this model in his 1985 book Gross SalesSupport ActivitiesPrimary ActivitiesCompetitive Advantage Figure 34 The Value Chain19Value is in fact the contribution that businesses make to customers and for which customers are then willing to pay The aim is to create value for customers that is higher than what it costs to create From this perspective costs are not always as important in determining competitive position as value isTechnology has made possible the fragmentation of the value chain – Suzanne Berger Professor at MITValue Chain 20In an article entitled Value Chain Xavier L Comtesse and Jeffrey Huang clearly update Porters value chain to make it more applicable to the present  These chains unquestionably now involve consumers along with the other companies implicated in a given companys business processes The production of a good or service is to be seen as a collective initiative bent on the creation of a unique value = experience for a unique consumerThe fact that the creation of value is an activity not just reserved for companies but entangled in the interplay between producer and consumer means that Porters original notion of the value chain is no longer sufficient This collaboration with consumers identified by Comtesse and Huang as the Value Chain 20 is illustrated in Figure 35An extra dimension has been added to each manner of value creation within an organization These additions reflect the consumers involvement in the business process As a result collaboration with end users and other companies is a requirement for continued survival in the new age Figure 35 The Value Chain 20The System is the ProductOn June 17 2006 Adam Richardson expressed his frustrations concerning the new Motorola cell phone on his weblog  In his blog posting he uttered his new mantra The System is the Product For a product to feel harmonious with the user the system that surrounds it must be harmonious No product is outside of a system though not all products are systemsLikely it is the iPod and its associated system iTunes that is among the first examples of his philosophy Apple introduced its newest product on October 23 2001 With this mp3 player Apple promised consumers complete and continuous access to their music collection All your music always with you The success behind this mp3 player was not due to the device but to the underlying software Specifically the iTunes program makes it possible for the bulk of functionality to be left off the iPod and run on the PCThe launching of iTunes Music Software was tantamount to a fully fleshedout implementation of Richardsons mantra It enabled consumers to stream all types of media video as well as audio to their iPods The iPod became as it were the interface for the entire underlying system It provided an entirely different manner of enhancing and personalizing the consumers experienceThis type of thinking is now being applied more frequently All kinds of social web applications are currently being offered through APIs allowing users to outfit their products with their own chosen content Consider for example Microsoft Virtual Earth or Google Maps in which various types of extra information can be added augmenting the program with almost any new type of visual dataMashup and the CloudThe Gartner Hype Cycle in Figure 36 shows the various elements of cloud and Web 20 in their different stages of adoption including the term Web 20 itself The complete Web 20 movement has caused all kinds of new social applications to spring to life Wikipedia Flickr Blogger Digg YouTube and Facebook are probably the most important examples of this phenomenon They all rely on the consumers input and therefore offer a full range of APIs in order to provide the greatest possible means of incorporating the needs of their end users To help web consumers negotiate this jungle of social networks large companies are offering platforms in order to make it easier to create mashups link networks and programs together Examples of such online infrastructure are Amazon Web Service Yahoo Pipes Microsoft Popfly and Microsoft Live Mesh Google App Engine and the IBM CloudCompanies are also using these platforms They are placing increasingly more of their core business in the cloud and linking to the core activities of other companies In this way completely new business processes have been created along with corresponding new forms of collaboration outside the walls of any one company Figure 36 Hype Cycle for Emerging Technologies 34 ConclusionThe world is verging on a fundamental transformation the likes of which we have never seen The ways in which companies are accustomed to doing business will change for good How can this be possible Companies have stood alone on the bridge of commercial enterprise for decades They have laid the course along which the consumer economy has had to sail They have dictated the products that consumers would like to possess They have greatly benefited from the absolute power that is now crumbling away at a furious rateThe emergence of new technologies has undermined this corporate regime by completely altering the product paradigm Technology has made globalization possible No longer is there any distinction between here and there Neither is there any distinction between sooner and later transactions are now occurring in real time The technology has also come into the hands of consumers enabling them to have immediate input into the production process Using the technology to assert a newly acquired advantage consumers can make their own wishes known and demand customized products or experiences from companies Companies must provide specific concrete responses to the unique needs of each individual consumer But how can companies achieve thisNo single company is capable of satisfying these unique needs all by itself Any company must therefore collaborate and enter into dialogues with other companies and with its consumers as well This will have a great effect on the position of the company in the market and will also have effects inside organizations And not least it will have an effect on the role of the IT department within the organization In the next chapters we will go into these topics and we will also talk about the workings of collaboration and its tools  Case  Holland CasinoCase Holland Casino Turns to Combination of Collaboration and Organizational Change as Strategy for the FutureTechnology with Insufficient Functionality Was Hampering Recruitment Fueling Travel CostsIt wouldnt be a stretch to say that Holland Casino has been an ideal candidate to benefit from collaboration technologies Since its founding in 1975 the governmentowned company had grown into a €750 millionayear network of 14 casinos serving gaming and entertainment devotees throughout the Netherlands That footprint and possible expansion left the company little choice but to acknowledge in 2007 that new technologies needed to be introducedFor instance on the recruitment front the lack of realtime collaboration tools such as instant messaging video conferencing and mobile document sharing is now and certainly in the near future is going to be a handicap in Holland Casinos efforts to hire promising young talent They prefer to work where cuttingedge technologies are at their fingertips Young people coming in are very well aware of all the new technologies says Ruud de Haas director of information and communication technology They use them at home and they want to use them at work as wellAmong current employees the lack of access to those same tools meant that efforts to coordinate on projects or simply to communicate with each other were relegated to email sending around documents and phone calls These are seen as increasingly inefficient tools for evolving realtime business environments Moreover the companys network of casinos had employees driving from one property to the next when working closely with coworkers in multiple locations whereas a webbased collaboration platform would render many such trips unnecessaryBig Bet on Collaboration Tied to Organizational Network EffortsWhile Holland Casino was trying to address its collaboration deficit it was also beginning to restructure around a new organizational model that reflected the changing business The companys management agreed that implementing a platform for collaboration would help that new model succeed It proceeded to deploy Microsofts SharePoint 2007 online collaboration software as well as its video conferencing technology in conjunction with a rollout of Office 2007 In the fall of 2008 it launched a proofofconcept rollout of SharePoint which was to run for three months to a crosssection of employeesAt this time the companys advisory board planned to assess its impact based on user experience reports business process results and whatever necessary organizational changes become apparent Full rollout of SharePoint to the companys 2000 information workers the company employs around 4700 people in all is expected to occur during 2010 The effort dubbed InfraNext will combine the wider SharePoint and Microsoft Office rollout with the establishment of a unified communications platform for ensuring that employees are able to keep in the loop at all times About a dozen IT staffers will be working on the project with help from Microsoft Workspace management consultancy Getronics NV already prepared the foundations for this projectDespite the substantial resources behind it de Haas sees the success of the collaboration effort as being tied to the fate of the companys network capabilities for which an upgrade strategy was being laid out The strategy was approved by the board to ensure the level of performance needed to support dynamic realtime applications The success also depends on the cultural change needed to really use collaboration tools according to de HaasNew Strategy Brings High Hopes – and Cultural ChangeThe kind of widespread change brought about by an effort like InfraNext isnt easy to institute In a company where facetoface meetings in specific locations have been the cultural norm moving to a technologyenabled collaboration strategy is a delicate operation Even exhaustive training of employees on using the new technology wont ensure the success of Holland Casinos evolving strategy Thats why de Haas believes the success of the parallel organizational change effort is so important This cant be a technology issue he says It has to come from the top not from IT Assuming it all comes together as planned de Haas has high hopes for what the changes will bring in terms of business value He expects substantial project management efficiency gains as the automated workflows inherent in SharePoint workspaces move Holland Casinos projects along more quickly than ever before SharePoints messaging and document sharing capabilities will enable employees to communicate more easily in realtime preventing important details from sitting in email and voice mail inboxes waiting to be addressed And at those times when a phone call is necessary SharePoints presence capabilities will enable employees to see whos actually at their desks further reducing the number of calls that go into voice mailIn addition to such hardtomeasure efficiency gains the new technology will help Holland Casino slash travel costs as employees adopt the tools to coordinate with colleagues at the various casinos According to de Haas those savings alone could save the company a substantial amount of money each year4 On Productivity41 IntroductionThe world around the organization is changing Value chains have opened up and there are new pressures on businesses But what does it look like inside the organization How do these changes impact the inner workings of the company In this chapter we will look at developments and what is happening to the organization then look deeper into what is happening inside company boundaries Furthermore this chapter will also spell out some directions for corporate IT on its journey to become the enabler that it always aspired to be Imagine the bestrun organization that could be an organization where you can benefit from the creativity support and initiatives of all your colleagues where autonomous units within the organization are responding properly to every business challenge and opportunity Where there is a structure for knowledge management that makes the company more mature every day leading to better decisions and evergreater efficiency Such an organization will have a culture where people are valued for who they are and where people are free to express themselves People in management roles function as facilitators and are open to feedback and suggestions for improvement from anyone with whom they are in touchWe all seem to have an idea what such a great company could be like – how much fun it would be to work for one and how easily such an organization would respond to change and even benefit from changes in the market Still most organizations continue to function in the old way Since the early twentieth century management practice has become increasingly professional Research has been done in the fields of metrics and incentives We have looked into subdividing tasks and assigning roles We have gained some insight into how to evaluate and motivate people Yet when examining the progress in this area we must conclude that management structures are largely unaffected The way we manage people is still the same as it was forty years ago while the markets around us have been changing While management has been practicing and honing a commandandcontrol management style the job market and the regular market of consumers have changed dramatically over the years42 Changing MarketsIn the previous chapters we discussed Michael Porters work on value chains Another topic Porter is famous for is the five forces model with which you are probably familiar Michael Porter introduced the model in 1979 to describe markets and specifically the competitiveness of a market It has become a tool companies may use to analyze their market position their threats and opportunities In short the Porter model looks at the choices that are available to every player in a specific market and how they impact the role of a producer new and existing competitors buyers suppliers In the model the job market which has its own dynamics is not explicitly mentioned but it could be viewed as a suppliers market supplying companies with the human capital essential for productionThe five forces model though 30 years old is still a valid way of examining markets Still when we look at the model we immediately see that for many markets the present use of the internet has greatly increased the competitiveness of these markets This is especially true for markets where no physical goods are produced are impacted For example it has become easier for buyers and suppliers to find each other and to organize bargaining power It has become cheaper for new players to enter certain markets this is possible in large part because a lot of IT support is readily available in the cloud The power companies used to have over their brand and marketing is slowly eroding thanks to different kinds of media YouTube and the blogosphere are much more difficult to direct than radio television and newspapers Ongoing Conversations Change the Pace of BusinessA powerful global conversation has begun Through the Internet people are discovering and inventing new ways to share relevant knowledge with blinding speed As a direct result markets are getting smarter – and getting smarter faster than most companies – wwwcluetraincom  Figure 41 The Five Forces We have discussed the Cluetrain Manifesto and Naked Conversations in previous chapters Since the Cluetrain Manifesto was published online in 1999 and on paper in 2000 we have been talking about markets as ongoing conversations between consumers producers suppliers and partners A peopletopeople market very different from the businesstobusiness world we were used to The Cluetrain Manifesto serves as a call to action for companies in a market where individual consumers are asking to be treated as individuals with personal service and attention Since the publication of the Cluetrain Manifesto in 1999 there has been a lot of change in the internet and business world and most of it is in line with what the manifesto proposedThe impact of these new market dynamics is in two main areas the many different consumers and the incredible speed of change To reach the consumers there is a drive towards extreme personalization The social nature of people leads to changes in consumer behavior which are happening at a faster pace than many organizations can handleAnd another characteristic of any conversation is that it is ongoing and incremental getting and staying in touch with consumers continuously Respond to changes in demand quickly Leverage the better insights by creating better services and solutions Leave the oneproductfitsall and offer more specialized solutions for specific groups or even individualsIn Naked Conversations 2006 Robert Scoble and Shel Israel put a more practical spin on the Cluetrain Manifesto giving the business world insight into how blogs are becoming an essential part of this conversational market as described in the manifesto Blogs fit the bill due to their interactive nature immediacy of communication and reversal of power Any individual could start a blog within minutes be discovered and thrust into the limelight thanks to social bookmarking and simple syndication of content If an idea or bit of information is worth spreading it will gain an audience in a matter of hoursWhere Value Comes FromAny organization looking to thrive in a competitive market will analyze its competitive advantage Strategists will think about how to increase the competitive advantage of the company by partitioning those parts of the company that represent its key products or commodities and which make it unique from others This might be very visible as in cases where part of an organization is outsourced It might also be an implicit way of controlling the flow of money and investments were no longer investing in the old products but spending time and money on developing new products and servicesThis also has an impact on IT strategy The fact that just maintaining existing IT systems costs money while not creating new profits puts pressure on IT As a direct consequence there is a need to on the one hand become more efficient and operationally optimized while on the other hand becoming more agile and responsive to business needsTherefore value is created in those parts or activities of the organization that are not a unique commodity Yet the parts or activities that are not a commodity are also the parts that are hardest to optimize This is a major problem for organizations the need to change innovate or even just respond to changing circumstances is enormous yet the parts of the organization that are involved in this change innovation or response are impossible to optimize We need a good way to improve the productivity of this sector of the organization The Rise of the Consumployee as Source of InnovationsWho initiates technological innovation within an organization We like to think its the CIO or CTO basing his initiatives on research and business needs This is increasingly not true In reality often the individual employees are the ones driving the demand for new technology Consumers try out the new technologies in their private lives then they bring these technologies and expectations to work The first smartphones were brought in by the people who liked this cool new thing The first websites were built by creative technology people playing around with the new technology Instant messaging was brought into organizations by people using it in the personal sphere MSN Facebook LinkedIn online video and many other examples started purely in the consumer sphere then found their way into organizations creating new opportunities for networking sharing of information etc First these innovations were primarily driven by technical people now they are occurring enterprisewide When it comes to Web 20 tools in the private sphere there are little restraints and the tools are freely and quickly available so the pressure on corporate IT to adopt them is large In most organizations the IT department is ill suited to respond to these kinds of innovations They either expressly forbid the use of any nonauthorized tools or simply ignore the problem Autonomy and Responding to ChangesWhenever an organization gets to a certain size the management and coordination of the whole starts to become more difficult That is probably the main reason why many companies never grow beyond a few hundred people it becomes a different game to manage anything beyond that size In particular responding to change takes a lot of extra time and effort in the larger companies How do we change that and remain nimble regardless of the company sizeIn a book by Ori Brafman and Rod A Beckstrom called The Starfish and the Spider The Unstoppable Power of Leaderless Organizations a possible solution is presented They use the analogy in the book title try to create organizations where every unit or subentity is an autonomous viable part that could in theory be its own company like species of starfish which can regenerate limbs or grow new starfish from a single tentacle instead of a centrally controlled organization where everything depends upon the central core like spiders who will die from losing their legs – excuse the cruelty of the metaphorThey go ahead and explore the characteristics of starfish organizations taking cues from AA Alcoholics Anonymous but also AlQaeda among others The book focuses on themes of cultural change and catalysts – the people who can help bring change and manage the delicate balance between centralized and decentralizedThere is a bigger trend that indicates that successful organizations are flatter due to continuous focus on decreasing the number of layers of management and try to delegate authority down the chain of command This also gives rise to practices such as 360degree feedback and selfsteering teamsIT support in the kind of organization that consists of numerous fairly autonomous units is also different from IT support in centrally coordinated topdown organizations Instead of one client for IT there are many Instead of one solution there may be a need for many different solutions based upon slightly different needs and strategies The game of finding and supporting the commonality between different units and embedding these in an enterprise architecture becomes all the more importantWorker Productivity Needs to Improve DramaticallyIncreasing productivity is ultimately what determines the living standards of people the competitive advantage of organizations and the wealth of nations – Erik Brynjolfsson Director of the Center for eBusiness MIT How we motivate people in a business context and how we provide them with the right incentives to reach their highest productivity is a continuous challenge to organizations Changes in culture and tools bring new challenges in the effort to make people productive The introduction of new tools and channels of communication also brings challenges on a very personal level people struggle to avoid being distracted from work by email internet messaging twitter and numerous other interruptionsProductivity in a business context is not measured as the sum of the productivity of all individuals It is really only the productivity of the collective Only if all people work well together can an organizational unit be really productive If everybody is busy creating papers and emails that might be perceived as a high personal productivity but it might not be the greatest collective productivity Evidently any group of people working together needs to have a continuous process that examines the productivity of the group This process must be continuously open to improve how people work together When were talking about collaborative culture this is what we mean everybody is responsible for maximizing the productivity of ALL collaborative groups they are a part of and perhaps even of groups they are NOT part ofThis is very much a bottomup culture The classic example is that it was only when carmanufacturer Toyota started taking the ideas and insights of factory workers seriously that they could improve the quality and productivity of the whole production process of their cars In the process they changed their entire way of managing production and quality Similarly when we want to improve administrative organizations we need to have a way to structurally involve all information workers in optimizing the whole processProductivity is an interesting metric We like to think of productivity as a very concrete number that can easily be quantified but in reality productivity of an employee or company as a whole is hard to determine Yet we do realize that productivity is directly related to the revenues and ultimately the profit of the company if we can do more work with fewer people we are bound to earn more and spend lessThe first major steps in improving productivity were made in manufacturing optimizing the environment processes and necessary skills to maximize the output of the factory while minimizing the number of people involved This is also the base for Six Sigma Lean and the CMM methodologiesLater attempts to apply these to the office side of organizations or to entire administrative organizations turned out to be more difficult Some parts eg claims processing or data entry were fairly similar to manufacturing so the same lessons could be applied There are great examples of how groups of typists were trained and optimized to be highly productive However some parts were harder to optimize oneoff projects creative processes adhoc responses to new situations How to optimize the productivity of a team that develops new products and servicesCollaborative Knowledge ManagementOne of the elements that will improve productivity of those parts of an organization that are unlike production is knowledge management build corporate knowledge and use it to provide people with guidance and support A lot has been written about knowledge management and it is a bit of a holy grail a lot of promise but eversohard to achieveThe newer tools that support collaboration seem to help a lot in embedding knowledge management into everyday processes One of the challenges has always been that people are willing to USE information once its there but they are NOT willing to create or add information if it is not in their immediate best interest A special situation arises when we use the right tools to support people in their everyday processes just by using the tools they will add information to the whole By categorizing information for their own purposes that cataloged information may be shared with others By prioritizing tasks or bits of information for your own use you can share this evaluation with others By selecting certain links or words over others statistical information is created that ranks relevance Figure 42 Tagcloud of this chapter generated at WordlenetOnce we take information and processes out of the email tool and start using other tools that are focused on retaining information in a structured way portals wikis we can improve the way we collect and create information Improving access and lowering barriers to adding even more information will create even more and better knowledge A lot of the tools would also provide the support to enable structure and meaning to emerge automatically For example tagclouds that show the contextual use of words could give a quick idea of the topics discussed in certain passages or most visited links could give insight into which pages are most likely relevant on an intranet especially if we could see most visited by your peers ie social bookmarkingDigital Natives Accelerate the ChangeThe digital natives the newer generations will bring some extra impetus to all the initiatives mentioned above Once they start thinking about business sales marketing and optimizing organizations expect to see extra pressure on newstyle productivity flat and autonomous organizations and a different view of work and commitments The changes we can expect are rooted in the beliefs and characteristics of the digital nativesThe world is flat They live with an international view of the world While they have some geographic home its easy for them to connect to people and business across the globeThere is a greater dislike of bureaucracy and some disrespect for authority Motivation is not accomplished through exercising power but through inspirationSocial in nature Having many friends and connections they actively use this network in personal and business lifeTheir commitment is based on deliverables rather than on a span of time Ideally they work whenever they like as long as they deliver on timeTechnology is not seen as technology its simply there to be used When you are born in a world full of computers and websites the technology behind it is a lot less interesting than the possibilities in practice This will also mean they will have higher expectations of what should be possible using technology since they are less interested in the complexities of implementation If this website has it why cant weFor the digital immigrants its hard to imagine what it means to live with the assumptions and expectations of a digital native There are a lot of great examples of generational differences between natives and immigrants from the father who could not explain why at the vacation address television shows were not available on demand there was no TiVo like at home to the mother who cant grasp that her daughter has over 300 friends online most of whom she has never met Or even the older brother who cant see why or how his younger sister only uses email to communicate with older people like himThe new generations want to be productive and use the tools available They want to work in autonomous teams that are free to control their own structure and dynamics They want to think and work across organizational boundaries They will bring a new wave of technologies and expectations to the corporate world43 Consequences for ITWe started with the insight that many innovations in technology were not instigated by the CIO or the IT department So what is the role of an IT department in supporting all these developments What does an IT department need to do to enable all these new ideas and initiativesITs first instinctive reflex will be to try and stay in control create strict rules that everybody has to abide by and create a governance structure that checks if the rules are not broken This will give the organization great control over the technology portfolio but it will stifle innovation and business dynamics Business doesnt simply want to introduce IT complexity for complexitys sake it is usually in pursuit of business goals or ideas A more flexible and dynamic view on IT is needed to find the right balance between control and freedomResponding to business change means supporting a more flexible IT This is where enterprise architecture IT architecture and specifically serviceoriented architecture are positioned trying to use an architectural style that is aimed at reuse and supporting agility The IT department will look for ways to optimize the IT portfolio and create the best set of IT assets The focus shifts to a portfolio view that is focused on today and tomorrow and not just on delivering projects The IT portfolio is the set of tools that are offered to the business users to let them create or configure their own solutions instead of only offering ITcrafted specific solutionsThis fits neatly with the organization that is comprised of autonomous business units If the tools are part of a portfolio that business units can select from as they like they are free to create solutions that fit their specific needs If there is also a way to include external services offered from the cloud it means the organization has the flexibility to leave parts of its business to the market if the market has started to offer a specific business task at a better price and/or quality Think of the business as a portfolio of business services created and managed by autonomous business units supported by IT services that are internal and external depending on the marketThe digital natives will expect these tools and platforms to be available once they start entering the workplace They will introduce new technologies and expectations themselves The IT department needs to get ready to support this drive for innovation Denying its existence or its value will not work nor will strict guidelines that forbid innovation the digital natives are likely to vote with their feet and try to find employment elsewhere – with your competition perhapsOne thing that the IT solutions should allow regardless of the fractured nature of the business or the complexity of the IT portfolio is building corporate knowledge The reason for doing certain business tasks in the context of an enterprise is to be able to create efficiency and lower the cost of transactions needed to complete the tasks One important way of creating efficiency and lowering the cost is to build corporate knowledge learn from experiences and let people share knowledge across the network In practical terms this means that knowledgebuilding should be ingrained in the platformsTagging adding classification information to assetsBookmarking categorizing and selectionRating evaluating valueGroups help people find peers to exchange information that fits their needsTrends give insight into popularity and the way the wind is blowing making for better business decisions andCollaborative sensemaking create an open dialogue that interprets information and tries to uncover hidden meanings allowing better insights and better decisionsThe tools for collaboration illustrate exactly the role IT should be playing in modern business IT doesnt create or DO the actual collaboration it simply provides the tools IT doesnt determine HOW people use the tools it simply makes sure they are flexible and available IT doesnt have to be involved to create new collaboration initiatives all it has to do is respond to new channels and tools that are entering the industry The business users will find out what they want what will work IT gives them the pen typewriter and brush and lets business discover what works for its needsReal Change is Business ChangeIT will need to respond and provide the right tools for business but it is up to the business side of the organization to find the best new structure As Gary Hamel argues in his book The Future of Management the only sustainable innovation is the innovation of management Only when we examine the workings of the organization itself and optimize the way people manage and work can we hope to increase business productivity and business innovation As Hamel wrote the exact design of what such an organization looks like will be different for each company but the themes of flat organizations increased autonomy bottomup initiatives and better collaboration will be present in any organization But here lies a challenge since the structure of management is also the hardest to change The people that have to initiate change will also be the people mostly impacted by it Case  Sydved ABCase Sydved Transforms Complexity into SimplicityIntegrated Interface Shifts Focus from Administration to Timber Procurement When youre in a business like wood procurement the last thing you want is to have folks stranded at their computers drowning in repetitive business processes But that used to be the situation at Swedish timberpurchasing firm Sydved AB which made the decision several years ago to move much of its administrative staff to other functions This left employees in the field to manage the myriad of details involved in coordinating with forest owners wood harvesters transport companies customers and colleaguesAt the time those field workers had to swap between some 2530 applications used for tracking harvest information managing contracts generating reports and handling the countless other processes comprising a single contract Worse yet they had to toggle between the various contracts creating an inefficient combination of wasted time excessive processing demands and operational complexity when what they really needed to be doing was visiting harvest sites negotiating purchase agreements and executing contracts And as difficult as it was to enter and manage the information it was equally challenging to view it what with all the switching back and forth from this contract to that and from one menu to anotherThe drain on the company went even further because the complexity of the system required that Sydved provide heavy support for its field staff whose normal job functions are related to purchase of wood and managing the harvesting of forests not performing administrative tasks The result was a whole lot of questions Before they were never taught how to navigate the system environment so they didnt know what to do says CIO Roland PerssonPEA Collaborative Business Process Management at Its SimplestThat all changed when Sydved tapped its IT teams Net and C programming skills to build a new system that greatly simplifies the coordination of so many moving parts The resulting system called PEA is an achievement in collaborative simplicity It combines more than two dozen applications into a single rowandcolumn interface that allows field staff as well as other staff throughout the company to access all the contracts theyre working on along with the status of each project all in one view PEA enables realtime collaboration and its ability to keep employees uptodate on their joint efforts with colleagues delivers truly collaborative business processes And field staff have no idea what application theyre working in at a given moment – nor do they need to – as PEA leads them through the process step by step without ever leaving the main interfaceNot that development of PEA occurred without hiccups For instance initially the system was designed to update interactively in real time but the added drain on computing resources was more than Sydveds IT environment could accommodate And that meant unacceptable latency caused staff to wait inordinate amounts of time for requested information Response time is important because users are impatient all of the time says Persson Even if youve got a lot of information they dont want to wait even 20 or 30 seconds To solve that problem Perssons team tweaked the system to update in batch mode each night easing the drain on IT systems during business hours Also if the user needs to he can choose to refresh the interface wait these seconds and get the most current data it is up to the userNow Sydved employees not only can use a single view to see the status of all current contracts they also get visual clues as to the status of each process as well as reminders of specific tasks that await their individual contributionsEmployees on the Same Page = Competitive AdvantagePersson estimates that PEA which runs on IBM iSeries servers calling an Oracle database has enabled field staff to reduce their administrative workloads by as much as 20 and has prevented Sydved from having to increase its administrative staff by at least 10 to keep up with the companys growth It has also yielded a more efficient staff by enabling employees to quickly view the status of everything from timber availability to harvesting difficulties to costs With a turnover of people of 40 in the last 3 to 4 years PEA has been very valuable in quickly getting new staff on track – they can immediately see what they have to do Combine that with the benefits of streamlined administration and improved information accuracy and Persson believes PEA is twice as effective as the mishmash of applications it replaced Additionally PEA was designed with sufficient flexibility to add processes or contracts easily on the fly an important consideration given the difficulty of predicting every task field that might eventually be needed to support a contractAny way you look at it Sydveds investment in PEA is money well spent The new system has reduced the volume of questions and issues that arise during the fulfillment of a contract and it has created a new level of collaborative business process management That translates to faster decisions which constitutes a competitive advantage Where confusion and misinformation reigned before says Persson Now we have different people talking about the same things5 The Anatomy of Collaboration51 IntroductionCollaboration and communication are different themes on the same spectrum only when we communicate can we collaborate and the part of collaboration that involves working with others is some form of communication Lessons in communicating teach us to consider the partners viewpoint and to create a connection to exchange information This is hard enough in facetoface communications and it doesnt get any easier when collaborating across a distance The many tools available are striving to approach or even improve upon the facetoface communication we are all used to In this chapter we will discuss the ingredients of collaboration and some of the tools that are available to support itAs Marshall McLuhan eloquently put it We become what we behold We shape our tools and then our tools shape us  The first tools available mimicked the tools we used in real life collaboration people meeting people facetoface Over time the tools have improved and are now using the new capabilities that come with the medium Regardless of the medium were using or the tools we have to help us we can talk about the capabilities we need to collaborate effectively We have the need toHave a place to store and add information to create deliverables and build knowledge Write it down on paper record it on tape or store it in the cloudInteract communicate exchange information between people the main difference from doing something aloneKnow and share status is someone available Can I approach you with a questionKnow more about with whom we are communicating know the identity role and position of someone we are working with Know the social networksDiscover information search and find Have a way to structure informationBe notified we dont only want to search we also want to be notified of important events and information Integrate we want all these elements to be seamlessly connected and integratedTo take collaboration and communication to the next level within the company and within the value chain it helps to understand the types of collaboration that take place and know how the available platforms support enable and enrich these collaborationsThere are a fair number of tools and websites available to support some kind of collaborative work But do they offer what businesses need Do they have the feature sets to automate and to support the information worker Do they offer the collaborative environment that business needs an environment where everybody works together in a seamless collaborative way and is able to work from any place in the world unleashing the creativity and innovation of the individual and the crowd collective intelligence GUFSIEMBBMIPVSEBKJF4PVSDFT8JLJQFEJB JTUPSZPG5FMFDPNNVOJDBUJPOMFYJDIBFMFO4BMUFS mPCJMFBSLFUJOHnFigure 51 Timeline of Communication Tools 52 Electronic CommunicationThe most widely used collaboration tool is of course email It was preceded by other electronic communication tools invented years ago first the telegraph then the telephone and later the fax Samuel FB Morse invented the practical use of the telegraph on May 24 1844 The first official message in Morse code What hath God wrought was sent from the old Supreme Court chamber in the United States Capitol to Morses partner in Baltimore  The word telegraph was derived from Greek and means to write far which is exactly what the telegraph does and what it was meant for to communicate over a long distance Its hard to grasp how the telegraph changed the perception of distance In those times the only ways to send information to another place was to physically bring it there or use a complex system of watch posts and signals or smoke To send a message across America would take 10 days one way using the Pony Express3 And there was no guarantee that the message would even arrive Around 1870 Alexander Graham Bells telephone added an extra dimension to the limited electronic communication tooling of the time Different from the telegraph it enabled synchronous communication with speech Not everybody was convinced of the value of this innovation In England the most prominent man of the time in the field of communication the Head Postmaster was quoted saying No sir The Americans have need of the telephone – but we do not We have plenty of messenger boys Admittedly it must have been hard to imagine that the telephone would evolve into something every person could own and that might be used by teenagers to call each other and ask where are youCommunications support grew from physical delivery to remote communication to synchronous communication and then to synchronous communication by everyone all in only a hundred years or so In the 1930s we had the telex basically a longdistance typewriter and in 1950 a century after the introduction of the telegraph Bell Laboratories came up with the first DataTelephone It was the first implementation of a modern modem with a speed of 50 kilobytes per second From there it took another twelve years before Ray Tomlinson developed a system for sending messages between computers that used the @ symbol to identify addresses The internet was in sight and in 1988 email became more widely adopted through the development of the email client Eudora by Steve Dorner The basics of email and its use have been the same ever since and evolution has been almost idle for 35 yearsOriginally email was a communication tool intended to send and receive information As it became widely adopted it also became peoples preferred way to work together It has taken over the telephone as the most important facility any business must have and it has become part of many business processes Just think of what would be worse for business the phone system down for a day or emailThe Problems with Email Email wasnt invented with a wide spectrum of collaborative work in mind It was invented for sending letters in the standard format of the preelectronic era A telling sign is that the CC refers to carbon copy from the time when letters were duplicated with carbon paper placed between multiple sheets of paper in a typewriterIn the time of the Pony Express people knew better than to send multiple copies to their peers asking them to change something and return the revision by Pony Express They knew then that it would end up in a complete mess Multiple copies multiple versions and multiple people who can change the data leads inevitably to a nightmare With email colleagues who receive and modify a document are going to be calling each other arguing about who has the most recent version and whether all the changes the others made are also present in that version It often ends badlyThe problem with email is its versatility which results in an email overload problem Or as Chris Rasmussen put it Email is not bad its simply overused Its a when you only know how to use a hammer all problems are nail type things  Figure 54 Email vs Wiki CollaborationMany organizations did research into to the email overload problem in the last decade They all reached the same conclusion people will get email paralysis if we keep sending mails as we are We need to reexamine our use of email since it is costing us productivity and performance An article in the New York Times with the title Struggling to Evade the EMail Tsunami warns that email has become the bane of some peoples professional lives Email administrators are asked to allow for bigger attachments and more storage capacity Some email providers have started providing email boxes that span many gigabytes This might sound very handy but keep in mind that most email is unstructured it is often redundant and the information in the archive is not shared with anyone There are now even special courses for managers that teach them how to handle email a tool has become a task in itself The courses arm the manager with simple advice such as dont keep your email program open the whole day only answer mail at beginning or end of your work days With advice like that we are taking a step back in the direction of the Pony Express era send a letter and wait a few days not knowing when the receiver will answer itSteve Whittaker from the University of Sheffield conducted research into how email could more effectively be used for task management He summarized the problems people have with using email for working together under the title Why Email is Not Enough  He noted that there are numerous problems with using email for task management Users relying on email complain aboutForgetting commitments to themselves and others tasks that they owe or are owedTracking global task status its hard to abstract from multiple messages to determine where a project currently standsDetermining whos involved in a complex taskIntegrating information across different technologies people may communicate about a task in email voicemail or using IM – and its often hard to combine information andManaging attachmentsHow many times have you had an email exchange with someone that took many more emails and a lot more time than expected when picking up the phone would have made the conversation much quicker and more efficient53 Other Tools Other ActivitiesThere are limitless options these days There are so many new tools available to communicate and collaborate with others that for any situation there is a tool that will fit Email platforms will lose the battle even though they have expanded over time Most professional email platforms have facilities for assigning tasks tracking and reporting progress on these tasks automation and even scheduling and archiving functions but the use of these functions is limited Email is competing with many new tools that are better at collaboration task management instant communication sharing persistent data etc Many new tools also belong to the Web 20 world The online places where we can talk and converse with others are all part of the global conversation as we discussed in Chapter 3 When we collaborate with others we can do so in many places We can use online conference calls that record our meetings we can use online whiteboard to collectively draw diagrams or we can jointly edit Excel spreadsheets We can post our findings on our blog use Twitter to communicate about minor updates or use MSN Messenger to ask questions directly Figure 55 from The Conversation Prism by Brian Solis and Jesse Thomas shows what the current landscape of communication channels looks like and the options we have when choosing our tools  CustomersSource Brian Solis  Jesse ThomasService NetworksFigure 55 The Conversation PrismUltimately the different tools will enable different behavior Deliverables and information that is meant to be longer lasting will find a more persistent medium than email Reference materials find a resting place where people know how to find them Fleeting information will fade to the background if it is sent using the right channels instead of being mixed with the persistent information The activities of people will focus again on the value they can add to a conversation a deliverable or a process We are no longer managing our email but managing value deliverables performance and innovationSo lets take a closer look at the capabilities that make up collaboration What are the things we can use tools for We will discussBuild deliverables build knowledgeInteractionPresence and statusRelations and social structureDiscovery search and find create structure  NotificationsIntegrationIn the rest of this chapter we will elaborate on these elements and give examples of tools that can be used to address themBuild Deliverables Build KnowledgeCollaboration is about working together and creating deliverables People may work as a team to write a document come to a decision or evaluate a product ie produce an evaluation The most basic level of support will help us build something together combine our efforts into one and find a way to store our efforts and make them available to us and othersEarly on people realized that collaboratively building a deliverable could be used to improve knowledge management Knowledge management is like a holy grail for organizations To learn from experiences and to build a dataset that can be used to respond to any circumstance is a highly desirable goal If we can entice people to contribute to a corporate knowledge deliverable did we find the grail yes of course The crux being in if we can entice…There are many tools to support the creation of deliverables and build knowledge and perhaps not surprisingly the simplest is the most popular A wiki supports both creation editing in a browser and storage onlineA wiki is a page or collection of Web pages designed to enable anyone who accesses it to contribute or modify content using a simplified markup language Wikis are often used to create collaborative websites and to power community websites The collaborative encyclopedia Wikipedia is one of the bestknown wikis Wikis are used in business to provide intranets and Knowledge Management systems Ward Cunningham developer of the first wiki software WikiWikiWeb originally described it as the simplest online database that could possibly work – from http//enwikipediaorg/wiki/WikiWikis have certain advantages over email such as the option of versioning maintaining one central point of storage where data can always be accessed instead of being hidden on someones computer see also Figure 54 Mostly wikis allow the users a lot of freedom to continually interact and improve the information Wikis but especially the more advanced tools such as SharePoint that enable the collaborative building of deliverables offer extra features that support reliable collaboration such as versioning and security Also there are collaborative tools that use peertopeer models to store and share information While the term still has an ambiguous ring to it due to illegal downloads that use the same models P2P is actually a valuable way to share information without the need for centralized servers storage and control  Using Groove clients will attempt peertopeer connectivityFailing that due to firewalls offline clients etc Groove will use a Groove Relay Server to queue the deltas untill the clients can be contactedFigure 56 PeertoPeer ArchitectureMicrosoft Groove is a peertopeer solution that has the ability to share information so that people inside and outside the company can work together on the same documents It is simple to use and doesnt need central servers to store data though it can be connected to SharePoint Instead documents are spread out among all the collaboratorsOffice Groove 2007 is a collaboration software program that helps teams work together dynamically and effectively even if team members work for different organizations work remotely or work offline Windows Live Mesh is a Microsoft product currently in beta version only that is a more infrastructural approach to sharing information – something like sharing a folder on a network Live Mesh has the ability to share folders between different kind of devices so you can share any kind of file type with friends family and colleagues so long as they are in your socalled Mesh It can also be used to synchronize the data on multiple computers A feature related to the tools above is tagging with tagging people add metadata to assets Tagging is not a single tool but a feature found in many tools and websites It is essentially a method of building knowledge classifying or categorizing things based upon the keywords or terms tags people use to describe them If tagging becomes a habit the value of information will increase rapidly leading to more accurate information Like tagging a rating mechanism is found in many tools did you find this page helpful Both are aimed at gathering metadata about assets We will give some examples of tagging when we discuss discovery below InteractionThis is what we think of when we talk about collaboration to interact with others Interaction is what drives the whole conversation economy We connect to others and interact with them Interaction with customers helps build trust gives us knowledge about what drives them and lets us gather feedback or even advice about the services or products Interaction with other people helps drive innovation and the creation of new ideasSome wellknown tools for interaction are online conferencing tools using audio video and presentation direct messaging making it possible to chat with someone directly chatboxes forums VoIPcalls Figure 58 ICQ Instant and of course emailMessagingInstant messaging ICQ I seek you was the first internetconnected instant messaging program released in 1996 It enabled users to send each other messages send files and see each others availability These programs got more and more sophisticated and at this moment they offer video conversations gaming and sending offline messages on a multitude of platforms including your cell phone Currently ICQ has lost its market leadership to Microsoft Google and Yahoo who each provide their own instant messaging solutionFigure 59 Cell Phone Instant MessagingAlso blogs and microblogging are a great example of the readwrite web the internet where twoway communication is the norm information is published on a blog and someone else can comment on it Anybody can start a blog Many companies already have blogs to keep customers suppliers and the rest of the world uptodate on whats happening within the company such as product upgrades or just to encourage people to feel good about the brand and get to know the company through seeing some real peoples words and faces associated with the company nameA blog a contraction of the term Web log is a website usually maintained by an individual with regular entries of commentary descriptions of events or other material such as graphics or video Entries are commonly displayed in reversechronological order Blog can also be used as a verb meaning to maintain or add content to a blog – from http//enwikipediaorg/wiki/Blog While a blog may seem like a oneway tool publishing posts the reality of successful blogs is that for the most part authors of different blogs respond to each other engage in longrunning debates and form a network of likeminded people that is indeed very interactive 9 Source TechnoratiWhy blogSpeak my mind on areas of interestShare my expertise/experiences with othersMeet/connect with likeminded peopleKeep friends/family updated on my lifeGet published/featured in traditional mediaMake money/supplement my incomeEnhance my resumeAttract new clients to my businessFigure 511 The reasons people give for blogging Microblogging is similar to blogging with the difference being that the posts entries are short sentences that share status events thoughts or observations Twittercom is the leading service for microblogging There is also Yammer which is much more focused on the corporate world by allowing only people within a company to connect and share It might address some of the security and confidentiality questions related to cloud offerings but it has the disadvantage of not enabling the interaction with people outside the organization that can be so beneficial Presence and StatusWhere are you What are you doing Can I interrupt What are you working on Do you have time for All very common questions when working with someone facetoface The same is needed online to find who is available or what channel to use when communicating with someone Sharing your status ranges from a simple free/busy status to a status that tells people this is where I am this is what Im doing this is how long it will take these are the people Im working with In the same sense even the outofoffice assistant used when people are on vacation is a presenceindicator as are your MSN Messenger status and your phones voicemail announcement Knowing the status of people is essential for using more realtime communication tools and it allows for more dynamic flexible and autonomous behavior Instead of simply waiting the sender can take the proper action to ensure the best chance of a quick and correct response Instant messenger tools were the first to explore status more closely because the way these tools work is by interruption which creates the need to manage interruptions You wouldnt want to be interrupted with instant questions all day when trying to get some important work done It also asks of your colleagues to actually respect your status very similar to reallife Cubicle/Office Etiquette Tip 2 and Tip 3Dont interrupt someone who is on the phone by using sign language or any other means of communication Think twice before interrupting someone who appears deep in thought Instant messaging programs usually show icons representing your contacts and for each contact it will show the status MSN Messenger  Microsofts instant messaging program and its corporate version Office Communicator have for example options to set your status to Busy Away On the Phone and some other default statuses Youre also free to enter your own status Feeding the baby This gives friends and colleagues who are connected through an instant messenger program an easy way to see whether you may be contacted  Figure 512 Buddy ListStatus might be quite detailed On the most informationrich end of the spectrum are the microblogging sites such as Twitter FriendFeed and Jaiku Here the messages originally intended to convey status information have become part of an ongoing discussion with short messages They now provide a way to let everyone know what youre doing whats on your mind and what interests you You might see it as a crossover between blogging and cell phone short message services text messages allowing users to write brief text updates 140 characters and publish them And the frequency of messages itself can also become relevant given this example from someone who used it to figure out why people werent responding to his emailI checked into their Twitter stream to see what theyd been up to throughout the day In one case the person was at a conference In the other I wasnt sure but the person hadnt sent a message in hours so he or she was clearly offline for a duration of time Microblogging has even been used by a teacher to keep track of students to send them messages and interact with them in order to understand what keeps them busy The continuous stream of someones status updates thoughts event and responses to others is the online representation of his or her life they are socalled Lifestreams Increasingly lifestreaming is part of everyday life for people Relations and Social StructureWe like to work with people we know and trust Building and maintaining connections between people and business is important for us in our working lives but also in our private lives We like it more and we can be more productive if we know what to expect and we know who is the expert on a certain topicIf we know the network we know the people If we know the people we can properly evaluate their roles and know what added value each brings to a collaborative initiative The famous Metcalfes law13 states that the value of any network is proportional to the square of the number of endpoints Or at a personal level having connections to twice as many people is four times more usefulIn the real world we build networks and make friends during social or business events People meet exchange ideas talk about their interests or simply start by giving each other their business cards Online relations and communities share these characteristics with facetoface networks and relations A connection between people can be made when there is something in common when there is the same interest or when one has information the other is interested in Figure 513 A graph displaying a social network It is interesting to note that networks are dynamic and selforganizing Although facetoface networks and online networks share the same characteristics and benefits creating and maintaining relations is different Online there is a bigger difference in how people know each other and how strongly they are connected with someone In the digital world people tend to build much more expansive networks than in real life a contact list of hundreds of contacts and friends is common Some of these contacts will be true friends others we might be less intimate with We could call a relationship with someone who shares the same interests and the same network of friends a high value relationship a strongly tied friendship In a strong relationship you know each other very well In the real world these kind of strong connections are very common The other connections we have we could call weakly tied relations They are people you know whom you may have met briefly but do not know well You know enough about them to keep in touch know where they work and what they might help you with In online communities these weakly tied connections are made easily – easier than in the real world Both types of relationship have their benefits… strong ties are unlikely to be bridges between networks while weak ties are good bridges Bridges help solve problems gather information and import unfamiliar ideas They help get work done quicker and better The ideal network for a knowledge worker probably consists of a core of strong ties and a large periphery of weak ones Because weak ties by definition dont require a lot of effort to maintain theres no reason not to form a lot of them as long as they dont come at the expense of strong ties Nodes often arent as important as the connections between them Reductionist science and analysis from the 19th and 20th centuries focused on nodes I believe 21st century science economics political science and computer science will use more complex systems theory to understand the interactions between chemicals speculators nations and users15The first tools that supported the storing of contact information were not all that different from using a rolodex a list of people you know with some key contact information The Contacts section in Microsoft Outlook also has this rolodex functionality with the added option to store extra information about your contact such as birthday manager their kids etc The additional information is used to be courteous to your contacts and to maintain a good relationship with them It is always nice if someone seems to remember the names of your seven kids The main problem with this way of storing contact and additional relationship information is that its hardly ever up to date or even worse it may be incorrectThis is where the online social network software comes in It allows you to stay current with your relationship information Your contacts will maintain their own information A site like LinkedIn a social network focused on professional relations gives the user the ability to connect to colleagues former colleagues clients and partners When a colleague moves to another job you are still connected and have the newly updated information available In this way maintaining relations and staying connected is much more accurate and convenient Social networks are expanding too sharing more information and giving you not only the names of your contacts seven kids but also their vacation photos MySpace and Facebook are examples of popular social networks that are used more in the private sphere though private and personal are increasingly mixed Figure 514 MySpace cofounder Tom Anderson arguably the most popular individual on the internet with 240 million MySpace friends he is added by default to every MySpace account Figure 515 LinkedInSNS [Social Network Software] lets users build a network of friends keep abreast of what that network is up to and even exploit it by doing things like posting a question that all friends will see All of these activities especially the latter two seem like theyd be highly valuable within a company especially a large and/or geographically distributed one where you cant access all colleagues just by bumping into them in the hallway Online networks commonly have the ability to import contacts from a client application like Outlook contacts This is useful during the initial phase of a social network but isnt the killer feature on which to build and find new relations and contacts Its importing your strong ties those contacts with whom you already have a good relationship Finding and connecting to new contacts coming up with new ideas for innovation and new sources for knowledge sharing – that is more important Intuitively speaking this means that whatever is to be diffused can reach a larger number of people and traverse greater social distance ie path length when passed through weak ties rather than strong If one tells a rumor to all his close friends and they do likewise many will hear the rumor a second and third time since those linked by strong ties tend to share friends If the motivation to spread the rumor is dampened a bit on each wave of retelling then the rumor moving through strong ties is much more likely to be limited to a few cliques than that going via weak ones bridges will not be crossed Social Network Software is enabling the creation of these valuable weakties in different ways For example most of them have functionality like maybe you also know these people or viewers of this … also viewed This technology helps to build bridges between networks and helps to connect with the rest of the world Figure 516 Six Degrees of SeparationSix degrees of separation if a person is one step away from each person they know and two steps away from each person who is known by one of the people they know then everyone is an average of six steps away from each person on Earth The translation of the six degrees of separation into business terms is that if you need someone to help you the best person in the world can be reached in at most six steps  Figure 517 Three degrees of LinkedIn connections of one of the authorsUsing the Network for Rating and ReputationAn interesting thing happens when our friends and contacts start rating information they find We know who they are and we trust their judgment So now we have a method that almost automatically starts to make sense out of the information overload things online my friends deem valuable must be valuable But also companies product and services that my friends deem valuable must also be valuable to me Even if we dont know the people statistics can help us Its almost a democracy the more people are drawn somewhere the more valuable the information from that place probably is There are several mechanisms that support rating of webpages The biggest examples are social bookmarking sites such as StumbleUpon Delicious and Digg These sites enable the social discovery of anything of interest online Diggcom is a website that makes it easy for people to share review information and news posts while other people can vote and comment on those articles It helps to sort through the multitude of blog posts and articles and bring up the most valuable content of the moment It is also is a trendsensitive mechanism if something is hot it tends to push other things to the background  Figure 519 Delicious is another social bookmarking site that provides mechanisms for rating and adding comments and tags to sitesThere are many other sites  that use a similar kind of voting mechanism DZone is a linksharing community site for developers that uses voting buttons which have the power to move an article up or down a list Figure 520 A message on DZonecom with Vote up/Vote down buttonsThe social bookmarking sites also help you find new friends for example StumbleUpon will analyze your preferences which pages did you like which not and match these with the preferences of other stumblers The people that most closely match appear on your friends page where you can look at their profiles read their blogs and perhaps get in touch Be forewarned when trying out these tools they can be addictive because they will lead you to increasingly interesting internet resources For businesses the most volatile examples are sites where the subject of evaluation is the companies and their services If you use Amazon and eBay to do trade the trust people will have in you and thus the amount of business you will do is directly proportional to the evaluation scores people give On sites such as Angieslist or Yelp people evaluate companies on TripAdvisor they evaluate holiday destinations on JobVent they evaluate workplaces etc For any kind of product service or company a place can be found online where people can share their opinionsDiscovery The socalled knowledge worker spends a considerable amount of time looking for documents and information by browsing the intranet portals or searching the internet The internet gave people access to billions probably trillions of articles news items songs videos and other kind of information With millions of people connected to it many adding more information daily we end up with more than we can handle Just as an example at YouTube 13 hours of video is uploaded every minute which is roughly equivalent to recording 800 TV channels simultaneously According to Technorati in 2006 there were 15 million blog posts in a week which means there were about 10 new blog posts every second  And every one of these posts videos presentations and comments could have information that is relevant to your business to your strategy or to your clients Figure 521 Blog VolumeAccording to a study from Basex  information overload is now costing businesses $900 billion per year in wasted productivity The first most basic solution available is a search engine such as Microsoft Enterprise Search or the Googlebox Nowadays we can install search solutions to find all sorts of internal and external information Old email archives reports graphics numbers anything can be indexed and found provided you know the right search terms That is where the problem lies a lot of information is not stored with the right metadata Some search engines can be configured to deduct metainformation from the place where things are found But there is another solutionWe have seen rating as a mechanism to make valuable information easier to find Another similar mechanism is tagging Where rating basically adds a thumbs up or thumbs down as metainformation tags can convey much more information They help catalogTagging makes searching for information easier Adding tags to documents photos videos or something else is classifying the content with simple oneword descriptions Think about how stock photo companies have long been using this type of metadata in the keywords they use to catalog photos  All kind of tools have the ability to add tags to content Many file formats even embed the information in the files themselves As an example with Microsoft Word you can add tags to documents in the save dialog screen and Windows Live Photo Gallery has a sophisticated mechanism to recognize faces so you can add tags for contacts from your address book Also most blog engines have tagging capabilities Figure 523 Windows Live Photo GalleryTagging is using human insight to classify things For humans tagging is simple and straightforward It should become a natural part of everyday life for people whenever we open or touch an item we would quickly add some tags to help us and others find it later We could tag documents and pictures as well as people and events If it is easy enough to do this might be how we would achieve the holy grail of knowledge management For computers it is still a great challenge to get the right grasp of concepts and context Serious scientific effort is put into automating analysis and classification of items Natural Language search and Artificial Intelligence are the topics of research One of these efforts is ALIPR which is a service that strives to automatically tag images and make them searchable   Figure 524 ALIPRWeb 30Understanding how to query the web is valuable and having relationship ties is even more important Connections are helping us find information by adding metainformation and rating content Meanwhile there is already talk of Web 30 where the web of pages Web 10 and the web of people Web 20 are enhanced with even more logic and functionality that should make the web intelligent turning it into the semantic web and forever solving our problems with searching the web From ReadwritewebcomWeb 30 offers detailed data exchange to every point on the Internet a machine in the middle with three main characteristicsSmart internetworking   The Internet itself will get smarter and become a gathering tool to execute relatively complex tasks and analyze collective online behavior  Seamless applications   Web 30 theories suggest that all applications will fit together a continuation of open standards where all applications will be able to communicate APIs will read data from any platform and provide a single point of reference  Distributed databases   Web 30 will need somewhere to store very complex and memoryintensive information It will require ontologies to establish relationships between information sources search millions of nodes and scan billions of data records at once NotificationStaying up to date is an important part of working in a collaborative environment You need to know what your peers are working on and from a company point of view you need to know what people are saying about your product and also what your competitors are doing We dont want to be continuously searching for the things we need to know and also want to be notified immediately when something important happens Figure 525 Office Communicator News FeedsWeb feeds are a bit like a subscription to a webpage Using a standard message format called RSS Really Simple Syndication we can use a special reader to alert us whenever for example a webpage has changed new information has been posted or new comments have been added RSS is a family of Web feed formats used to publish frequently updated works – such as blog entries news headlines audio and video – in a standardized format An RSS document which is called a feed web feed or channel includes full or summarized text plus metadata such as publishing dates and authorship Web feeds benefit publishers by letting them syndicate content quickly and automatically24Most collaborative systems like blogs portals with document libraries such as SharePoint wikis and microblogging sites offer RSS feed by subscription Initially RSS was used for updates on web pages only but now it is used to communicate notifications of all sorts of events Even system administrators can use RSS to communicate about server status updates RSS feeds can be read and aggregated in different client software tools or on specialized websites Microsoft Outlook and Internet Explorer both have a mechanism to subscribe to RSS feeds The benefit of using a feed reader or socalled feed aggregator or news reader is that information from different places on the web can be read in one place all notifications all news is in one place and no longer in our email inboxFigure 526 RSS FeedsThe webbased readers offer the same functionality as client software they aggregate feeds and make them accessible from one place They also offer some interesting extra functionality that takes advantage of its inherently cloudlike nature such as sharing feeds with friends through social networks  Notifications Are InterruptionsMost of us are already used to notifications When we receive an email a small balloon will pop up on the desktop to notify us that there is new email Also Windows itself has many kinds of notification messages youve seen one when the connection to the network is lost Other kinds of tools also have notification mechanisms MSN Messenger can notify you when your friends come online or when they want to talk to you Twirl a desktop version of Twitter can notify you when someone whose twitter messages you want to see posts a new message a tweetSome notifications are important some not it depends on what you are doing Since many systems are sending notifications to you and trying to get your attention you might end up paralyzed by the information overload It is easy to subscribe to lots of valuable sites and blogs and it is important to stay up to date about important people information companies and projects but other feeds might have lower priority The generation of digital natives seems to be able to cope better with the constant interruption but the resulting continuous partial attention has an enormous effect on people Linda Stone has written about this Like so many things in small doses continuous partial attention can be a very functional behavior However in large doses it contributes to a stressful lifestyle to operating in crisis management mode and to a compromised ability to reflect to make decisions and to think creatively In a 24/7 alwayson world continuous partial attention used as our dominant attention mode contributes to a feeling of overwhelm overstimulation and to a sense of being unfulfilled We are so accessible were inaccessible The latest greatest powerful technologies have contributed to our feeling increasingly powerless A filter mechanism is a must You need some filtering to be able to distinguish between company related project related personal or system related notifications and to stay focused on your job Managing your interruptions is important if you want to ensure that you actually get your work done Some notifications need to pop up the moment they arrive your project has a new deadline and it is tomorrow Some notifications you need to see when you have the time for them we will have a new team member Other notifications may be left for you to read on your phone in the moments spent waiting for the bus the new company slogan is…To manage interruptions it helps to get accustomed to prioritizing according to groups Think of how a cell phone will allow you to create different ringtones alerting you to calls from different groups or individuals Microsoft Office Communicator has a similar notificationfiltering mechanism called Interruption ManagementInterruption Management You can assign a Team access level to other contacts to create a preferential list of people who are allowed to communicate with you when your Presence status is set to Do Not Disturb In addition you can manually set your Presence status to Do Not Disturb from the Presence menu or from incoming IM Call or Conference alerts When your Presence state is set to Do Not Disturb you see by default only urgent alerts from Team membersIntegrating All into a Personal MixThere are now so many information channels that people are looking for ways to connect and integrate them One way is to use tagging with social networks to prioritize blog notifications Interact with a group of colleagues and receive updates when someone posts a message to your forum There are many ways to integrate from complete automated processes to a simple portal interface that displays different components A special type of integration is the mashupMashup IntegrationMashup is a term used for a solution that is created by combining and configuring multiple underlying services It is a flexible way to easily create solutions that fit the individual need When we want to optimize collaboration between organizations or we find recurring patterns in our collaboration eg we have a certain meeting every week for which we need to book a room order meals and organize transport mashups can provide support at a suitable scale for this collaboration Wikipedia defines mashup as a web application hybrid which combines data from more than one source into a single integrated tool The most common examples are the combinations of geographic visualization with other information like the visualization of Outlook contacts within Live Maps or Google Maps with location information of realestate data The combination of the two different services makes a new application with more functionality than the different parts provide One of the main advantages of mashups is they are selfservicing employees businesses and consumers can make their own mashup gather services  they use and combine them however they want in whatever way works best for them They are the true tools of a prosumerMashups can be created manually using the interfaces available from the different services But there are also webbased mashup editors Microsoft Popfly and Yahoo Pipes are examples of tools where you can drag drop and link different services to each other thereby creating your own mashup in a matter of minutes without the need for much programming The underlying technology of web services provides standard interfaces that allow easy though sometimes still fairly technical combination26 Figure 528 Enterprise Mashups in the Web 20 Era   Figure 529 Microsoft Popfly offers a draganddrop integration solution to create your own mashup in minutesA much simpler way to integrate is to link different services together without many steps or tools but by simply using a readymade tool that integrates a number of services There are specific tools available that connect multiple media and feeds into one And the tools themselves have started to knit together as well Most social networks have RSS feeds available and social networks have integrated status information or can connect to Twitter Facebook for example has the where are you what are you doing box in the upper right Yahoo Fire Eagle takes the concept of presence awareness even further with various tools including the ability to share your geographic GPS location with friends Another simple example of integration is Xobni inbox spelled backwards It shows what is possible when you combine online communities with an email client Xobni is an addin for Microsoft Outlook It automatically shows information about your contacts and how many emails you have sent and received It also connects with LinkedIn to show a selected contacts public photo and contacts that are related based on your email traffic It is a simple tool that can help prioritize email and enhance the social structure of collaboration Perhaps the ultimate combination of news and social networks is FriendFeed it allows you to combine all your activities online into one location This information in turn is available to your network In this way you can stay current and keep others current FriendFeed can integrate your blogposts Twitter tweets YouTube videos slideshare presentations and any other online activity It also has the option to use rooms for filtering information  Figure 532 FriendFeed Finally The End of EmailA revealing statement that hints at what a new way of work could look like was made by a student talking to her older brother I only use email to get hold of older people like you It might seem impossible but try to imagine a workplace where the first thing in the morning is not opening the email If we were to use all these other options we have available our email traffic could greatly diminishPeople for information about people informal notes on what they are doing profile information and notes on their friends and their activities we go to a personal profile page their FriendFeed Deliverables documents events corporate decisions for working on items we want to be working on we visit an online place where the deliverable is central which may be a project document event or any other lasting assetQuestions and answers we post them to a forum that is visited by our colleagues and friends An archive function allows us to look up questions that were asked beforeUpdates notifications these are grouped in our newsreader We prioritize the notifications and can decide to take action on some and ignore othersChecking if someone can be reached quick interactions we use instant messenger for quick questions or chitchatTask assignment and progress reporting is better done using an online project management toolEtc If we think this through very little remains for which email is really the best and only solution And once other solutions have the majority of the traffic email will quickly die down if everybody is posting messages on my Facebook wall thats where Ill go for messages and Ill start to forget my email And although information can be spread over many sites this does not mean we have to surf to all these locations personal portals and dashboards will integrate all of them into one enhanced by mashups and other integration tools on the desktop Even your email client can be the one that integrates them all54 Collaboration – Software MatrixEvery kind of collaboration needs specific tools What kinds of collaborations are there In the previous paragraphs we discussed several types of collaboration tooling and their capabilities – what they are and what you use them for But when to use which is a more difficult question People use email for all kinds of collaborations but in what kind of situation might you prefer to use blogs or wikis To answer this you have to separate collaboration into several parts Up until recently we still used the telephone as our primary realtime synchronous communication tool When we have to work in groups we use it for conference calls Not something Bell thought of when he invented the telephone There is a key difference between synchronous and asynchronous communication just as there were key differences between the early telegraph and the telephoneSoftware that facilitates communication and collaboration is available in many forms from mail systems where communication is provisioned in an indirect way to interactive games where people play in realtime against each other The timeplace matrix  pictures this This matrix has two dimensions in which collaboration differ place and time People may sometimes work on the same project at the same time in the same place – for example in the overly used onsite meetings People may also work on the same project in different places at different times – for example by outsourcing to India where there are different time zonesIn practice email may be placed in the different time / different place quadrant and instant messaging will be more directed to the same time / different place quadrant Each quadrant represents a specific need that must be met by collaboration tools When communicating in the different time / different place quadrant there is a need for a message box such as an inbox where we will receive our messages This is different when we want sametime communication In that case there is no need for an inbox but rather a notification mechanism alerting the receiver that someone wants to communicate and alerting the sender as to whether the person he wants to contact is available neither of which is necessary in a differenttime situation  Figure 533 TimePlace MatrixBesides the capabilities the tools must have the people who want to communicate must understand what kind of communication is to take place Sending an urgent message via a tool that supports a different time / different place mechanism isnt the best option This is something you see happening often people sending an urgent email expecting that the receiver will read it in realtime not knowing that he just did the management course How to overcome email overload where they told him to open email only twice a day Figure 534 TimePlace Matrix Applied on eLearning ManytoMany ManytoOne OnetoMany OnetoOneAnother dimension to explore is the one of how many people are involved Collaboration can take place with just the interaction between two people or with larger groups Different ways to interact ask for different tools We can draw a matrix and use it to situate the different collaborative tools Depending on the intended use different tools will be better suited in certain situations than others As you can see at a glance most tools are well suited for asynchronous collaboration between many people in this context many being more than two Figure 535 Collaboration tools mapped on the TimeWay Matrix The number of people working together is recorded on the Way axisKnowing and understanding in what quadrant your communication and collaboration takes place helps to choose the right platform Each tool has its pros and cons in any specific quadrant being better suited for one than another There is no platform available that provides seamless communication in every quadrant Even email as a conversation tool is really only ideally suited for a onetoone situation where we have to keep in mind that it is asynchronous the receiver might first read your mail three days from now 55 ConclusionAs we have seen there are great tools available Out there on the wide open internet they have a following of their own and nothing is keeping us from either joining these tools on the internet or if we have to implementing our own internal copy A corporate Twitter might relieve email overload A corporate Facebook could improve the social cohesion within the company While email will be longlived in the business world expect to learn new tools and start working in the different ways enabled by these tools And if we really want to enter the conversation we have to be out there in the real world Once the groundwork is in place we can select the right tools or leave people to find the right tools themselves The provisioning model for these tools and services will be a hybrid some things will be internal to the company while others will be in the cloud What this looks like what the impact is and how we can make the right decisions is the topic of the next chapters We will discuss Software  Services and get more specific on the topic of Social Computing for business  6 Groundwork for a New OrganizationThe ways that people work together shift over time which can affect your culture of collaboration More important the introduction of collaboration technologies can also change the culture of collaboration If handled properly the tools and the culture will coevolve – Dennis Kennedy 61 IntroductionCollaboration is essential to business and many tools are available to support the various capabilities that make up collaboration But just installing the tools or using a tool from the cloud does not make you an Enterprise 20 overnight The build it and they will come adage does not apply This chapter will go into the groundwork the other elements that need to be in place before collaboration can be successful It will address the soft part of collaboration addressing elements such as trust culture and rewardEmpty SharePoint Messy Wikis – a Disaster ScenarioUnfortunately the scenario described below is all too common It describes how incorrect adoption of tools can hinder collaboration before improving it It is the case of an organization going about the implementation of collaborative tools in the same way they select and implement any other kind of software toolImagine a company where knowledge workers are becoming less and less productive because they spend more and more time searching for the right information in the vast directory structure of their network It is hard to document repeatable tasks and knowledge leaves the company whenever an employee finds another job The company sets out to solve these productivity and knowledge management problems and forms a team of IT and some business people to find a solution A topdown centralized decision is made and the company decides to install SharePoint as their primary collaboration platform After the decision is made and the software is acquired the company starts with the necessary investment in infrastructure and begins training operations how to install organize and manage the collaboration platform When everything is in place the infrastructure is working backup procedures are in place guidelines are written and the operations team knows what to do the transition to support is done and the new collaboration can start Most likely the initiative now has a fancy name and perhaps a sloganSenior management then sends a companywide email announcing with pride and joy that there is a collaborative platform available for anyone to use There are private sites for every employee wikis blogs discussion forums and portals Management expects new forms of collaboration to start soon and is happily calling their organization 20ready They surely have solved all the productivity and knowledge sharing problems The employees receive the announcement emails and are pressed to use it every day Curious and perhaps a little excited some – not all – of the employees visit the enterprise portal and look around fill in their personal details and perhaps upload a profile picture  Figure 61 SharePoint Personal Details PageLooking further the employees discover that there isnt any real information yet the wiki pages are empty blog posts are rare and the forums dont have any questions After a quick look around the employees go back to their work thinking I will get back later this week when there will be more information Meanwhile most of the employees still use email as their primary collaboration tool not knowing about or not comfortable with the collaborative aspects of the newly introduced portal  After a slow start some people will have found their way to the portal Most likely these are the newer generation or the people that had specific needs or were tired of using fileshares to transfer large files With some people using the new platform and most people still using email they will have the worst of both worlds for example there will still be uncertainty as to which is the latest version of any document People will be looking in several places to find the current status and its unclear If people from different departments were to try to create a proposal together a lot of effort would go into coordination and integration of the different pieces of the proposal The wiki pages intended to be changed and updated by all to provide a common knowledge base are empty or contain temporary notes scribbles or information copied from elsewhere The team portals and project portals have some structure but most elements are empty or outdated The collaborative space looks like an abandoned town with empty houses It is clear that this scenario is not the scenario management had in mind when introducing the new platform It is also clear that introducing a new platform in itself will not change the way people work Of course in real life many organizations realize that the human aspect of any implementation is important and doubly so with collaborative technologies Not addressing the use beforehand seems like a beginners mistake So what do we need to think about in order to avert disaster scenariosExpecting MiraclesBefore we go any further we need to examine the expectations Especially we need to compare internet scale to corporate scale If we see successful sites online with a thriving online community where people are adding and improving information every day eg Wikipedia and uploading many interesting documents continuously eg Slideshare we cannot help but want this for our own company too The problem is that you probably do not have enough employees to achieve the same traffic to an internal site Only 1 of all internet users in your country is still a multiple of 100 of all of your employees And since some initiatives need some critical mass to survive it pays to look for alternatives It might just be possible to become part of the external community instead of recreating one indoorsAlso dont be disappointed when only 10 of your workforce joins an online community the numbers are still much higher than in the general public62 What You Need to SucceedCreating new modes of collaboration supported by technology can only be done by addressing the human aspect More specifically we need to address some of the worries and obstacles people encounter when collaborating using technology The three most important concerns areTrust Trust is a condition for social interaction People will only work with people companies tools and information they know they can trust Before we can expect collaboration to take off online there must be a way for people to get this trust And a topic closely associated with trust when it refers to people is IdentityCollaborative culture If one individual is the greatest collaborator in the world he or she is probably not getting anywhere Only when all people involved are part of the same collaborative culture will new levels of creativity and productivity be reached A collaborative culture consists of many things including Collaborative leadershipShared goalsShared model of the truth and  Rules or normsReward Changing the way people work takes effort so it must be clear for the parties involved what they will gain at a personal level from collaborating in a new way Surprisingly a reward for successful collaboration is most often of a nonfinancial nature  Figure 63 A graphic used in a blog discussion by Sam Lawrence of Jive software to explore how in his view collaboration is composed of coCreation coOperation coLearning coOrdination coRespect and coSolving with the individual me back on top 63 A Model for TrustWho can you trust these days And when you expect someone to collaborate with you how can you prove they can trust you and the information you provide And if I trust you and you trust your friend Joe can I also automatically trust Joe Does Joe make good on his promises to me as he does on his promises to youTrust and its closely related cousin transparency come to mind when talking about the internet and technology Or as a great quote goes Trust is the business word for love For some companies trust is what builds the brand Larry Page cofounder of Google said this in discussing GoogleOne of the big assets we have is a big consumer brand It is very clear that our users are everybody and that is who we are answerable to We need you all to trust us or else we have no business Sunday Times Why would Google have no business if we the consumers didnt trust them It seems obvious that if we didnt trust the search results we wouldnt use Google It is the same with other aspects of collaboration if we dont trust the people tools or information we will not use them In some instances its not even that we dont trust the information but that we have no way of knowing if we can trust the information Someone writing on Wikipedia suggests Trust is a relationship of reliance A trusted party is presumed to seek to fulfill policies ethical codes law and their previous promises  If we are confident that the promise will be fulfilled we will trust If we see a recurring pattern of promises that are being fulfilled our trust will increase If we see even one broken promise the trust can evaporate instantly Trusting PeopleTrust is not a hard fact or a number we can quantify like a credit rating There are no universally accepted certificates of trustworthiness We are most familiar with the concept of trusting people our family and friends gain a reputation based on our prior experiences with them and the social structure to which they belong It works the same way within collaborative initiatives the more we know of someone and the more we know of the organizational and social structure they are part of the better able we are to determine whether we can trust that person or not This is also where online social networks provide value In collaboration across geographical or organizational boundaries trust gets special attention if I have never seen the person I am working with it is harder to build trust If we cannot look someone in the eye trust doesnt come automatically This is one reason why the collaborative tools that focus on supporting online conferencing are including videofeeds as much as possible In 2007 the Economist Intelligence Unit published a paper sponsored by Cisco with the title Collaboration Transforming the Way Business WorksThe paper reported that there is a widespread imperative to adopt collaborative business models and noted that trust is a critical building block in collaboration However those seemingly simple conclusions can quickly become complicated in todays business world where the forces of globalization and the knowledge economy are converging with technology and demography  Figure 64 Requirements for Excellence in CollaborationIn this study the researchers found different levels of intensity with which people are working together It starts from a situation where there is no trust at all and people dont really collaborate but processes need to be explicitly coordinated As levels of trust increase different methods of collaboration become viable and move through cooperation up to true collaboration where people are working together to achieve a shared goal  Figure 65 Trust levels and several collaborative tools  This diagram shows how different tools or media can be used with different levels of trust closer to me means greater trust It is also a great illustration of Marshall McLuhans statement that Technology is an extension of the human body7IdentityIn order to trust someone I must be able to identify that person To delegate trust I must be able to accept the reference of someone I recognize To enable collaboration you need a way to identify people Especially when we are using cloud computing to offer collaborative solutions this is an important challenge Microsoft was one of the vendors offering Microsoft Passport as a way to uniformly identify people online but due to other trust issues this didnt gain the wide acceptance expected or hoped for By now the focus is on open identity standards OpenID that are being implemented by a range of vendors among which Microsoft but also Facebook and others Here we also see the crossover from trusting people who is working with me to trusting the technology can the technology prove to me who Im working withWhen an organization is looking to initiate collaboration between multiple parties it must first find a way to identify the different parties in such a way that they can start to build trust Trust and TechnologyWhenever we are using technology to communicate and collaborate the platform itself becomes a factor in the collaboration If our email is unreliable the process will break down If the site we are using to exchange information is not secure we will not post our materials there If people can take on other identities I am less likely to build a trust relationship based on someones avatar their online representationTechnology itself can also get in the way of trust for example this popup message below will appear when a web browser needs to update itself to show a certain webpage For a common businessuser this might look puzzling the browser asks if I trust the website but it does not tell me why I should trust that site or how I can find out if a site is trustworthy Users must figure out by themselves without any additional information whether to trust this site  Figure 66 Trusting a WebsiteFrom an organizational perspective this means the end users must become websavvy they must develop a sense of security online What sites can I trust what are normal procedures how do I recognize a secure site etc Which when you think about it is not all that different from what banks do to train their customers when they are banking onlineServiceLevel AgreementsTrust depends on how much someone is able to deliver as promised so describing the promises makes sense In the Software as a Service space this means describing the servicelevel agreement In these cases we replace blind  trust by an actual contract or at least a formal expectation Most services that are generically available online which you can use without needing to sign a contract have a very simple servicelevel agreement if the service is up its up if the service is down its down There are very few guarantees and if you want better guarantees you most likely will have to pay for them What happens when your Gmail goes down for a day Or what if your website disappears and there is no backup What do you do if the free website statistics engine you are using messes up the statistics rendering a years worth of data useless Who can you call and how fast will they respond There are many different approaches to building trust Salesforcecom the CRM as a service provider offers insight in their uptime to gain trust from their customers trust through transparency When we as consumers can see what the uptime is of their services at least we know what we are buying Amazon has an approach where they simply offer money back in case their AWS cloud services fail It remains to be seen whether you are adequately compensated for your loss if the compensation for a day lost in sales is just the rebate of one days fee for hosting the services but at least it is an explicit SLA  Figure 67 Trustsaascom is providing realtime insight into the status of several services available online  Figure 68 AWS Service Health DashboardDowntime or more explicitly not having access to online services depends upon the weakest link Especially when combining services from multiple locations and multiple service providers the combination may soon prove to be too unreliable to use for important business processes On the other hand inhouse technology is not without its downtime either and SaaS providers generally have better uptime and response times than the internal IT systems For the end user only the reliability of the whole solution matters as illustrated by fragments of a discussion on LifeHackercom on the topic Do You Trust the CloudI dont trust the cloud anymore than I trust my hard disk In fact Ive had more trouble with the cloud than I have had with hard disks – from site outages like yesterday to cable outages to a beehive in my cable box that killed my cable with honey Ubiquitous sync is the answer The info should reside on PC Smartphone and Net otherwise it is only partially usable Are we talking downtime data loss security privacy or what I trust in Google maybe because they havent bitten me yet But Ill change my tune in a second if they lose all my data or lock my account without reason And then there is the reliability of the service provider itself the one hosting the technology Especially in these financially turbulent times the choice of service provider warrants some extra attention You dont want to choose a service provider that might go out of business anytime soon Especially among the providers whose services are paid for by advertising this unpredictability leads consumers to be extra cautious perhaps to the advantage of wellestablished vendors like Microsoft And one smart move to limit risk is always to back up your data elsewhere just in caseTrusting InformationWe can trust the people we are collaborating with and we can trust the technology and the provider of the technology but can we be sure that a given document in our online portal is indeed the latest report we need If you have ever used an online collaborative solution you have probably come across documents where you werent sure whether they were drafts or final versions or whether the information in the document was really true Or you may have seen a poll on the intranet but you are not sure who participated and what the value of the poll really is People and companies that use collaborative tools need to make a conscious effort to create information to turn data into information to add value to statistics etc A great example is Wikipedia how can you trust the information somebody has written about a topic People who love Wikipedia will say that the crowd will make sure the content is correct Yet anyone can edit a wiki so who can say that vendors or competitors are not polluting Wikipedia with marketing statements instead of real information Or what about the topics that are most heavily debated A good sample of pages with a dubious history can be found in Wikipedias own very long list of Most vandalized pages However the quality of entries is uneven sometimes entries are even factually incorrect Wikipedia founder Jimmy Wales admits on the website that on any given day [the quality of] any entry might be up or down Truth be told Wikipedia has cleaned up drastically in recent years by putting more emphasis on references removing original thought and checking if for example politicians or companies are editing their own information in their favorThe way in which Web 20 technologies can help make information more reliable and trustworthy is by combining the trust in people platform and information Trust Needs a NetworkOn Amazoncom we can see product evaluations On eBaycom we can rate the seller AND the buyer Reputations here are extremely valuable On LinkedIn people are encouraged to recommend the people they think stand out Many other sites use the opinions and evaluations of the crowd to help customers make decisions by helping to make sense out of the multitude of options They build trust by creating the right expectationsIf we want to have people technology and data we can trust they must be connected and part of the same networkInteresting developments in the collaborative sensemaking area are solutions such as IBMs experiment with Many Eyes  where visitors were invited to add opinions and ideas to datasets and visualizations In a corporate world this could be applied to sharing important information with all people inside and outside the company to try to make sense of the data predict possible new developments and come up with new ideas to respond to the trends in the market Another interesting experiment online in the same space is Debategraph13 that facilitates online debates by structuring the arguments of both sides One can imagine what this could have done for many financial institutions had it been possible to use these kinds of tools earlier 64 Collaborative CultureThere definitely is a cultural aspect to collaboration For one thing the perception people have of the possibilities for and consequences of collaborating will determine their actions Or as one employee once said Im trying to develop an area of expertise that makes me stand out If I shared that with you youd get the credit not me… Its really a cutthroat environmentThe quote above was collected by Wanda J Orlikowski in 1992 while doing research into the adoption of Lotus Notes in an organization After this research Orlikowski concluded …in competitive and individualistic organizational cultures – where there are few incentives or norms for cooperating or sharing expertise – groupware on its own is unlikely to engender collaboration Such products will be interpreted as countercultural and to the extent that they are used they will promote individual not group aims14 The organization Orlikowski followed in 1992 was one where the CIO initiated the implementation of Lotus Notes Because this implementation was technologydriven employees who needed to work with the new tools were not involved Actually they simply arrived at work one new day and were surprised to find new software installed on their desktops not knowing what to do with it This is a scenario very similar to the disaster scenario mentioned aboveNo More TaylorPrevious innovations that aimed to improve productivity and performance were often based on the ideas of Frederic Winslow Taylor He stated http//ccsmitedu/papers/CCSWP134htmlhttp//collaborationmitreorg/prail/IC_Collaboration_Baseline_Study_Final_Report/3_0htmIt is only through enforced standardization of methods enforced adoption of the best implements and working conditions and enforced cooperation that this faster work can be assured And the duty of enforcing the adoption of standards and enforcing this cooperation rests with management alone  Taylors scientific management consisted of four principlesReplace ruleofthumb work methods with methods based on a scientific study of the tasks Scientifically select train and develop each employee rather than passively leaving them to train themselves Provide Detailed instruction and supervision of each worker in the performance of that workers discrete task Montgomery 1997 250 Divide work nearly equally between managers and workers so that the managers apply scientific management principles to planning the work and the workers actually perform the tasksTaylor used examples from manufacturing to create a standard approach to all sorts of work Divide processes into smaller chunks optimize each chunk and the result will be improved performance Repeatability quality control and a fixed set of tasks and actions are elements of a Taylor work method In Chapter 3 we described how Taylor puts a lot of responsibility for innovation and coordination with management and takes these responsibilities away from the ordinary worker with many counterproductive side effectsCollaborative LeadershipIn recent years Taylor has lost some of his attraction We have seen other innovations that allowed more flexibility and gave workers more responsibility and room for initiative In a collaborative environment topdown managementdriven decision making does not always produce the best results Yet old habits are hard to break In the example of the research done by Orlikowski in 1992 we saw that people started to use the new groupware primarily for personal productivity not for sharing and working togetherBusinesses and people have a backpack of thirty years of Taylor like work experience Employees must start getting used to working together and sharing information Managers must facilitate cooperation must learn about how to encourage people to contribute in online forums to generate trust facilitate teamwork etc… The cultural and social structure of the organization and its employees must breathe collaboration The aim is that teams and people should become selforganizing Knowing that they are going to make the decisions they take responsibility because they are not afraid of consequences and because the culture of the organization stimulates creativity and innovationHere we can take lessons from how people use internet and collaborative services in their private lives In that sphere people are already very familiar with collaborative services They share their online profiles Facebook MSN MySpace enjoy a low barrier in communicating with their network using Twitter email chat and even share knowledge wherever possible Wikipedia weblogs FriendFeed reviews and ratings People will bring the experiences and expectations from their private lives into the corporate world They expect the same kind of services to be available within the organization And they want these tools to be available whenever they need them without any restrictions Does it make any sense to govern or limit their use of the same tools within the organization New services providing collaborative functionality will require new insights into governance principles and culture To quote one IT managerIs it fair to govern 21st century tooling using 20th century principles Would you put restrictions on how long people talk to others they meet in the hallway Would you forbid people to talk about their vacation over coffeeCollaborative Mindset a New Look at ProductivityCulture is organic and is made by people If collaborative culture takes root people will work and think differently or conversely if people start to work and think differently collaborative culture will have taken root In the intricate interaction between culture tools and work habits the usability of the tool the work experience plays an important roleIn a blog post by Leo Babauta on Zenhabitsnet he puts forward eight new rules of work that show the shift from oldschool productivity and Corporation20 productivity  These are of course general rules of thumb but they do give insight into the changes that are occurringDont Crank – Work with Deeper FocusOld School Crank it out The old school of productivity taught us how to crank out the tasks Each task is a widget that needs to be cranked and the more we crank out the better Speed is important and cranking out more tasks is the ultimate criteria How many tasks can you finish in a dayProductivity 20 Deep focus The new worker isnt as obsessed with speed He allows himself to slow down and work at a more leisurely pace He clears away distractions and allows himself to focus on the task at hand He gets passionate about important and exciting tasks and gets into Flow This allows for a new kind of productivity – one where quality matters where amazing things are produced at an intense rate where there is a passion and satisfaction in completing a taskMinimize Meetings and Planning – Just StartOld School Lots of planning is important Hold numerous planning meetings draw up specs or detailed timelines make sure things are well planned out before committing resources This however meant that things took time That was fine when the world moved at a slower paceProductivity 20 Just start Forget all the detailed planning Meetings are a waste of time usually Instead figure out the minimum requirements to launch get those done as quickly as possible and launch in beta mode Improve as you go along Things dont have to be perfect at launch Paperwork Is Out – Automate With TechnologyOld school Crank through tons of paperwork The old productive worker had tons of incoming papers and lots of paperwork to fill out And productivity methods taught him how to crank through that paperwork  Productivity 20 Automate with technology Many workers are learning to go paperless And because everything is becoming digital you can use technology to process it faster  Many little tasks that used to be performed by humans can now be automated through computersDont MultiTask – MultiProject and SingleTaskOld school Multitasking is productive Juggling tasks shows how productive you are says old school productivityProductivity 20 Multiproject and singletask Its more about about singletasking – focusing on one task at a time to be more effective but multiprojecting has its uses too Lets say youre working on Task 1 of Project A – you should singletask while working on Task 1 But when its done you might need to wait for a response from your boss before moving on to Task 2 In that case while youre waiting you can work on Task 1 of Project B singletasking while doing that When youre done with that you might need to hear back from a client before moving on to the next task of Project B – in which case you can either return to Project A if your boss responded or move on to Project C Singletask while working on any one task but working on different projects to make your time more efficient can be a useful skillProduce Less Not MoreOld school Produce more Again the idea was to crank out as much as possible Good managers tried to get as much productivity out of their workers as possible Good workers produced moreProductivity 20 Produce less More isnt necessarily better The old thinking can lead to a big pile of crap Instead focus on quality on innovation on creativity Focus on the important stuff Lets take a software engineer as an example one engineer can write tons of code knocking out one program after another But a second engineer can focus on a really innovative program and though he has produced much less code and fewer programs and has spent more time on a single program … his software can change the industry It can win awards and recognition It might even be the companys main source of income if it catches on Produce things that change the world with a longlasting impactForget About Organization – Use TechnologyOld School Be organized The productive worker of the past had drawers full of files all organized thoroughly so that nothing would ever be lost He had a Filofax full of contacts and appointments He organized his computer files into folders and subfolders and subsubfolders and on and one It took a lot of time but it was worth it Productivity 20 Tag archive and search With technology thats not necessary Tag a file with a certain label archive it and find it later through its label or through search This approach saves a lot of time a lot of effort and a lot of headaches You can spend your time on more important tasksOut With Hierarchies – In With FreedomOld School Hierarchy The old way of thinking is that hierarchies are more efficient After all in a dictatorship the trains run on time no Well thats not always true Hierarchies require a lot of topdown decisionmaking and a lot of upanddown communication The bottom level is often left powerless to act until the top level makes decisions and the top level is often left without important information necessary to make those decisions because they arent down at the bottom in the trenches As a result theres a lot of inefficiencyProductivity 20 Independence freedom and collaboration Hierarchies are being flattened out In fact whole new forms of organization and collaboration are being created all the time People more and more are working independently either within a company or as freelancers and consultants They take on jobs as they like and collaborate with others at will Workers are empowered to make decisions communication is more efficient and people with freedom are generally happier with their jobs and more passionate about the work they produceWork Fewer Hours Not MoreOld School Work longer hours Work long and hard Be a top producer Burn out by age 40 Working long hours earned you points with your boss and there was a competition to see who worked the most and the hardestProductivity 20 Work fewer hours With more freedom workers are realizing that work isnt everything and that its more important to be happy to produce important work to have the freedom to be creative and innovative to be passionate about your work … than to give everything you have for something you dont care about As a result more people are working from home More people have flexible working hours working early and leaving early or coming in late and leaving late More people take naps in the afternoon when their productivity normally flags and wake up refreshed and ready for a productive round 2 More people are setting limits to their working hours and realizing that with those limits they actually make better use of the fewer hours they workCulture Is of the PeopleCulture is ultimately determined by the people and any company will most likely have different kinds of people working together We have mentioned the digital natives before They are a special group when it comes to collaboration and using technology But of the others the digital immigrants so to speak some will also be able and willing to join new initiatives Forrester Research uses a ladder analogy to show how people can develop in several steps from inactive to fully fledged creators see Figure 612 When initiating collaboration within an organization companies often rely upon the people who are already higher up the social technographics ladder in order to create successful pilot projects that will attract and inspire people who are newer to the technologies If we look at the statistics the different generations show their true colors of the younger generation people between 18 and 21 years old about 37 is a creator and only 17 is a complete inactive Compare this to the people in their fifties where only 15 is a creator and a shocking 61 is inactive Note that the ladder is merely an illustration of the different levels and suggests a natural progression It does not imply that everybody will move up the ladder or even that all steps in between have to be taken in order to reach higher levels  Figure 612 Social Technographics LadderAnother way of involving all kinds of people is not to bring the people to the technology but to bring the technology to the people A great example of this is in research conducted in 1998 by the University of California in Los Angeles Julian Orr an anthropologist did research for a photocopier company to find the best way to use technology to support their repair technicians Orr took a broad view of the technicians lives learning some of their skills and following them around Each morning the technicians would come to work pick up their company vehicles and drive to customers premises where photocopiers needed fixing each evening they would return to the company go to a bar together and drink beer Although the company had provided the technicians with formal training Orr discovered that they actually acquired much of their expertise informally while drinking beer together Having spent the day contending with difficult repair problems they would entertain one another with war stories and these stories often helped them with future repairs He suggested therefore that the technicians be given radio equipment so that they could remain in contact all day telling stories and helping each other with their repair tasks The example also shows the importance of the social structure for collaboration We are more likely to share information with people we know and like65 Goals and RewardsCollaboration is working together towards a shared goal Or is it It helps if the goal of the collaboration is clear and known to the parties who are taking part in the collaboration but peoples individual goals may vary So before we can look at improving collaboration we need to examine how we set goals and – what is closely related – how we motivate people Collaboration like most activities will only occur when people know they will get something in return They wont do it for nothing but they might do it for free That is by accepting rewards other than money or goods People collaborate in many different ways We already have hundreds even thousands of years of experience in collaboration and communication From the first forays into communication while hunting mammoths to the social networks on the internet nowadays we have known that if we did not work together towards the shared goal of killing that mammoth the whole group would starve and some team members would probably be flattened in the process Goals are mostly topdown While solutions innovations and ways to achieve the goals are best left to teams to discover the goals are usually externally assigned Allowing teams to set their own goals tends to lead to long debates and soul searching without any progress The assigned goal may of course not be too detailed The goal is to write this one paragraph for this newsletter is easily translated into a work order On the other hand The goal is to involve people and communicate our progress leaves more open to the team collaborating and will lead to better and more creative results The goals are part of the vision The Vision Ignites the Fire and the Rewards Keep the Fire BurningThe most crucial element underpinning the vitality of effective collaboration is a shared vision This might be a vision of a specific collaboration We will build the product set of the future or it might be a vision of how the company wants to work generally We will be the most engaging and inspiring workplace Once this vision is determined and communicated throughout the organization including internal staff and external stakeholders clients etc it becomes the reference point for future action As such the critical focus of the leaders attention is on developing and crafting the collaboration vision More than just a sentence on a document the vision should be brought to life and embedded in the activities reward mechanisms and key performance indicators of the entire organization The reward mechanism demands some extra attention since money and bonuses will most probably not be the most effective motivatorsPeople will not share anything unless they stand to benefit from the experience Employees must know how and why their opinions and contributions to the collaborative system will make a difference They should be encouraged and rewarded Management guru Peter Drucker inspired the adage Knowledge is never conscripted but only volunteered… The adage is still very relevant today and is not taken to heart often enough Get this right and you will go much further towards a successful collaboration platform/initiative regardless of what tools you use portals mashups team sites instant messaging web conferencing blogs wikis… Rewarding people with bonus money or other gifts is ingrained in many organizations but research shows that by rewarding someone we take away the inherent pleasure of the task itself  The reasoning behind this is that people unconsciously think if I must be rewarded to do this it apparently is an unpleasant task that needs rewarding to get me to perform it So giving people money to write blogposts or contribute to the corporate wiki might backfire turning a fun thing into another work task  Figure 613 Maslows hierarchy of needs is often depicted as a pyramid consisting of five levels the first lower level is associated with physiological needs while the top levels are termed growth needs and are associated with psychological needs Deficiency needs must be met first Once these are met seeking to satisfy growth needs drives personal development The higher needs in this hierarchy only come into focus when the lower needs in the pyramid have been satisfied When looking for ways to motivate people we might reexamine the classic Maslow pyramid According to Abraham Maslows Hierarchy of Needs things such as love belonging to a group respect for others and by others are needs that must be satisfied before creativity and problemsolving are perceived as valuable If people dont experience respect and confidence their first drive will be to find this respect and confidence before they will start looking for creativity and spontaneity as inherently fulfilling activities Shared Model of the TruthWe need trust culture and the right way of thinking and motivating people and then we can get to work Before people can start to work effectively together they also need a shared model of what is true People must have access to the same information and must have a way to verify the information For example if one party is operating under the belief that shipping costs are low and another is operating under the belief that shipping costs are high it will be very hard to come to a joint solution for optimizing shipping and stockingCreating a shared model of the truth means giving people access to information When working across corporate boundaries with partners or with clients this means these other parties must get access to the same information And if we are creating autonomous units where people are free to respond to changing circumstances they might need access to different kinds of information than what they needed access to before If we want to allow people to create mashups a combination of services to meet a specific need we will get many different solutions using a lot more obscure sources of information For true autonomy and spontaneous collaboration expect to open up the corporate data stores This in turn means detailed access control and auditing Freedom RulesSo far we have seen that an open culture where bottomup initiatives are valued and people are free to build social networks in the workplace gives the best results for collaboration At the same time we also have corporate responsibilities that need us to limit our liabilities and guide our investments How do we set the rulesRules of the GameOne of the aspects of a collaborative culture is that people use norms cultural values to guide their behavior Just like we would not tolerate people yelling in the hallways we do not tolerate people misusing the corporate collaborative tools When we are collaborating across organizational or even national boundaries examining the rules of the collaboration game is extra important Implicit expectations about how to behave can lead to troubles in the communication and collaboration Have you ever worked with someone from China How do they respond to jokes What is their view of deadlines and quality Just as any traditional project needs to examine the rules of engagement a collaborative initiative which can be part of a larger project also needs to define the ground rules And then there are the corporate rules aimed at limiting liability while trying to allow the good things to happen Prevention and FreedomHow do you prevent your valuable information from being lost or misused How can you capture new ideas and stimulate innovation Do traditional IT rules and guidelines which are already in place take into account the multitude of aspects related to knowledgebased and informationsharing technologyInnovation demands the creation of new ideas Collaboration supports idea creation Implementing new ideas requires flexibility But when you take a look at internet guidelines currently in use by many companies they are oldfashioned and dont stimulate innovation and collaboration It looks like there is some kind of fear around blogging instant messaging social networks and all kinds of other 20 technologies just as in the early ages of the internet and the rise of emailThese rules and guidelines are made with risks in mind Risks may include safety legal issues ethical concerns costs or system overload and certainly productivity is an important impetus behind some rules Why dont these rules always produce the best result They are made with the aim to control Control is good for the oldfashioned manager the Taylorstyle manager but isnt necessarily good for an innovative environment where it would be better to have collaborative leadership in place Workers need to communicate with customers and other kinds of interest groups in order to share knowledge about the product or service They need to be inspired while browsing the internet watching videos reading blogs and having direct contact using instant messages and micro blogsFor example most organizations have blocked accessstreaming media as one of the internet guidelines But why arent employees allowed to use streaming media Probably organizations are worried about system overloads or perhaps they are worried that an employee might watch the Tour de France during business hours and be unproductive for the duration But these days video is widely used in communication and collaboration platforms to exchange ideas pilots and even user guides On YouTube there are many videos delivering new ideas as well as user guides Microsoft used streaming audio to broadcast an important keynote address from Ray Ozzie during the Professional Developer conference where he launched the new Windows Cloud Platform Azure so employees didnt have to travel to a large event but could still get the information first handEvery company will have to find its own balance and ways to enforce the parts that need to be enforced Determine which data people can share across units and which data is very sensitive What can be published and what needs to stay very secure What data should never end up on a memory stick in a rental car The theme we are running into here is of course an element of IT governance information security A valuable approach with regard to collaboration is to attach to every information asset some metadata that describes for example the confidentiality need for integrity and availability of the asset Combined with a structure of roles and access rights collaborative systems can then determine if people can have access to certain information and how it can be shared Given the ease with which information can be shared it pays to draw a plan before launching any initiatives to address the most basic security questions Figure 614 Live Webcast of Launch of Azure Example Blogging RulesThe fear of sharing too much information is very prominent with blogging Another worry could be whether employees will spend a lot of time writing blogs reading other blogs or searching for new blogs How do you know if the blogging is work related or personal Many companies have trouble trying to regulate this kind of new informationgathering and collaboration Most of them find that there has to be some kind of guidance A crude but simple code of conduct is the not allowed to blog rule adopted by some companies There are many reasons to allow blogging and there is only one reason why you would want to prevent it and that is the fear of losing control you are not in control of what employees are saying about the company As Technorati founder and CEO David Sifry saysIts scary The lesson everyone learns in Marketing 101 is Control the message Blogging turns that on its head and thats very frighteningBut the advantages far outweigh the disadvantages as this article on WebProNews a portal about how to build a better online presence says Banning work related blogging activity by members of the organization actually hurts the business By failing to take advantage of the blog benefits including transparency conversation and community building and relationship development a company blogging ban does more harm than good Instead of forbidding blogging its better to define some important blogging rules Many companies already have an online presence in the blogosphere The guidelines from these companies most of which are publicly available are an interesting starting point for establishing rules to guide the online presence of employees For example the blogging guidelines from Yahoo23 can be found online and might be a basis for your companys set of rules Most of the blogging guidelines include the rule that the writer is personally responsible for his writing and most of them want the blogger to use a disclaimer IBM says Blogs wikis and other forms of online discourse are individual interactions not corporate communications IBMers are personally responsible for their posts Actually this rule is a little bit strange because they are corporate blogs But this disclaimer also gives the employee freedom to post anything he wants to even about the company or the products they make Beside the common rule about responsibility most guidelines also include more obvious rules such as keep secrets and be respectful Yahoo has also included a rule that ties the new world of blogging to the old world of marketingIf a member of the media contacts you about a Yahoorelated blog posting or requests Yahoo information of any kind contact PRBlogging is just a small part of the whole collaboration spectrum but talking in terms of rules freedom publicity innovation and added value establishing blogging rules is a good starting point for adopting Web 20 technologies When you are considering the rules for blogging dont let them stand in the way of success It takes some freedom to have a successful corporate blog Robert Scoble the previously mentioned author of Naked Conversations is also a wellknown blogger and former Microsoft employee who introduced more open communication within Microsoft with the start of Channel 9 In 2003 he proposed a weblog manifesto for companies In the manifesto he shared his ideas on how to make a corporate weblog successful How many of these things could your employees currently do Does that mean they can or cannot write a blog about your companyThe Corporate Weblog Manifesto Thinking of doing a weblog about your product or your company Here are my ideas of things to consider before you startTell the truth The whole truth Nothing but the truth If your competitor has a product thats better than yours link to it You might as well Well find it anywayPost fast on good news or bad Someone say something bad about your product Link to it – before the second or third site does – and answer its claims as best you can Same if something good comes out about you Its all about building longterm trust The trick to building trust is to show up If people are saying things about your product and you dont answer them that distrust builds Plus if people are saying good things about your product why not help Google find those pages as well3.	Use a human voice Dont get corporate lawyers and PR professionals to cleanse your speech We can tell believe me Plus youll be too slow If youre the last one to post the joke is on youHave a thick skin Even if you have Bill Gates favorite product people will say bad things about it Thats part of the process Dont try to write a corporate weblog unless you can answer all questions – good and bad – professionally quickly and nicelyDont ignore SlashdotTalk to the grassroots first Why Because the mainstream press is cruising weblogs looking for stories and looking for people to use in quotes If a mainstream reporter cant find anyone who knows anything about a story he/she will write a story that looks like a press release instead of something trustworthy People trust stories that have quotes from many sources They dont trust press releasesIf you dont have the answers say so Not having the answers is human But get them and exceed expectations If you say youll know by tomorrow afternoon make sure you know in the morningNever lie Youll get caught and youll lose credibility that youll never get backNever hide information Just like the space shuttle engineers your information will get out and then youll lose credibilityIf you have information that might get you in a lawsuit see a lawyer before posting but do it fast Speed is key here If it takes you two weeks to answer whats going on in the marketplace because youre scared of what your legal hit will be then youre screwed anyway Your competitors will figure it out and outmaneuver youLink to your competitors and say nice things about them Remember youre part of an industry and if the entire industry gets bigger youll probably win more than your fair share of business and youll get bigger too Be better than your competitors – people remember that …Be the authority on your product/company You should know more about your product than anyone else alive if youre writing a weblog about it If theres someone alive who knows more you damn well better have links to them and you should send some goodies to them to thank them for being such great advocates66 ConclusionWhile there is no golden recipe to ignite collaboration we can say that you can make it harder or easier on yourself To set the stage and create the right environment where the spark of a great inspirational vision and goal may set off productive collaboration at least you need to address the things discussedCreate a structure that enables trustCreate the environment where a social network can live or become part of existing networksShare the vision the goalsMotivate people in the right way and give them the freedom to deliver andTalk about the rules that guide collaborative behaviorAfter that it could still be a challenge to make people from four different time zones cultures and organizations work together but if at least they can trust each other know what the rules are and – of course – if they can trust the technology to support them in any way they see fit there will be people who will take to this new way of collaborating And once one group is on board others will follow by either joining existing initiatives or by creating their own   Case  Publishing CompanyCase Its Taken a Circuitous Route but Publishing Company on Verge of Collaboration BreakthroughScattershot Attempts Prevent Company from Knowing What It KnowsTo say that the road to a collaborative business environment has been a bumpy one for this business information supplier would be a gross understatement But after several years of ineffective efforts to infuse the company with technologyenabled collaboration the $9 billionayear publisher of journals for the medical legal and business sectors believes its about to get it rightThe companys first attempt came with the deployment of Microsofts SharePoint 2003 collaboration tool But with an insufficient infrastructure to support it it choked the network rendering it more a source of frustration than anything Complaints rained down on IT until in 2006 the company upgraded to Microsoft Office SharePoint Server 2007 cobbled together an improved infrastructure and watched the tool start to take off growing to 10000 SharePoint sites But performance began to suffer as familiar problems surfaced Again the environment grew increasingly constrained by the infrastructure it wasnt assigned to the right hardware and because there was no strategic communication about it people had to discover it themselves Without an effective collaboration environment the informationintensive company that had grown primarily by acquisition was plagued by islands of information that made it difficult – if not impossible – to effectively reuse its vast pool of intellectual property It was a case of You dont know what you know says the senior director of enterprise architecture Were an information company and we had all this information out there but we couldnt make the connectionsRampedUp Commitment Expected to Yield More CohesionThats when the company made the decision to take the effort up a notch It started by building a standard Active Directory structure to ensure all of its 32000 employees were in the same environment Then it earmarked budget to clean up its SharePoint environment turning to Sogeti for help in designing an infrastructure that could support SharePoint for years to come An interim architecture built with Sogetis assistance is enabling SharePoint to run more effectively until the more permanent infrastructure is completed most likely in early 2009Concurrently the company has begun a twoyear effort to condense its scattered portfolio of disparate applications into a smaller number of instances all to be accessible via a SharePoint intranet portal That combined with the decision to standardize globally on SharePoint will enable the company to collaborate more effectively by functioning as a cohesive unit Persistent Connections Yielding Early ResultsEven now the interim infrastructure has brought a stability that is helping the company see SharePoints true potential Global teams are assembling to collaborate and then disassembling very quickly without any need for IT involvement Project teams are moving faster delivering products on more aggressive timelines And general areas of concentration are emergingOne of these concentration areas centers on knowledge discovery with people using SharePoint sites to exchange expertise Then theres a category of sites where project teams focused on specific processes or outputs can collaborate globally by frontending their work in one location where colleagues can easily get at it The company also has created a process for calling out the militia as the director of enterprise architecture puts it enabling teams to quickly and collaboratively tackle isolated situations such as IT security incidents new business opportunities or acquisitions Were fostering discovery which is what Im really after he says Were creating persistent connections between business units and experts who can help each otherAnd to make sure the company doesnt lose control of the environment again theres increased attention being paid to governance In addition to previously established guidelines about expiring sites after a period of inactivity the company also is working on security ensuring that employees are clear about whats appropriate to share with a wide audience and that anyone who sets up a SharePoint site can easily assign permissionsEventually the company will extend its SharePoint 2007 environment to update its extranet which currently runs on Microsoft Content Management Server and the company also is looking at bringing Microsofts Groove realtime collaboration tool into the mix7 Mixing Software  Services71 IntroductionThere are fundamental changes that continuously occur within our industry related to the price of different types of components and the cost of communication About every five years or so weve found the need to reevaluate the right architectures for systems based on changes that are occurring Today the confluence of cheap computing cheap storage and cheap communications is again causing us to reevaluate where we put computing in order to deliver solutions and solve problems – Ray Ozzie Chief Software Architect Microsoft As we saw in Chapter 2 we are on the top of another Kondratiev wave of fundamental change We are entering a time of rapid change for business In previous chapters we have talked about collaboration and the organizational side of this change In this chapter we will focus on the delivery architecture for the services and tools that enable crossboundary collaboration The developments in technology around SOA Web 20 and SaaS combined with the rise of the conversation society offer us new possibilities and new flexibility to deliver solutions that specifically fit any user any situation any location any device etc The rapid changes in society also impact IT itselfIn the IT industry that will cause considerable disruption and change for the IT and enterprise vendors and for the way that people perceive and use technologies This disruption is similar to the earlier PC and internet revolutions in terms of scope and effect It touches millions of people generating diverse models of commerce and of IT consumption as well as diverse ways of generating revenue and value It has already spawned new marketplaces industries and multibilliondollar companies and it has had a dramatic effect on multiple aspects of business The driving forces behind this radical disruption are the same ones that have driven the previous disruptions we have seen in the IT space creativity communication and commerce People want to be creative and unique and they want to innovate and to build to make new things and generate new ideas Additionally people want to communicate and share with one another both locally and globally This desire to share and the value that can be created by collaboration is tearing down organizational barriers It is blurring the distinctions between consumers suppliers and business It is making all enterprises more transparent Finally businesses want to expand and create new products services marketplaces and revenue Figure 71 Technology at the Base of DisruptionThe main force driving this disruption as in the previous ones is technology The falling cost of bandwidth the availability of computing in new and diverse and cheaper forms and devices the emergence of cloud services and platforms and the increase in productivity and ease of use has caused a massive uptake of webbased applications This disruption is in its early days and the full ramifications of what will happen have not yet emerged but the opportunities are all around us so those people and organizations that recognize them early will benefit mostIn the next three sections we will examine key catalysts that are collectively driving and creating the emerging Software  Services model We will then outline the key principles underlying the Software  Services model We will conclude by discussing some of the implications of the Software  Services model on IT at large  72 The Evolution of ServiceOriented Architectures Service orientation has changed our view of IT and architecture dramatically It is an architectural style that can be used to guide the design of distributed systems At its most abstract level service orientation views every business capability and every IT asset – from the mainframe application to the printer to the shippingdock clerk to the overnight delivery company – as a service provider SOA looks to create an architecture for the organization as a collection of business services that mirror organizational capabilities These business services in turn are composed of many layers of more technical services Some of the standards that are used to implement SOA are also used without the overarching architecture for simple integration purposes making it easy to connect to mainframes or other technical entities Service providers expose capabilities through interfaces and serviceoriented architecture maps these capabilities and interfaces so they can be orchestrated into processes The service model is fractal the newly formed process is a service itself exposing a new aggregated capability At a macro level from an architecture perspective SOA may be thought of as being about the space between the systems or services The core tenets of SOA are focused on the harmonization of these systems or services and one can view service federation and service composition as the primary capabilities of serviceoriented applicationsSOA represents the collection of a lot of prior knowledge about IT combined into one coherent architectural style SOA provides us with a model that allows us to improve IT maturity and guides us in the way we can design develop build and manage applications The concept of application is no longer viable since we are looking at services as independent reusable entities These services can then be combined into processes that may be very dynamic and/or personal SOA has fundamentally changed the way that we think about design develop build and manage applications Gone is the model where the application was a static entity – today the application is usually a dynamic ondemand composition of services directed by the end user This has paved the way for IT to conceive of itself as a collection or portfolio of services Predominantly these services are inhouse – but the genie is out of the bottle and this shift in mindset and in execution is paving the way for IT to investigate and pursue services that may be outside of the organization Organizations that gain experience with SOA discover that the boundaries between internal and external services are consequently blurring both are very similar in their provisioning and use Because those organizations have their internal IT in order they are almost automatically ready to connect to the cloud The Evolution of the WebThere was a time when the web was about delivery of information to the end user This design pattern traces its heritage to the hypermedia information systems that led to the invention of HTTP – the protocol used to transfer text and webpages across the internet However this pattern was quickly superseded by the use of the web to enable transactional commercial scenarios – where the end user was able to transact with a provider Prominent examples of the Business2Consumer B2C pattern are sites such as Amazon com and eBaycom among others In the last decade these web applications have undergone some tectonic shifts Gone are the days of the web application as a means of information delivery or even transactional commerce Todays web applications have become hubs for people and for communities to create and share and to do so collaboratively Websites and applications have become bazaars where users can come together to share rich content to engage with one other and to discuss and create entirely new content in ways that were unimaginable in the early days of the webRise of the Conversation SocietyIn the book Me the Media Rise of the Conversation Society the impact of disruptive technologies is seen through the lens of the media Media play a central role in our lives an observation that is also literally what the word means Traditionally a medium is found in the space between sender and receiver Since there are various ways of communicating or mediating across this space onetomany onetoone manytomany visual auditory textual and via associated devices it is possible to distinguish different types of media What the internet has done is to gather the preexisting media – radio TV newspapers magazines telephone etc – into a single multimedia environment that is personal and social at the same time This trend is greatly troublesome to traditional mass media and the organizations that tend to support communications in such massmedia forms In the digital Middle Ages every medium had its own distinct footprint but these distinguishing features have now become fully interwoven in the multimedia internet forming what has now become a single undifferentiated media mass Due to the emergence of this personal and social multimedia internet experiences of the brand and identity have gained enormously in importance Phrases such as service at your fingertips and the customer is always right can now be given new meaning The first examples of this new development are discernible in the ways that some companies engage critical bloggers to help them maintain a competitive edge while others are involving online customers in innovation and marketing A similar observation about consumer empowerment was made by the Business 20 magazine American Innovation and later by Time magazine when they placed You at the top newsmaker spot at the middle and end of 2006We shall now briefly consider each of the three great media revolutions in our history the most epochmaking media events that have occurred since the development of writing 5000 years agoThe First Media Revolution Type Letters and Printing PressThe newspaper was the final development of the personal media revolution This revolution resulted from the introduction of type letters and the printing press in Europe and subsequently around the world Modern printing makes it possible for everyone to be kept informed about the latest developments In the Wild West posters clearly advertised the reward for a captured outlaw for exampleThe Second Media Revolution Electronic Mass MediaIn addition to the explosion of newspapers and magazines radio and television are the major innovations of the mass media age This second media revolution exposed us to multimedia broadcasts over the airwaves The resulting forms of communication and socialization combined with print media in a fruitful crossfertilizationThe Third Media Revolution Web MediaThe internet the PC and mobile telephones with cameras are characteristic of the present phase of the third media revolution We are currently living through the transition from the traditional mass media to a single massive multimedium in which everyone can personally participate as a prosumer For we are now all able to both consume and produce texts images and audio using such devices as our mobile telephones which have developed along with the PC into the most prominent forms of internet hardware This third media revolution means even more communication and socialization since individuals can become personally involved whenever they like They can also organize themselves into nontraditional associations such as the Best Buy customer the housewife the New York Times subscriber the jazz lover or the liberal Figure 72 Cover of Me the Media Rise of the Conversation SocietyThe Emergence of Cloud Computing and Software as a Service SaaSThe viability of the internet as an extended platform facilitated by cloud services and managed offerings is driving interest in sourcing services from the market aka the cloud and Software as a Service as opposed to buying/building applications and managing them onpremisesWikipedia  defines SaaSSoftware as a Service SaaS typically pronounced sass is a model of software deployment where an application is hosted as a service provided to customers across the Internet By eliminating the need to install and run the application on the customers own computer SaaS alleviates the customers burden of software maintenance ongoing operation and support Conversely customers relinquish control over software versions or changing requirements moreover costs to use the service become a continuous expense rather than a single expense at time of purchase From an enterprise perspective SaaS and cloudbased services open a plethora of possibilities From the MSDN view  some of the benefits of SaaS and cloudbased services are as followsManaging the Risks of Software AcquisitionTraditionally deploying largescale businesscritical software systems such as ERP and CRM application suites has been a major undertaking Deploying these systems across a large enterprise could cost hundreds of thousands of dollars in upfront licensing fees and usually requires an army of IT personnel and consultants to customize and integrate it with the organizations other systems and data The time staff and budget requirements of a deployment of this magnitude represent a significant risk for an organization of any size and often puts such software out of the reach of smaller organizations that would otherwise be able to derive a great deal of utility from itThe ondemand delivery model changes some of this SaaS applications dont require the deployment of a large infrastructure at the clients location which eliminates or drastically reduces the upfront commitment of resources With no significant initial investment to amortize an enterprise that deploys a SaaS application that turns out to produce disappointing results can walk away and pursue a different direction without having to abandon an expensive onpremises infrastructureAdditionally if custom integration is not required SaaS applications can be planned and executed with minimal effort and rollout activities creating one of the shortest timetovalue intervals possible for a major IT investment This has also made it possible for a number of SaaS vendors to offer riskfree and often literally free test drives of their software for a limited period such as 30 days Giving prospective customers a chance to try the software before they buy it helps eliminate much of the risk surrounding software purchaseManaging IT FocusWith SaaS the job of deploying an application and keeping it running from day to day – testing and installing patches managing upgrades monitoring performance ensuring high availability and so forth – is handled by the provider By transferring the responsibility for these overhead activities to a third party the IT department can focus more on highvalue activities that align with and support the business goals of the enterprise Instead of being primarily reactive and operationsfocused the chief information officer CIO and IT staff can more effectively function as technology strategists to the rest of the company working with business units to understand their business needs and advise them on how best to use technology to accomplish their objectives Far from being made obsolete by SaaS the IT department has an opportunity to contribute to the success of the enterprise more directly than ever before On top of Azure multiple services are hosted Currently five services are available Live Services SQL Services NET Services SharePoint Services and Dynamics CRM ServiceWindows Azure will provide a Windows server in the cloud giving developers the ability to run their applications from the cloud instead of from onpremises hardware The services on top of Azure either provide the building blocks for creating custom applications or are configurable readymade applications such as SharePoint or Office One Cloud or TwoThe term the cloud has become very popular these days Everybody is busy developing or embracing their own variations of the cloud As we saw in Chapter 1 this leads to a lot of confusion and many different definitionsAnalysts from Forrester Research described their vision of cloud in great detail They see roughly two dimensions of cloud One is the cloud as a software thing all sorts of applications and services on the web that provide users with unique ways to interact and collaborate These are the services such as YouTube Flickr Salesforce and the likeSoftware platform as a ServiceVirtual infrastructure as a ServiceThe other dimension they see is the cloud from a infrastructural side the massively scalable on demand infrastructure that is being offered by Amazon Google and now Microsoft Companies will likely choose to use cloud computing for one or both of the dimensions either needing services or needing a massively scalable infrastructure that can be turned on and off on demand 	Physical infrastructure as a Service	Traditional data center services market such as collocation or managed hosting		4PVSDFPSSFTUFS3FTFBSDIFigure 74 Three Cloud Infrastructure as a Service Markets Are Just Emerging73 The Software  Services Model There are three overarching principles that guide this model – principles informing the design and development of technological capabilities for both individuals and business The Web is the Hub of Our Social Mesh and Our Device Mesh The web is first and foremost a mesh of people Elements of this social mesh will be a primary attribute of most all software and service experiences as the personal of the PC meets the interpersonal of the web Whether in work play or just life the social element of software will continue to transform the ways that we interact with people All applications will grow to recognize and utilize the inherent groupforming aspects of their connection to the web in ways that will become fundamental to our experiences In scenarios ranging from productivity to media and entertainment social mesh notions of linking sharing ranking and tagging will become as familiar as File Edit and View Were also living in a world where the number and diversity of devices is on the rise not just PCs and phones but TVs game consoles digital picture frames DVRs media players cameras and camcorders home servers home automation systems vehicle entertainment and navigation systems and more To individuals the concept of My Computer will give way to the concept of a personal mesh of devices – a means by which all of your devices are brought together and managed through the web as a seamless whole After identifying a device as being yours its configuration and personalization settings its applications and their own settings as well as the data it carries will be seamlessly available and synchronized across your mesh of devices Whether for media control or access scenarios ranging from productivity to media and entertainment will be unified and enhanced by the concept of a device mesh The Power of Choice as Business Moves to Embrace the Cloud Most major enterprises are in the early stages of a significant infrastructural transition – from the use of dedicated and sometimes very expensive application servers to the use of virtualization and commodity hardware to consolidate those enterprise applications on computing and storage grids constructed within their data center This trend will accelerate as enterprise applications are progressively refactored from a centralized scale up model to the horizontal scale out requirements of this new utility computing model Driven in large part by the highscale requirements of consumer services the value of this utility computing model is most clearly evident in cloudbased internet services By extension cloudbased enterprise utility computing infrastructure services and enterprise applications are all becoming a reality affording IT a range of new choices in how to deploy solutions across and between enterprises within their own data center in a partners hosting facility or with the vendor itself in the cloud Software built explicitly to provide a significant level of server/service symmetry will enable IT to balance factors such as cost and control and to leverage the skills of its key personnel most effectively It will afford choice and flexibility in developing operating migrating and managing such systems in highly varied enterprise deployment environments that are distributed and federated between the enterprise data center and the internet cloud Small Pieces Loosely Joined Within the Cloud and Across a World of Devices Application design patterns at both the front and backend are transitioning towards being compositions and in some cases loose federations of cooperating systems where standards and interoperability are essential At the frontend lightweight technologies have become ubiquitous Terms such as REST and AJAX reign here The standards make it possible to integrate a broad variety of components seamlessly for the user at the surface of the browser Other standards such as RSS and ATOM allow the creation of information feeds that have become lightweight channels and queues between software components Declarative languages such as XAML allow for rapid UI innovation and iteration where simple declaration takes the place of more complex programmingAt a higher level myriad options exist for delivering applications to the user the web browser unique in its ubiquity the PC unique in how it brings together interactivity/experience mobility and storage the phone unique in its extreme mobility Developers will need to build applications that can be delivered seamlessly across a loosely coupled device mesh by utilizing a common set of tools languages runtimes and frameworks – a common toolset that spans from the service in the cloud to enterprise server and from the PC to the browser to the phone At the backend developers will need to contend with new programming models in the cloud Whether running on an enterprise grid or within the true utility computing environment of cloudbased infrastructure the way a developer will write code deploy it debug it and maintain it will be transformed The cloudbased environment consists of vast arrays of commodity computers with storage and the programs themselves being spread across those arrays for scale and redundancy and loose coupling between the tiers Independent developers and enterprises alike will move from scale up to scale out backend design patterns embracing this model for its cost resiliency flexible capacity and geodistribution 74 Implications of Software  ServicesSoftware and Services SpectraThe concepts of SaaS and cloudbased services are increasingly well understood by IT departments in organizations worldwide and with this familiarity has come the desire to explore ways in which IT capabilities can be sourced to reduce cost and improve their ability to innovate In particular the notion of business capabilities such as CRM or email or user collaboration being available from multiple different sources onpremises hosted managed SaaS or cloud with differing service levels and differing models of control and governance and at different price points has recently gained a lot of attention in IT organizations Whilst it may be some time before this scenario becomes mainstream the advantages of being able to dynamically select a service depending on the business needs are so attractive that many forwardthinking organizations are very actively investigating the idea indeed some organizations have the provision of these services as a major element of their future strategyImagine an organization that based on a business architecture has defined an IT service portfolio that describes all services the business requires or would like to use This portfolio can actively be managed where services are being provisioned to best fit the budget requirements Services can be provided by the internal IT organization inhouse or onpremises they can be developed by the internet IT organization but be hosted elsewhere hosted or at the other end of the spectrum they can be developed and offered by the cloud In this case managing IT becomes very much a case of finding and mixing the best services possibleLifes a BalanceWhen we choose between the different models internal onpremises services or cloudbased services we have to take into account the different characteristics of each service For example where cloud services might be cheaper the options for customization may be limited Or where our internal services might be easier to extend there is less flexibility when it comes to replacing or upgrading The choice between Software onpremises services or Service cloud service determines among other thingsThe amount of control we can exertFlexibility PricingTrial optionsInstrumentationOperationsCustomizationExtensibilityIntegrationRichness of UI featuresRisk of continuity…These aspects are all relative it is not that one service will have NO richness and another will have FULL richness its more a matter of more versus less The user or company requirements then determine if a specific service and its provisioning model fits the need  Application Management  8IPNBOBHFTUIFBQQMJDBUJPOTPGUXBSFFYQFSJFODF 4 Figure 75 Delivery Model shows some dimensions of services versus control An example of a building block service is Amazon EC2 an attached service could be Microsoft Exchange hosted services and a finished service could be Microsoft Office Live OnlineObviously there are implications for IT visibility and for the concomitant governance models based on the choice of where in the spectrum the organization decides to source a service Clearly onpremises application infrastructure enables the most robust governance models whereas cloudbased services pose challenges with respect to data privacy data harmonization and control and archivingHeterogeneous User Profiles and PopulationsOrganizations are realizing that they have a wide and diverse set of users ranging from usually centrally located knowledge workers through production/task workers contract employees freelancers then partners suppliers and more and more customers with whom they have to support and interact – the organization providing the appropriate access and security for each individual For example many new real estate companies often have only relatively few centrally located employees and the vast majority of the people whom they must support may be independent selfemployed real estate agents some of whom may even be freelancers working to their own constraints and schedules On the other hand an organization such as Microsoft has a large number of fulltime and often centrally located employees and comparatively fewer freelancers vendors and contractors In either case IT is responsible for the productivity of all of the individuals in the organization and the business as a wholeMore and more organizations with significant manufacturing units ask why they need to provision onpremises email capabilities for their shopfloor employees While email may be the vehicle to share organizationwide information and updates it is quite likely that many of these employees will not access their email and that a manager may print out the information and put it up on a bulletin board They usually start by asking Why not use a socalled consumer provider for these user populations And while some may benefit and derive value from these services many are more likely to look to and leverage commercial grade cloudbased services for these user populationsThis brings us to the realization that there is not only a spectrum of services from internally provided to cloudprovided but there is also a spectrum of roles ranging from centralized corporate users to customers or even the general public Each of these roles will have a different expectation for the services they consume for a corporate user robustness might be essential while for someone in the general public the level to which the service can be integrated for example into Facebook might be more important MultiHeaded User ExperiencesThe customer experience for the future appears to be multiheaded There will be a diversity of software and services for the various channels and devices – and the user experience will be tuned to the channel and the interaction – whether it be a mobile device a TV a PC or the rich web And probably the best example of this is the email experience Microsoft delivers to its customers today around Exchange A user could find the richest experience through Outlook for a more casual experience a user with a browser could also use Outlook Web Access and a user could use Outlook on a variety of mobile and other devices A user plowing through a lot of email is more likely to use Outlook If the user is on an airplane trying to catch up that user will most likely only use Outlook But if the user is doing some casual email or is at an internet kiosk that user is more likely to use Outlook Web Access And a user on the go can use the most convenient available device to access the same emailBusiness Implications IT organizations of all sizes and from all industry sectors are looking for a model of IT usage to support their wide spectrum of user profiles and user populations and a spectrum of available application services is promising the ability to meet this need Organizations are seeing the potential of being able to provide the right levels of service to their entire range of IT users in a dynamic and agile fashion that is the ability to provide a new level of Differentiated IT Businesses are thinking in terms of their IT portfolio – a portfolio of capabilities that they want to intentionally partition across both onpremises and cloudbased software75 ConclusionThe IT industry has historically been defined by a sequence of inflection points in the way consumers and businesses benefit from computing and we are in the midst of another momentous shift – a services transformation Software as a Service SaaS is fundamentally about service delivery – changing how we think about both deployment and delivery of new software applications Many of the core assumptions and constraints in the conventional approaches to software deployment and delivery are being challenged and overthrown with this inflection point in service deliveryServiceOriented Architecture SOA is about the harmonization of multiple systems and services – and fundamentally about service federation and service composition Many of the basic assumptions about applications have been challenged with the emergence of SOA The notion of an application as a static entity has been replaced by the notion of an application as a dynamic composition of services often directed by the end userWeb 20 is about the social and collaborative experiences made possible through services on the cloud and about their monetization Many of the core assumptions about the user experience and monetization have been overthrown by the emergence of the Web 20 model Together SaaS SOA and Web 20 are converging to create a new software architecture model – a model of Software  Services This model is based on the premise that the harmonization of onpremises software and cloudbased services will be superior to either of those approaches in isolation As we move to software interaction through a broader mix of digital devices and form factors the multiheaded experience will become the default user scenario for both consumers and business users Being able to optimize the mix of software and services gives IT the visibility and control to understand whats happening both inside and outside the corporate network as well as the necessary flexibility in intentionally partitioning which capabilities are best delivered onpremises versus in the cloud Software  Services will ultimately enable business to have the optimal portfolio of capabilities to meet the unique needs of each and every individual userThe social and inherently crossboundary nature of collaboration drives the need for such a userfocused architecture that can quickly deliver highly useable and flexible support for collaboration Self provisioning and quick response to new collaboration partners are essential When information and activities need to cross borders the technology needs to support it smoothly driving towards this architecture where all sorts of services can be combined into one portfolio that the user can pick and choose from Also when organizations are looking to more extensively collaborate and combine their services to a new offering to the market the integration of IT needs a pragmatic approach with the ability to make strategic decisions about who does what the essence of SaaS and Software  Services  Case  ITAGroupCase ITAGroup Leaps Into the World of CollaborationNeed to Share Intellectual Property Spurs MoveIf there were ever a company ideally suited to benefit from collaboration technology it would be ITAGroup Until now the expert in people performance management has relied on the rudimentary internal collaboration capabilities built into the Microsoft Office suite – not well suited for such a dynamic teamoriented organization Given the nature of ITAGroups $200 millionayear business the lack of a collaborative environment has been a glaring hole in its quest for greater productivity In its role helping clients establish and administer sales employee and channel performance improvement programs ITAGroup assembles teams of people plucked from six different functional areas to support programs that will run at least a year Each team must coordinate skills ranging from program development and marketing communications to technology support and client interaction Plus an internal survey found that on average ITAGroup employees work on 6 teams at a time and up to 20 during the course of a yearWithout an effective collaboration platform managing so many variables has been a challenge Communication has been handled via email resulting in attachments at varying stages of development being scattered among multiple inboxes and choking storage resources Additionally the lack of a standardized methodology for storing critical data has made finding information nearly impossible The biggest challenge we faced was effectively bringing new team members into a project says John Rose vice president of information technology We needed to be able to share information ideas and documents more easilyGood Technology  Employee BuyIn = Faster Path to SuccessITAGroup believes it has found the answer in the form of Microsoft SharePoint Server 2007 SharePoint features an interface thats familiar to anyone accustomed to working in Windows Its easy to set up a collaborative site it has powerful search capabilities embedded in it and it can take all that scattered information from email inboxes and desktop file systems and make it easily discoverable to anyone with permission to access itITAGroup is in the midst of a companywide rollout of the technology with the first collaborative elements introduced to all employees in January 2009 The companys intranet is running on SharePoint where its storing corporate data such as ISO documentation and employee information A team has been working with consultants from Sogeti to implement an outofthebox SharePoint environment thats requiring less than a $400000 investment over 3 yearsBut as anyone whos overseen a collaboration deployment can attest merely acquiring technology – no matter how well suited it is to the job at hand – is only half the battle Bringing people up to speed on the new tools showing them how to find information and work in tandem more effectively takes time Change is never easy says Rose But we know that a gradual implementation progressively adding features and functionality will ease the transition and build excitement The result is adoption of an improved collaboration environment which will ultimately result in a better company Rose has been encouraged to hear reports that many SharePoint customers who experience some initial frustration adjusting to the technology soon see the technology as a cantlivewithoutit toolGetting the Most out of SharePoint Simplicity Breeds MomentumAdvancing SharePoint to that status however requires employees get behind the technology And Rose says hes learned that when it comes to a collaboration environment marketing and positive reinforcement of the technology is important To make a tool like SharePoint really work employees have to know about it and they have to be convinced to use it To that end Rose says its critical to keep things simple even if SharePoint has myriad capabilities that can be rolled out at once Things have to remain simpleOnce the company builds the momentum it needs the anticipated benefits of SharePoint run the gamut Rose expects it to spur big savings by simply cutting the amount of time employees spend searching He also believes that the combination of more timely information and faster responses will lead to improved customer satisfaction That satisfaction figures to rise further a year or so down the line when ITAGroup hopes to extend the SharePoint capabilities to an extranet that will enable clients to log in and approve copy or artwork access workflows or simply get the latest data on their various performance improvement programsPerhaps most importantly SharePoint will gradually help lower the information sharing walls that have existed between ITAGroups teams and team members And with everyone on the same page management will be able to think outside the box more than ever armed with a tool that can evolve to make employees jobs easier8 Social Computing for Business81 Introduction We have been looking at markets the concepts behind collaboration and cloud computing We have explored the anatomy and success criteria of collaboration and we have looked at a model of a mix of Software  Services that gives us the flexibility needed to achieve it all In this chapter we will discuss more scenarios in which Web 20 technologies and the model of Software  Services can be used inside organizations The areas and scenarios discussed are also the areas where you can find value some of which can perhaps even be included in a traditional business case for a project A lot of the scenarios in this chapter revolve around the theme of A New World of WorkThe New World of Work2In a new world of work where collaboration business intelligence and prioritizing scarce time and attention are critical factors for success the tools that information workers use must evolve in ways that do not add new complexity for people who already feel the pressure of an alwayson world and everrising expectations for productivity We believe that the way out of this maze is through integration simplification and a new breed of software applications and services that manage complexity in the background and extend human capabilities by automating lowvalue tasks and helping people make sense of complex data – Bill Gates82 The Emergence of Social ComputingIn the current conversation economy we are seeing enormous changes in the way that we publish and consume information on the internet Rather than simply viewing information on static web pages we are now publishing rich content through blogs and wikis and on photo and audio and videosharing sites Instead of solely being consumers of pages downloaded from the web people are now sharing collaborating and creating new content and entire online communities Indeed people are now combining data and content from multiple sources to create their own custom personalized experiences and applications Many of these evolving concepts and capabilities were dubbed Web 20 in a seminal discussion paper by Tim OReilly in September 2005  In essence it is the collective realization that the ability to use the web to write as well as read rich content along with support for social networking and the rapid spread of broadband access combines to allow people to interact with the web online content and one another At its core this is about fundamentally changing the ways people interact with content services and with other users to provide a platform for harnessing and promoting collective intelligence4Wikipedia defines collective intelligence as shared or group intelligence that emerges from the collaboration and competition of many individuals People are no longer just passive consumers of content and data they are active participants and in many cases they are creators – creating content and interacting with a multitude of services and people Sometimes referred to as the network effect this increase in participation collaboration and in content creation presents new opportunities to involve the end user in deeper and more meaningful ways We are only just beginning to see the opportunity for these emerging concepts and capabilities both inside and outside the organization but it promises to have dramatic and longlasting impact on business In the rest of this chapter we will look at more specific scenarios in which technology can be used to be productive in Web 20 we will talk about the technology side of the conversation economy and we will discuss the roles cloud computing and Software  Services play in the space  83 Collaboration and Social Computing in the EnterpriseOrganizations of all types and sizes from startups to Fortune 100 companies and from all industry verticals have seen the explosive growth on the web of social and community sites in the consumer space such as MySpace YouTube and the deluge of Web 20 sites They have seen the response by major web players such as Amazon eBay Live and Google and Yahoo in adding social and community elements and they have seen the interest and demand that this has created They are now actively investigating – and in many cases building – new communitybased portals and businesses for their own organizations Web 20 is moving into the enterpriseThere are two primary areas in which organizations are interested in using these concepts and capabilitiesEnterprise 20 Inside the organization to improve efficiency and productivity andB2C 20 Connecting with customers to improve profitability and customer satisfaction The use within organizations is commonly called Enterprise 20  and is typically the first phaseUsage to connect to their customers and consumers is similar to BusinesstoCustomer B2C activity but with a social and community basis and may be termed BusinesstoCommunity or B2C 20  Interest in this use of the community as a customer is rapidly growing That is one of the reasons why popular social networks like LinkedIn Facebook and MySpace with all their customer profiles are worth billions of dollarsEnterprise 20This is the kind of organization that we described throughout the earlier chapters of this book It is the enterprise where employees are autonomous and collaborative and where bottomup initiatives are well valued Where socalled information workers can be in charge of their own user experience and hence create for themselves a more intuitive and efficient work environmentBusinesstoCommunity B2C 20Businesses of all types and sizes from startups like Plentyoffish to Fortune 100 companies such as General Electric have been keen to enter the dialogue with consumers and communities using 20 tools While it is always smart for businesses to keep in touch with potential clients the necessity in this case is especially critical it is a matter of survival but it can also be an enormous opportunity On the opportunity side the reasons for this interest and activity in the space are fivefold1 Revenue and growth  The opportunity to enhance existing revenue streams and to build completely new revenue streams by utilizing community and social networking capabilities In particular the cost containment of the recent past has given way to an interest in the business side in innovationdriven growth and revenue The rapid growth and innovation in the Web 20 space is seen as something that companies want to emulate2 Webbased economies of scale  Organizations see that they can dramatically decrease the cost of capital equipment and resources by using a web based delivery model to serve communities of their customers3 Flexible employment models  The use of contract and agency staff for delivery allows flexibility and agility Agency and contract staff can be thought of as another specialized community and so supported like customers4 Community creation as evangelism and support  Customers are the organizations best sales marketing support and development resource The creation of communities effectively outsources at a very reasonable cost all these cost centers Indeed with the inclusion of targeted advertising to the community the present cost centers potentially become profit centers5 Community leader advantage  Community dynamics are such that the first successful community is by far the most powerful so the organization that owns this community is the one that controls the vertical For example MySpace focused on new music bands and created a community in that space which it now effectively dominates so it has become a major force in the music industry The converse of this is that if an organizations competitors are first in the community space they will have a very significant competitive advantage To make the concept of entering the dialogue a bit more specific we can explore five particular areas where social computing capabilities can be leveraged in working with customer communities1 Innovation and New Product DevelopmentAs discussed before open innovation or crowdsourcing are important trends of the day A very large percentage of new product ideas and innovations in organizations come from suppliers and customers rather than from inhouse labs or RD organizations These new product ideas are more likely to be successful as they have come from the end users of the product and are also typically less costintensive Clearly organizations that can build a system to harvest these ideas can benefit from these innovations and derive significant benefits The use of social computing based customer and supplier communities as discussion forums and marketing focus groups for new product ideas and incubation is a powerful simple and costeffective technique for gathering these ideas Many organizations are actively investigating the use of community forums and discussion groups to provide new product ideas and uses in the product development process Indeed it is possible to envisage a program that automatically scans communities for new ideas and sends them to interested parties An additional benefit of this communitybased innovation and new product development is that the customers have a better understanding of the product or service delivered as they were involved in its gestation so the customer uptake of the product is significantly enhanced2 MarketingProbably the bestknown application of social computing techniques in organizations is around viral marketing The examples of community and rich content such as video being used to generate and spread buzz about products and services are legion There are two elements to this viral marketing initial interest generation and then the viral dissemination The initial interest generation is best done with the use of innovative image video and audio content – it is not unusual to get millions of downloads of creative video within hours or days of their release and ongoing interest in a product can be sustained by having an informational or tutorial element to the content The dissemination of this material is done by the internet community at large using chat and messaging email and community forums Again this dissemination can be very rapid and widespread It is not unusual for a new product or engaging video to be passed to millions of people in hours and to reach the mainstream media such as TV or newspapers in days There are a couple of caveats about viral marketing Firstly the target audience needs to be well understood and even then the material may not gather community interest and buzz – this is much more an art than a science Secondly an organization cannot control the spread or use of materials the use of viral marketing videos in unanticipated ways by the community is well documented and can cause significant sideeffects for an organization A great example of a viral video is the movie The Machine is Us/ing Us  by the famous cultural anthropologist Michael Wesch This movie explains in a nutshell the whole Web 20 concept and how it is changing our society The video has been viewed more than 85 million times3 SalesThe cost of sale is normally a nontrivial element of the overall cost of a product or service In a BusinesstoConsumer organization the community can act as the collective salespeople thereby dramatically reducing the cost of sale in many cases to nothing The customers themselves act as spokespeople and salespeople for the organization There is no need for a high pressure and high expenditure sales organization in communitybased businesses in fact in many cases it might actually be counterproductive4 SupportSupport is probably the second most popular area in which social computing techniques are used Firstly there is the use of messaging and chat and other collaboration techniques for realtime support of their products and services by organizations Secondly there is the use of image capture and video for problem communication and resolution Finally and most importantly there is the use of communitybased product experts and self help discussion groups This selfhelp technique has been shown to work very well in communities such as the shared and open source movement and is a simple and lowcost way of providing very highquality support As with most socialbased systems however the actual operation of these selfhelp groups is not simple and requires significant thought and expertise Support is one of the reasons why Apache and Linux have become so popular The hardcore community behind these two open source initiatives is famous for its online support If a problem arises it is sure to be solved promptly It is interesting to see that Microsoft picked up on this and has built a vibrant technical support community for their technologies5 Training and EducationProbably the least explored application of social computing in the enterprise is for training and education The online availability of highquality audio and video and other rich media provides a very lowcost and frictionless way of providing training as well as how to and other learning materials This rich content when integrated with influential subject matter experts and the online communities and discussion groups enables a very powerful environment for education and training84 Web 20The recent history of the internet has shown some very significant and farreaching changes Ten years ago there were no websharing sites or applications merely sites composed of static pages and ecommerce Organizations had customerfacing websites to connect with internetsavvy consumers and used the internet as a way to market and sell their products Internal corporate intranets were used mainly as a place to post news and policies in the company portal More recently websites have become destinations for communities of users to create and share rich and complex data such as music images and video and to discuss and rate that content People are no longer just consumers of content and applications they are participants creating content and interacting with different services and people More and more people are creating blogs contributing to knowledge bases such as Wikipedia and using peertopeer P2P technologies Sometimes referred to as the network effect this increase in participation and content creation presents new opportunities to involve the user in deeper more meaningful waysThere has been a huge amount of discussion on what exactly is meant by Web 20 Tim OReilly originally defined it as the followingThe web as a platformHarnessing collective intelligenceData as the next Intel insideEnd of the software release cycleLightweight programming modelsSoftware above the level of a single device andRich user experienceThese can be grouped into three areas The use of the web as a platformThe web as a place to read and write rich content and  The social and collaborative use of the webThe Web as a Platform Web 20 systems use the web as a platform conceptualizing the internet as a huge range of interconnected devices that can provide a new level of rich immersion for the user an easytouse and lightweight programming model for the developer and a rapid and flexible deployment mechanism for the supplier Web 20 uses the web to provide a new perspective for the user developer and supplier a new way of thinking about the internet all of which allows new and creative uses of the internetIt should be noted that an important concept underpinning all connected systems which of course includes Web 20 is that of a service A servicebased system supports the concept of separation of concerns by the use of loose coupling and concomitant message passing This loose coupling allows functionality to be created as a service and delivered over a network so for example diary functionality can be provided by a blog engine and be delivered as a service to the end user or blogger over the internet Software as a Service is this delivery of software functionality over the internet and it underpins most Web 20 systems todayLooking at the internet as a platform we can see that it has to provide a number of important platform concepts such as device independence rich and common user interface a common programming interface and a software deployment and management mechanismSoftware Above the Level of a Single DeviceWe are very familiar with software on a server providing services to software on a PC in Windows or in a browser which then consumes or displays them While this is a common and well understood model it does not cover a number of common cases such as peertopeer systems or delivery to nonPC devices like music players phones or navigation devices We need to have a model that includes these cases and covers a higher level of service than HTTP the protocol used to transfer the pages of the internet we know and use to connect them it needs to address the concepts of a music service such as Napster or a communication service such as Skype We need to have a model that addresses software above the level of a single device and a single service but which includes rich high level services interconnecting a mesh of different device types in a symmetric mannerProbably the best example of this type of highlevel service is in Microsofts game computer Xbox Live where gaming services are supplied between specialist hardware devices working in a peertopeer manner This model is the generalpurpose case of servicebased computing and is the Software  Services model of computing Rich User ExperienceThe value of rich and immersive user experience has been well understood in the PC world since the advent of Microsoft Windows and this has been a focus of browser based applications for many years Standards such as JavaScript and DHTML and technologies such as Flash and Silverlight were introduced as lightweight ways of providing clientside programmability and richer user experiences commonly called Rich Internet ApplicationsIt was a true revolution when a web browser was first able to provide this Rich Internet Application functionality For the first time web application could approach the experience of real desktop applications Microsoft broke ground in this field by porting the familiar email client Outlook to a webversion that very closely resembles the original Outlook Web Access OWA also used JavaScript and DHTML to provide rich interaction The collection of technologies used to provide these rich and dynamic browserbased systems has been called Ajax standing for Asynchronous JavaScript and XML Ajax isnt a single technology or even a set of new technologies but rather a set of several technologies being used together in powerful new ways to provide Rich Internet Application functionality Ajax includesStandards that help styles and presentation using XHTML and CSSDynamic display and interaction using the Document Object Model Data interchange and manipulation using XML and XSLT  Asynchronous data retrieval using XMLHttpRequest andA programming language using JavaScriptAjax is an important component of most Web 20 applications and is providing the ability to create web applications nearly as rich and dynamic as Windowsbased applications Indeed we are now seeing the advent of Ajaxbased applications that can work whilst disconnected from the internet and so provide offline functionality similar to Windowsbased clients like Microsoft OutlookThere are also sets of technology other than Ajax that are increasing the value of user experience in areas such as communications voice and video Instant messaging IM is heavily used in Web 20 applications to provide instant communications and there is a wide range of agents and delivery options available for IM systems VoiceoverInternetProtocol VoIP systems allow voice and teleconference communication over the internet as part of the user experience Finally the provision of real time stored or broadcast video rounds out the client experienceRecently technologies like Flash and Silverlight have also been playing a big role in enabling mediarich interactive user experiences facilitating sophisticated readwrite and streaming scenarios among othersThe breadth richness and flexibility provided by these technologies moves the user interface well beyond a dynamic UI to a fully interactive audio and video experience which provides new and powerful ways for people to interact with systems and with one another that are still to be exploredLightweight Programming ModelsIn Web 20 the programming models concepts and techniques are significantly different from those that have been used before Whilst they are very heavily servicebased and reliant upon the concept of messagepassing using Representational State Transfer REST protocols they focus on simplicity and ease of use This has a number of implicationsWeb 20 programming is based on the concept of separation of concern using a loosely coupled messagepassingbased model on top of an internetbased and standard set of communications protocols http which is often called RESTful programming It implies notions of syndication and composition where services are provided without knowing how or if they are used This is very different from a conventional tightly coupled transactional and often objectoriented system It has a different set of benefits such as flexibility and speed of implementation and challenges such as integrity and managementThe languages such as Perl or Iron Python and the frameworks used are simple and dynamic which provides a low barrier to entry and reuse and high productivity The frameworks have builtin support for common design patterns such as Model View Controller MVC and methodologies such as Agile They are quick and easy to pick up use and become productive withWeb 20 applications are inherently composable and compositable because they are built with lightweight programming models and standardsbased services new applications can be created by composing or mashingup present applications and services Mashups are where applications and services are composed at the UI composition is the more general case of services being reusedEnd of Software Release Cycles and DeploymentThe platform concepts behind Web 20 strike a new balance between the control and administrative ease of centralized systems and the flexibility and user empowerment of distributed systems Web applications are by nature centrally deployed so central services can manage applications and entire desktops automatically Software as a Service builds on this concept to provide the idea of software and services delivery over the internet and Web 20 builds on top of Software as a Service to provide social and content services over a webbased mechanismThis usage of SaaS by Web 20 as a deployment and release methodology provides all the wellknown SaaS advantages of simple deployment minimized management and administration and probably the most important instant update and repair Thus one of the mosttouted features of Web 20 and SaaS is the concept that the system is being constantly updated and enhanced often in realtime in response to users requests Of course the issue with this perpetual beta that the community needs to come to grips with is what happens when downstream applications rely on services or functionality that the application is providing The Read / Write Web The second important area of Web 20 is the focus on data and content and in particular the ability of people to create and interact with rich content rather than just consuming it If the original internet provided read access to data then Web 20 is all about providing read and write access to data from any source This ability of anyone to create content has caused an explosion of available content from all sources and of all types of quality At the same time it has created a whole new set of issues around vandalism and the integrity of dataAs the bandwidth available to the end user continuously increases the richness of the content that can be sent over the internet increases The original internet was all about text but Web 20 started with music and images and moved into voice and video Now TV and movies are the content areas that are being investigated as part of Web 20Whilst people and organizations have been searching uploading and downloading all this explicit data and content on the web they have all the while been creating a huge amount of implicit data showing where they are going and what they are doing This implicit or attention data of Web 20 can be used to predict future behavior or provide new attentionbased features Of course the collection storage and use of this implicit data raises challenging questions about ownership privacy and Intellectual Property IPAnother issue with the huge amount of data on the web is finding and navigating it Search engines use the implicit data to find textual data but they will not work with audio image or other binary data In addition the search engine often does not have enough contextual information to provide a valid result In these cases tagging the data becomes a valuable way of assisting with data navigation Web 20 applications use tagging and tagclouds extensively as a way of finding and navigating through the vast amount of data available on the webTag data is data about data or metadata One of the major concerns with data and content on the web arises from the lack of consistent standards for metadata and schema It is impossible to cut and paste something as simple as an address on the web because there is no standard format for addresses We need to understand the different levels of metadata and have standards for what that metadata is in order to liberate data on the web and in particular to allow composite applications to compile data This standardization of metadata in the Web 20 space is similar to the goal of the Microformat effortThe Social and Collaborative Web The third key element of Web 20 systems is the concept of social networks community collaboration and discussion People naturally want to communicate share and discuss this communication is a key part of understanding learning and creativity The unique element that Web 20 brings is that of social networks and community which are typically enabled by blogs discussion groups and wikis In Web 20 the sheer scale and number of people on the internet creates an architecture of participation where the interaction between people creates information and systems that get better the more they are used and the more people who use them This harnessing of the collective intelligence creates systems that have more and better information than any one person could generate it becomes the wisdom of the crowdThere are a number of different types of collaboration that can occur in Web 20 systems Contentbased  This is where groups gather and collaborate around a piece of news or content typically in a blog or a spacestype environment Groupbased  In group collaboration people gather around an idea or interest such as a hobby and discuss it in discussion forums Projectbased  In projectbased collaboration groups work together on a common task or project such as a development project a book or even something as large as an encyclopedia using wikisAll three types of collaboration can be used in Web 20 systems and in many cases more than one can be used Figure 81 Different Types of Web 20 Collaboration85 Software  Services Enabling Social Computing We discussed Software  Services as the realistic hybrid model to use SaaS in combination with onpremises software It provides a model to create rightsized IT for every user and gives the CIO the power to optimize the IT service portfolio Software  Services is the model for situating cloud computing in a real business scenario If we look at collaboration and how cloud computing could help there are four main reasons why cloud computing and collaboration is a natural fit Autonomous users want self provisioning of tools Both are crossing boundariesCollaboration is about being part of the conversation economy plusTraditional reasons such as cost or performanceSelf ProvisioningWhen we want to stimulate bottomup initiatives and make units teams projects and people more autonomous in creating value for the company we need to give them tools Traditionally the IT department designed and supplied the tools but we have seen that the prosumer users demand more control and faster service Self provisioning and configuration is of the essence here Cloud computing fits the bill perfectly since a lot of the services being offered online are extremely easy to provision try out and configure Most likely business users in every organization are already using them Crossing BoundariesThe best collaboration is across boundaries bringing together value from two different sources to create something new and better This is hard to accomplish when the tools to support the collaboration are strictly controlled from within one organization or unit If we want to address identity issues and answer security and ownership questions a cloud tool is usually more suited for this than onpremises corporately controlled tools Conversation EconomyThis is an important reason to use the tools from the cloud If we want to be part of the ongoing conversations and join the discussions in progress on the internet then we have to be where the social networks are And the social networks are in the cloud Also we will start to exploit the benefits of cloudserved solutions We could for example use LinkedIn to update our CRM system or use YouTube to do recruiting or scan blogs for trends relevant to our business etc Traditional ReasonsThe reasons for choosing SaaS as stated in Chapter 1 are as valid for collaborative tools as they are for other tools If collaborative tools are becoming part of the IT infrastructure a SaaS version is most logical Hosted email solutions are well accepted as are collaboration portals messaging software etc  Figure 82 In the Cloud 86 The Web as the HubThe value of the knowledge found in employees heads and in the databases and unstructured documents found across the organization has been well known for a while and there have been many attempts to collect it into knowledge management systems with varying degrees of success Clearly when people can quickly find critical information and subjectmatter experts and then work seamlessly together productivity will soar This has been difficult to achieve in the past but new technologies such as dynamic workspaces wikis and enterprise searches for people and data may lower the barriers to knowledge management and provide a platform for collaborating on complex and creative tasks Applications will grow to recognize and utilize the inherent groupforming aspects of their connection to the web in ways that will become fundamental to our experiences In scenarios ranging from productivity to media and entertainment social mesh notions of linking sharing ranking and tagging will become as familiar as File Edit and View As we noted however the real barrier to knowledge management is around social and value issues in organizations rather than technical ones These are not addressed by the technologies per se and hence the expectations from the technologies themselves should be tempered with the right investments across people and processes87 In Conclusion – Where Do We Go from HereThe most appealing and potentially most rewarding uses of emerging collaboration and social computing techniques in the enterprise are in the customerfacing areas of organizations The entire customer relationship management cycle will be transformed by the tools and techniques that are in common use in the consumer space In marketing the opportunity to exploit rich interactive media and to enable a new world of digital customer intimacy through wikis blogs and online communities will provide new ways of reaching out to and engaging with potential customers In customer support the use of video and other rich media to assist with problem resolution and the use of online communities towards selfservicebased models promises to create entirely new models and frameworks for supportBeyond the customerfacing business capabilities it is product innovation where customer involvement and participation in product design and development using blogs wikis and discussion forums is heralding fundamental shifts in what is commonly thought of as the cocreation of innovation The other business domain that has the potential to benefit from these techniques is training and mentorship where the use of rich media messaging and chat and other collaboration capabilities has considerable potentialOverall the use of new and emerging social computing and collaboration capabilities in the enterprise enabled by a platform of onpremises software  cloudbased services promises to have profound and far reaching effects on how organizations function and create fundamentally new and powerful ways of innovating marketing and selling to and delighting customers  Case  UVITCase UVIT Embracing Collaborative Technology to Benefit from Recent MergerUnivé and VGZIZATRIAS are all active in the insurance business Univé is an allinsurance company the others focus on health insurance On January 1 2007 the companies officially merged into a new company with the temporary name UVIT For the benefit of the readers we will use the temporary name throughout this case The new name will be announced at the end of 2009 The core business is to provide health insurance coverage for 43 million people – a quarter of the Dutch population Their other nonhealth insurance products cover 800000 insured This makes UVIT one of the largest insurance companies in The NetherlandsAfter the merger the companies started a centralization process The plan is to reduce the number of office locations from 17 to 5 One of these 5 locations will be the new headquarters in Arnhem which is scheduled to open in September 2009 This office has a completely different interior design The architects of the building have designed the office with a limited number of floors leading to a very open work atmosphereMerger as a Time for New OpportunitiesAccording to Mr Jo Knippenberg CIO at UVIT the merger offers an ideal time to reflect It is a moment to closely examine the existing organizations and think about what the new organization will look like after the merger It is a time to zoom in on the details when you can look at what went wrong and what went right in the past and then take these lessons into account when developing the future organizationThe merger offers a a unique opportunity to create a new setting It is a moment to think about how people work how they collaborate with people inside and outside of the organization and how technology can help How do you forge all these elements together to form something that contributes to the greater good to your clients your prospects to your relationsChallenges in Collaboration and Corporate ImageMr Knippenberg faced a couple of challenges in daily practice during and after the merger and subsequent reorganization The first challenge that arose during the merger process itself was how to best support communication because without communication there is no collaboration and without collaboration you essentially do not have a unified company A closely related question was how to stimulate and enable the employees to create effective collaborations among themselves How do you make sure everybody has access to the right information in the correct form at the right time and the right locationThis of course is not only relevant for employees but just as much for third parties The new possibilities here are way beyond the patterns used in traditional outsourcCollaboration in the Cloud ing New forms of collaboration can emerge as the result of the use of new technologies and tools It could very well be that the party that is best able to collaborate with his competitor is the party best positioned to determine the future Another challenge that UVIT is trying to solve in the process of the merger is how to fix the problems with the corporate image Traditionally an insurance company is not a sexy company to work for rather dusty and stuffy instead This poses challenges whey trying to recruit new employees since you also want to attract good people and young talent They represent the future of business How do you create an environment a workplace where these people feel at home The New OrganizationThe big question of course was how to respond to the challenges above One of the actions after the merger was to reduce the number of offices Of the 17 only 5 remained The new headquarters was designed to fit a completely new philosophy As a departure from the old situation people dont have fixed offices anymore but will be equipped with laptops and mobile phones The idea is to create a more open and flexible working environment offering new options in how to work and collaborateNext to this upcoming bigbang change other modifications have been made These changes were first implemented with a small group of people before addressing the larger worker population First see if the experience is satisfactory before implementing on a larger scale As an example currently people are using Microsofts Live Office Communications Server product This product can show the status of people free/busy which sometimes produces resistance Employees sometimes see it as Big Brother is Watching You Use of this function is voluntary and not required by the company Experience shows that groups start using it whenever they are ready for it If one sheep leaps over the ditch others will follow And greater adoption immediately leads to new questions Why would we only use Live Messenger inside our company and not directly with our customers and suppliers Where Communication Server saw slow gradual adoption Microsoft Office SharePoint Server was immediately more widely adopted One of the most interesting consequences is that during regular meetings attendees no longer use paper Meeting minutes and notes are directly recorded in SharePoint Employees including those who could not attend can read back the notes immediately after the meeting Here too the process of adoption is a gradual one It has to become part of the normal way to do your job On the one hand this of course takes some training on the other hand its a toolbox with a variety of tools from which employees have to select the right ones themselves198Case  UVITSocial Networks Build the OrganizationFrom the commercial side of UVIT there was a growing demand for Facebooklike applications At this point UVIT still maintained a strictly controlled environment where it was not possible to use all sorts of applications available on the web To meet the demands a new organizational mindset would be needed Such radical changes in policy are more or less prescribed by the outside world UVIT wants to find a way to accommodate those kinds of options but this is still undiscovered terrain Control versus freedom what is the wise course UVIT is currently in an exploratory phase when it comes to these issuesMeanwhile a special UVIT social network was born and is being used frequently by a small group within the IT department of the company Every employee is responsible for keeping his or her profile up to date with relevant information and added value to the company Using someones profile you can directly explore the relations between colleagues Who is close to whom Who is his or her boss Who is the boss of the bossIn the old days we needed all kinds of organizational diagrams This is no longer necessary The hierarchy has become selfmaintained A new employee joins the organization via his or her profile and can immediately contribute New employees are also immediately part of the organization An organization is very dynamic The organizational movement of people is a dynamic in itself and this social networking tool is great way to gain insight into this dynamic Technology and opportunities happen to enter your life The same happens to your company As a company you have two options you resist or you embrace At UVIT the latter obviously was the caseTrust Means EverythingUVIT is very aware that only providing the hardware and software will not make a new company To create a new firm the whole concept has to become part of the DNA of the employees It is a cultural thing not something ordered by the top management but embraced by the people on the work floorTrust is of the essence in this transformational process Without trust none of the things we have envisioned will happen We have to earn the trust of our employees Both employees and management have to trust each other trust that we are building the new company together Trust is one of the key factors of our new emerging corporate culture  9 Fourteen Questions to Guide the Revolution91 IntroductionThroughout this book we have been promoting a new way of looking at your organization of creating bottomup management and better supporting crossboundary collaboration by using Software as a Service All bundled up it amounts to nothing less than a revolution in the way you do business and the way you support business with IT This chapter will help you keep a cool head by providing you with fourteen questions to continually ask in order to keep the revolution pragmatic and to facilitate any initiative that will help to achieve your goals Fourteen questions to keep in mind to prevent being swept away by hypeMost of the answers to these questions will depend very much on your situation and on the specific project or challenge you are trying to address Still we can sketch certain elements that should be considered when answering these questions Questioning EvangelistsThe IT industry is composed of a myriad of vendors and service providers each offering their own value and point of view Trying to convince you of abilities and optimistic about the future they will offer to solve all your problems using the latest tool or insight So what do you ask when a vendor talks about a new collaboration solution What do you say when someone from the IT department stops by your desk to talk enthusiastically about the newest project regarding wikis or SharePoint You can use this list of questions to keep your feet on the ground Ask the questions below and listen carefully to the answers you get and whether the important points are coveredIf you are not an IT manager but a provider or someone inside the organization who wants to see changes in the way collaboration is performed this list will help you prepare the answers you will need to convince and involve others You will have to know the answers since the questions will be asked eventually even if not at firstThe questions and conceptual answers are based on frequently occurring businessIT alignment issues They are distilled from failed and successful collaborations or SaaS projects and from the personal experiences of people working in the collaboration or SaaS space Some relate to collaboration specifically while others are SaaSrelated 92 The Fourteen Questions to Guide the RevolutionWhats whatWhos collaboratingWhy collaborateWhats wrong with emailIs technology all we needWhen is the next version dueWhat does it costHow does this integrateIs it secureWhere do I startWhat is our competitive advantageWhat boundaries are we crossingAs a service Not as a service Mixing Software  ServicesWhen have I wonWhats WhatWhat does collaboration mean What is cloud And SaaS and SS What do we mean with all the terms Can we agree on terminologyThanks to the state of marketing and the chatty and dynamic nature of the internet numerous interpretations will surface for anything new Consequently an essential part of any project these days will be to examine the concepts assumptions and terminology used What do the different partners who are in conversation to create strategy understand the terms to mean What definitions and associations do people use when talking about the topic Is collaboration something focused on people or focused on corporate relations Is cloud something cool and userfocused or is it technical infrastructure You cannot expect two people to understand a term exactly the same way unless they have talked about it As we have seen even the term collaboration can cover anything from businesstobusiness platforms to email document sharing or conferencecall solutions To create this shared understanding of the concepts and terminology a brainstorming session a workshop or ongoing conversation can be very useful Experience tells us that peopletopeople contact works better than reading a website or one person defining meaning for everybody This book a website or an external expert can effectively provide the starting point for your own discussions Whos CollaboratingWho are they Where are they What kind of people Are they from our department Are they only IT people Whos collaborating with whom Are they from another company In what time zone are they What language do they speak Whos managing themTraditionally collaboration is seen as something between colleagues or even between people working in one project As we have shown collaboration is a much wider field addressing all interactions between people or even companies crossing organizational and cultural boundaries The question of who is collaborating will give you insight into the challenges you will or will not face when creating a collaborative culture Looking beyond job titles or department names and really examining the people and their characteristics gives you an idea of what you will run into in trying to stimulate or enhance their collaboration Once you have established whether the people belong to one organization or work across organizational boundaries you will be able to look into the matters of provisioning ownership and confidentiality What to look for in an answer in trying to formulate an answer to this question talk about which people will collaborate across which boundaries and what kind of people they are technical business older or newer generations etc A good way to define the different collaborations and see who is involved is to create scenarios storylike illustrations that describe a day in the life of the different audiences Another way to find and define the different collaborations is to make use of existing technologies Tools are available that help you explore your social network at the push of a button The result of this network crawl can then be used to visualize the different kinds of relationships that exist inside and outside your company The results will surprise you Some people within your organization may play pivotal roles as community leaders of which you werent aware before3 Why CollaborateWhy would they want to collaborate Why do we want them to collaborate Are they already collaborating Whats in it for me Whats in it for themPeople collaborate by themselves automatically They will seek help delegate tasks and look for contributions from others When addressing collaboration from a corporate view the question should be asked at these three levelsWhy do we need to address collaboration from a business perspective What will be the corporate gain for addressing it across the enterpriseWhats the benefit from a teamperspective of better collaboration What will get the project manager or team leader excited about this initiativeWhat will motivate an individual to collaborate differently Whats in it for the individual userIf you are missing one of these levels it will make the adoption of new ways of collaboration problematic The goals you are looking for with these questions can have the form of real business cases for example saving time and money that is currently wasted on ineffective collaborations or increasing the quality of work in an administrative environment through improved communications and response to anomalies Goals can also be more indirect helping recruit and retain people by making their daily activities more fun and engaging thus creating a better work environment and better knowledge retention leading to ultimately some higher business goals The cascade of goals must be rooted in reality preferably confirmed by the actual people who will be impactedAs we have learned from Open Innovation or Crowdsourcing at the individual level the drivers for collaboration can be very businesslike money efficiency in achieving goals doing a better job or very personal peer status social interaction curiosity Any answer talks about the goals at the different levels how they strengthen each other and how they ultimately contribute to explicit corporate goals Every good answer will also include a personal answer for the person asking the question and whats in it for you personally is …Whats Wrong With EmailWhy has email worked so far Will any new solution be better than email Why Will people stop using email Do you still use emailAnyone coming in to talk about new ways of collaborating will need to have a vision of how email fits in the picture If email remains the preferred medium alternatives will never reach full acceptance and maturity As an analogy if we were to give people bicycles but they still wanted to keep their feet touching the ground all the time – because they always have – we would not gain any speed by distributing bikes if anything it would slow people down The same goes for the introduction of better support for collaboration the advantage must be demonstrated tried and proven to be a true advantage over email Document creation in an online environment is much more efficient then sending links emails and keeping multiple copies yet only when this is shown in practice can it convince people to stop using email for this purpose Email itself has been too good for its own good Its ease of use and wide adoption have made it the burden of presentday working life for many people There are now even special courses in how to handle email efficiently In essence email is an old concept ported to new technology the paper letter replaced by the fax then by email still sticks to an old distribution model You write something address it post it let it go and wait for someone to respond – or just hope it all works out depending on the kind of message More advanced solutions as an addon to email have been around for a long time project management tools delegating tasks and reporting are examples found in most presentday email tools Yet the actual use of these features is very limited and they are hardly ever used correctly and to full advantage Not because the features arent useful but because nobody ever explained them or people never got used to them or there was no corporate or team advantage and thus no real support was given to implementing the features A different way of looking at email is needed in order to move forward By asking this question repeatedly you can begin to more sharply define how any new solution can and should be used An answer talks about the current role and use of email in your organization perhaps the problems of email with regard to process control versioning knowledge management governance etc It will talk about how different solutions will take chunks out of everyones email load leaving email as a minority medium used only where the other tools and communication channels do not suffice Is Technology All We NeedWhat else needs to be done What needs to change What does your solution NOT address How much should we spend besides this solution to get the right results What is the most difficult partIf a vendor or service provider states that collaboration will be greatly improved using a piece of software or a service in the cloud perhaps sticking to the build it and they will come adage it clearly shows an incomplete perspective A tool or service can definitely help but as we have demonstrated it is only a minor part in the larger effort to change how people work and how we provision IT support for it Creating a strategy that is founded and funded on the business side of an organization with a practical approach to delivering shortterm visible results would be the ideal combination A strategy would also include statements as to how to anchor the collaboration strategy in the organization whos the owner what are the activities that we are trying to improve or support and how will this evolve over time What to look for in an answer it will talk about the role and impact of the tool or the service the architecture how the tool or service will fit in an everchanging environment the people usage and the change that is needed in culture and managementWhen Is the Next Version DueIs this a stable solution How long before we need to change this to the next version What will be next Will this work in ten years time Can we radically change business processes and structure and still use it One lesson from the past is that we can always expect change Our vendors will keep offering new features and solutions but even more importantly our businessenvironment will keep changing rapidly In a globalized world business is changing at realtime speed Especially in the SaaS world versioning and upgrades are a continuous process Currently it is hard to change to a different set of tools user and developer training new licensing costs and other investments have to be made Given the certainty that the only thing we can count on is change we need to plan for change in the IT space too We must ensure that either replacing tools with the latest version or different tools is extremely cheap and easy or that a particular solution will be able to withstand a lot of change going forwardBoth solutions easy to change and resistant to change revolve around the use of standards and the actual design of the tools and services themselves Also the architecture surrounding the tool or service must be ready for change When selecting technology attention to open standards insight into future product roadmaps or testing changeability can help to prevent getting stuck with any solution For example a good way to test the changeability of any solution is to start a proof of concept where a radical change to the configuration has to be made within a minimal timeframe demonstrating that future investments can be kept low when changes are needed Also it pays to examine the way a vendor or service provider handles changes is there an open forum discussing features and updates or is it a closed innovation process How long are multiple versions supported if at all How accessible is the service provider for feedback and support All these will give insight into the risk of change their change and your riskA lot of software available on the internet is in a permanent beta stage Can your company handle that In the past most companies waited for the next version or the first service pack before implementing something This attitude might not be wise in the now realtime economyAn answer to this question will show how change will not affect the core elements of a solution how new needs can be met with the current tool or service Also backward compatibility open standards and release planning are part of the answer Ultimately the solution should fit your overall IT and business architecture which in itself of course is also adaptable to changeWhat Does It CostHow are we paying for this What are the costs over time What will it cost to abandon this choice in the future Does this make sense from a financial planning perspectiveSaaS is often sold using the payment model payforuse with little upfront investment It is often a variable cost making it attractive to bookkeepers and giving the illusion of being easy to scale down in times of need In reality comparing cost is actually hard to do How do we get the real cost of hosting something onpremises and how do we compare it to renting something as a service taking into account an uncertain future When contemplating the model of software and services and finding the right balance an honest comparison must be made Take into account risks costs of human resources costs of downtime and ongoing licensing and usage costs for different provisioning scenarios In particular the cost of extra support or unexpected growth in usage can drastically tip the scales in the comparison if a contract is not carefully screened in the course of this comparisonWhen estimating cost we look at the contracts of any as a service offering and compare it to onpremises numbers and estimates of support staff overhead etc Pay special attention to the cost of scaling and exceptions A useful method can be to describe several scenarios then for each scenario estimate the likelihood that it will become reality and then calculate the cost as a range How Does This IntegrateHow does the tool or service fit into my existing IT infrastructure Are we using the same user database Is it integrated into my Enterprise Search How does it impact my content management system Is it integrated with my email system What else needs to be done to assure good integration How do we migrateThis is one of the hardest questions for any new IT solution and even more so for functionality provided as a service Often the simplicity of the solution provided in the cloud will look attractive but soon there will be a demand from users and management to integrate it with existing systems perhaps initially for signon credentials but quickly also for integration of all kinds of other data such as client information stock data or internal financial feedsWhen looking at collaboration services the need for integration with existing data might not be obvious at first but soon it will become evident that it is closely tied to information management tools content management business intelligence governance structures users rightsmanagement and existing presentation platforms websites portals desktop environment The closer systems integrate the lower the barrier to using the technology and the greater the chances of widespread adoption A platform that is disconnected from everything else will either slowly die out due to the difficulty of keeping data in synch with other sources or it might become the new authority leaving data in other sources more and more unreliable Existing basic interfacing standards allow for integration but beware of solutions needing lots of manual integration it might make the as a service solution more expensive and more difficult to maintain than something installed onpremisesWhen talking to vendors or service providers or when selecting a service from the cloud the options for integrating should be among the most important decision criteria Over time as the cloud matures we would expect more and higher level standards to become available making integration slightly easier At the same time this is also an important question to ask your own IT staff since their estimate on ease of integration might differ from that of the service provider What to look for in the answer it will demonstrate how the technology fits in with – and makes use of – existing data it will show how business intelligence and integration of information will help create better interactions between people 9 Is It SecureWill others be able to access my data Can my employees do things that are bad for our company Will we run into trouble with the auditors This question is brought up most often by people who say that SaaS will never happen Because how can we rely on the confidentiality or security of a service running somewhere in the cloud on physical hardware that is shared with perhaps thousands of unknown others from countries all over the world Yet there wasnt much concern when the business world introduced the Blackberry mobile email solution confidential email flows through servers worldwide without it causing too many worries In the end its not just about security but about trust and service levels can we trust the service provider to observe the terms of the SLA regarding security and availability confidentiality and all the other elements that make up an SLA You do need to keep asking this question perhaps not so much as to prevent SaaS from happening altogether but to make sure that any SaaS solution you introduce fits corporate policySurprisingly sometimes the people who are the most notorious detractors can turn into true supporters if engaged early An auditor will help find solutions to keep an external service conform to auditing rules a security officer can be engaged to scope an SLA to cover the basics needed to keep new services from compromising corporate security etc10 Where Do I StartHow do I start How do I manage this change What are the first steps How do we maintain progress over time Is there an easy way in or are you proposing a big bang How can we be sure every step of the way has valueAs a rule big bangs dont work Experience tells us that the best way to improve IT or organizations is to create an active evolution toward a future goal allowing for many small steps and minor or even major detours along the way Improving collaboration is no different start small where the chances of success are largest and a win is likely Create support among techsavvy early adopters but aim for greater late majority adoption by the less techsavvy users by listening to their demands and feedback Start small with a perhaps selfselected group of enthusiasts and then spread the initiative using the early adopters as seeds within new groups The first advances will be in the easy category perhaps only later involving different organizations time zones languages cultures etcAlso when trying new ways of collaborating freedom is key dont oversystemize the initial solutions but allow for enough freedom to see what works best If the freedom is essential to the collaboration it might even be part of the final solution If the freedom is too much for the later adopters the early experiences will tell you how and what to formalize It is important to note that this freedom is not just in the tools and support but even and especially in the way the collaboration team is managed Experiment with different levels of involvement to find what works The answer to this question for your organization will talk about the scope the people and a situation where improved collaboration will provide immediate value to the participants Expect to think small to select a small group of people who will start using the new technology If they are happy with the new technologies they will automatically become your evangelists who will spread the tool within the organization11 What Is Our Competitive AdvantageWhat are we doing that competitors arent doing What are we doing differently How is that visible in our IT investments How is that visible in our systems architecture How quickly will this competitive advantage erodeIf you are replacing a proprietary solution with a commodity solution either as software or as a service you need to ask whether you dont squander your competitive advantage by the move And vice versa when looking to find specific solutions for parts of your business that are definitely NOT differentiating you from the competition it wouldnt make sense to build a custom solution when a commodity is available Especially when contemplating SaaS solutions and the levels and methods of customization a keen eye is needed to define and manage competitive advantage Most often its not the tool or service itself that is part of your advantage but its the way you use the tool the processes and configuration that make the end result unique So if you are in the market to procure a service that allows for NO custom process and NO configuration dont expect to beat the competition with itAs an example imagine a small web retailer that has thought of a unique way to checkout and pay for online shopping while a competitor would differentiate on price assortment branding shipping or some other element Our web retailer would probably choose to build or configure a specific shoppingcart and checkout solution while the competitor would be happy to choose one of the many standard shoppingcart services available While the example is one dimensional when creating a businessandIT strategy these are the choices to be made Answer this question by looking at the business strategy and the competition and translating it to focus on areas within the IT portfolio that are more important than others that have higher priority than others and should probably be more flexible than others too What Boundaries Are We CrossingAre we connecting to anybody or anything outside our department or company Are we crossing internal or external boundaries What is the rationale behind these boundaries Where do they come from Will people object to crossing these boundaries How can we make it easier to crossIn business and IT more often than ever we are crossing boundaries we are using external resources we are working with people from other organizations Be it in a supplychain collaboration or in a buyervendor relation we are not simply exchanging money and goods but were also exchanging information So when thinking about any project the specifics that define your interactions with the environment are the ones that ultimately define your company You ask this question to find the best model for provisioning solutions You also ask this question to get an idea of how difficult it will be to create or improve collaboration the more boundaries you are crossing the more work needs to be done to cross these boundaries If all people collaborating report to the same manager it will be easier than if they report to different managers let alone different companies In answering this question look for a description of wellknown explicit boundaries but also look for implicit or cultural boundaries that might handicap successful collaborationAs a Service Not As a Service Mixing Software  ServicesCant we host our own Should we host our own Should everything that we have be as a service Is there an actual business case for SaaS in this situation How do you address the insecurities that come with SaaSYour instincts might still tell you that to minimize risk and dependence upon others owning and hosting everything inhouse or onpremises is still best The case for providing Software as a Service has been made many times over yet the reality is that owning IT solutions gives the illusion of greater control When exploring the right mix between services that are in the cloud and services that are owned maintained and hosted internally a rational view of costs and risks is essential Include the actual costs of onpremises people hardware licensing and increasingly power and compare them to the costs of an offpremises solution Compare the features of generic solutions with the features of custom created solutions Compare the risks of downtime and disasters to SLAs that are not met etcAs mentioned before any conversation around the as a service question will revolve around setting boundaries What functionality is generic what is specific to us It will also talk about the different levels of as a service hosting and support on a hardware platform servers is a different level than generic partsolutions collaboration portals and is different from a business solution or service CRM The technologylevel services might be easiest to use but the businesslevel services will provide the greater benefit provided they offer the right services needed by your organization Finally keep in mind that the model of as a service is also a valuable model for IT departments to use in defining and offering their own services The choice then becomes not so much between as a service and not as a service but rather between as our own service and as someone elses serviceWhat to look for in the answer it will take organizational strategy and input as to competitive differentiators to define what to have and what to hire what to produce and what to procure The answer will define the boundaries of the organizationspecific IT 14 When Have I WonWhat defines a successful implementation How can we measure success What would be the endgoal for this solution Can we track progress Who will determine if this was a success Are there early warning signs of failure we should watch out forFor any project or initiative the definition of success is probably half of the success itself Once we know specifically what we want or need it becomes easier to get it Collaboration is an area that is often ventured into without a clear definition of success we want to support people but we cant predict how they will adopt it or even We want to install a portal While the exact usage might be unpredictable and perhaps even intentionally so there must be a way to track progress There must be a way to know if things are evolving in the right direction or if perhaps corrective action is needed The measure of success for any organization will of course depend on specific circumstances At the same time there should be early signs of change in the way people work and the tools they use Some things to look at could be  Email is used lessWhen people come to work they start their portal tool collaboration environment before they open their emailUsers start talking about their collaboration portalNews and success stories are no longer spread using email but posted in the right places and People start experimenting with new additions to the tool create mashups or start requesting new featuresAfter the new situation is starting to become normal the real business benefits should become visible better knowledge retention better responses to exceptions quicker answers to questions from clients or suppliers improved success rates on proposals etc Ideally the signs of success are visible at the personal team and organizational level When answering this talk about how to track progress how to correct or stimulate if progress is not as expected Look for the early signs of success and know how to measure business success Case  Toyota Material Handling EuropeCase Toyota Material Handling Europe Sheds Paper Processes for Mobile CollaborationIT Seizes Opportunity to Establish Collaborative Best PracticesA few short years ago the service operation at Toyota Material Handling Europe was living in the dark ages technologically speaking Toyota Material Handling Europe TMHE began operations in 2006 to manage the Toyota and BT materials handling business in Europe With more than 100 years of combined Toyota and BT experience they are active in more than 30 European countries TMHE provides a complete range of Toyota counterbalanced forklift trucks and BT warehouse equipment supported by services and added value solutions TMHE is the European regional organization of Toyota Material Handling Group TMHG which is part of Toyota Industries Corporation TICO the world leader in materials handling equipmentThat is service personnel at TMHE were drowning in paper processes Technicians got their monthly maintenance plans on paper Then they submitted paper worksheets with feedback on customer calls and it took back office staff sometimes up to two weeks to update customer histories invoices and parts replenishment data Whats more the processes differed by country And with 4800 technicians spread amongst 30 countries making more then 35 million service assignments a year that meant a lot of unnecessary complexity as the back office was asked to translate widely varying types of forms and data into useable intelligence about customers maintenance needs The technology team looked upon the situation as an opportunity to implement best practices that could help standardize data introduce a layer of collaboration and give back office staff and field technicians better access to more uptodate information The result would be improved service efficiency and thus increased service revenue Because more than 3300 of the technicians work from vans mobility was a key considerationTeam Approach Yields Streamlined Mobile ProcessWorking with a team of 10 back office staff service technicians and IT personnel from Toyota Material Handling Sogeti consultants joined representatives from Lawson Software and field service automation specialist Intermec in building an integrated solution that today combines a Lawson ERP environment a webbased interface used by back office staff and a mobile Intermec application built on Microsofts Net framework That application resides on the 2200 PDAs that have been deployed and are in use by mobile technicians When a customer initiates a call with one of 400 service center personnel and a nearby technician is located the service dispatcher is able to immediately push information such as the customer history and directions to the customer site to the technicians PDA The back office can continue providing the technician with any information he or she needs such as safety and inspection rules or even broadcast bulletins to all mobile technicians Conversely technicians can communicate directly with the back office to provide information such as a corrected vehicle identification number that can be instantly updated throughout the system The PDA is also equipped with the diagnostic tools and documentation needed for technicians to troubleshootWhen a service call is completed the PDA is used to capture the customers signature and submit it electronically along with the job worksheet directly to the back office allowing service center staff to review the job immediately to better ensure quality of serviceSystem Designed to Combine Standardization and Local TouchDespite the obvious gains it knew it would achieve in transitioning from paper processes to automated ones Toyota Material Handling Europe also made sure to take into account the myriad of potential cultural impacts such a project can have says Bo Sivenius director of IS promotion Among the factors considered were how a core process is implemented across operations in multiple countries that are running sometimes widely varying technologieshow to standardize forms sufficiently to create uniformity while also leaving room for local customization andhow older employees are affected when asked to embrace a new technology Solution Delivers AwardWinning ResultsThe total project – inclusive of a wholesale business process change extensive education and training for all the technicians and significant software and hardware purchases – required a substantial undisclosed investment It was money well spent says SiveniusThe payoff has been quite good he says Were being perceived as a more innovative efficient company In addition to improved customer satisfaction Sivenius says the effort has resulted in more accurate information improved cash flow increased efficiency among technicians and a 30 reduction in back office costs It also has garnered recognition as the third release of the system was named Best Mobility Solution at Microsofts Net Awards in Sweden last year The fourth release with updated technology as well as support for additional countries and languages will go live in the first quarter of 200910 Debunking Collaboration Myths101 IntroductionThere has always been collaboration Our ancestors worked together in their struggle to survive and their descendants did the same Up to now collaboration has always occurred among small bands huddling together in strongholds Companies organizations and groups of people act as selfreliant entities in battle with the outside world After all there can only be one bestThe internet has however brought about a change The introduction of new technologies has caused small cracks to appear in these bastions New forms of collaboration have been created by people who are looking beyond their companies own wallsThis expanded work sphere introduces an extra measure of complexity as it involves more than just collaboration between people It also requires collaboration among companies as reciprocating entities that together generate a completely new chain of value Furthermore it requires collaboration among computers in the form of mashups and the cloud for exampleThe Internet of the future will be suffused with software information data archives and populated with devices appliances and people who are interacting with and through this rich fabric – Vint Cerf Extremely wild rumors are circulating about all these new forms of collaboration This chapter is intended as an antidote to these myths Each section will debunk a specific tall tale that is currently making the rounds102 The MythsThe Tool Is All You NeedMake me a community is a request that companies are frequently heard to make They think that by simply installing a social tool such as a wiki blog or forum the entire company is immediately transformed obtaining the stamp of Web 20In practice the process runs entirely differently Tools do not make an organization people do The employees have to put these tools to use they must propagate and spread the underlying ideasCompanies have a history a period of existence during which they have succeeded in creating a certain culture now more or less inscribed in the companys DNA And this genetic inscription is not so easy to rewrite or overwrite It has become a feature that distinguishes one company from anotherTherefore change must occur gradually It can be stimulated by employing a social tool but must certainly also be reinforced by a small group of people capable of motivating and inspiring other employees to use these applications Only when the number of users of these tools has reached a certain size at which time a certain tipping point is reached in the organization only then can this behavioral change be written into the companys chromosomes and as a result a new species of company with a new form of collaboration be created It is consequently an evolutionary process and not a revolutionHumanity was not created in one day There is 7 million years of evolution inscribed in the human genomeMust Be Invented Here Many programmers fall into this trap Code that is written by another developer and has to be applied to a companys own program never fully satisfies one hundred percent of the requirements The code must always be tinkered  with in order to make the best possible use of it The code is made the companys own so to speakThis attitude has been adopted by many companies even in terms of all the web services that they now use anything not built by the companys own IT department cannot be any good This view is of course incorrect Hidden behind service providers are often communities of intelligent people attempting to earn their living by means of the service as incredible as this may sometimes seem How could such an industry possibly survive if its practitioners were only offering ramshackle services If such were the case would they not just be digging their own gravesIn fact the web runs on trust Parties throw in their lot together in order to jointly profit from a given situation As soon as one of the parties harms this online relationship then the defaulting party will suffer the consequences The web is especially unforgiving in this regard It is a big machine that relies on reputation and holds a grudge thanks to search engines past indiscretions are remembered forever Parties must therefore always endeavor to put their best foot forward failing to do this means that they soon will be stepping into their grave In order to earn trust organizations have to be transparent Show the outside world what youre doingIt is certainly good to realize that a great many services exist in a permanent beta stage they are never finished Millions of people currently depend on online services for example for email that have never been officially released from the beta stage of their development There are often no guarantees Data Cannot Be Made Secure Except Behind a Closed DoorMany companies are afraid that the internet is not safe A great deal of money is spent by IT departments on making company computers secure against all kinds of external viruses and other marauding threats from outside The danger is however somewhat of an illusionMany employees have complete freedom on their home computers and want similar freedom at their workstations as well When the IT department does not cooperate in this regard then employees are often smart enough to find ways to nevertheless realize their desiresIt would be much better for companies if they schooled their own employees on the dangers of the big bad internet Every day sees another new article posted on the internet describing how yet another company has allowed sensitive information to leak out Often these breaches of security result from the thoughtless actions of company employees A slight lapse in thinking and information is suddenly available on the street Companies need to alert all their employees to all types of dangers that risk loss of information Policy must be geared to this point and this policy must be embedded in employee minds As a consequence security is a continuous process in which companies are constantly training and retraining their employeesTools Change People Were seeing an evolutionary change The people in the next generation who are really going to have the edge are the ones who master the technological skills and also facetoface skill Recent research has suggested that the internet is causing our brains to function in a completely different manner than the way to which we were accustomed With increasing frequency we are basing decisions on rightbrain activity creativity and intuition are becoming progressively more importantTools such as the internet transform humanity and humans then modify their tools The result is a circle from which there is no escape but that may indeed be virtuous It is important for humanity to take the lead in this dance Without human beings there are no tools Therefore close and constant examination of human behavior is crucial in order to adapt the tools to meet human needs in the best possible wayIn the near future there will be all kinds of intelligent tools agents bots swarming the internet tools that are intelligent on their own tools filled with all kinds of artificial intelligence that will help us These tools will facilitate our on and offline collaborationTools are therefore subordinate to human beings extensions of man They are only facilitative and can never be dominant Collaboration is DifficultPeople need each other in order to achieve certain goals If we did not band together with other people we would still all be living in the Stone Age We would be cowering in holes in the ground in order to escape as much as possible from the various threats of the external worldPeople are therefore accustomed to working together which is not to say that collaboration is easy Collective action has always been complex and the internet has only multiplied the degree of complexity In a previous age you could look each other square in the eye when making a deal now you have to rely on a virtual personality who might be located on the other side of the world How do you build up a relationship with such a remote person in order to do business together and realize a shared goalExamples from practice such as the online encyclopedia Wikipedia show how it is possible to work together on the internet and collectively attain results Such web collaborations make completely different demands on companies and their employees Instead of closed they have to be open organization is no longer topdown but bottomup And consequently in the year 2009 there is an entire list of traits that employees and companies must now acquire in order to surviveThe section above has already suggested that an entirely new race of humans is emerging and that these new men and women are using their brains differently than their ancestors did This development is literally and figuratively an evolutionary process Companies will therefore have to invest in training in order to bring these types of employees to maturity and enable them to realize their full potentialThere Is No Final SolutionEvery company is different What works for one company does not necessarily work for another In fact companies can be viewed as a kind of jigsaw puzzle so to speak As result of the most recent technology the pieces forming the puzzle of company profitability can now also come from locations outside the company itself Companies must therefore actively be on the lookout for external pieces that fit well with what they already have One assembly of the jigsaw can result in a picturesque landscape another might end up as a self portrait while a third attempt may yield a cubist interplay of linesNo single solution can be the right one forever We live in an aroundtheclock economy in which people are always capable of contacting each other The possibility of radical overnight change makes it necessary to constantly reexamine the ways in which companies must operate And this continuous monitoring of feedback also evolves as it adapts to a type of business organization undergoing constant revision in response to changes in the environment and in anticipation of future developments All this looping occurring as far as possible in real timeIt Must All Go OnlineMashups and clouds are now making it seem as if everyone just simply has to release everything and publish it on the web This is of course untrue Critical information that enables a company to distinguish itself from competitors is typically information that companies want to keep within their own walls There are of course unforeseeable consequences of allowing competitors free access to such information Commercial distinctiveness is then instantly lost and a company is robbed of its ability to provide added value Companies must certainly keep a close eye on their own value chains Parts of the business process can undoubtedly be easily outsourced The issue is therefore to identify the element that can be replaced by a service in the cloud Making this determination could result in the creation of new value chains extending beyond company walls In such circumstances companies must focus on what they do best outsource things that others do better and keep an eye on the ecosystem of services on offer A service that is perfectly good on one day might on any subsequent day be replaced by a still better service from another party The field in which companies are now operating is so dynamic that they cannot permit themselves the luxury of overlooking any opportunities The IT department in collaboration with the business department must constantly be looking out for the next environmental changeIt Is All or NothingIt is best to begin small A company cannot of course transform itself all at once In just procuring the tools a company is far from finished its process of adaptation It is ultimately the companys people who must be changed along with the associated business cultureA big bang scenario will only run into resistance It is much more sensible to follow a gradual approach Allow the company to first become familiar with a blog or a wiki When one of these tools is accepted by everyone and people are capable of using the tool on their own the next tool is then made availableBe ready in advance to accept the fact that not every tool will be accepted by everyone If such is the case dont try to enforce acceptance but withdraw the tool Attempt to discover what is causing the resistance gain consensus and offer a new tool in order to achieve similar results Tools might be used differently than was envisioned Dont be scared let it happen the results might positively surprise youIt is a question of trial and error Web 20 might not only transform the heart of the business but also most assuredly the hearts and minds of employees Collaboration Will Not Work in OldFashioned HierarchiesOn the web we can find all kinds of examples in which people collaborate One of the common myths within all these examples is that they only work within bottomup organizations Leaders are no longer needed The old hierarchies have to die in order to create a true collaborative organizationClearly this is not true Organizations without leaders cannot exist However the role of a leader – whom we have implemented within our organizations since the rise of the industrial revolution – has to change Leaders can no longer work the oldfashioned way by pushing orders topdown to the working peopleThe new leaders must listen to the ideas of the working people Encourage the people and their ideas Empower them in order to spread and embed those ideas within the organizationsManagement guru Seth Godin has written a book Tribes We Need You to Lead Us  about this subject From his point of viewManagement is about manipulating resources to get a known job done Leadership on the other hand is about creating change that you believe inLinda Dunkel and Christine Arena published a white paper called Leading in the Collaborative Organization How Collaboration Drives Innovation and Value Creation in Todays Corporations  In it they sayCollaboration is not about shifting from commandandcontrol to coax and cajole Instead collaboration is an essential tool for the new kind of business leader – the facilitative leader – one who engages relevant stakeholders in solving problems collaboratively and works to build a more collaborative culture in his or her organization or communityThe Credit Crunch Will Kill CollaborationWe are now experiencing difficult times Every country every organization is facing the fact that the financial crisis has vaporized zillions of dollars How are we going to survive this disasterOrganizations now have the choice Do they stop investing or not Will they sit on their money and wait for better times or will they invest in innovations hoping that they will be the new leaders in the near futureCollaboration within and outside the organization can make a huge difference in these harsh times By using all kinds of online tools organizations can tap into the collective mind of their customers Plug in to their knowledge in order to improve products invent new ones and survive Collaboration is the killer application for murdering the financial beast103 ConclusionIn an interview on November 1 2008 Wikipedia founder Jimmy Wales stated that Were really just at the beginning still of collaborative efforts In video right now were still back in many ways in the Web 10 era If you look at almost everything on YouTube its individuals doing videos either funny cat videos or drunk girl videos seem to be quite popular there What we havent seen yet in video is largescale collaborative projects And with this observation Wales hit the nail on the head Considering the time that has passed since the introduction of the internet as well as the disruptive effect of this new technology on entire industries it is impossible not to conclude that the impact on our society has been enormous And the repercussions are only just beginning Some of the worlds most prominent companies online have only existed for less than 10 years All of us are standing on the threshold of a fundamental transition The ways in which we are accustomed to doing business are based on an organizational form created in the industrial revolution an historical phenomenon that reached its zenith at the end of the eighteenth century The blessings of the internet have entirely transformed our overall worldview It is therefore no longer possible for us to continue following the road that we first began travelling back in the seventeenth century We are at a crossroads but do not precisely know which road to takeAccording to Al Gore the technologies behind the internet can save us allNow is the time to really move swiftly to seize these new possibilities and to exploit them… Web 20 has to have a purpose The purpose I would urge as many of you as can take it on is to repair our relationship with this planet and the imminent danger we face It is therefore not so strange that a large number of horror stories are now cropping up However we should not let these ghost stories frighten us Instead we need to continue to probe to the very heart of the matter in order to discover the new types of collaboration that the future has in store for us In this way we will be able to make use of the emerging new practices in our own companies as quickly and adaptively as possible About the AuthorsErik van Ommeren – http//wwwlinkedincom/in/erikvanommerenErik van Ommeren is responsible for VINT – the International Research Institute of Sogeti in the USA He is an analyst public speaker and coauthor of the books SOA for Profit A Managers Guide to Success with Service Oriented Architecture and Open for Business Open Source Inspired Innovation Sander Duivestein – http//wwwlinkedincom/in/sanderduivesteinSander Duivestein is a senior analyst at VINT the International Research Institute of Sogeti He is also an internet entrepreneur Prior Sander was a software architect at Capgemini He is coauthor of the book Me the Media Rise of the Conversation SocietyJohn deVadoss – http//wwwlinkedincom/in/jdevadosJohn deVadoss leads the Patterns  Practices team at Microsoft His responsibilities include platform and architecture strategy for the Microsoft application platform and developer tools He has over 15 years of experience in the software industry and has been at Microsoft for over 10 yearsClemens Reijnen – http//wwwlinkedincom/in/clemensreijnenClemens Reijnen is a software architect at Sogeti and was awarded Microsoft Most Valuable Professional MVP by Microsoft Clemens has a broad background in software development and architecture He also runs a technical blogErik Gunvaldson – http//wwwlinkedincom/pub/0/103/558Erik Gunvaldson is a senior program manager for Microsoft Online Services He manages the partner customer and community programs of Microsoft Online He has over 20 years of experience in IT  IndexAAjax  187188Akst Daniel  63ALIPR  111Amazon  34 108 134135Amazon Web Service  67Apple  66Application Service Provider see ASPArena Christine  224 ASP  1112 attention continuous partial  115 autonomy  7778 Azure  166167BB2C 20  181185 innovation  183 marketing  183184 product development  183 reasons for  182183 sales  184 support  184185 training and education  185Babauta Leo  142Barabási AlbertLászló  4Beckstrom Rod  77Bell Alexander Graham  89Berger Suzanne  65BernersLee Tim  35 Blogger  67 blogging rules  152154 Bloglines  114 blogs  76 98 volume  109 bookmarking  107108 boundaries crossing  2630 212 Brafman Ori  77 Brynjofsson Erik  78 bullthorn acacia  55 business reality  17 BusinesstoCommunity see B2C 20 butterfly effect  4CCarfi Christopher  5Carr Nicholas  60 Cerf Vint  217 changemanaging  210211 responding to  7778Channel 9  153 Chaplin Charlie  46 cloud see also cloud computing  930and mashup  68 definition  918 202203 guiding questions  201214 relationship with SaaS  1215 stages of adoption  6768 terminology relating to  14 two dimensions  167cloud computing definition  910 emergence  164167 enabled by Software  Services  192193 examples of services  15Cluetrain Manifesto  5758 7576Coase Ronald  27collaboration  930anatomy of  87124 and innovation  2426 by whom  203204 conversation economy  192193costs  208 crossing boundaries  192 212 dealing with versioning  207208 definition  1826 202203 evolving  44 free  24 goals and rewards  146155 groundwork for successful  125155 guiding questions  201214 impact of technology on  4244 in enterprise  181185 in nature  56 integration  208209 joint proposal by two parties  2728 managing change  210211 measuring success  214 motivation for  2324 204205 myths  217226 reasons for  192193 requirements for excellence  131 role in evolution  55 rules  149154 security  210 self provisioning  192 shared vision  147 software matrix  121124 tools  92121 218 220 types of  191 within a project  22collaborative culture  128 138146 collaborative leadership  141 collaborative mindset  142 collective intelligence  180 Collins Charles  1communication  8792 electronic  8992 tools  88 92121competitive advantage  211212 Comtesse Xavier  65 consumer demands unique experience  3637 consumployee  77 continuous partial attention  115 conversation economy  192193 Conversation Prism  59 92 conversation society  162163 conversations markets as  5758 naked  5860 ongoing  7476 society of  5661credit crunch  224 crossing boundaries  192 212cost  2630 reality of  2930crowdsourcing innovation  2426 culturecollaborative  128 138146 determined by people  144146DDarwin Charles  5154DataTelephone  90Debategraph  138 Delicious  107108 deliverables building  9497 deployment and release  189 Digg  67 107 digital natives  23 8183 115 145 discovery  109112 disruption  159160 Drucker Peter  3334 147Dunkel Linda  224DZone  108EeBay  34 108 efficiency  47 electronic mass media  163 Ellul Jacques  39 email  8990end of  120 problems  9092 role of  205206 versus wiki collaboration  91 Enterprise 20  125 181 evolution theory  5254 experience unique  3637FFacebook  34 67 104 fax  89 financial crisis  12 67 224 finch  5253 Fire Eagle  118 firm new nature of  5169 five forces model Porter  7475Flickr  67Ford Henry  36FriendFeed  34 100 118119GGartner Hype Cycle  6768 GENYers  23 goals  147155 Godin Seth  5 7 224Google  130Google App Engine  67Google Maps  67 Gore Al  225 government competetion in  29Groove  9596HHamel Gary  62 84Holland Casino  7172Huang Jeffrey  65Hype Cycle Gartner  6768 Hyves  34IIBM Cloud  67IBM Many Eyes  138 ICQ  97 ideation  24 identity  128 133 Illich Ivan  18 60 industrial revolution  36 4647 innovation  2426 consumployee as source of  77 new age of  6263 of management  84instant messaging  97 100 188 integration  116119 208209 interaction  9799 internet influence on business  23 interruptions  114116 invisible hand  4445 iPod  6667 Israel Shel  76 ITconsequences of technological innovations  8284 facilitating knowledgebuilding  83 ITAGroup  177178 iTunes  66JJaiku  100KKennedy Dennis  125 knowledgebuilding corporate  83 building  9497 convergence of  3536knowledge management  94 collaborative  8081Kondratiev Nikolai  4041Kondratiev Wave  41 Krishnan MS  62Lleadership collaborative  141 Levine Rick  56 lifestreaming  59 101 LinkedIn  103 105106 Live Mesh  67 96Locke Christopher  56Long Tail  63Lorenz Edward  4MMackenzie Deborah  43 managementfuture of  62 innovation of  84Management 20  6168Many Eyes IBM  138 Margulis Lynn  55 marketing viral  183184 markets as conversations  5758 changing  7481 Marx Karl  36 mashup  63 68 116118 integration  116119 Maslow pyramid  148149 mass media electronic  163 McLuhan Marshall  3940 61 87 media  162163 media revolutions  163164 Meltdown Monday  1 5 Metcalfes law  101 microblogging  98101 Microsoft Groove  9596Microsoft Live Mesh  67 96Microsoft Popfly  67 117Microsoft SharePoint  7172 125126 157158 177178 Microsoft Virtual Earth  67 mindset collaborative  142 Modern Times  46Morse Samuel  89MSN Messenger  100 114 MySpace  34 104 myths  217226NNaked Conversations  5860 7576 news feeds  113114 notification  112116interruptions  114116OOffice Communicator  113 open innovation  24 OReilly Tim  185 organization and collaboration  1826 autonomy  7778 impact of change  73 responding to changes  7778Orlikowski Wanda  139Orr Julian  146Ozzie Ray  159 166PPage Larry  130 paradigm shift  34 Perez Carlota  42 Pipes  67 117 platform concepts  186189 Plato  38Pony Express  8990Popfly  67 117Porter Michael  6365 74 Prahalad CK  62 presence reporting on  99101 printing press  163 productivity  7384improving  7880programming models  188189 prosumer  36 publishing company  157158RRasmussen Chris  91 rating  107109REAAL Verzekeringen  4950 real time  6061 relationships  101109Representational State Transfer REST  188 reputation  107109 REST  188 revolution industrial  36 4647 media  163164 technological  3348rewards  129 147155 Rich Internet Application  187Richardson Adam  66Rifkin Jeremy  63 RSS  113114 rules  149154blogging  152154SSaaS  159175 choice for  213 conceptual map of  1617 costs  208 dealing with versioning  207208 definition  165 202203 downside  14 emergence  164167 examples of services  15 integration  208209 managing IT focus  166 managing risks of software acquisition  165 relationship with cloud  1215 security  210 servicelevel agreement  134136 terminology relating to  14 upside  13Satir Change Model  56Schumpeter Joseph  42 Scoble Robert  5859 76 153 search engines  110 190 Searls Doc  56 security  210 219220 self provisioning  192 service as a service  1617 servicelevel agreement  134136ServiceOriented Architecture see SOAservice providers map of  17 services mixed with software  159175 213Shared Service Center  11SharePoint  7172 125126 157158 177Sifry David  152 Simon Herbert  3 six degrees of separation  106 Smith Adam  4445SNS  105106SOA  174 evolution of  161167social bookmarking sites  107108 social computing areas of use  183184 184185 emergence  179180 enabled by Software  Services  192193 expectations  194195 for business  179195 in enterprise  181185social networks  34 67 102108 191 Social Networks Software SNS  105106 social structure  101109 social technographics ladder  145 society of conversations  5661 Socrates  3839software mixed with services  159175 213Software as a Service see SaaS software matrix  121124Software  Services model  159174 213 enabling social computing  192193 implications  170174 principles  168170 Solis Brian  59 92 speed of business  6061 7476 Spencer Herbert  54 starfish  7778 status reporting on  99101 Stimmt AG  3132 stock market  12 Stone Linda  115StumbleUpon  107108survival of the fittest  54 Sydved AB  8586 system as product  6667Ttagcloud  8081 190 tagging  97 110111 190 Taleb Nassim Nicholas  5Talpa Creative  26Taylor Frederic Winslow  4546 139140 Taylorism  4546 140 technographics ladder social  145 technological revolutionsimpact  3348 six  4048technology  206 adoption curve  43 and trust  133134 impact on collaboration  4244 impact on community  3437 influence on business  23 negative consequences  3740telegraph  89 telephone  89 telex  90Thamus King  3839Thomas Jesse  59 92 Thoth god  3839 timeplace matrix  121122 timeway matrix  123 TiVo  34Toffler Alvin  36 60 Tomlinson Ray  90 tools collaborative  92121 133TopCoder  25Toyota  79Toyota Material Handling Europe  215216 transparency  60 130 trust  128 and technology  133134 levels  132model for  129138 need for network  137138 shared model of  149 trusting information  136137 trusting people  130132 trusting websites  134 Trustsaascom  135 truthiness  60 Twirl  115 Twitter  34 99100 type letters  163Uuser experience multiheaded  173174 rich  187188user profiles heterogeneous  172173 UVIT  197199Vvalue source of  76 value chain by Porter  6465 unbundling chains  63 unbundling  64 Value Chain 20  6566 viral marketing  183184 vision  147WWales Jimmy  225 web as hub  194 as platform  186189 collaborative  191 evolution of  162read / write  189190 social  191Web 20  11 175 180 185191 communication tools  92 definition  185186 focus on data and content  189190 platform concepts  186189 programming models  188189 stages of adoption  6768 types of collaboration  191 191 Web 30  112 web feeds  113 web media  164Web Science Research Initiative  35Weinberger David  56Wesch Michael  184 Whittaker Steve  92 wiki  95collaboration  91 definition  95 empty  127Wikipedia  34 67 116 137Windows Azure  166167Windows Live Photo Gallery  110111 Woloski Matias  1617 work new rules of Babauta  142144XXbox Live  187 Xobni  118119YYahoo Fire Eagle  118Yahoo Pipes  67 117Yammer  99YouTube  34 67 109 In todays world organizations in different branches are using more and more agile ways of working As the operational environment is constantly changing and organizations are forced to keep up the pace to stay alive they might not be able to survive by following only the old inflexible methods However thorough consideration and preparation needs to be done before changing into agile In many cases organizations are so used to follow traditional models such as waterfall that they do not realize that the organization itself needs to be changed as well not just the method they are following The number of agile pitfalls organizations are facing is endless but there are a lot of same mistakes many organizations are doing one after another These common issues are the most interesting ones and therefore highlighted in this thesis In this thesis the most common pitfalls of agile software development are investigated and suggestions how to avoid them are introduced The thesis is not related to any specific organization or technology but common issues identified by having some informal interview discussions First a preliminary literature was written in order tohave a hunch on common issues before starting interview discussions and preparing current state analysis Based on current state analysis conclusion topics for the literature review were identified After literature review initial proposal for tackling the most common agile pitfalls in advance was prepared and validated by agile professionals These agile professionals were partly representing same persons that were interviewed for the current state analysis Finally after initial proposal was validated the final proposal was written The topic for the thesis was decided based on authors own passion and interest The author has been working as a scrum master and wanted to gain more knowledge in order to develop the use of agile methods in her own job She had experienced a lot of positive implications because of agile way of working instead of traditional methods However she had faced also some severe issues and wanted to drill down to learn whether other people are having same experience and how these could be avoided This thesis is not built around any case organization and therefore people interviewed are representing couple of different organizations Interviewed people were chosen based on suitable background and their willingness to participate and they are all having agile experience Though the thesis is not done to any specific organization the outcome of it can be considered as a checklist for any person or organization that are either planning to go agile or already are using agile but facing issues and would like to improve way of working The business challenge of this thesis is that managers in software development adopt agile as some sort of cure all without consideration to the challenges that are likely to be encountered for this particular field of work The business challenge is not related to a single organization but common issues The objective of the thesis is to develop a checklist how to overcome issues in agile software development Target audience for the checklist are people like the author individuals who are using agile methods in their job and would like to improve the way of working to embrace agile benefits However the checklist could be useful also to persons and organizations that are only planning to go agile The output of the thesis is a validated proposal in a form of a checklist answering to a question how to overcome some of the most common issues in agile software development By taking the checklist into a consideration when planning to go agile organizations can avoid the most common agile pitfalls As the use of agile methods has been a rising trend in many organizations in all branches and not least in the software development agile pitfalls is very actual topic Despite the popularity of agile surprisingly many organizations do not familiarize themselves with careful preparations but are getting an illusion that agile simply means lightening or even skipping the planning and project management tasks Software development is demanding and there any many possible stumbling blocks that are not fading away by just saying that traditional methods will be replaced with agile Agile methods are not curing all the problems and not leading to a successful end without seriously going into it The output of this thesis should help organizations to understand the preconditions of agile and things to consider before going agile software development In the next chapters first the research method and material used is explained Then the summary of the preliminary literature is written following by the current state analysis After and based on the current state analysis the conceptual framework is introduced Last an initial proposal and its validation is described ending to a final proposal in addition to conclusions This chapter describes the research design and data collection methods Qualitative research method is used due to its suitability to the thesis In addition to the current state analysis and literature review also preliminary literature review is done to gain a hunch of the current issues The design of the research process is illustrated in below figure First preliminary literature review is carried out in order to get a hunch of the most common issues in agile software development Though the issues that are collected from the literature are not exactly similar to the ones identified based on interview discussions they are still directional and a good starting point In the literature issues are introduced from all over the world from different kind of organizations and different technologies Most of all the issues in the literature are mainly more generic compared to the ones identified by discussions with individuals After the preliminary literature review the current state analysis is drawn up based on informal interview discussions with people involved in agile software development Current state analysis is introducing the current strengths and weaknesses of agile software development Interviewed people are representing scrum masters and developers from different organizations In the next phase of the thesis a literature review is done the main concepts related to the summary of the current state analysis are explained such as agile software development scrum traditional software development waterfall method differences between agile and waterfall change management and agile transformation The literature review is targeting to conceptual framework that will be a base for the initial proposal a checklist how to overcome most common issues in agile software development Initial proposal is validated by couple of the interviewed persons the initial proposal is finetuned based on their comments and the outcome is the final proposal When considering the validity of the research process it can be stated that above mentioned was valid for this case because there was no case company involved Also the subject is so new and broad that discussions instead of a questionnaire were more suitable Data collection for data stage 1 was done via informal facetoface discussions with people involved in agile software development With some of the people discussions were not just onetime but continued couple of times Originally the purpose was to have few more discussions but it became obvious rather soon that the answers were started to repeat themselves Hence it did not make sense to continue discussions There were total five people discussed with representing both scrum masters and developers As illustrated in below picture four scrum masters and a developer were interviewed from couple of different organizations Discussions were done informally and incognito in order to get honest and independent opinions from people Field notes were done by the author to record the discussions Data was analysed by pickingup the main points from the answers and to coming back to those in cases where it was not clear enough what the interviewee was trying to say All the interviewees were having their own point of view a very unique way to express things and hence it required some analysis and rediscussions to be able to crystallize the main points After the main points from the answers were pickedup they were categorized under few topics to be able to identify the areas of issues This was helping to understand the big picture and the areas where the biggest issues were lying Also the identification of the literature topics was much easier after the categorization As shown in below table data stage 2 was done by introducing the thesis as a whole and especially the initial proposal to two of the interviewees participating to data stage 1 Informal discussions with two individuals were done and the author prepared field notes Their comments and suggestions were taken into account when the final proposal was prepared Comments and suggestions were compared to the theory of the thesis and the initial proposal to figure out how they could be put into practice and finetune the initial proposal In this chapter findings from the preliminary literature are introduced The purpose of this chapter is to gain preliminary information before starting the interviews and current state analysis to have a hunch of the most common agile issues In the study of Gandomani Ghani Ziaei and Zulzalil 2013 the obstacles and issues in agile software development are categorized under four themes organizational and management related challenges people related challenges process related challenges and technology and tools related challenges Many of the current challenges are stem from the culture and structure of the organization which is serving needs of traditional methods Organizational culture is affecting to agile transform Organizational culture is a vague term covering numerous things such as prevailing attitudes norms and values Iivari  Iivari 2010 Gandomani T et al 2013 are using a term The agile transformation process when discussing about organizations moving from traditional methodologies into agile Organizations are often making a mistake by underestimating the difficulty of the agile transformation process and not investing it this is making challenges even more difficult Organizational issues in agile software development are coming from too narrow thinking of the meaning of agility Organizations are often stating they are agile though it usually means only software development The software development is failing in agility in cases where the organization around it is not agile enough The software development projects and teams cannot fully use their agile potential unless the organization is not supporting them and getting rid of traditional thinking and old habits When the agile software development team is lacking agile support from their organization it tends to lead situations where people are not feeling safe to share identified issues and mistakes this is reducing agility and impacting to end results Gothelf J 2014 According to Moczar 2013 agile is promising too much when stating that it would be a solution to problems faced with traditional methods Moczar 2013 has identified several times that agile is partly falling to same issues than with other methods Organizations are counting too much on pure agile method and forgetting the importance of agile thinking In cases where only the agile method has been followed without changing the mindset as well it has sometimes leaded even to bigger catastrophes than by using traditional methods and changed the good intentions totally upside down One of the common issues is that organizations are not considering carefully whether the use of agile is worthwhile Moczar L 2013  Since agile is all about people people related challenges are playing a significant role especially in cases where the organizations have earlier been using traditional software development methods One of the common people related weaknesses is the difficulty for people to change their mindset and behaviour into agile mode During agile transformation there is not always enough training and coaching from agile expertise though it would be needed People related issues are concerning both customers and vendors and both can have overwhelming impacts Gandomani T et al 2013 For instance the agile principle of early and continuous delivery is sometimes leading too hasty outcome in detriment of quality This principle is allowing developers to neglect to bugs The consequence of too fast delivery might be the growth of defect backlog ending up to excessive work Moczar L 2013 The manifesto for agile software development is encouraging to development over planning This has been often an issue though the original idea has been to make things easier There are often issues because the size of the changes is varying from a tiny to huge ones Though agile is welcoming changes even late in the development it is still commonly causing problems because the development is constantly ongoing and there might be unsolved defects making it even harder to success in agile Moczar L 2013 The plan to have a totally selforganized team without a project manager who would be responsible for the whole project is not always working as desired What happens often is that the scrum master is forced to act as a project manager to keep things going on but without a project manager mandate For instance the prioritization of the tasks to be done is an issue faced in the real world often timepressure is so high that an additional prioritization is needed In practise it is difficult for developers to manage all the priorities and dependencies by themselves Moczar L 2013 The outcome of the preliminary literature review are some the most common weaknesses of the agile software development on a highlevel The weaknesses of agile software development are for instance organizations are not agile enough and therefore not able to provide support for the agile software development teams people with experience on traditional software development are not able to get rid of their old habits and mindsets and preventing the successful use of agile agile processes are not properly used due to lack of agile knowledge When reading the results of the preliminary literature review it needs to keep in mind that though the issues mentioned are partly similar than in the current state analysis they cannot totally match due to fact that CSA is done by interviewing Finnish ITprofessionals while literature is from the wider perspective Still the preliminary literature is providing a hunch a useful overview In this chapter the most common strengths and weaknesses of agile software development are being introduced The current state analysis is prepared based on informal and anonymous interview discussions Based on interview discussions the following strengths of agile software development were identified intense and good cooperation easiness to plan work in small pieces possibility to correct mistakes rather easily and quickly allocated resources if preconditions are in place the quality is usually good Though above mentioned are considered as strengths they still cannot be taken for granted but can be achieved only by treating agile method with conscious Agile strengths can turn to weaknesses in a quick manner if agile principles are not followed actively First people discussed with were having positive experience on cooperation and communication between different parties such as the project team and customers Especially when sitting at the same premises and having extended facetoface communication the cooperation has been much more informal and therefore better compared to traditional approaches Communication can be done without delays and so called Chinese whispers –effect can often be avoided also threshold to open discussion is low One of the scrum masters highlighted the easiness of the cooperation when all project members are sitting on the same premises he had experienced that good cooperation usually requires people locating on same premises and as soon as part of the scrum team is located for instance in another country communication gets poor All interviewees mentioned good cooperation and communication as the most valuable thing agile can offer However they all had experienced the fragility of good cooperation meaning it can easily be spoiled This will be elaborated more in the next subchapter Another identified strength of agile software development is the easiness to plan work in small pieces This is a great advantage because the changes in the schedule and error estimates are not causing as much issues as with traditional methods The socalled snowball effect can be avoided rather easily and the possibilities to adjust the overall schedule works better One of the scrum masters stated that it is unrealistically to even think that all the smallest details could be planned in the beginning of the project due to nature of the software development and especially regarding bigger software projects Hence he appreciated the possibility that agile is offering to plan work in pieces Third strength of the agile software development was identified to be the good possibilities to correct mistakes and bugs easily and relatively early People were having unpleasant experience on traditional methods where mistakes are not often noticed until at the end of the project but they considered agile way of working to enable faster issue fixing People noticed that for example in scrumming you are learning sprint by sprint and eventually be a master The scrum master 1 was praising agile due to its mercifulness in hes experience software development done by traditional methods is harsh and punishing people for all mistakes they are doing especially in the beginning of the project when agile method is often offering a possibility to fix mistakes during the coming sprints Hes opinion was that in agile software development the learning curve of the scrum team members is much better because it is actually possible to learn by mistakes fast within the project and not only after the project is about to end or even finished Allocated resources are also one of the agile strengths people mentioned Allocated resources are a great benefit because they know the product that is developed but also other project members enabling to proceed smoothly In perfect situations resources are allocated 100 to the agile project itself this is something that is unfortunately not always happening but when it does it makes agile life easy One of the interviewees a scrum master stated that everything is much easier by using agile because there are designated resources and they are mainly allocated to the same project Despite all the strengths there are also several weaknesses in agile software development such as agile methodology is used though there are not prerequisites lack of sufficient planning or documentation or testing too early delivery communication and cooperation issues due to resources located in different places issues due to cultural differences when projects are international resources not always able to concentrate 100 to agile work due to other responsibilities changes in staffing affecting agile projects heavier than traditional ones agile methodology and principles not known bigger risks to break existing functionalities because the big picture not always known due to constant changes done Three of the most common weaknesses are explained in detail in this chapter though there is not much difference between the answers by the interviewees Also to mention some of the weaknesses are almost overlapping One and the most common of the weaknesses observed and discussed was that in many cases all agile resources are not 100 allocated to agile work due to other responsibilities This is causing delays to the development work and makes it difficult to plan schedules Even one person with less than fulltime allocation may cause tremendous issues As the developer that was interviewed said since things are unfortunately often depending on individuals the nonattendance of even one person can spoil the whole thing and undercut the benefits of agile Even too early delivery meaning lack of sufficient planning documentation and testing is also a big issue regarding agile software development Some of the people interviewed stated this issue to be concerning the whole project covering all the steps and starting from the project planning they felt that in some cases the project team thought that the use of agile would justify defective quality Though agile is encouraging to iterations and welcoming changes over planning this was sometimes misused When using agile there is sometimes pressure to deliver outcomes earlier than what would be wise and realistic leading to careless development and lack of proper testing Especially lack of planning and documentation is sometimes making bug fixing difficult and causing too much dependency on individuals Without proper planning there are often conflicts between the development done by other people within the same agile team or even other projects Poor planning is often leading to quality issues and bugs as well In cases where also the documentation is negligible the defect fixing is even more painful and time consuming In addition the software around is constantly changing making it harder to identify the root cause for issues and corrective actions The third biggest weakness discussed was the use of agile methodology without having preconditions to adopt it People were having bad experience of projects originally planned to be done with traditional methods but for varied reasons the method was changed to agile these situations were often leading to confused situation where agile method was supposed to be followed but the organization around the project group was not acting agile at all Some of the people were considering agile as a trendy concept that is rather often used without really focusing on it and the conditions it is requiring Typically the thought is to run a project like with waterfall method but without any specifications and with minimal testing One of the scrum masters was even having experience on agile team developers not at all familiar with the agile method itself leading to waist of valuable time reserved for the development work He used a lot of time during several sprints for teaching agile principles and scrumming to other team members Strengths and weaknesses based on interview discussions are listed in below table Interviewees were overall satisfied with the quality of work in agile projects They all though in many cases agile approach works better than traditional one Due to designated resources and emphasizing the communication and cooperation risk to fail is less Especially good and intense cooperation and designated resources were appreciated However there are several weaknesses as well such as all resources may not be 100 allocated to agile work due to other responsibilities misusing agile approach by working carelessly and using agile though all the preparation work was not done As the interviewees were speculating most of the issues are due to lack of proper preparations and underestimation of agile approach Interesting observation was that people identified more agile issues than successes An interesting observation is that many of the strengths and weaknesses are opposite to each other meaning that the advantages of agile can be gained only with careful consideration and preparation and without this they can turn into weaknesses When rushing to agile without preconditions in place the results are not always positive as expected When discussing with people about what should be done differently to succeed with agile a common denominator seems to be that better change management and learning agile deeper would be needed In the next chapter literature review based on findings from the current state analysis is introduced In this chapter a conceptual framework of the thesis is being introduced Topics are identified based on conclusions of the current state analysis The purpose of this chapter is to support the understanding of the thesis and to prepare the proposal The current state analysis revealed that the most common issues are related on a highlevel to either agile transformation the differences between agile and traditional methods or change management The idea of the agile software development is to have an adaptive team which can deliver frequently and rapidly and welcome changes in the requirements The advantages of the agile software development are the ability to respond to the changing requirements of the project Balaji S  Murugaiyan S 2012 and the improved communication between the customer and the development team Agile method is usually more profitable and suitable for smaller projects One of the issues in agile software development is the demand for seniorlevel resources agile developers should be able to do decisions and be selfimposed Balaji S  Murugaiyan S 2012 Manifesto for agile software development Individuals and interactions over processes and tools Working software Scrum n A framework within which people can address complex adaptive problems while productively and creatively delivering products of the highest possible value Schwaber K  Sutherland J 2013 Scrum has empirical and iterative approach aiming to control risks and highlight predictability According to empirical approach there are three main principles to follow adaptation inspection and transparency The purpose of transparency is to keep the whole process visible to the people who are either performing or accepting the work Inspections are referring to the idea that scrum artifacts should be inspected enough to detect the unwanted side effects but not exaggerate Adaptation is aiming to adjustment of the artifact in case the inspection is revealing that the artifact is unacceptable Schwaber K  Sutherland J 2013 The product owner development team and a scrum master are formulating a selforganizing scrum team that should not be depending on outsiders The scrum teams are having needed competencies to deliver the artifacts incrementally and iteratively Continuous feedback is desired to develop the competence and productivity Traditional software development is approaching things from the predictive point of view Traditional software development is based on detailed plan with a complete list of items that must be developed All the changes are going through a change control management GhilicMicu B et al 2013 Traditional and one of the oldest and most popular ways of software development is the document driven sequential waterfall method The catch of the waterfall method is to follow the predefined stages and milestones and to invest on early planning An output of a stage is an input for the for the coming stage At first requirements are gathered and right after that follows the design phase After the design the implementation ie coding and testing is done and the final phase is handing to maintenance Bhuvaneswari et al 2013 Balaji S  Murugaiyan S 2012 The advantage of the waterfall method is the easiness to understand and implement it due to its linear model Waterfall is useful on mature products and weaker teams can benefit more from it However one centric pain point of the waterfall method is the unrealistic expectation that requirements in the beginning of the project could be strict and unchangeable leading to issues in the latter phases of the projects In this model issues cannot usually be solved in one phase completely leading to quality issues in the final outcome As the final deliverable ie the actual software is delivered at the end of the project possible issues are identified late leading to expensive changes Bhuvaneswari et al 2013 Balaji S  Murugaiyan S 2012 According to Kotter change management refers to a set of basic tools or structures intended to keep any change effort under control The goal is often to minimize the distractions and impacts of the change 2011 There are several alternative approaches to change and the selection should be done case by case taking all the circumstances into account Lockitt 2014 has roughly divided change management strategies into five different approaches directive expert negotiated educative and participative However these strategies are not exclusive and can be used alongside One of the change management tasks is to make a decision what strategy or strategies to use and how and when to implement them Lockitt B 2014One of the five strategy approaches directive strategy emphasizes the authority of the managers even without other people involved in the decision making This approach is allowing fast change but not taking other involved peoples opinions into account The disadvantage of this strategy is often strong change resistance and lack of ideas from other stakeholders Lockitt B 2014 Another strategy approach expert is looking the change management from the problem solving point of view and it is suitable especially for the technical cases such as new systems being introduced There are often specialists leading this kind of changes which is bringing both advantage and issues as well though this approach is enabling rather quick implementation affected people may not share same views than experts driving the change Lockitt B 2014 Negotiating strategy approach is highlighting the negotiating between the management and people affected The management is letting stakeholders to express their views and is willing to do compromises regarding how and what is to be done By following this approach the change is having slower tempo and the predictability of the outcome is not complete however people affected are more involved and there is less change resistance Lockitt B 2014 Educative strategy is trying to change peoples way of thinking leading them to support the change Different kind of activities is used within this strategy such as training and sweet talking by experts and consultants Naturally this approach is timeconsuming but as an advantage it is involving and committing people and reducing the amount of change resistance Lockitt B 2014 In participative strategy all affected people are involved and their opinions are taken into account In case experts and consultants from the outside are used to facilitate the change management process they are not allowed to do any decisions This approach is offering a possibility to learn and grow up for both individuals and the organization around them In addition it is committing people and making them to support the change As a disadvantage this kind of change process is taking a lot of time and may be expensive Lockitt B 2014 When moving to agile a strategy for the agile change management is needed Agile transformation is sociotechnical process that requires a lot of time and patient There are three different approaches to use when moving to agile tailoring localization and adoption Tailoring is aiming to fewer changes in the organization and it was popular especially in the days when agile methods were introduced Tailoring approach may not always be the best way to implement agile but rather a way to have the disciplined process and agile side by side Gandomani T et al 2012 Instead of tailoring localization is accepting essential changes but not all agile activities Some parts of agile might be ignored totally and some are customized Especially in organizations that are taking their first steps towards agile and lacking experience some practices are still done by following traditional ways Gandomani T et al 2012 Adoption approach is emphasizing major changes to adapt organizations with agile When using adoption approach agile methods are tried to be used completely without any limitations Agile adoption is considered as the best way to achieve agile method Gandomani T et al 2012 Challenges in agile transformation have been categorized as follows management and organizational challenges people challenges process challenges and technology related challenges Impacting to peoples mindset is one of the biggest challenges it is impossible to achieve overnight and besides time it requires mentoring as well Individuals as members of a project team may cause severe issues because of their habits ambitions and different cultural backgrounds Coaching towards agile is unique comparing to other methodologies and therefore requires an experienced and professional mentor in order to succeed When changing to agile people must change and forget old habits and roles for example project managers with strong experience in traditional methods must learn new way of working and forget being a commander Also the role of a customer is changing radically because of the agile way of working forcing them to contribute in a different way Gandomani T et al 2012 From the management point of view tacit knowledge and minimal documentation are causing issues and can be treated as barriers Still one of the biggest management relates agile issues to be considered is the group decision making which is totally opposite when comparing to the traditional software development Besides group decision also letting individual project team members do selfgoverning decisions is part of agile but can sometimes be hard for the management to implement in practice Gandomani T et al 2012 In many organizations changing processes from traditional life cycle model to more iterative and evolutionary agile is difficult This change affects many levels such as strategies peoples roles and measurement practices In organizations where operations are spread to different locations process related barriers towards agile transformation are playing even a bigger role and challenges regarding communication and cultural differences needs to be taken into account as well Gandomani T et al 2012 As a conclusion transforming from the traditional software development methods to agile is never easy but a timeconsuming process that needs to be treated with a conscious and understand the importance of it Everybody involved in agile transformation needs to be aware of the challenges and sufficient training and coaching must be provided In addition as there are several different agile methods to choose organizations should carefully study them to find the most suitable one for them All in all in order to succeed agile transformation requires a professional change management strategy plan and resources Change management strategy from a wider perspective is mandatory for successful agile transformation Purely technical point of view concentrating on software development process is not sufficient but all aspects as illustrated in below picture should be taken into account Agile transition is change oriented not methodology oriented process that is touching all levels in the organization Gandomani T et al 2012 Selection of a method  selection of an approach  creating a change management strategy  creating and following the execution plan  In this chapter initial proposal to overcome issues in agile software development is introduced Initial proposal is prepared based on data 1 which is current state analysis and literature review  Initial proposal is trying to take all the previously introduced aspects in to account to offer a useful checklist Initial proposal is telling who what and when certain actions needs to be done The aspect why is not mentioned in below figure because the lack of the case company the thesis is based on common issues and not related to a specific organization  There are several things that organizations and individuals should be taken into account when planning to go agile At first a careful consideration which one traditional or agile method would be preferable should be done Comparison between these two different methods should always be done case by case and understand the unique features in every project There are cases where agile is not suitable at all despite of all the benefits it is offering When doing the comparison also the characteristics of the organization are crucial some organizations are more traditional and rigid having a lot of bureaucracy It can be extremely challenging or even impossible to bring agility to organizations like this After careful consideration and selection of the method desired approach should be defined As introduced in earlier in the literature review there are roughly three alternatives to select from tailoring localization and adoption When selecting the approach all aspects must be considered realistically from the project and organizational point of view One major thing impacting to the selection of the approach is the former experience on agile or the lack of it A change management strategy should be created by considering all known and common challenges meaning management organizational people process and technology related aspects should be considered The creation of a change management strategy must be done in the planning phase after the method to follow and the approach has been chosen before the actual project starts As explained in the literature review first the most suitable change management strategy approach to achieve the desired change needs to be defined When defining the strategy all aspects of the change must be taken into consideration the organizational culture the scale of the change expected change resistance schedule budget and risks of the change An execution plan is needed together with the active followup It is crucial to plan in detail how the actions will be executed the plan itself is not enough but it needs to be followedup as well The initial proposal is validated and commented by two of the interviewees participating in current state analysis a developer and a scrum master 2 Validation was done via email and by having informal discussion Also comments from the thesis supervisor was received The developer commented that the initial proposal was good and realistically She is working in a software development industry and using agile methodology in her work currently Her company is struggling with same issues mentioned in this thesis and hence planning to start implementing similar phase than the selection of approach phase in the initial proposal they came into a conclusion that a phase like this is a must in order to avoid facing same agile pitfalls over and over again The company did the decision without knowing the initial proposal introduced in this thesis which is a notable example of the necessity and usefulness of this kind of a checklist The developer was thinking that the way agile methodologies are used in Finland may be different than in other countries and especially other continentals In her experience Finnish companies are not yet too familiar with agile software development and therefore the initial proposal would probably not be as usable in other countries but suitable in Finland The scrum master 2 evaluated the initial proposal as simple and doable In her experience this kind of checklists needs to be simply enough and the correlation between commonly known issues and the checklist needs to be clear to get people interested about it She stated that in case companies would not like to execute all phases they could still pickup certain phase or phases and execute them individually this is an alternative that should be highlighted and explained The thesis supervisor highlighted the lack of the named resources in the initial proposal there is only mentioned either project team or management However this is not sufficient but leaves it too vague and raise a question how to make sure things will be done In addition the thesis supervisor was missing a more concrete checklist with actions and their subtasksSince there was not identified any major changes during the proposal validation the final proposal is rather like the initial proposal with a comment that in case companies do not want to implement all the phases they can also pickup an individual phase and execute it it is not recommended but better than ignoring the whole checklist There is also more depth added to make sure that things will be done there must be a responsible person pointedout regarding all the steps in the final proposal In the initial proposal instead of individuals there were mentioned that either a project team or management should be responsible for certain steps It was too vague definition creating a risk that things will not necessarily be done and certainly not on time In the final proposal it is suggested that named person can be either from the project team or management it is depending on the project and organization which one is more preferably A detailed checklist with all subtasks is also added to the final proposal The checklist is covering all stages of the proposal and its purpose is to offer more concreteness Selection of a method to select between traditional and agile methods Responsible person is named individual from the project team or from the management of the organization To succeed the person responsible requires sufficient knowledge of the organization Planning phase To consider what kind of change management strategy would be the most suitable The final proposal is trying to take all the previously introduced aspects into account to offer a useful checklist The final proposal is telling who what when and why certain actions needs to be done There are several things that organizations and individuals should be taken into account when planning to go agile At first a careful consideration which one traditional or agile method would be preferable should be done Comparison between these two different methods should always be done case by case and understand the unique features in every project There are cases where agile is not suitable at all despite of all the benefits it is offering When doing the comparison also the characteristics of the organization are crucial some organizations are more traditional and rigid having a lot of bureaucracy It can be extremely challenging or even impossible to bring agility to organizations like this There must be a named individual responsible for the selection of a method responsibility on selecting a method cannot be shared Naturally it is essential that responsible person is cooperating with other stakeholders and if needed also consults subject matter experts but he or she is responsible that the decision will be done appropriately and on time Without a responsible individual who is having sufficient preconditions there is an increased risk that this step will be done carelessly or ignored totally Also support from the management is needed the way the support is needed is depending on the situation but a minimum requirement is principled support Sometimes also financial support may be required Selection of a method is a big decision that should not be done without a support from the management Despite of a good plan the first mistake is already done if responsible person with management support is not pointed out After careful consideration and selection of the method desired approach should be defined As introduced in earlier in the literature review there are roughly three alternatives to select from tailoring localization and adoption When selecting the approach all aspects must be considered realistically from the project and organizational point of view One major thing impacting to the selection of the approach is the former experience on agile or the lack of it As in the first step of the proposal selection of a method also selection of approach requires an individual responsible with managerial support A change management strategy should be created by considering all known and common challenges meaning management organizational people process and technology related aspects should be considered The creation of a change management strategy must be done in the planning phase after the method to follow and the approach has been chosen before the actual project starts As explained in the literature review first the most suitable change management strategy approach to achieve the desired change needs to be defined When defining the strategy all aspects of the change must be taken into consideration the organizational culture the scale of the change expected change resistance schedule budget and risks of the change The successful creation of a change management strategy requires also a named person who is in charge Especially in this stage the management support is crucial due to fact that changes may touch all aspects of the organization and have a significant impact on its customers as well An execution plan is needed together with the active followup It is crucial to plan in detail how the actions will be executed the plan itself is not enough but it needs to be followedup as well There must also be resources enough to execute the planned actions As with previous step deep and sustainable support from the management is important The management is also needed to provide sufficient resources and finance to secure the implementation of the execution plan The thesis is not built around a case company but done from a common point of view Though the amount of people interviewed is not much it was obvious that the answers and opinions were starting to be repetitive hence there was not more interviewees involved When considering the results of this study it needs to keep in mind the preconditions such as geographically location since this study was done in a small country as Finland it is obvious that the sizes of the projects are minor meaning that the use of agile is different than globally In addition the way agile methodology is used is also depending on the organization Some organizations are more agileoriented than others and therefore better aware of the possible pitfalls Out of the five interviewees three of them were working as consultants at the time of the interview discussion this is also a fact worth to notice since consultants may have different kind of possibilities to impact their customers way of work and especially the way they are adopting agile and doing all the prework During the proposal validation the developer commented that the outcome of this thesis is probably serving best Finnish people due to fact that the current state analysis was done based on interview discussions with Finnish people and the assumption that the use of agile methodologies is not yet very advanced in Finland This is a useful view when considering the credibility of the thesis When considering the facts mentioned above it can be said that the study is credible enough but the preconditions needs to be kept in mind If a similar study would have been done in another location or in a selected case company the results may have been a bit different However the issues identified in the current state analysis are matching to the preliminary literature in a highlevel It was really educating to drawup a study like this the topic is near to my heart and I have been really interested on agile methodology and luckily have had the opportunity to use that in practise I had originally a totally another topic suggested by my employer of that time I found this original topic to be too wide and it was difficult to seize that hence I decided to do my thesis without a case company and select a topic that really fascinates me most That was at the same time a really good decision but it also felt difficult to do the thesis without a case company supporting in a background knowing there is nobody particularly ordering a study like this Still I think the outcome of the thesis – a proposal how to overcome agile issues in a form of a checklist is valuable and useful for the companies planning or going agile pment project Chapter 5 fulfils this goal even though the application of the theory is singular at the moment The secondary goal of this thesis was to examine and document the practical application of the theory of test platform prioritization for functional testing in a smartwatch application development project This goal was fulfilled by the investigation presented in Chapter 6 The findings of the investigation also support and elaborate the theory presented Test platform prioritization as presented in this thesis has practical applications but it is not viable for every project It can also be utilized in projects outside of the gaming field The investigation showed no difference in testing results between the Apple Watch device and the Xcode Apple Watch simulator in functional system and regression tests There were some functionalities of the application that could not be tested on the simulator so testing without the device would have left some gaps in the test coverage It would be a more feasible strategy to conduct unit testing tasks when possible with only the simulator Installing the application builds to the device takes considerably longer and because unit test are conducted in a phase where more changes are still made to the project this would lead to significant benefits to resources and work flow The prioritization of testing platforms can be carried out on an ad hoc basis or it can be planned ahead utilizing tools such as the ones presented in this thesis More tools can be created or discovered in the future to cover a wider range of scenarios and development frameworks Test platform prioritization for unit testing would be an interesting topic for future study since unit tests are very different in nature to functional system tests and with unit tests there is a greater possibility of affecting the time usage through prioritization Test automation would be another field where test platform prioritization could yield interesting results Running automated tests on an actual device would be significantly more challenging compared to a simulator There are frameworks for controlling mobile device functions through the desktop computer interface such as Appium which can be used for automation but maintaining the test sets and the devices in working order for executing automated scheduled regression or smoke tests would certainly present complicated issues Before setting up this kind of a system it would be important to first discover if running the automated tests on a physical device would produce greater results to justify the additional effort The increasing focus on efficiency and optimizing the way people think and work has led to a new area of serious gaming – cognitive games The rise of modern web rendering technologies has enabled the creation of visually interesting cognitive games on browser based technologies The goal of this study was to assess the applicability of using modern browser technologies to create a user centric cognitive gaming platform and the use of mathematical formulas in organic rendering The approach discusses the current market situation and the products and methods of cognitive gaming as well as the technologies involved The user centric approach is studied through user experience design as well as graphic design and animation aspects The reference implementation is project CCA a user centric cognitive gaming platform built on top of Adobe Flash that uses seemingly organic movement rendering The technical implementation is discussed from the platform clientserver aspect as well as an overview of the structure of the front end architecture The rendering engine methods go through the 2D –based rendering of mathematical formulas the use of continuous Bezier curves in organic movement and the creative ways of using Perlin noise to generate textures as well as movement Optimization of complex rendering and platform building is an essential part of the process The results show the viability of using modern browser based technologies in the creation of a cognitive gaming platform Through the use of optimization and creative mathematical solutions as well as tending to user experience needs a successful product is built The project platform is used in medical trials as well as the Science Changing the World Exhibition shown in science centers around Europe This study stands as a testament to the possibilities of cognitive end user training and a guide on the aspects of building a successful gaming platform Humanity as a whole has seen increased focus on optimizing the way we work the way we consume and the way we think This need for ever improving performance has made us consume smarter perform more efficiently at work and waste less time During recent years the world of gaming and digital entertainment has seen a growth in a new area one that was not focused solely in time consuming entertainment the coming of so called brain fitness Gaming suites and platforms offering rather simple basic mechanics that provided the user various activities they could do to train their brains The central tenet in these games was that through exercise powered by game related reward models one could improve the way one thinks and track that improvement as well The brain games became very popular and spread throughout the entertainment ecosystem from mobile phones to modern console platforms The games varied from simplistic memory exercises to the ones based on largely theoretical premises and offered actual data as back up of their effectiveness For the most part the games have been shown to increase your ability to memorize things but the actual science proven benefit is still under much debate Aside from the more mass market oriented products the world of science started to take interest in the inherent interest  reward models and game environments and how they could be used in rehabilitation and analysis of different diseases and medical conditions that affect our cognitive functions Cognitive gaming is based on the concept of neuroplasticity the ability of the brain to physically adapt to new stimuli At the same time the front end tools for web development have improved to a level closer to full desktop experience The introduction of RIA Rich Internet Applications technologies like Adobe Flex and Microsoft Silverlight in addition to the rising support for the next W3C standard HTML5 and CSS3 have enabled high production value fully fledged user experiences across platforms within the browser This study focuses on the key issues in building a user oriented gaming platform and the applicability of organic movement and shapes with mathematic formulas in cognitive gaming The reference implementation is a consumer oriented platform for cognitive gaming built for Neuroware Group a browserran application to measure improve and develop cognitive abilities The goal of this thesis is to study how to develop a rendering system for seemingly organic movement and a fullyfledged cognitive gaming platform with a user experience focus for improving peoples lives using modern web front end technologies The present study does not explore the medical theories of cognitive gaming nor is it a study of software engineering methods themselves This study focuses mostly on the front end platform implementation where the game logic and rendering lies and only has an overview of the backend system of project CCA The first part of the study focuses on the basis of the reference project the setup of the research question and key factors and challenges The second part provides an overview of brain gaming and introduces the user experience needs and specifics in consumer market oriented platform building as well as details on CCA platform implementation The third part introduces the technological core of the platform rendering engine specifics optimization approach and implementation and how the challenges were overcome After going through the theory and reference implementation the final part discusses the merits accomplishments and future of the project CCA From current user base and use cases to possible future uses as well as the related branch of the project currently presented at the Science Changing the World exhibit The reference project is collaboration between the concept owner and CEO of Neuroware Group – Matias Palva PhD and Niilo Säämänen the author of the present thesis Neuroware Group is an innovative small company focusing on neurogaming and cognitive training games that are based on theoretical research on the fundamental workings of the human brain The project began based on the research done by Satu and Matias Palva at the University of Helsinki Neuroscience center Matias Palva had started a prototype of the creature rendering system on the LabView platform to test the possibilities of making a consumer oriented cognitive game based on his research The project CCA started in 2008 on the 18th of December with a meeting with Matias Palva Based on the prototype idea a consensus on the viability of creating the platform with modern web technologies was found The aim of the project was to create a consumer oriented platform for cognitive gaming based on the research done by Satu and Matias Palva on neuroplasticity and the application of organic shapes and movement in the realm of brain fitness The target was to make a browser based solution with a fairly large portion of the platform logic and controls coming from the backend solution enabling a modular and extendable solution for brain training Figure 1 shows the approximate funding partners and their relative contributions to the project during its lifetime The project had Tekes funding for the initial prototype and development Development was started without certainty about future budget options During the development additional funding was acquired as a grant from the Runar Bäckström Foundation and closer to the final stages of the project the trials with HermoPharma funded further development The approximate total external funding in project CCA was 70 000€ The projects first release version was finished during the spring of 2011 after 2 years of development Since the 1st release version the platform has been further developed and optimized The main goals of the project have been achieved and the whole platform had been heavily reworked and evolved to a point of maturity The project work was delegated as follows The front end platform development rendering engine development and UXdesign by Niilo Säämänen graphic design by Mikko Häkkinen and the game design mathematical theorems and backend solution by Matias Palva Mostly the project was an intense collaboration between Matias Palva and Niilo Säämänen The reference project is a cognitive gaming platform for consumer use The tool is meant to be used by end users of all ages and trades and to be easily approachable and trustworthy tool for measuring and improving cognitive processing efficiency From a users perspective the platform is a webbrowser based game system with user authentication and personal account based training programs The general user flow in CCA is straightforward users either have an account or register for one and log in to the game platform The users have free choice over which order they plays their games in and can choose from various available games to them as seen in Figure 2 The different game modes depend on the users account and targets Each user has a specific user account based training program that allows for them to play a certain amount of games per day Most games last around one minute and in each day a different set of games is played The total duration of a training program is approximately 30 minutes per day The games vary depending on the type but contain various amounts of moving or static visual objects called creatures The users task is to perceive andor memorize creatures or the target visual states thereof according to the instructions The target state of a creature is a brief contrast color andor shapechange In some of the game modes the target state change is relative based on a calibration round played before the actual measurement portion Calibration changes the size of the target state so that the subject detects around 6090 of the changes in a two creature game phase The detected amount of target states is expected to get better with training When the target state is perceived there is a limited window of opportunity for reaction within which the user must react by pressing a key Configurable but by default it is the space bar The platform measures the hit rate and reaction time of the user per target state per creature on screen The hit rate HR is the main measurement used in CCA to define user capability After a successful play the users are given a summary of their performance The performance metrics shown are based on the hit rate and reaction time of the user The main numerical feedback given is called capacity Capacity is defined as where N is the number of creatures Capacity is given as a total value for the playthrough and as a separate value for each phase of the game In addition to the numerical metrics the users are also given trophies achievements and stars based on their performance These categories provide user friendly feedback on how well the users are doing without the need for detailed metrics The detailed metrics are available but not shown as the default content The purpose behind the trophies is to empower the user and give them solid clear and maintainable goals for their training After the game end screen the user returns to the main interface of the CCA platform where he can see his statistics review his achievements change his information and play another game The projects target user groups were divided into two sections The general public and the clinical trials The general public users were divided into two sections Elderly citizens whose interests would be to both test attention and working memory as well as train to improve them and school aged children who would benefit from attention disorder testing with an automated platform The current methods for testing and diagnosing attention disorders is work intensive and expensive and automating the testing would yield significant savings These users focus was on daily life and they suffered from no known disruptions in their cognitive capabilities Because of the scientific nature of the platform and the precise data collected from our users the platform could be used as a reference in clinical trials to see how and if people improve their performance with the use of a medicine This is important in studies of new medicine and helping people with brain trauma The platform enables a way of measuring and making training programs specific to studying the target groups differences with medicine and without Since a portion of potential users is visually challenged significant emphasis was put on making the platform visually appealing and simple to use The focus was on pleasant user experience instead of pure efficiency To make the users feel safe with the program and to trust it a solid user experience was necessary The work started soon after the first meeting and the focus was on creating a prototype of the basic rendering mechanics as a proof of concept This was necessary as the rendering power of browser based solutions is still far behind native language based compilers and it was needed to see if it was possible to create the rendering engine on such platforms As seen from Figure 2 CCA was a comprehensive project spanning many years The project was divided into separate phases each phase consisting of a number of sprints The first priority was in the rendering and game engine construction to make sure it was viable to build a full scale platform with the technologies chosen After a few weeks of development a first prototype of the rendering engine was developed and published After some tweaking of parameters and bottleneck analyzing it was considered to be a viable solution and a sufficient base for building the platform Figure 4 shows the first version of the platform rendering engine The result contained only one stationary creature comprised of a nominal amount of points rendered just so one could see how it would work and what the possible performance bottlenecks were The initial version rendered the stationary creature with barely 30 FPS and was quite heavy on the CPU  After the POC was finished the full development of the platform began Development was done in an agile way with sprints of 24 weeks 1 where one aspect of the platform was tackled at a time Since the most difficult part of the project was the rendering engine itself the first 45 months were spent solely on creating optimizing and fine tuning the first version of the core rendering engine  Once the basic engine was built a fast pass over visual style was done for the project an initial GUI for test users and funders to see and test the progress Work was started also on doing reusable UI components for various user prompts and game information needs Beyond the GUI needs the first game mode logic was coded and the first actual play through of CCA was possible  As depicted in Figure 5 the first GUI version took the metaphor of the circles quite far and was oriented around cells that contained smaller cells The first version of the UI was fully implemented and contained growth animations for chosen cells and a small amount of fluid dynamics however it was quite confusing to use and more of a game in itself  After the initial GUI pass the focus turned to the game modes of the system The final version of CCA supports 4 different game modes for cognitive gaming from a single creature reaction observing to feeding multiple creatures In addition to building the initial game modes the first bug fixing and improvements for the rendering engine were implemented  Once the game modes were done a sprint was dedicated for properly introducing a full GUI to the platform and further separating UI components After the initial logic and games and UI were done the focus was on Login and register functionality as well as user support mechanisms such as account handling and performance graphs and information graphics in general  After the phase three of the project the official Beta release of CCA was ready The beta interface can be seen in Figure 6 The platform supported all the basic game modes user accounts a full play through experience and statistics to prove it Once this version was done the work for the Heureka science exhibition called Science Changing the World started It was a separate version completely independent from the backend and running a local copy of the rendering engine modified for 4 player multiplayer needs  After the Heureka version the phase 5 concentrated more on improving different parts of the platform at a time The addition of medical imaging indicator helped automated testing of the platform the improved target location in path finding enabled smoother movement and the GUI code had a thorough overhaul to separate it more from user logic  Phase 6 was the 2nd major round of refactoring and improvements for the platform The introduction of training programs changed the way the platform worked and the new game modes added a lot of variety to the package Engine code refactoring helped further development of game modes especially and the target state calculation change was done in order to make it more manageable for the server to fine tune the user account based training regime  In Phase 7 the way users were rewarded and progress communicated got a much needed overhaul changing the direction of the feedback to a much more user friendly result In Phase 8 the release version of the platform was finalized the final GUI was put in place the performance was verified and the user experience perfected  There were a lot of features and ideas in the beginning some of which were discarded along the way as unviable for us to implement with our schedule and needs among those features were evolutionary algorithms in creature generation  The technologies enabling browser based heavily mathematic rendering for the front end were evaluated before the project begun and a choice was made in the beginning on which platform to pursue the solution on Since much of the projects core engine is based on mathematics that are rather universally supported by the different platforms the porting of the platform to a different technology is not considered to be an impossible choice in the future  One of the technologies evaluated was Silverlight the RIA solution from Microsoft It is a browser plugin similar to its main competitor Adobe Flash An overview of the Silverlight technology stack is seen in Figure 7 The runtime is based on the popular C language and UI components are marked in the more human readable XAML Extensible Apllication Markup Language XAML is also used in Windows presentation foundation WPF and NET framework of which Silverlight is a sub sect of Silverlight implements the same version of the Common Language Runtime CLR as the NET framework  30  Silverlight offered similar performance to Adobe Flash and being a part of the MS solution package it comes with a wide range of solid development tools and support with Visual Studio family of products Silverlight supports the integration of multimedia graphics animations and interactivity in a single runtime environment Similar to Flash Silverlight also supports vector rendering in addition to the typical Bitmap based rendering  The main hindrance in Silverlight and one of the reasons it was not used is that it is far behind Flash in adoption rates Where the Flash Player 10 adoption is around 987 in mature markets 3 Silverlights adoption rate was around 22 4 at the time of the project start Whilst being impressive in its abilities the Silverlight technology was rather young in 2009 and sorely lacking in features  The technology chosen was the de facto standard for rich content online in 2009 the Adobe Flash platform As mentioned in relation to Silverlight it is the most widely spread browser technology in the world and has a mature and solid development environment and support Flash supports vector rendering as well as bitmaps and the newest versions from 11 onwards support using OpenGL based GPU rendering as well When the project started the player version target was 101 but the release version of CCA is targeted for the Flash Player 11 platform 7  Whilst requiring the installation of a plugin the wide spread adoption of Flash made it a clear and easy choice for a consistent user experience across platforms and browsers There are up–todate versions of the player for Windows Linux OSX and Android platforms  The heralded revolution of standards based online development – HTML5 – was not around in a prominent way back in 2008 After studying the possibilities of using HTML5 and Javascript based solution as an open alternative for the plugins it was found that the performance of Javascript VMs and especially the rendering performance was subpar when compared with the plugin based technologies  This has changed a lot from the year 2009 and the newest JITJust in Time compiler in Google Chrome is starting to surpass some of the plugin solutions in pure crunching power However the rendering capabilities of browsers vary a lot based on the versions and the adoption rate of new browsers remains one of the larger obstacles in generating modern standards based rich solutions online  Being a consumer mass market solution we aimed at all three big players in the market on consumer platforms Microsoft Windows family Apple OSX and Linux based operating systems needed to be supported The support for our decided solution platform – Adobe Flash is wide and envelopes all our target platforms  The benefit of choosing a browser ran online platform was also the generic platform independent nature of the technology With a single solution we could reach and deliver a fully functioning solution to the whole user group As seen in Figure 8 the project supports the main versions of Microsoft Windows from XP onwards OSX from 106 onwards and Linux Red Hat Enterprise 56 or later openSUSE 113 or later and Ubuntu 1004 or later in both 32bit and 64 bit varieties 7  In principle a twopersonproject the CCA was a significant undertaking The viability of using browser based technology for creating a complex mathematical rendering engine for clinical and consumer use was something not many had done before  The biggest challenges of the project were on the pure engine building level The rendering of creatures in real time without prerendering or cheating in the rendering pipe line and the performance of path finding logic and path wrapping The mathematics used were performance intensive and the rendering of several creatures with hundreds of rendering points and surfaces in addition to the path creation with continuous Bezier curves was difficult for a CPU based rendering engine  The goal of building a medical platform for consumer use had its own challenges The combination of a number based mathematics oriented solution and attractive user experience provided many obstacles Keeping the UI code separated from the engine and ensuring a solid separation of concerns was essential in building a working platform  On the platform front the separation of as much presentation and user logic to the back end for control and data based optimization of training posed several challenges to overcome The parameterization of almost all UI elements and processes from game modes and their configurations to even trophy icon generation was a complex issue to solve  Cognitive Gaming  A type of gaming and exercise that is designed to help and improve cognition Used in the aid of recovering from brain trauma also used as recreational activity believed to be beneficial to the mind  RIA  Rich Internet Applications Browser based technologies that enable creation of desktop like features in the world of internet Example technologies include Flash and Silverlight  Modern Browser technologies  A technology stack that contains all browser ran technologies HTML5JS Flash Silverlight Java   Neuroplasticity  The theory that the brain is capable of physical change and improvement based on outside stimuli  Creature  The main target in the project CCA A collection of mathematical formulas that are rendered based on a path finding engine to represent an organic creature  A single unit target of the platform  Bezier curves  Smooth parametric curves based on 2 control points and a start point and an end point Essential in project CCA  User experience  A combination of usability user interface interaction design information architecture and animation to cre ate a complete use experience for a user  Platform  A combination of technologies that form a coherent reusable and deployable whole An extensible combination of modules that works as a basis for building content on top of  Figure 9 Key concepts of the study  Key concepts used in this study are shown in Figure 9 They cover the areas of cognitive gaming and modern browser based game development as well as the principle technological choices in organic rendering  Cognitive gaming is an exciting new area for consumers and scientists alike There have been various studies and trial projects about using games as platforms for advanced learning learning through play and using virtual worlds as class rooms for learning Cognitive gaming takes the elements of gaming such as repeatable tasks reward models user tracking and fun of play and combines them with medical research and recuperative methods for a game experience that also benefits and improves cognition  The basis for all brain exercise games and all is the concept of neuroplasticity or brain plasticity the ability of the brain to change physically throughout life in response to stimuli The times when changes happen in brains are in the beginning of life when injury hits the brain and whenever something new is learned and memorized 8  Up until recently it was believed that the connections in brains remain fixed with age and physical changes are impossible However recent studies have shown that brain keeps on changing through learning and stimuli throughout our lives 9 As an example when comparing professional musicians to amateur musicians and nonmusicians the actual physical volume of grey matter in areas involved in music such as motor regions anterior superior parietal areas and inferior temporal areas was larger with the professionals who practiced over one hour per day 10 These changes were also greater when measured over time  Another example came from a study done on extensive learning with German medical students They used medical imaging to monitor the brains of the students before their medical exam and after and compared the results to similar students who were not studying at that time The students brains showed anatomical changes in grey matter in different areas of the brain including the parietal cortex and posterior hippocampus parts of the brain known to be involved in learning and memory 11  Despite the recent studies and interest in training programs to be used there have been very few long term studies in the effects of cognitive games and training While there are studies that show the short term implications of training 12 especially on those suffering from early stages of cognitive impairment there have not been sufficient enough studies to show whether the training can postpone the effects of such impairments as dementia 13  In 2005 the size of the brain health market globally for software and biometric applications was estimated to be around 210 million dollars The estimated value of the market in 2012 is over one billion dollars and by 2020 it is estimated to reach six billion dollars in value 14 This rapid growth comes in part from recent research and in part from many medical professionals and researchers trying out the possibilities of using their research in helping people on the consumer market  As seen in Figure 10 the biggest growth expected to happen in the growth of cognitive gaming and brain fitness is the consumer market by a large margin The growth of self service training portals with training regimes directed for home use and selfimprovement are already growing fast and are expected to do so in the future as well The other big growth area is in the area of insurance and health care as well as elder living The benefits of preventive training in age related deterioration are substantial as well as in the rehabilitation of people with brain related injuries The savings generated by such actions could be monumental Aside from the main growth areas the uses of cognitive games in school systems as well as employee care are expected to grow  The cognitive game market is divided into 2 different parts the pure software products and the biometric applications that require actual hardware to measure physiological responses applications Examples of biometric products include products that measure hart rate variability or brain activity through EEG Electroencephalography the recording of the brains electronic activity over a short period of time through the scalp Our reference project belongs to the software category and does not need any hardware to function  In the area of cognitive games the reference project lies the focus on perception attention and working memory there are various competitors and products on the market To keep the subject more valid and tied to the subject of this study the focus is on online browser based cognitive gaming platforms and their use cases technology and popularity  The biggest company in the online cognitive gaming market in 2012 was the Lumositycom Lumosity is partnered with researchers at Berkley Harward and Columbia and works with numerous health care organizations to help create cognitive gaming experiences The service has over 25 million users and it provides comprehensive and personalized training programs based on user accounts 15  As seen in Figure 11 Lumosity online training program conveys facts and scientific information about what the tasks you are performing at the moment provide and manages to create a solid user experience with enough information and play to make it interesting The training program is fitted to your needs based on a simple questionnaire The first steps in Lumosity are free but after a few games you get to a point where you cannot benefit from the service without subscribing  Technology wise the frontend base portal of Lumosity is based on standards based HTML5 and CSS3 providing the general test framework and admin functionality in the portal The actual games themselves are made with Adobe Flash technology similar to the reference project The games are rather simple in function and are very event based in nature In addition to the web interface Lumosity also has a mobile application available for the Apple iOS platform  From a user experience and game design point of view the portal comes across more as a collection of different games and a framework that ties them together as seen from Figure 12 The different games use different visual cues and styles based on the subject matter and are not uniformly under the same visual design aspect as the main Lumosity portal However they do contain repeating elements in the introductory controls to provide similar functionality across games  In addition to providing a cognitive gaming service Luminosity has scientists actively working in the field of research and finding out how to best use cognitive gaming to benefit the human condition In a recent study it was shown that there was improvement in the working memory and visual attention of the target group when using a web based training application outside of a clinical trial setting 16  The other competitive platform taken as an example in this study is the My Brain Solutions portal at wwwmybrainsolutionscom Similar to Lumosity the portal provides a brain assessment in the beginning based on which it generates a user profile and a training program for you to follow Unlike Lumosity the brain assessment is a quite a comprehensive test of memory comprehension emotion and other cognitive functionality and lasts around 30 minutes up front 17  As seen from Figure 13 the My Brain Solutions contains a personalized training solution as well as charts on how the users brain and performance range on the variety of test subjects In addition to points which Lumosity used in this platform the user also has badges a reward mechanism similar to achievements to convert the arbitrary numbers and progression into more human readable terms The platform allows you to set your own goals and encourages you to set actions for yourself to keep you busy  Technology of the My Brain Solutions follows that of Lumosity and others the main site is a web portal built on web standards while the individual games are based on Adobe Flash technology In addition to the web portal My Brain Solution has various applications for different mobile platforms targeting a specific feature such as MyCalmBeat that focuses on lessening stress and increasing focus through slow breathing  In comparison to Lumosity My Brain Solutions has a more unified gaming platform with all the games showing similar introduction screens button layout and statistics  in game information when doing tasks As seen in Figure 14 the different games all feel as if they are of similar family and go well together visually with the main visual identity of the portal From initial testing the games seem to have more complex interactions in them as well  From the more entertainment oriented area of gaming recent years have shown increased popularity for titles such as Brain Age 2005 for the Nintendo DS console that sold over 1896 million copies as well as its sequel Brain Age 2 20052007 1483 million copies to date 18 While these products were very popular and studies have been done in order to evaluate their possibilities 19 Nintendo has distanced itself from the use of scientific proof of benefit in the games 20  An example of the Brain Age line of games can be seen in Figure 15 In the Brain Age games the user is expected to play a small amount each day according to a training program This is a common approach to brain fitness and is employed in the reference project as well The tasks a user performs orient around simple calculations and mathematical questions memory exercises Stroop tests as well as Sudoku puzzles  The cognitive gaming platforms focus on a certain set of games each targeted for training different parts of the cognitive system These areas are close to the ones used in clinical neuropsychology but understandably are not as exact or as precise due to the consumer market approach These target areas can be roughly divided into three categories memory attention and executive function Some platforms such as My Brain Solutions also contain tests related to emotion and human understanding  Memory games test and train our ability to memorize items words patterns and other objects There are different variations of memory games a commonly used one involves working memory or short term memory These tests are called Nback tests where a user is presented with a sequence of objects and the task is to react when the current object matches the one shown N objects before 21 The Nfactor can be adjusted to make test more or less difficult Short term memory tests are well suited for consumer market testing the test gameplay does not take overly long and the results are parsed live An example of an Nback game is seen at the top in Figure 12 as well as another type of memory exercise seen in the middle  Attention games focus on our ability to perceive and react to our perception in a given controlled environment The idea of attention tests is to train the cognitive processes of focus and visual search as well as long term attention The tests often focus on pattern recognition as well as visual attention The way to test attention implemented in the project CCA is to have multiple moving objects on screen that the user has to follow and react whenever the object achieves target state Target state is a change in the shape colour contrast or form of the object Numerous examples of attention tests are found in the reference project platform as well as the example game at the bottom of Figure 12  Executive function is an umbrella term used for various cognitive processes and sub processes working together  Executive functions are those involved in complex cognitions such as solving novel problems modifying behaviour in the light of new information generating strategies or sequencing complex actions 22  In the reference platforms for brain gaming the tests for executive function include games related to arithmetic quantitative reasoning planning verbal fluency and task switching An example of an executive function test can be seen in Figure 16 This task is about connecting a series of nodes with as much area as possible without ending in a dead node a task that requires both planning and reasoning  The reference project is mostly based on various attention games as well a testing mode for Nback tests to improve your working memory and a continuous change mode that falls in part into the domain of executive function  The term game engine came to be around mid1990s in reference to 1st person shooter games such as Doom by ID Software Game engine is a platform for game development that employs data driven architecture to create reusable software components such as a threedimensional graphics rendering system collision detection system and a physics simulation 23 The line between an engine and a game is often blurry and to date there are few engines that can adapt to more than a few genres of games  The engine defines how the game is rendered on screen and influences a lot of the design around the core concept of the game Some game engines include tools for building the actual game content on top of them such as Unity Unreal Engine or CryEngine 242526 This is often called scripting since it is most often done using a scripting language Many of the engines contain their own scripting language 26 or use a set of commonly supported high level languages Unity includes support for C Javascript and Python based Boo 24 Some engines provide only the actual rendering engine for abstracting the hardware level such as Ogre 27  Game building in general has gone through a renaissance of sorts where the middle ware is increasingly important in creating game experiences in the industry as well as in the education of game development 28 A large portion of games today use middleware such as Bink video 29 Havok physics engine 30 or the Adobe Flash harnessing Scaleform 31 for game user interface building These middle ware programs provide a necessary relief from the complexity of building a modern game each solution doing its part providing a polished and optimized way of handling one aspect of the game  In the field of medical and cognitive gaming some are using the same game engines as entertainment oriented gaming uses 32 while many use their own engines based on a higher level programming environment such as Flash Silverlight or Java to create their own base engines Many cognitive games are not yet complex enough to have the need for specific engines but with the increase in demand development budget and competition it is only a matter of time  When talking from a more conceptual level of how game interaction and gameplay is handled there are two types of game environments static and dynamic In the context of this study this distinction is defined as the way game events are controlled in the game and the way the engine handles rendering and interaction  The traditional game engine is static as in everything in the engine is defined by an artist or a developer every move you make is the result of a careful calibration iteration and concepting Static engines allow for absolute control for the games designers and are in many cases more optimal from rendering and calculations point of view In the comparison products Luminosity My Brain Solutions and the Brain Age all fall into the static game engine section They are all also very event based in their approach everything happening in the games is not based on real time calculations but on specific events happening at specific times and the reaction to those events  The downside of static game engines is that in controlling everything they lose the element of surprise in some ways When everything is designed there are no happy accidents nor odd gimmicks that a player can find that cannot be reproduced and everything in general works as it does always the same reliable way Many static engines also employ a basic physics model with hardbody physics In such an engine the physical values are tweaked and set by the designers in a way it replicates some simple form of physics but does not really allow for proper surprises  While being more obscure and harder to control dynamic game engines base the world they render on a set of rules These rules may be physics different path algorithms and so forth but they all have in common the lack of direct control over what happens inside In a dynamic engine the game designer gives the objects targets creates behavior and sets up boundaries but how the engine executes these is left for the engines internal system to decide  Some games use the dynamic game building in ways to create randomized environments procedural content based on a set of rules Games such as Diablo 33 and Minecraft 34 have used procedural generation to create whole levels or in the case of MineCraft a whole planet The traditional problems with computer generated content in games have been the repetition of content and unnatural and uninteresting combinations Some newer games use this dynamic or procedural generation in the generation of items for the player to use such as the weapons in Borderlands and Borderlands 2  35  In gaming physics a more solid way of doing physics in a dynamic way is called softbody physics where a physical object is a collection of its sub parts physics This is immensely heavy in calculation and is only now emerging with the new Cryengine 3 25 and other new game engine platforms The project CCA is not in a world of fully fledged physics nor is the world building in any way overly complex but the engine and the way the creatures work is entirely dynamic  User experience UX has many different definitions depending on the subject matter it is related to A classic example is from 1996 from the first annual ACM Interactions Design Awards  By experience we mean all the aspects of how people use an interactive product the way it feels in their hands how well they understand how it works how they feel about it while theyre using it how well it serves their and how well it fits into the entire context in which they are using it 36  The NielsenNorman group define user experience as  User experience encompasses all aspects of the endusers interaction with the company its services and its products The first requirement for an exemplary user experience is to meet the exact needs of the customer without fuss or bother Next comes simplicity and elegance that produce products that are a joy to own a joy to use True user experience goes far beyond giving customers what they say they want or providing checklist features In order to achieve highquality user experience in a companys offerings there must be a seamless merging of the services of multiple disciplines including engineering marketing graphical and industrial design and interface design 37  In principle UX is everything a user feels sees experiences when in contact with a company andor a product The point NielsenNorman make is to go beyond the needs of the user the realization that UX is far more than providing the user with what they need but how they experience it as well The joy of use aesthetics and message all combined in a thought out package  User experience design UXD is an umbrella term that covers the different facets of expertise required to create a wholesome UX The roots of UX design come from Human Centred Design HCD HCD can be summarized as  Positioning the user as a central concern in the design process  Identifying the aspects of the design that are important to the target user group  Developing the design iteratively and inviting users participation  Collecting evidence of userspecific factors to assess a design 38  In addition to the methodology of HCD UXD builds on top of HCD with more complex cultural and business factors While traditional HCD based usability factors were about performance and smooth operation UXD brings along aspects of social interaction the importance of aesthetics both very culturally complex and context requiring concepts  Since both UX and UXD are very broad subjects in the context of this study the focus of UX is confined to how our project platform uses UXD to create a suitable UX within the platform itself The areas presented in relation to the reference project are interface design  aesthetics interaction design  movement information design  communication and playability in the cognitive gaming context There are also various UX concepts such as presence immersion flow fun involvement and engagement that try to describe the UX in games 39 These theories while important are not discussed within the scope of this study  What the success of iOS and mobile platforms has taught us user experience matters People are willing to pay for better suited and thought out solutions The wakeup call provided by the mobile markets sudden rise has not gone unnoticed in the world of gaming Especially in large companies there has been a surge of focus on hiring user experience designers and front end designers to focus on the neglected parts of games the game UI and the interactions with it  Games have been on the forefront of human computer interaction for the past 20 years as the playability of a game essential to the overall experience is uniquely a quite complex endeavor from a user experience point of view But while focusing on playability inside the engines and games themselves they often forget the first thing a user sees when they enter a game the user interface of the game itself The navigation settings and saveload these top level controls affect a lot on how one perceives the game experience  Often these are badly designed riddled with flaws inconsistencies and hiding of information It is not uncommon in games that you are not sure what will happen when you adjust a setting or a parameter The inclusion of UXD methods and experts has made games more userfriendly and easy to use In building a platform for cognitive gaming the unified UX for the games within the platform as well as the overarching user interface was an important part  As has happened throughout time the advance of technology and tools for creating visual communication have changed and evolved at a rapid rate in recent decades Despite this change the essence of graphic design remains unchanged to bring order to information form to ideas and expression and feeling to artefacts that document human experience 40  The international typographic style has been a significant and influential style in graphic design for the past 50 years Its origins come from Switzerland and Germany in the 1950s and it is also known as the Swiss Design The visual characteristics of the style thrive for unity though a solid mathematically based grid objective photography sansserif typography and a solid copy that presents information in a clear and factual manner 40 More than pure style the importance of the international typographic style lies in the attitude and approach its early pioneers adopted The role of graphic design was formed more towards shaping information and communication then personal expression and artistic eccentricity  This clarity of style and information is a basis of much of modern online communication and it is the basis for the style in CCA as well In addition to a clear and unified visual communication in CCA the focus was also on the visual narrative animations movement and flow that affect how a user perceives a product  Typography is the art of type the act of arranging to make language visible In the modern day the term envelops many crafts from the traditional typesetters and compositors to graphic designers and artists In the context of this study the term is used to mean everything we do with type in a digital platform Font sizing characters and legibility  Typography exists to honor content  Like oratory music dance calligraphy – like anything that lends its grace to languagetypography is an art that can be deliberately misused It is a craft by which the meanings of a text or its absence of meaning can be clarified honored and shared or knowingly disguised 41  In modern digital communication the use of solid typography to create a unified and visually attractive legible message is an essential part It is used to both communicate efficiently as well as to add a feel personality and grace to the communique  The technological choice in CCA the Adobe Flash Platform is a good choice for working with typography as it contains a much more advanced text rendering and font support engine than the traditional browser solutions With the support of fully embeddable type the platform could take use of proprietary fonts with full fidelity and control With the recent advances in CSS type support it is even possible to use proprietary or special fonts on standards based online communication 42  In project CCA the main typography is provided by the fairly common Myriad Pro originally developed by Adobe in 1992 and widely used by companies such as Apple Walmart and Wells Fargo 43  Myriad Pro is a versatile sansserif font family designed for mostly digital use The family contains a wide variety of weights and widths to suit the needs of the CCA platform It is a simple elegant font with excellent readability An overview of Myriad Pro font rendering with examples can be seen in Figure 17  It is often a problem for digital platforms to provide similar design and typography in different languages as the Latin based languages have their own typography and the Chinese Arabic and various other languages have their own typeset Rare fonts support even the whole plethora of European language typesets from Cyrillic to Greek Myriad Pro is a good choice for a language versioned platform since it provides a complete support for Greek and Cyrillic characters which enables the use of same visual fidelity across languages  The Flash Platform also provides the tools to embed different character sets for the use of completely different language characters such as having Myriad Pro for Latin based languages and a specific font for the Chinese market Which font to use is decided during runtime when dynamic content is loaded based on the character sets involved in the UTF8 encoded content  As seen in Figure 18 CCA uses Myriad Pro in two different weights regular and bold The main headers for each screen are 36 points big allowing for a clear distinction from the sub headers and regular copy text As CCA is not a very text heavy platform the most used texts are the buttons and sub headings All elements that indicate something that can be activated or form a separate section of information are bolded for effect All regular text information notices explanations and such are in a readable 14 pt size  In interface design the essential glue that keeps a view a page a dialog together is the composition also known as the page layout Composition can be thought of as the link between art and mathematics the use of relations numerical patterns such as the golden section and geometric shapes to create a formula that organizes content in a meaningful and clear way 44 Composition can be based on various styles from a single visual to perhaps the most used element of composition grid theory An effective tool is also to use Gestalt Laws of grouping the principles of how the mind organizes visual data in creating coherent compositions 45  In CCA the whole composition is based on a centred grid with a focal point always at the horizontal and vertical centre of the display area All content containers and controls as well as all UIelements are aligned based on the centre point As seen in Figure 19 the centre based composition is followed by the main playground the user menu at the bottom and the game menu at the top as well as the main information dialog at the start of the game During the actual game the main visual time indicator is at the centre point as the most obvious visual element for a user  An important aspect of composition is the use of spatial relationships The space can be created by content using images texts icons lists logos or just plain text – or it can be created by the space between content called negative space or white space The space can be actively used to create a point or it can be passive there just because the layout process requires it 44 The use of negative space is essential in giving air and increasing legibility in digital design It can be divided into two categories macro white space – the distance between major content elements – and micro white space – the distance between elements within content elements such as lists  A challenge in composition in a digital medium is the dynamic nature of the viewer setups The layout can be viewed with a screen from 1024*768px up to 2560*1440px with varying pixel density DPI This creates quite unique issues for the use of white space and the arrangement of elements The traditional approach in digital design is to set a certain target resolution a compromise that contains reasonable resolutions and provides a good enough result for those not matching the target resolution  As Figure 20 shows the most popular screen resolution of users in Europe has grown from 1024*768 to 1366*768 in the last 4 years Beyond just the resolution the size to resolution ratio or DPI Dots per Inch has also grown and diversified The rapid change in resolution as well as the fragmented size variance has resulted in a situation where designing for just a single base resolution is not a preferable option anymore This is true even without taking into account the boom in mobile browsing and devices with smaller physical size and screen resolutions  As a response to the ever growing resolution and varying DPI the modern way of doing digital layouts is in a state of transition an ever larger amount of platforms and frontend technologies enable the use of adaptive or responsive layouts A responsive layout adapts to the viewing area by resizing reorganizing and resampling the information presented to the user 47 It is often based on set of steps within which the content scales and when a step changes the content arrangement and  or organization itself changes  A key challenge in CCA was the requirement of being completely resizable upwards from a minimum size 1024*800px The elements must fit the screen from the minimum onwards while maintaining a suitable space and visual feel to them A part of the solution for this was the centerbased grid design Because the platform can be resized both vertically and horizontally basing the user controls in the middle of the screen enabled efficient use of both axes  In CCA the whole viewport scales based on user resolution but the game area itself has a minimum and a maximum stop for both performance and playability reasons The size of the game area affects how big an area the user has to visually scan in order to notice changes This area cannot be too large otherwise it starts affecting users performance scores  In addition to the scaling layout size all the main views in the UI of CCA can be dragged around the screen by the user as needed This empowers the user to arrange the UI in efficient ways depending on their own resolution It also helps solve the problems of using extra space in the GUI Each element has an active drag area everywhere where there is no control or content presented The dragging works with slight simulated physics calculating a primitive velocity of the object when a user drags and releases it and using a static coefficient for friction decreases the elements velocity until it comes to a stop  An important part of understandable GUIdesign is the use of repeatable recognizable visual controls and containers also known as elements The use of repeatable controls is also of benefit from the development point of view as it is substantially less time consuming to create base classes for repeatable elements such as a content container or a button and button sub type Repeating elements are also essential in creating a coherent user experience where the user can predict how a certain view behaves and how to interact with controls  CCA contains a set of containers and controls for the users to use that is largely based on drag able containers that automatically centre on screen when created and later on allow the user to place them wherever they please As seen in Figure 21 the main container is the base of every content element It is vector based stretchable and contains the dragenhanced functionality  Other common elements for most of project CCAs layouts are the main header the general button element and the close button In prompts the close and the general button provide the same functionality with different messages From these base elements most of project CCAs views have been built  Aside from the floating elements a typical CCA UI contains the user control menu This element attaches itself to the bottom of the game screen and displays user specific information such as the account name and the amount of points the user has as well as providing logout functionality During a game play the UI also has a game related menu with information about the game you are playing as well as controls to get back to the main UI  For proper interaction every element needs three basic states for a solid user experience normal hover  active and clicked  reaction Examples of project CCA button elements three states can be seen from Figure 22 They all follow the same logic having a subtle highlight specific for the type of the button and a full fill recognizable action state that is clearly indicated Providing these states allows the user to always be in control to have a predictable response to every interaction  CCA contains also elements that are not meant for interaction by the user but for mere show of information During the game there is a large clock displaying how much time is left in the current game and how many sections are included in it Another element that a user can view is the indicator for performance in the current game The symbols stars and crosses show the user how well he is doing during the exercise whilst being inconspicuous enough not to distract the exercise Examples of the clock and the starcross performance indicator can be found in Figure 53  Colors contain a lot of meaning The selection of colors for a user interface and especially for branding of a platform is an important step Colors while being culturally dependent and highly subjective offer a lot of meaning and interpretation and can greatly enhance the user experience and aesthetics of a system Whilst there is a plethora of articles about color in physics psychology and other disciplines in the context of this study color is used to discuss the merits of the use of color in the reference project from a user and artistic point of view  Figure 23 shows the main color themes in CCA The main color is a green shade R 184 G 227 B 115 normally used in a radial gradient with multiple stops between the end and start points The gradient form is to create a feeling of enlightenment a subtle focus on the content in the middle The green background and color scheme is used throughout the buttons and modal dialogs wherever something related to the main game is encountered The blue R 91 G 166 B 218 theme is used in information and statistics where there is a need for more contrast with the elements and a distinct visual appearance  Other than the dominant background and highlight colors the main color for text in CCA is always white with a subtle drop shadow to bring it forward from low contrast backgrounds In buttons and other interact able controls the active color is always dark grey or black to highlight the change and alert the user to an action  In addition to the use of color in the platform interface the games themselves have a specific use of color Each creature in the game has its own color scheme containing various hues and shades to create an organic creature  The use of shapes and colors in the game relies on feature integration theory a theory of attention suggesting that perceiving stimuli can be divided into two separate tasks features and objects Features can be registered fast and in parallel whereas objects are slower and separately identified Visual searches regarding these two tasks are called feature search and conjunction search Feature searches are fast sweeps targeting only one feature such as color or shape while conjunction searches work with a combination of features 48  In CCA there are game modes targeting both types of visual search The game types focusing on multiple creatures on a neutral light grey background focus on the use of feature search where the target mode of the creature is a change in contrast color and  or shape The game platform also has a mode for rendering a specific Perlin noise background matching the color set of the active creature aimed at the use of conjunction search The use of a background where the creature easily blends in forces the user to focus on finding the shape by combining color and shape information The task of separating the object from the background layer is called figureground separation 49  Motion is a powerful tool in the world of modern digital communication when creating a unified user experience especially for the consumer market By creating a specific visual narrative a repeating pattern of interactions and movement that builds upon the visual style and reinforces it with suitable movement one can build a memorable and effective user experience  In the realm of traditional animation there are 12 principles for believable movement 50 of which the following are relevant to transitions in digital media and the CCA platform  • Anticipation o To prepare the audience for the action and to make the action feel more realistic  • Slow in and slow out o To simulate most movement in the real world Most of human movement and gravity based movement have an inout easing curve it builds up and it builds down  • Arcs  o Trajectories are followed by most natural motion most living creatures have structures that enable them to move in certain ways follow certain paths Emulating these trajectories creates organic looking movement  • Secondary action o A complementary animation that adds to the main animation enhances its effect  • Timing o Correct timing makes objects more real like they were following the laws of physics  • Appeal  o Anything the user sees and finds likeable pleasant design a quality of charm simplicity and communication Originally about drawn characters but applies to graphics as well  The sections of movement and animation discussed in this study can be divided into three categories Timing and flow movement patterns and easing and nonlinear motion  Timing is the part of animation that gives meaning to movement 51  The key in efficient visual narrative is twofold timing and delays Timing is the essence of transactions interactions and transitions within the platform How long does an element transition how it appears how it behaves Delay is essential in combining different views and states With the combination of these two the base structure of visual narrative is achieved  As Figure 24 shows most transitions in the project platform last between 300ms and 600ms In views the time in is always 500ms and the time out varies based on view but the change trigger is always 750ms when a transition from a content state is initiated the out animation call stack is started and the 750ms delay timer is initiated at the same time Regardless of the time the view takes to animate out in 750ms the animate in of the new state is called and started This overlap makes it possible for a user to interact during transition and the loose coupling between view animations makes the user always be in control  On average for a transition to be within acceptable limits it has to be between 100 and 1000ms Immediate response for actions like clicking requires some indication of reaction within 100ms of the action If you go over the 1000ms limit people will start to think the system is sluggish and unresponsive 37 In CCA the times are based on iterations of visual aesthetics feeling of motion and professional opinion Motion is a rather tender art and the feel of a transition is based on the shapes colours graphics elements and the surrounding elements of the target There is no unified rule that can be quantified for every solution it depends greatly on the context and surrounding platform  The buttons have the same click animation time always to make the reaction based on a user intent unified The difference in animation in and out time is essential since the shapes and means are different in a menu button the line colour and shadow are animated in the close button the shape and colour are animated and in the general button the gradient shadow and text colour are animated All the buttons have different hover state animations however they are visually coherent and unified to represent the same indication for the user  Tweening or inbetweening is a method of interpolating the change in value between points A and B A basic tween is linear consisting of a predictable amount of points between A and B Dynamic tweening is adding acceleration and  or deceleration effects to animation by using easing algorithms 52 The problem with static  linear tweening is that the movement looks fabricated and clunky Dynamic tweening solves this problem by adding natural feeling movement patterns such as the slow in slow out principle of animation  In programmatic animation easing algorithms enable the feeling people perceive in physical movement This effect is done by applying an algorithm to the interpolation of movement of the object In principle the ease affects how the steps of movement are shown where there are more steps and where less along the path of the animation Figure 25 shows the most common easing algorithms used in tweening libraries on various platforms Most are based on Robert Penners work on easing and tweening  52  In CCA the use of easing algorithms is subtle and a mix of the following  Content boxes o BackeaseOut for background o QuadeaseOut for fades  Menu elements o QuadeaseOut for fades and color  o StrongeaseOut for the line  Countdowns o StrongeaseOut for scaling  Drag o Based on inertia a custom easeOut calculated based on mouse movement and velocity  Most of the transitions are elegant simple based on the subtle curve of Quad based movement The parts where there is a need for highlighting movement as in dialog boxes and the pop out of elements the Back based movement is used For fast movement and to give the feeling of control of pace the Strong movement is used  For user reactions the easing type of easeOut is almost exclusively used in CCA The easeOut means the movement starts fast and slows down as it decelerates in the end It works well for user reactions as the start of the movement is the user initiated action the response needs to be fast and clear and easeIn algorithms tend to make the transition seem sluggish EaseIn is only used when animating a background out with scaling of the height of the element Since the Back easing algorithm has the bounce effect and the background is closing it is visually appealing and logical to have it do the bounce in the beginning  After timing and easing are taken into account what is left to create a solid visual narrative is the actual movement of objects There needs to be a recognizable pattern of movement that strengthens the visual branding and solidifies the feeling of the user experience In CCA different containers have different movement but the general movement patterns are very similar and create a solid feeling of a unified platform  The button animations all contain the same click effect animation and timing When a user clicks a button the text or main shape of the button turn black in colour over 300ms The menu button is a good example of the secondary action principle of animation The main action is the colour animation to black the secondary supporting animation is the line growing from the left to right underneath the text The line animation does not dominate the visual but builds on the effect and supports the feeling of highlighting Examples of button visual states can be seen in Figure 23  Dialogs and content views all appear in a similar fashion the background animates in first by scaling the height to 100 from 0 while animating the opacity of the element to a full 10 form 0 These two combined make the background appear smoothly from thin air The content inside dialogs is animated in with a sequenced animation fading in elements and in the case of lists also animating the x coordinates of the element from –N to 0 Where N is subjective to the size of the element The sequence timings can be found from the Figure 24  The basic pattern of animation in user interface elements for project CCA is the fade in  fade out the simple vanishing and appearance of objects It is supported by secondary actions such as colour animations movement scaling of horizontal and vertical size depending on the object animated  From development point of view creating a visual narrative requires a specific way of creating your user interface classes When transitioning between states many platforms and systems make the mistake of queuing the transitions in a way that makes the user wait for interactions between states This breaks the narrative flow and lengthens the response time of the system to the users frustration  When creating a solid platform with UX in mind it is necessary to plan in advance and to create methods for handling view states concurrently In project CCA when states are transitioned every control taking part in the transition executes their hide  show mechanic There is a set delay between calling the next state when current state is transitioning out and there is no queue As soon as a control starts to form on the screen a user can interact with it The user can stop the current transition and use another control while transitions are in place as well  Creating modern user experiences requires the developer to take this into account when creating their solution It saves time and effort to solve the issue of state handling and transitioning between states already in the beginning or planning phase of the project When building individual controls it is beneficial to have a set template to start from that contains all standard functionality needed for state handling Whether to implement this via inheritance or other methods is up to the developer  In CCA every visual element from single controls such as a button or content views such as the main menu to modal dialogs and user menus all of them contain basic functionality for handling their own view state The repeated functionality in all visual controls of CCA are show hide and dispose as seen in Figure 26  A necessary requirement and in creating modern interfaces with programmatic animation is the ability to override ongoing animations Overriding enables the stopping transitions and interacting with controls mid transition to keep the user in control Modern UI technologies either contain suitable Tweening libraries inbuilt or one can use one of the many open source alternatives out there In CCA the library in use is an extremely robust and extensive tweening platform called Greensock Tweening Platform 54 by Jack Doyle wwwgreensockcom GSAP is available for Flash and HTML  Javascript based platforms  GSAP abstracts many of the verbose methods needed in creating programmatic animation such as the setting of filters timelines and sequencing by offering a simple but powerful syntax that allows for the tweening of any numeric property alongside platform specific properties such as CSS transformations It is also quite robust efficient and contains a full open documentation The platform is used by many of the top sites and digital experiences in the world such as big movie productions big brands and art projects 54  In CCA there are two different systems for showing performance and ability The user has access to pure numbers and graphical visualizations of performance and numeric data The platform provides statistics on all game modes as well as training program based data for the user to analyze The idea is to enable the user to transparently go through how well he has done and how he has improved and draw his own correlations about the program Example of the histogram approach is shown in Figure 27  The second way the user is communicated the importance and performance of the data is via a very UX oriented method – achievements The user is provided different rewards in three different categories of achievements achievements stars and trophies The different rewards are all catering to a certain part of the measured data and user response Every achievement has their own icon and a badge the user can see once they log in  The achievements provide a very human understandable way of communicating complex data oriented events The user can for example get a trophy from doing better than previous runs or for having a perfect run where there were no missed reactions The rewards vary between the different game modes In comparison to the raw data shown by the histograms that do provide a valuable way of seeing visually where you improve the achievements allow the user to gain specific feedback for specific parts of their game performance Example view of achievements in CCA is seen in Figure 28  CCA is a user experience driven cognitive gaming platform harnessing the power of the Adobe Flash Platform in the front end implementation and the LabVIEW system design software as the server side solution  The general architecture of the platform is based on typical clientserver architecture Overview of the architecture is seen in Figure 29 The client connects to the server and authenticates the user account after which the server sends a packet of data that determines what options are available for the user with the account The user interacts with the options available and a new request is sent to the server to which the server responds by providing wanted data be it user account details or a list of games  As is common for Flash based web applications there are no page loads or page requests during the application run The state of the page is managed by the application platform itself and all interactions and reactions by the user are parsed processed and handled without the browser As such the platform itself can be ran with a flash player outside of browser environments as well  Labview short for Laboratory Virtual Instrumentation Engineering Workbench is a programming and system design platform built by National Instruments It uses a dataflowbased programming language which enables users to program logic with the use of graphical block diagrams 55 The platform provides a very visual way of creating functional systems  The platform front end is based on Actionscript 30 AS3 The Ecmascript based 3rd iteration of the Actionscript language is an object oriented programming language running on the Actionscript virtual machine The version of the virtual machine that supports AS3 was built from ground up to support the new OOP nature of the language In combination with the Flash development tools like Flash Builder and Flash CS5 the AS3 is a powerful tool for interactive media  The front end is built to be modular and much of the control comes from the backend This datadriven architecture helps keep the platform modifiable for different purposes and user programs without making changes in the front end implementation 23 The server side provides lists of menu items and games available as well as statistics and user account data The front end has different tracks of interaction and progression through the solution that are triggered by the server messages  Figure 30 gives an overview of what packages views and controls the CCA front end platform contains In AS3 projects there is always a root stub that initiates the solution when started The main solution initiates the application model and main UI elements and queries the server for language versioned UI definition XML After that the navigation service handles user interactions and along with the UI state handler keeps the visual and logical state of the application in sync  The model package contains main application logic and data related functionality The application model handles application states with the navigation service and holds references to the different parts of the application The navigation is integrated tightly with the UI state handler that handles the UI state changes and controls what the user sees Data services handle all server communication and data parsing  The creatures package is the home of the main rendering engine All the heavy lifting mathematics with path generation and creature drawing is done by the package The pathfinder handles the generation of new path points and general route handling The creature handles the algorithms creature specific calibration and state information The brain is responsible for combining the input from path finder and the creature and to pass off the necessary values to the renderer which then does the actual plotting and drawing of the creatures A detailed description of how the rendering core works is in Chapter 6 of this study  The game handling package handles all game related logic Once the game data has loaded and parsed the game handler initiates the correct game mode The game mode handles the states within the game everything from info prompts to countdown to the different phases of the game data tracking user performance and the end and fail conditions Each game mode has their own conditions for game complete and their own goals  The UI package contains all UI elements views and controls Essentially the GUI is constructed via the UI package The main UI class acts as an interface for the UI elements initiating them based on UI states and disposing them as necessary The game related UI elements are contained in their own package and are used and called from the game handling classes All controls are separated as usable entities any view can use any control as needed  The utilities package contains all extra functionality and helper classes that the views game handling and data services need It has sections for animation data loaders and handlers algorithm parsing various physics and inertia handlers as well as the core Bezier classes Utilities are used through the application model by any section of the solution that needs them Most utilities are initiated only once and kept in memory to be used as a static instance  The CCA platform was built to be highly manageable from the back end solution Even though the rendering engine and game logic resides in the rendering and game engine in the front end platform as much as possible of the configuration of the games and rendering options were separated from the engine and interaction logic  The communication between the front and the back end services is done using a fairly standard REST API Every interaction is based on HTTP POST calls with structured XML Data XML is an application profile or restricted form of SGML the Standard Generalized Markup Language 56 XML is a standardsbased and well documented human readable format for communication and configuration and a suitable choice for the platform Due to the nature of the game rendering engine XML was also suitable for conveying the mathematical formulas and sets of functions needed to create the creature renders The Flash platform also contains an in built support for E4X 57 the extension for AS3 that makes XML a native primitive in the programming environment making the parsing and use of XML efficient  When the user enters the platform the front end queries the server for the latest language information After the user inputs his credentials the server is queried with the user information and a request for the main user interface details for said user As seen in Figure 31 the server parses the request matches the user login information and returns information regarding the user account The front end platform constructs the personalized user interface of the user based on said instructions  The platform supports a user based exercise program that monitors how well the user is doing and calibrates it to match the performance and improve it When a user logs in they are greeted by a personalized message based on their previous performance  The user has specific games based on his  her exercise program and a set amount of exercised per game per day or a set time period he can perform The program can be managed and calibrated to each users needs based on progression and scores from the game  Most of the data is served to front end only when needed For example when the user wants to see a certain performance graph it is loaded only on users request and rendered with the latest data Each view of the chart is requested per interaction to limit the amount of data sent and to keep response times low In principle everything in the front end is based on the data transmitted from the back end services except for the rendering and game logic and user interaction  The front end game logic is highly parameterized and the games are generated based on the instructions sent by the server in a game XML  Figure 32 provides an overview of a single game In the initialization phase the front end fetches the gameand object parameters and initializes the correct game logic engine instantiates the creature renderers per creature in the configuration file and sets the initial failure success and timing of the game During the game the front end game logic handles all user interaction game events and data gathering In the event of a game over be it via failure or success the game termination is triggered The performance data is sent to the server and in return an analysis of the performance of the users in relation to their performance program is dispatched and shown in the front end platform  The game is initialized by using a set of configuration flags for the game logic and the rendering engine via the game XML The game information is parsed from the game XML to a game value object This VO is essentially the core of what a single game contains and it is used in the rendering and game logic engine  The game XML has 2 main parts the game configuration and the creature rendering The creature rendering is covered in detail in chapter 6 A simplified hierarchy of configuration details in the game XML is shown in Figure 33 In principle the configuration construes from 4 main sections The user settings game settings optional configuration and game object data The game object is a simple set of variables that define the user id game name game id and login id for the user session and game management with the server  User settings are related to the user account of the logged in user The player information is for UI notifications mode contains game related data for the user The mode determines what kind of a game logic instance is created and also contains information about the delay and timing of the users abilities in reacting to stimuli The mode related user information also contains an optional calibration property that determines the time and requirements of a calibration round before the actual game begins Calibration is done to ensure proper user difficulty in training  Game settings define alongside the failure condition the structure of the game The periods contain depending on the game mode the different sections of the game Each section has a target time performance and creatures that are used during that period The target timing contains an array of timestamps on when the rendering engine displays a target mode effect that prompts the user to react The target timing also contains information on which creature said target timing affects  The target timing was originally done in the game logic engine with only minimum and maximum times for the frequency of the target event given by the server But during development and testing there arose a need for a more fine grained control over target states especially with the integration of the user training programs With the server in control of the target times various iterations of the same game can be made without affecting the front end platform codebase  The optional settings include modules that are used in different game modes and a medical imaging and measurement helper The foto detector is an indicator that shows whenever a creature reaches a target state Its size and color can be configured depending on the use case It is used when recording user activity via medical imaging and other instruments as a synchronizing signal The background generation defines the needed values for the background Perlin noise generation from the actual noise to the color treshholding More information about the background Perlin noise generation is found in chapter 643  54 Game rendering event  The system in game engines that controls the ongoing simulation is the main loop It is the representation of time in your engine and the layer responsible for the game life cycle 58 In CCA the front end game engine is based on asynchronous events between different modules The core of the rendering engine is the renderer class Any visual interaction or module that requires a time based rendering pass such as the path creature and target rendering and calculations subscribe to the main rendering class render event and base their rendering passes on it The event is a custom event with a unique rendering time key that is then employed in the various time based calculations in the rendering Figure 34 shows the most important renderer event related dependencies  The separation of direct calls gives the platform a manageable and simple way to add new requirements on the render pipeline During the development of the platform the rendering core started from a simple mode of having creatures running free on the screen and slowly evolved to support dynamically moving bubbles for the background time rendering visualizations and calibration events all subscribing to the main rendering thread  The separation also allows for an important feature for any gaming platform the complete control over the flow of time With this control all forms of prompts countdowns user feedback and result screens can be efficiently and smoothly incorporated to the game designs at any time  55 User reaction tracking  In order to get a complete account of how a user performs during a game in the CCA platform all user reactions are tracked even those that dont end up showing any visual signs in the user interface Every reaction be it valid or invalid is time stamped and logged alongside all creature target renders and game phase changes The data is held in memory during the game period and after the game is over it is sent to the server for analysis  The game data tracked during a play is shown in Figure 35 The data contains events divided into the game periods they belong to for precise analysis Each event contains the timestamp in the relative time of the game rendering process type of the event handled and possible tags related to the event In addition every reaction or target state handled contains a list of all visible creature positions for further analysis The data also contains information about the actual rendering resolution of the game areaTracking enables the use of more detailed information about how people play and the strategies they employ in reacting optimally within the platform games For example the difference between a user trying to optimize hit percentage by reacting at a regular interval and a honest try at seeing the reactions can be taken into consideration with the analysis of the reaction data especially with comparisons to previous user account performanceThe core of project CCA is the rendering engine It is a 2dimensional engine based on a standard XYcoordinate system The coordinate system starts from the top left corner and expands to the bottom right corner The whole rendering area is flexible and user scalable and the platform adapts to the user resolution by adjusting the speed and size of the rendered creatures for optimal playability For the sake of rendering performance and playability there are set minimum and maximum width and height values for the scaling  The engine is a so called black box system where the game is generated based on set rules and the engines own logic This means that while being very controllable the engine does not mindlessly repeat control orders nor does it follow a linear human made path The engine draws and moves the creatures based on their paths and targets trying to find an optimum route but does so within its own limits and algorithms  Games often are made based on strict artistic control and very man made worlds as it allows for a more fine grained control and precisely deterministic outcomes but it lacks the surprising elegance of simulation based engines A good example of some simulation engines are the modern physics engines in games especially the ones based on soft body physics instead of rigid body physics  The basic principles of the rendering engine revolve around the concept of a creature Each creature is an entity with its own path finding and rendering logic and all game types work with changes in the creatures The core separation of the creature object is the shape and path as seen in Figure 36 The shape is the actual form of the creature the visual end result that the user sees on screen calculated based on a series of mathematical formulas and a set of parameters The shape consists of the actual calculations for solving the formulas as well as the rendering of said formulas on screen with a set of colors and transparencies and possible dynamic movement The shape also takes care of wrapping the shape on the actual path generated by the path part  The path is the core movement of the creature itself Each creature has its own path generation the role of which is to find out target locations for the creature and calculate optimum movement paths between them The path is based on continuous cubic beziers and recalculates itself every time the creature reaches the end of an interpolated target path and embarks on the next segment  The rendering of the creatures on screen is based on two main parts the plotting of the stationary creature on its own and the path wrapping calculation and plot The creatures stationary calculation defines how the creature looks is colored and how it performs its target state The path wrapping takes the end result of the stationary phase and integrates the creatures form into the shape of the path it is travelling on The creature can also be rendered without the path as a stationary object in its prime state  During the project lifecycle there were a couple of different ideas on how to render the actual creatures With the path generation separated it was possible to use hand drawn pieces of creatures and animate them on top of the path points to make them look more artistic and use less rendering mathematics as well But for pure organic feeling and keeping full creative control in the hands of back end generation it was deemed better to go with the option of drawing the creatures completely with mathematical formulas for full customizability  The creatures are rendered based on a server served creature XML that contains settings and configuration as well as the actual formulas for creature generation Figure 37 shows an overview of the creature generation The settings define features that the creature has as a whole such as the target states and length of the creature The calculations contain the essence of the creature Each formula has a set of configuration attributes that explain how the particular set of formulas is rendered  The formula settings define the look and feel of both the creature surfaces as well as the creature main lines In addition the values related to actual plotting of the formulas are introduced the amount of points to be rendered the start value of the plot and the distance the plotting is done on Each formula is a set of two X and Y coordinate plots The main plot is the first XY pair and the return plot is the XY2 pair when these are combined they form a surface with a fill that is a part of the creature  The actual creature rendering is a stacked plotting approach to rendering mathematical expressions spreading them over a uniform scale wrapping them up with return functions to create surfaces and mirroring the end result to optimize rendering The creature form is unified on both sides so only one half of the creature form is calculated and then mirrored to create the actual creature  Figure 38 shows an overview of how a creature is rendered from the set of algorithms provided by the creature XML The first line shows the first formula being plotted first the initial plot of points without lines drawn is shown then the fill with lines and shape Then the return formula XY2 is plotted and it forms a complex shape Then the return and the first formula are filled to create the end result This is done for each formula and drawn on top of each other on the creatures canvas and the formula 4 line contains the end result creature built from the 4 mathematical expressions  The creatures are based on two types of formulas The various formulas are plotted on the screen and the 2d coordinate system with the time seed value from the rendering event time loop  The base of the creature and most of the formulas are static formulas Static formulas are plotted once per creature per formula After the initial plotting of the formula the plotted points are stored in vectors typed arrays of AS3 and used for path wrapping and physics calculations before finally being turned into visual renderings  The amount of plot points is defined per formula as are the values the formula is plotted on The definition of the target state is given as a separate formula to be rendered when triggered All the times of trigger and densities and colors and opacity levels are configurable per formula  As seen in in Figure 39 a single expression of a creature consists of a set of parameters followed by the initial and the return formula of the single shape to be rendered Each static formula has a variable that is plotted with the given attributes This variable is named t and it is parsed in the rendering engine into AS3 native value along with the text representations of the math functions  In the example expression the t is plotted from valStart to the distance provided by valDist over the course of the amount of points defined in valNum After these have been plotted the points are stretched with the length factor of the creature XML The length factor is the length of the creature in ideal circumstances and is adjusted as needed based on the resolution of the game area during play  Some creatures based on rendering type and game mode also contain dynamic formulas Dynamic formulas are plotted per render loop every time anew This creates a relatively significant overhead for calculations for the processor Dynamic formulas are used sparsely and mostly to add some flair or to make it harder to recognize the target states of the creatures Figure 40 is an example of a dynamic expression the definition in CCA for a dynamic variable is c Typically dynamic formulas are circular so they create repeating smooth movement  The third type of algorithm every creature contains is the target state calculation formula Figure 41 This formula decides the look of the creature when it reaches target state the state when a user is supposed to react to the change in the creature The target state is parameterized in size and effect based on the user accounts previous score and calibration results It is precalculated at the start of a game and contains only static formulas The difference between a normal drawing function and the state one is that the state is drawn for a set amount of time as defined in the creature head part of the creature XML  Every formula the creature is based on was originally a one way formula forming a line but not a shape After some development time there arose a need to make shapes fills and surfaces in the creatures to add visual fidelity and concretize the creatures some more To solve this need the ability to render return formulas for the various algorithms was introduced  In principle a return formula is plotted on the same time values as the initial formula and in the same scale With the main formula it creates a complete shape or surface which then can be filled with color and opacity to make it more vivid Adding the return formula to the shapes enabled the creation of complex shapes interloping with opacity and colors for unique creature designs An example of how the return formula forms the body of the creature can be seen in Figure 38 and an example of the XML can be seen in Figure 42  The creature formulas plotted on their various paths form the natural state of the creature To create life and movement it needs to be integrated with the path provided by the path creator The integration of the creature to the path is called path wrapping  Figure 43 shows a visual representation of the difference between the creature in its natural state and the path wrapped bended version of the creature The red line at the bottom is the interpolated target path of the path calculation for the creature The target path generation is explained in detail in chapter 632  When wrapping the formulas on the path the formulas are plotted as usual to create the natural stationary creature but after that the whole plot is bent around the segment path so that lengthwise the center of the creature is on the current point of the path rendering progression Then both halves are calculated from the center onwards up to the point where the length of the creature ends The interpolated target path is the size of the creature and contains a point for each plot point in the stationary creature plot  After the target path is generated the creature is wrapped around the path point by point calculating the angle at each step This calculation is done per formula per creature each formula is bent individually  A simplified principle of the creature bending can be seen in Figure 44 First the integral of the segment as well as the angle of the segment at any given point is solved while making sure that the angle doesnt go over – or  pi Then the position of the creature is interpolated on the path and the point is rotated based on the angle on the frame path at the same point The creature is wrapped point by point on the ongoing path allowing it to react to every small change in the angle of the path individually enabling the organic feel  The movement of the creature is based on the path generation created by the path creator class The path creator calculates the targets where the creature needs to go and the path itself and then attaches the creature on the path The path is based on continuous cubic Bezier curves to create a harmonic organic movement of the creatures while maintaining randomized movement paths  A rendering of path creation in CCA can be seen in Figure 45 When the path is created it consists of 5 sections of points each section has a minimum amount of 100 points between control points A creature moves between the middle control points 34 and whenever it hits the 4rd control point a new control point is generated on the path and the last control point is removed When the game begins first 2 points are generated by random and after that based on the same rules as the new points in the game  Because the middle and ongoing segment of the path needs to be unchanged every decision reaction for the creature takes the time of going through the next 2 segment paths By making sure the length of the paths per segment is never longer than 05 seconds preferably around 0304s the creatures reaction speed is between 0608s which is around the same as the reaction speed of an ordinary human being in a nontrivial task  The paths are based on cubic Bezier curves Every cubic Bezier curve has two control points and a start and an end point When making continuous Bezier curves the start point of the next curve is always the end point of the previous curve Making Bezier curves that continue from each other is relatively simple and requires no complex calculations But when making smooth movement one needs to make continuous Bezier curves that are smooth and that means each curve affects the curve before it and after it An example curve can be seen in Figure 46  For the path finding and rendering system to work And because calculating continuous Bezier curves with the path wrapping isnt cheap the active creature path at any given moment must be a static Bezier curve The path will change only when the creature reaches the end of the current path then the next point on the path is calculated and rendering continues Because of this requirement the path needs to have the five segments or six points at all times  Once the point locations are generated the path needs to be smoothed Since the motion is continuous through the points the movement cannot have jumpy or unsmooth sections To achieve this the last control point of the previous curve needs to be on a straight line to the first control point of the next curve as seen in Figure 47 The line goes through the control point and the path point  For creating cubic Bezier curves Flash contains a native library called BezierSegment It is a collection of four point objects that define a single cubic Bezier curve and has methods for getting the value of the curve at any point on the curve itself 59 The solution in CCA is based on the BezierSegment class and more specifically on the work done by Andy Woodruff on the implementation of continuous cubic beziers with AS3  60  One issue when using continuous Bezier curves as paths for movement is that when the angles of the previous and next point collide in a straight line on the x or y axis the line drawn is a straight line and by default is calculated without any steps To overcome this in CCA the straight line situation is checked after the Bezier generation and a slight variation is added to the target point to get a proper curve between the points for calculations  In CCA the continuous Bezier curve forms only the foundation for path generation on which the actual movement is built Once the base path is generated the active segment is resampled for smoother movement and simple physics  In Figure 48 the different parts of the path generation are shown The RawInterpPath is the path created by the smooth Bezier curve generation After the RawInterpPath is calculated the active segment of the RawInterpPath is separated as its own entity called InterpPath The actual path used by the creature for movement is made by calculating the velocity and displacement of the InterpPath This path is called a FramePath  Figure 49 opens up the complex logic of creating the path The actual FramePath is not needed for the current segment of the creature for movement For that a FractionalFramePath is generated from the InterpPath which contains a fractional index that is used to find the actual movement point in the end The FractionalFramePath index is used by interpolating a point on the actual curve based on said index In principle the FractionalFramePath is the creature location on the InterpPath  The physics used in the game are fairly simple The main use for physics as such is to enable the creature to have different speeds based on its mood and the curvature of the path it is on This creates the feel of organic movement as the creature slows down to curves and speeds up when going long ways along a slightly curving path  The path segment where the creature is currently moving on during the rendering is never changed Because of this the physics and velocity of the creature are calculated when the path is calculated In general the path is calculated and physics applied to the movement only at points where the path is recalculated – when the creature reaches a new target location  Aside from the mathematics of actual path generation CCA also contains methods to create path finding for the creatures and boundary methods to keep the creatures rendered inside the actual game area  There are two modes to the path finding hunting and foraging When foraging the next path point is generated without a specific goal by using pseudo random logic In foraging mode the creature finds its own way around the game area without specific incentives When in hunting mode the creature has a target array of points where it aims to move to the closest target point is evaluated and an optimal path is generated towards said point With the closest one calculated the path finder finds out if it is close enough to nab it in this path if not itll create one path segment towards it first and then redo the calculation If the target is gone the creature falls back to normal foraging mode  Figure 50 shows a rough overview of how path finding logic works in CCA The next target location is generated by combining the orientation between previous two target points with a randomized target angle and a speed based on creature length and the angle of movement The core idea is to have a similar direction of movement to prevent too big drastic changes in the creature direction  If the next natural point in a creatures path generation is outside the boundaries of the game area a bounce method is applied The bounce calculates how far outside the boundaries of the stage the creature was going for and changes the angle of movement so that the new point is inside the stage This is done to ensure the creature is always within the visible target area and does not wander off the sides of the screen  After the path generation and bounce factor have been taken into account the distance between the previous and the new point is calculated to make sure the points are not too close to each other In case of them being too close a small speed fix is added to the points location in order to keep the length of the path relatively uniform The length of the active path segment is set to be 1…2x the length of the creature to keep the reaction times in check  Perlin noise is a procedural texture or a pseudo random gradient that is used abundantly in modern computer generated art and graphics for its unique attribute of seeming organic and natural The algorithm interpolates and combines random noise functions into a single function that generates more naturalseeming random noise Perlin noise has been described as a fractal sum of noise Developed by Ken Perlin back in 1980s for the movie Tron Perlin noise has been used in CGI and had a huge impact on computer generated graphics ever since 61  Aside from using Perlin noise to draw patterns or textures it is also very useful for creating organic smooth movement The movement is achieved by using the luminance values of a black and white Perlin noise and attaching it to the velocity of a particle engine Due to the nature of the noise one can create the noise field once and then change the offset of the x and y coordinate space of a single octave of the noise With this the noise moves without recalculating the whole noise while maintaining the same visual similarity Perlin noise is also quite optimized and suitable for performance use  Figure 51 shows a Perlin noise field used in project CCA to move the background particles around This noise field is randomized on every application run and resize event so the movement of the background pattern seems organic and never repeats itself Perlin noise is rarely used to just create visuals but rather to blend other visuals together or to generate objects landscape and other items that benefit from natural seeming patterns 61  The way Perlin noise is used in the background rendering of the CCA system is quite simple Every resize and init event a a 2dimensional Perlin noise is generated to fill the game background layer no need to make it whole screen sized It is restricted by the same resolution restriction stops as the main game area thus simplifying the sizing issue to a single place This Perlin noise is not visible to a user and is used only to generate movement  The noise field is approximately the size of the gaming area and it is populated with N amount of particles The amount of particles varies based on the users resolution to create an optimum look versus performance The particles are simple round shapes with opacity randomized to make them look like abstractions of bubbles In the start the particles are positioned randomly along the noise field  As previously mentioned the game has a single renderer that all time based actions are performed on On every render tick the position of each particle is changed according to a set offset that is unique to each instance of the particle  The opacity angle speed and scale of the particle are calculated from the brightness of the pixel at the new position of the particle Figure 52 The brighter the noise at the coordinates the faster the particle moves and the larger it becomes and vice versa In addition to this noise based movement each particle has a unique wander property that affects the navigation of the particle to make the movement a little bit unique With the smooth coherent noise we get from Perlin noise it provides a unique and performance efficient way to generate organic movement  In addition to working as background organic movement and mood setting the particles served another purpose During gameplay correct reactions triggered a coloring effect on the background particles closest to the creature whose reaction was targeted There was a set radius around the active creature within which the particles had to be  The coloring was gradual a smooth green shade with opacity stops per reaction was introduced An opposite effect was also introduced when a user repeatedly triggered reaction without a cause outside of the reaction window of the target state the particles were gradually colored with light shades of red One could get an overview of a game situation at a glance  In addition to making movement with Perlin noise Perlin noise is used in the reference project as a creator of coloured randomly generated backgrounds that fit the colour scheme of the creatures The colour match is important as it makes the spotting of creature target states harder when there is less contrast between the background and the creature itself  Figure 53 shows an example of how Perlin noise is used in the backgrounds of certain game modes It was implemented by partially replicating the Perlin noise in use in the background animator The actual effect was developed in true to platform fashion in a parameterized way where the server provides a possible array of colours to be mapped into the background noise with the creature XML These colour values are then mapped to the noise based on the brightness values of the noise  The colours are mapped to the noise by first taking the highest and lowest brightness values in the black and white noise then mapping the colours to the noise by assigning a brightness index per colour In CCA a vector of values was used since it is faster to iterate through an array of bytes than to manipulate an actual bitmap object  A small trick was used in the resizing of Perlin noise fields since they are quite heavy to instantiate and generate and there was a need to generate one anew every time the stage was resized In calculation or rendering heavy tasks it is not viable to run them every time a user or a program resizes the window but rather set a small delayed call to the refresh function and overwrite that delay every time the resize gets called With this small optimization one most of the time gets only one rerendering round even when the user holistically drags the window around  Optimization should always be done holistically Look at the big picture first and then drill down until you find the specific problem that is slowing your application When you dont optimize holistically you risk fruitless optimizations 62  The amount of computing resources available to a game is finite Optimizing code increases the amount of work the engine can achieve per unit of computational power and maximizes the efficiency of the system 62 In optimization there are many ways to speed up the code but it is essential to identify and solve the actual performance bottlenecks within the laws of diminishing returns The main tradeoff to think about when optimizing is the development time versus complexity  In a project with strict performance requirements such as the CCA one has to be quite strict and diligent in handling performance bottlenecks However untimely optimization leads to chaos and low quality code it is imperative to find out exactly what needs to be optimized before jumping in Performance optimization is a balancing act between readable solution code and pure performance gain  There are many ways to approach optimization but the fundamental basis of the optimization lifecycle is testing or benchmarking  Figure 54 shows the basic idea of the optimization lifecycle When optimizing the essential point is to measure the gains A benchmark is a point of reference in the game that serves as a comparison against future implementations Benchmark should be reliable quick and it should represent an actual gaming situation The main use of benchmarks is that they enable relative comparisons  After the comparison points are gathered the detection step starts The point of detection is to find the biggest return on investment for the optimization In detection phase it is important to start from the big picture and analyse layer by layer to the finer problem points until a suitable issue is found After the detection it is a matter of solving said issue Solving can be about fixing a bug toggling a flag rewriting the algorithm involved or changing the data structure 62  Once the solving is done the check phase begins When checking the benchmark is ran again and measured to see if the solution changed anything in the performance After checking it is all about repeating the same progress again until the biggest performance gains are figured The idea of this cycle is to find optimization hotspots and bottlenecks Hotspots are the points in your program that consume a lot of processing power Typically hotspots are small amounts of code with a big hit on performance Optimizing hotspots leads to significant performance benefits Bottlenecks are particular points in the system execution that clog down the performance of the whole system  Optimization can also be approached on a broader level by dividing it into three categories Figure 55 On system level optimization the focus is on the use of resources of the target platform The point is to find an implementation that utilises system resources with balance and efficiency System level optimization is about planning a platform that utilizes the available resources without overusing in a sustainable way  Application level optimization can also be called algorithm level optimization It is the choices made in the data structures and algorithms that make up the platform The idea is to use a good profiler tool to find out call hierarchies and time and iteration amounts of systems most used parts Algorithm level optimisations are the crucial backbone of optimizing code The identifying of a key part of code that uses a lot of processing time and optimizing it or the node that calls it with a more efficient solution is of the best return on development time  Micro level optimizations are the most common stereotypical types of optimisation The small hacks on as low level as possible to get a bit of a performance boost out of a specific thing The optimization of inner loops or pixel rendering routines that are called extremely often and that benefit from the most miniscule of an improvement because of the sheer amount of times they are called during an application execution  In project CCA the concentration on optimization was made on all three levels A structural planning oriented performance review was conducted in the initial phases of the engine building The algorithmic level was benchmarked and reiterated throughout the application development cycle and the low level micro approach with rendering and mathematics was made in the engine building phase to smooth out the heaviest of operations in the engine  Beyond the processes of optimization one must be wary of the pitfalls along the way It is easy to get stuck on assumptions about performance problems and optimizing prematurely without knowing if it is a big performance sink Optimizing based on your own machine or with debug builds gives misleading results as debug builds are often slower and riddled with processes not found in the final release build In CCA during development there was a point where the focus was on micro optimizations on the engine rendering loop for too long and without the aid of a profiler the real optimization pitfalls would have remained hidden  A common trait in game rendering engines is to render only what is needed and reduce the level of detail dynamically whenever possible In fact many games could not function without such optimizations due to the sheer size of the game worlds and the amount of rendering In CCA the amount of rendering is relatively small compared to many of the 3dimensional engines but due to the measurements and the nature of the medical side of the platform it was necessary to render everything on screen without any reduction of rendering quality or level of detail Because of this in the creation of the rendering engine it was necessary to try and use all the tricks possible with a managed language to make it as fast as possible  A good approach to optimizing holistically is to focus on detecting and solving issues on the system level whenever possible then the algorithmic level and only if no solution is found resort to micro level optimization  The optimization of the platform can be divided into 3 main sections seen in Figure 56 In data optimization the focus was on the issues of latency and communications with the clientserver architecture The point of data optimization was to minimize the latency of backend calls and limit the amount of data sent between the back and front end platforms The front end requests data for a view be it the main user interface a data visualization component or a single game only when initializing the view for the first time  When non changing data such as the main user interface localization and configuration is loaded it is cached on the front end and not refetched until the user next logs in again When a response from the system takes between 200ms and 1000ms a user is prone to lose the feeling of flow and the user experience of the system is hampered 37 For the sake of responsiveness and continuous feedback the user interface shows a loading indicator whenever a data request takes longer than 400ms The indicator is not shown on shorter loading times to keep the user flow uninterrupted  When optimizing the application level the focus was on making the rendering engine and game logic separate and to manage garbage collection and memory handling with specific initialization and dispose functionality The separation of the engine and the game logic optimized not only the development time of the platform but the way responsibilities were divided between the two  The initialization of objects in managed languages is costly and can easily cause variation in the FPS of the system as well as random hangs in program execution Also the disposing of objects for garbage collection GC can cause the GC to run at times when the user is performing an important part of the platform The ways of observing smoothness in a system are somewhat immaterial and cannot be found purely from profilers Extensive testing and visual assessing is needed to make sure the user experience flow remains unaltered More information about garbage collection and GUI elements can be found in the Flash specific section 73  The deepest iteration of optimization was done on the micro level thinking about the mathematics used and testing different operators and in lining function calls and other hacks that produce less readable code but provide a performance boost in a time critical section of the platform Mostly micro optimizations were done in the core rendering engine on the calculation and drawing of the creatures and their movement  The way to find important performance issues in project CCA was done with using the Flash Builder profiling tool for performance profiling Figure 57 Profilers are the most common tools for optimization gathering information about resource usage most often the CPU load during a session Profilers typically provide information about the amount of calls selftime and total time of function calls as well as memory used in them 62 In CCA the profiler helped in generating memory footprints and locating high resource hogging sections of the rendering code as well as finding bugs in disposing of objects  In CCA the creature rendering model is a very versatile implementation of a data driven architecture The creature parameters define how they each formula in them is plotted on screen and the detail level of each creature can be adjusted per formula as well Each mathematical formula has an amount of points that are calculated and plotted to render out its true form and these can be adjusted and manipulated with relative ease The lower the amount points rendered on screen the less fidelity in the creature but on low end machines or situations where there are many creatures on screen it increases performance smoothly  Figure 58 shows the clear difference in rendering quality based on the amount of control points or dots used in the creature generation The overall shape and visual style of the creature is the same between the fill render and the 4x fill render but the subtle details of the creature are more smooth and round in the 4x fill render than the normal fill render The dot render shows the inflated dots generated by the plotting of the formulas the dots themselves are used in the rendering only as the control points for the lines that shape the creature itself  The difference in cumulative CPU calculation time in milliseconds between the 1x and 4x dot renders is shown in Figure 59 The values benchmarked by using the Flash Builder profiling tool set show that the rendering of 4x the amount of points on the same creature increase the calculation time approximately 32 The difference is to be expected as the fidelity increases so does all the major computation in the creature rendering and path bending More difference could be obtained by adding a formula for the creature generation itself instead of adjusting the points of the formulas calculated  Different creatures can have different dot amounts for visual fidelity Some formulas work better with low amounts of dots while others require much more to look and behave properly When the creatures are moving the tighter turns they take the more clear the complexity of the creature if the creature consists of a low amount of dots the lines between start to show when doing above 90 degree turns quite visibly There is a small implementation in the path creator that tries to optimize creature movement in a way that it never does such high degree turns  Due to the data centered design of the rendering the creatures can be optimized based on user accounts and user machine performance to create the best possible outcome For most formulas in CCA the conversion of the formulas to native code could be done in the initialization and the calculations for the necessary values out of them only once and then reuse the plotted model of the creature in all the path bending operations without recalculating the plot The performance gains were significant In addition to the fidelity of the creatures additional features can be optimized such as the background Perlin noise particle animation can also be toggled as needed  The Flash platform or more specifically the Actionscript 30 AS3 execution model is single threaded Because of the lack of threading all sufficiently heavy actions on the UI thread slow down or stop the rendering of the whole UI layer While being robust for a web technology it is a severe limitation of processor use AS3 is also a managed language interpreted by a runtime which leads to system managed memory and garbage collection  There are various issues when profiling with managed languages the nature of the byte code leads to more native level calls than pure native languages The way AS3 handles array actions is more involved than native languages every array is implemented via a generic data structure so every array load means hitting that data structure and querying it for type information 62 Because of the cost of even basic operations can be much more varied than in native code it is beneficial to set a good baseline for performance data to know what to avoid  The basic approach in optimizing for AS3 is to always use strictly typed operators everywhere in your code it provides a significant runtime performance boost Another way of optimizing what is needed in AS3 came with the Flash Player 10 release Flash had no history of supporting typed arrays before the introduction of Vectors in FP10 Vectors have a significant overhead in declaring and wiping them but they are extremely robust when iterated over significant amounts of values 63 The difference between array and vector use is significant as seen from Figure 60 The reading speed of vectors is roughly 40 faster than with traditional arrays The writing speed difference is even greater roughly 170  In AS3 the construction of new objects is very costly When creating the creatures and parsing the formulas from XML to AS3 native math functions it was useful to use a technique called object pooling In object pooling you declare your objects only once and store them in a list Instead of creating new objects when you need them you use the objects already declared and stored from the list Figure 61 The benefits of object pooling are greater when using more complex primitives the increase in performance is 5x when using a basic type such as a Point and over 80x when using a complex Sprite type 62 In CCA each creature is object pooled but so is each set of formulas that form a part of the creature  To prevent the UI from slowing down in CCA the initialization of objects into the object pool was also optimized In the game handling initialization the creature classes belonging to the game are instantiated The Perlin noise backgrounds are created as well as all the creatures When creatures are not on screen or do not belong in the current play mode they are hidden from view and removed from the display render loop Since the game core is about constant perception of the target moving it is imperative not to have high changes in frame rate on average even if the rendering frame rate is not the full required 30fps the users arent as phased by the difference as long as it maintains a steady pace  Alongside the optimizations mentioned here there are great many smaller point solution optimizations used in CCA and available for the Flash platform Links to further reading on the subject can be found from the associated references in the end of this thesis  One of the basic optimizations to do is to remove loop invariant code When doing large quantities of iterations the cost of declaring iterators within loops and inner loops can be quite high especially in managed languages such as AS3 It is useful to try and minimize the amount of variables declared inside loops when it is not necessary even predeclaring the loop iterators let alone variables used in the loops saves performance 62 64  Another optimization that tends to be bad for code readability but that can help optimize code that is heavily used during rendering is the inlining of functions in your code When moving functions inline one must be wary of the cost for code readability it significantly hampers the future development of the code base Inlining calculation heavy functions can amount to a performance increase of 400 in AS3 64  Other uses for micro optimization are the manual redo of language specific mathematical libraries There are many ways to solve basic things such as the absolute of a number via casting to int or using a ternary operator that can be faster than using the platform inbuilt Mathabs Small optimizations in AS3 can also be gained by using the right iterators for the right loops for example when doing while loops it is faster to loop through in reverse order than forward looping 64  In CCA the micro level optimizations were used solely in the core rendering and path finding engine There was an effort to keep the unmanageable only in the areas identified by profiling to require significant tuning and optimization Function inlining is used in some processor heavy mathematical operations and reimplementations of things like Pointdistance and Mathabs to improve low level efficiency For most of the platform code algorithm and system level optimizations were preferred  Another way of optimizing performance heavy bottlenecks in calculations and iterations is to use bitwise operators instead of verbose solutions Since computers operate in binary using bitwise operators can be a powerful programming tool Often a few quick operations can easily replace what would otherwise be complex and heavy code In certain situations using bitwise operators can even amount to clearer code 62  Common bitwise techniques in AS3 revolve around using the inherent quality of binary operations multiplying and dividing by the power of two integer conversions sign conversions modulo absolute value minimum and maximum seeking and color conversions Figure 62 shows common alternatives to operations in AS3 by using bitwise operators for efficiency  Performance benefits for using bitwise operators in AS3 can be immense Figure 63 shows the benefits of using bitwise instead of natural logic in common logical operations The benefit is especially clear when using the bitwise method instead of the Mathabs an increase of roughly 2500 can be achieved Basic functions such as multiplication and dividing run around 300350 faster than the basic solution When testing the modulus of a number the performance is around 600 similar to testing if a number is even or uneven  When optimizing the CCA rendering engine the approach was to benchmark key rendering situations and find out the most performance intensive mathematical operations and optimize them The most used and heavy functions were optimized with various micro optimizations including removing loop invariant code declaring variables together and bitwise operations  Flash supports both Vector Not to be confused with the Vector of typed arrays in Flash and bitmap rendering Both of them have their own benefits drawing with vectors enables crisp smooth and scalable rendering as well as quite handy tools for handling the graphics Whereas drawing with bitmaps allows for easy manipulation and bitmap specific filters such as PixelBender and especially with complex and large graphical content performance benefits 64  There are many ways to optimize the drawing of content in Flash One good tool is to use the redraw regions debug option to visibly observe what regions of the screen are dirty and redrawn on every frame Even though content is opaque if it is on the display list it can still trigger hit tests and other runtime performance hogging tests as long as its visibility is set to true One can also set the platform level toggle on movie quality to a lower amount this affects antialiasing smoothing of scaled bitmaps fonts and animations  Other performance heavy operations in native Flash drawing are the use of filters and blend modes Filters such as drop shadow and blur need to be applied on the whole object they affect during render time unless specifically set to cached Efficient use of cached effects and minimizing the amount of alpha blending can also help rendering performance especially on lower end hardware  Most vector content in Flash can be cached as bitmaps and reused even changing the objects x and y coordinates does not affect the caching benefit However caching cannot be used for any content that needs to be redrawn every frame such as the creatures in CCA  In CCA both bitmap and vector based drawing methods were studied The target was to find which would be most ideal for rendering the CCA creatures Vector based rendering gives one smooth controlled geometrics in a fully scalable way enabling graphical fidelity and an ease of creation As opposed to vectors bitmap rendering is purely pixel based and as rendering goes it is typically significantly faster than vector based rendering Handling bitmap rendering is somewhat trickier than doing it in vectors especially if you go with just pixel painting since you need to manually take into account antialiasing and corners of the creatures  Since the creatures are plotted from mathematical formulas into vector shapes in CCA as the basis of rendering using pure vector based rendering on the Flash platform level was an obvious choice In the initial engine tests there were no significant increases in performance from using bitmap based rendering over vector based rendering as most of the CPU power went into path finding and creature calculations  8 Results and future implications  81 Current situation  After the past two and a half years of development project CCA is a mature bug free platform and currently deployed in clinical trials as well as touring the Science Changing the World Exhibit It has proven to be a useful tool for cognitive gaming in preclinical alpha version testing users were generally pleased with the tool There were no harmful effects or discomfort reported and 20 users out of 21 found the game appealing  Despite the fact that the project got off to a good start it was a large piece of software for two people to handle There were many ideas presentations and funding rounds to try and ramp up a company built around it to make it a viable consumer market platform for cognitive gaming but none of them have succeeded so far  Regardless of it not being as big as originally hoped it would be at this point it is a solid work of engineering user experience science and hard work CCA has performed well under tests and clinical trials It is currently being translated to other languages and has been developed to be fully multilingual based on different language resource XML files that are easily configured via the server interaction There is a single simple API for language versioning that can be triggered on any back end call so users can change the language at any point during platform use  After the current clinical trials there have been rough plans and ideas about the possibilities of reworking the core engine for mobile platforms as well as ventures into the world of 3D but the future of the platform remains open The online version of CCA can be found at wwwmybraincapacitycom  The current version of project CCA is being used mainly in ongoing clinical trials in conjunction with HermoPharma wwwhermopharmacom in their amblyopia studies There have been plans for a mass market version and an open account creation but for now the plans for large scale deployment are on hold  Possible uses for the project CCA platform range from helping aged people hone their cognitive abilities to helping the rehabilitation of people after experiencing brain trauma It has proven its capability as a cognitive gaming platform and can be used for accurate user statistics and ongoing analysis of users progress as well as measuring the effectiveness of drugs by providing detailed data about gameplay and improvement  The ongoing clinical trial at the moment of writing this Thesis is related to ambylopia also known as the lazy eye A fairly common disorder characterized as vision deficiency in an eye that is physically normal and able Ambylopia is estimated to affect the lives of between 15 of the population 66 Hermopharma are in the process of developing a drug based on antidepressants to prevent or heal damage caused by ambylopia wwwambylopiafi  In the trials project CCA is being used as a measurement and a training program for the test users to gain accurate information about how the medicine works and to see if the training of the affected is efficient in improving results Figure 64 shows the campaign for the clinical trial In addition to being used as a measurement device for the actual clinical trials a standalone implementation of a single game mode was made to support testing for one eye at a time The purpose for this version was to act as a campaign game for people interested in the possibility of amblyopia affecting their lives giving them a preview of what the trial was about and to raise interest and awareness for it  On top of the main game platform build we were approached by Heureka wwwheurekafi to do a multiplayer version of the game for a new exhibition called Science changing the world they were doing in conjunction with 3 other leading science centers The exhibition is a testament to science a playful series of exhibits to interact with and learn about new things in science and the ways health life quality even the planet can be improved with scientific methods The show started at Heureka from 1542010 Figure 65 up until 1512011 After that it has toured the Museon the Hague in 2011 the Cité des sciences Paris in 20112012 and it is in the Pavillion of Knowledge in Lisbon until 1572013 After Lisbon the exhibition is up for rent  What Heureka wanted was an offline version of the rendering engine with a specific game type and about 10 different game configurations and creature combinations It differed from the main game in a few important aspects It was not a training schedule based system having no server interaction or user accounts but a pure front end implementation and it was to be played by up to four people at the same time competing in cognition around who achieves the highest score The game was also in 3 different languages and the whole user interface of the game was redone to fit the resolution and language requirements  Due to the modular nature of the platform and engine code it was possible to separate the rendering engine with a game logic part and implement the new multiplayer requirements without too much trouble on top of the custom version It was mutually beneficial as some of the improvements made in the Heureka version ended up being ported back to the main game platform  An additional other challenge of the Heureka version was that it was run on a 46 touch screen on a full high definition television 1920*1080px That meant that the whole game area that the game runs on was relatively bigger than the typical CCA platform game area around 1280*800px Because of this it required extensive optimization and calibration of the points rendering and creature styles to make it perform well enough for show use  The multiplayer mode was a single mode of the Nobject capacity test the game featured 1 to 5 creatures at the same time on screen and the users had to react to each target state of any of the creatures within a set time frame Each successful reaction per player was colored with its unique shade of color glow around the creature the reaction affected to keep it clear on who did the reaction  Example gameplay of the Heureka CCA version can be seen in Figure 66 The game was played in a customized stand with the 46 screen facing upwards All controls for users were abstracted from the traditional keyboard and each player had an arcade style control button to use to react The custom controls were connected to a keyboard input and could be mapped into the CCA Flash application via normal keyboard events  In addition to the changes in the game mechanics the multiplayer mode required its own user interface In the multiplayer version there were between 1 to 4 players and the game needed be playable with any number of people between that range The UI had to work in four directions matching the four player slots around the stand  To enable the users to play at the same time a separate UI based on the same logic and graphical design of project CCA was implemented The UI was built so a separate instance of for each direction was created and shown based on user participation Each side of the game had an initial information screen informing the users about what they are supposed to do and the game started by anyone pressing the start button The game allowed hot seat style multiplayer where anyone could jump in during the game and start reacting to the creatures albeit coming in later would affect your score in the end of the game  Only the players who were active during the game run were shown the game end screen with detailed information about how they did In addition to the statistics a crown reward system was implemented the best user was rewarded with a gold crown the second player a silver one 3rd player got a bronze crown and the 4th one had to settle for nothing  As opposed to the online version of the main platform the Heureka version had to be a standalone installed package with everything needed inside a single installer file To do this with traditional Flash was impossible and third party extensions were not reliable enough Luckily Adobe itself had come up with a solution to the problem Adobe AIR or the Adobe Integrated Runtime is a wrapper for the Flash player that allows it to handle native functionality from file system access to running its own instance 69  Adobe AIR enables the use of web standard HTML and Javascript as well as Actionscript 30Flash to create native applications for all major desktop environments as well as Android iOS and certain smart TVs 69 It is widely used in creating casual games for mobile platforms as well as a lot of entertainment and ad campaign applications Air has full support for Stage 3D OpenGL API for accelerated graphics as well as a lot of native integration to iOS and Android services  Developing for AIR brought about some small changes to the CCA codebase because it handles certain things like stage events and window events as well as keyboard interaction a bit differently but all in all around 90 of the code could be ported to AIR within days The inbuilt installer application visuals such as icons and automatic updating functionality made the AIR version completely standalone and enabled a solid experience for the people keeping up the show at the various science centers  The biggest issue to solve in the Heureka version was the requirement for the game to run on its own for a minimum of 16 hours in a row The game was automatically started at the beginning of a day and the whole machine was shut down at the end of a show day The application needed to be maintenance free during uptime and it should be trustworthy and robust throughout the whole life time of the show  The approach to the Heureka version required a lot of profiling and optimization Before deploying the build in the actual environment a series of test scripts were implemented The scripts made the game play itself for a long time changing the game parameters and reactions to simulate a real user situation A logging of memory footprint and performance details was also implemented These features enabled the testing of the game before actual deployment  But as optimizations go it is rarely beneficial to test only in development environments as was the case in Heureka Using the scripts the game seemed to work fine for the required 16 hours without memory leaks or slowing down but during the actual show run it started to show problems We kept getting reports that the game randomly crashes after 2 hours depending on how many users have been using it and the rate of use during the day  Al lot of time was spent trying to track down what caused this elaborate bug by the Flash logs it showed to be running fine and that memory use was not stacking up and rendering times were normal But from the OS point of view the app slowly ate away at the memory space allocated and crashed in the end when it had consumed too much of the system memory  The reason this fatal bug was not noticed in the internal tests was because the tests always ran the game to completion to the end screen and then began a new round It became apparent that the crash happened when time the game was started but abandoned midway through without playing it to the end numerous times The game had a timeout function if it was not played for a bit it started showing instructions on what it does and encouraging people to play  In the end after some serious time spent with the profiler and taking memory snapshots in different benchmark situations a memory leak was found A creature rendering class in the core engine rendering system was being reset for garbage collection with a call to not the correct level of inheritance but the super class of it This left the game with a small amount of assets each game creation that werent being cleaned up and slowly started to eat away at the memory available  This problem had not been found during the main CCA engine development due to the fact that the main game was never supposed to be used for more than a maximum of about an hour at a time A lesson was learnt here profiling and being rigorous about testing the memory footprint is imperative and extremely valuable when creating continuous running software  Despite the problems during the development and the rework needed to create the Heureka version it is up and running around Europe at the moment The branching was a success and even if nothing else big will come of the project CCA at least it got to educate people around Europe about science and cognition  At the writing of this thesis Matias is doing the clinical trials with HermoPharma and the language versioning system enables fast and easy porting to other language areas The Heureka version is still circling Europe The current version is complete but there are a few routes that have been pondered about for where to take CCA next  One obvious platform for brain gaming and cognitive use that appeared during the development of the CCA platform and that offers lucrative amount of users and more importantly users that might benefit from the platform is the mobile space especially the rise of the tablets Tablets fit perfectly with the way the project CCA core works and could be easily a good place to start spreading out to  Due to the fact that Flash platform is not available for mobile except for Android it is not a viable option for creating the project CCA for mobile Even though the Air platform would support building to mobile environments the computing power required on top of the managed language would most likely be too high for mobile CPUs The heavy rendering and calculations required would need a native application to be built for CCA platform On the other hand the core of the CCA engine is reasonably easily converted since it is very mathematics based  The other way aside from mobile platforms is to evolve the CCA forwards towards the 3D space The same engine would not work in a similar fashion in 3 dimensions as it is currently a very much 2 dimensional solution but the same ideas and approach could be viable in 3D The serious gaming around platforms such as Unity wwwunitycom is growing at a significant pace enabling efficient solutions for beautiful yet useful gaming  There are some initial steps towards looking into the possibility of building a next version of CCA with the Unity engine toolkit Unity would provide a unified platform for all native computers as well as a way into consoles and mobile gaming Then again the advances in web standards and the rise of WebGL could enable a remake of CCA with Javascript and HTML5 Canvas based approach The future remains open  Cognitive gaming is a rising area of serious gaming The approach that a brain can be trained and moulded to perform better at tasks we perceive to be meaningful is a exciting thought In this day and age efficiency is one of the top most factors working life focuses on and the ability to improve not only working methods or tools but people themselves provides valuable and genuine possibilities  With the rising popularity of mobile platforms and the improvements in traditional web the focus has shifted from creating features to providing good user experience and servicing user needs Going beyond usability the importance of brand and aesthetics are also gaining ground in realizing products and experiences in the online world User experience matters  The approach to cognitive game platform building in this thesis is generable to serious game platforms as a whole The issues solved relate to common real world problems when creating performance heavy online implementations and provide ways to overcome obstacles in the creation of good user experience as well as system performance The project approach emphasizes a focus on user experience and solid software engineering  The reference project is a complete cognitive gaming product a feature rich user experience oriented platform for measuring attention and working memory It is based on a data driven architecture with proper separation of concerns for rendering game logic and user interface elements The modular approach allows good controllability and modifiability without changing the front end platform code  The rendering engine provides organic movement and creatures with a natural feel in endless combinations based on mathematical formulas It is optimized on the algorithmic and micro level to provide pleasing visuals and fast enough rendering for attention games Due to the separation of game logic and rendering as well as the heavy emphasis on mathematics in the rendering the engine is easy to export to different platforms in the future  The project was a success and is being used in clinical trials in Finland and Estonia as well as in the Science Changing the World Exhibit around Europe User feedback has been encouraging during the alpha and the current medical trials The future remains open but the direction is towards mobile and tablet use as well as employing 3D as a means for more immersed gameplay  This study and the reference project stand as an example of the viability of modern web technologies in creating complex serious gaming platforms It is a testament to the possibilities of cognitive end user training and serves as a guide on the approaches necessary to accomplish a successful cognitive gaming project  This postgraduate study was done as part of the Master of Engineering Degree Program in Information Technology for the Metropolia University of Applied Sciences The motivation for this thesis originated from personal interest in both the 4X strategy games and the game artificial intelligence in general having experienced this genre first time when playing Sid Meiers Civilization back in 1993 on the Macintosh LC II computer  While I have been working in the game industry for over a decade now the possibilities for exploring this particular area of interest have been limited so the possibility of using it as the topic of my thesis was a rather natural choice Although the scope of the research was very broad and the results did not reach the practical levels which I hoped for working on this project was highly rewarding and will further motivate me for years to come  I would like to thank my supervisor Ville Jääskeläinen my family and friends for their support and encouragement during the writing of this thesis  Instructors  Ville Jääskeläinen  Although computer games have been around for over half a century the gaming has matured to become a mainstream phenomenon in the past decade partly propelled by the breakthrough of mobile platforms which provide users access to a vast selection of games As the complexity and selection of games is constantly increasing there is growing pressure to provide meaningful artificial intelligence AI opponents in certain gaming genres  This masters thesis focused on finding a way to implement an AI player for a turnbased 4X computer strategy game As there was no suitable project at work to apply this research on at the time of the study the project was created as a personal venture with a theoretical game used as the source for requirements  In the research phase dozens of sources of existing literature and information about the field were analyzed which included extraction of the knowledge and technologies appropriate for this project The selected technologies were documented in the thesis and their use cases were identified and examples were created for most of them  A highlevel technical design for AI integration was created as an outcome of this thesis which describes a proposed architecture for the AI opponent and combines the technologies evaluated in the thesis into a modular framework These features can also be leveraged in the player assistance features such as micromanagement automation and advisor functions  The resulting design was supported by actual prototype development done in Unity Editor of selected key technologies These prototype implementations included a rulebased system inference engine A* pathfinding on a hexagonal grid map a spatial database for tracking map data such as player influence and tactical pathfinding leveraging the information in this database  Although a complete game was not created during the project the technical design and research done can be used as a foundation for building AI opponents in turnbased strategy games in future The practical implementations will most likely also provide feedback on the possible shortcomings of the design and possibilities for improvement which will be reflected back on the design  Keywords  Artificial Intelligence Computer Games Game Industry Prototyping Strategy Games Technical Design Unity Since the early days of first publicly available video games in 1970s the business around gaming has grown to a large and highrevenue business known as interactive entertainment or video game industry These are some key facts about the industry mentioned in Entertainment Software Associations annual report of 2015 to give perspective about the business case for this project 1 p 14  • Video game industry generated $23 billion sales in United States alone in 2014  • Gamers spent globally estimated $71 billion on games in 2014  • There are 155 million people in United States that play video games  • A total of 1641 video game companies are in United States alone spread along 1871 locations  • Education of video game industry is being offered in 496 programs in 406 schools in United States  The process of game production can be simplified as an illustration of the value chain of a production as seen below in Figure 1  Time Constraints in Production  In game industry the schedule of game production has a crucial role because it directly affects the development costs of the title Due to this there is limited time for doing specific pioneering research during game development as there is a lot of pressure to meet deadlines and to provide value for the producer Sometimes there are separate teams which will do engine and technology development but even in that case prioritization needs to be done for optimal developer resource allocation In the authors experience Artificial Intelligence AI development often ends up being one of the fields which will get less focus 3  Role of AI Programming  Because of the previously mentioned factors especially in small and mediumsized teams there usually is not a dedicated AI programmer but those features will be rather added into the game by gameplay and generalist programmers This allows better flexibility in the team resource management and usually generalist programmers do have good basic knowledge on the topic to create usable and functional implementations However when the requirements for the AI features and quality increase dedicated skills in AI programming will prove to be invaluable  Comparison between Academic AI research and Game Industry  When people talk about AI they are generally referring to the academic AI which is quite different from the game AI While academic AI aims to solve problems requiring intelligence the purpose of game AI is to give illusion of intelligence and entertain the player The goal of AI research in academic setting is usually to create publications and do original research while game AI development aims to create a game Academic AI research is often funded by grants academic institutions or sponsorships while game AI is funded by the game publishers Its said that there is strong division between the two fields but in practice both parties have the possibility to benefit from each other with help of the results of academic research game developers can add increasingly advanced AI to the games while academic AI research can benefit from game engines which they can use for their research 3  11 	Scope of Thesis and Research Design  The goal of this thesis was to produce a technical design for adding AI features into a turnbased strategy game This included  • Figuring out the requirements imposed by the game for the AI  • Determining which of the existing solutions in the field of AI knowledge were appropriate to satisfy those goals  • Designing the best methods for integrating them in the game  • Choosing proper balance between AI programmers and game designers workload by necessary tools  A full working game where the technical design would have been implemented was out of the scope of this thesis but limited amount of prototyping was set as a secondary goal during the writing process to provide some practical analysis on the feasibility of theoretical choices made in the thesis The research design is illustrated in Figure 2  The research design Figure 2 shows the phases of the project and the respective thesis and prototype work involved in each of them Most of them match the chapters in this thesis in the following order  1 Introduction chapter outlines the goals and motivations behind the thesis work and gives introduction to the game business to which this work relates to The initial prototype work was also started at this point and the requirements it imposes for project were gathered  2 Background chapter contains brief history of AI in games and the related gaming genre and describes the prototype and lists the previously gathered requirements During this phase the relevant technologies were picked  3 Technology chapter was created through iteration of selected technologies during which the purpose of each one of them was described in the context of this thesis work Also some of them were integrated in this phase to the prototype  4 The Proposed Solution was developed initially during the technology iteration phase and finalized in the review phase This included outlining the highlevel system during which prototype was also refactored based on the results of earlier integration  5 In Evaluation chapter the feasibility of proposed solution is evaluated and this evaluation is partially backed by the data that was extracted from the prototype at this stage  6 The Discussion and Conclusions chapter analyses the results of the thesis evaluates the outcome of the research and contains suggestions for further improvements  In addition the project also produced a partially working prototype for which the source code was included in the appendices of this thesis 	  This section gives a brief overview of the history of game AI and the 4X strategy gaming genre which the project belongs to It also describes the technical starting point of the prototype and outlines its requirements for the AI  21 	History of Artificial Intelligence in Games  The concept of artificial intelligence itself is nothing new even in ancient times mankind has been interested in the concept of artificial life and intelligence This shows in the various fictional stories and attempts to build automatons even centuries ago However the major breakthroughs in electronics and computer technology in past century have allowed unprecedented advancement in research of artificial intelligence 4 pp 45  After the first general purpose programmable computers were invented it did not take long time until the first AI programs were developed One of the first published ones was Alan Turings chess program although at the time the computers were not advanced enough to completely run it 4 p 6  Although there has been constant academic interest in AI research breakthrough in game AI started in the 1970s when first video arcade games were developed They featured primitive computer opponents such as the aliens in Space Invaders and ghosts in Pacman Although the AI in those games operated on simple deterministic algorithms they gave the player impression of intelligent behavior Since the early video games the game AI development has slowly advanced and different gaming genres have their own specialized requirements ranging from tactical realtime NonPlayer Characters NPCs in firstperson shooters to complex strategic planning in strategy games and even artificial life simulation 5  Current State and Future Development  In the past decades there has been a lot of progress especially in terms of graphics quality and storytelling aspects of games With the increased complexity and depth of games turning into interactive entertainment there is increased desire for creating advanced artificial intelligence functionality to improve the gameplay experience and immersion for the players Thanks to the constantly advancing computing capabilities of modern computers there are now better possibilities to implement advanced AI than ever before such as increased focus on selflearning AIs that will adapt and learn from players 6 pp 35  22 	4X Strategy Games  The term 4X was created by Alan Emrich who used it in review of Master of Orion in 1993 for the first time  it features the essential four Xs of any good strategic conquest game EXplore EXpand EXploit and EXterminate In other words players must rise from humble beginnings finding their way around the map while building up the largest most efficient empire possible Naturally the other players will be trying to do the same therefore their extermination becomes a paramount concern 7  Although majority of 4X strategy games are turnbased there are a few examples of realtime games which are considered to belong to this genre However the classification of games in 4X genre can be difficult and sometimes a few additional criteria have been used to narrow what fits the definition such as empire economy control and diplomacy versus combatfocused gameplay in regular strategy games 8  Unlike many other game genres strategy games are highly dependent on skilled AI to provide meaningful gameplay experience for the player As the opponents tactics and strategy are foundation of the core gameplay challenge for player a poorly implemented computer AI would most likely be acceptable by casual gamers but advanced players would find it boring and unrewarding 5  Civilization  Sid Meiers Civilization series is one of the bestknown examples of turnbased 4X strategy games see Figure 3 The game has taken heavily inspiration from previous board and computer games such as Risk and Empire but also added novel features such as technology tree 9   Figure 3 The original Sid Meiers Civilization Macintosh version pictured  Originally released in 1991 the series has had six major releases and several spinoffs In Civilization the player takes role of leader of a nation which heshe will lead from stone age to modern day In the first game in the series there were only three conditions for ending the game world domination by eliminating all opponents building a spaceship to reach Alpha Centauri or running out of time in the year 2100 Later versions of game have gradually added more victory and endgame conditions along with many other features in each major release 9  Master of Orion  In Master of Orion see Figure 4 the players control number of different races which compete for control of the galaxy The game features exploration discovery of new technologies dealing with other players with diplomatic or military means and endgame conditions which are similar like to the ones in Civilization 10   Figure 4 Master of Orion II  Besides being spacethemed the game trades off the gridbased movement and exploration with a more restricted and simpler model where movement is only allowed between solar systems Other major difference is also how battles are handled instead of singleunit attacks the combat happens on a separate combat screen where multiple fleets of both participants are fighting at once Players have control over individual ships and weapons which can lead to complex tactical battles 10  The game has been influenced by some earlier games such as Reach for the Stars The original series had three releases and the franchise was recently rebooted by Wargaming on Steam 7 10  Galactic Civilizations  Galactic Civilizations see Figure 5 is a series of strategy games which was initially released for OS2 in 1993 by Stardock Corporation The game combines the turnbased grid map familiar from Civilization with space exploration theme like in Master of Orion 11   Figure 5 Galactic Civilizations Gold for OS2 12  Galactic Civilizations was one of the few games released for IBMs OS2 operating system and it received recognition even from IBM who licensed the game and included it rebranded as Star Emperor in their FunPak software bundle However the OS2 operating system had limited user base and later lost its remaining market share to Microsoft Windows on the PC platform thus later releases were made for the Windows platform most recently on Steam 11 13  23 	Prototype of Strategy Game as Foundation  Due to time constraints the implementation part of this thesis focuses on working on a theoretical prototype of the game This allows the development to focus on areas relevant for artificial intelligence integration  The game itself is be turnbased with each player performing actions in sequential order The game world is represented as a hexagonal grid map to which players have their own views depending on exploration and espionage status The game draws inspiration from Civilization and Master of Orion series creating mix of them with resemblance of the original Galactic Civilizations  Unity 3D  For rapid prototype development an offshelf game engine is used Unity 3D see Figure 6 is one of the most popular engines today which has not only gained popularity as mobile game development platform but also has seen use in recent AAAgrade desktop titles such as the recent remake of Master of Orion and cityplanning simulator Cities Skylines 14  Figure 6 The Unity3D Editor running on macOS platform  Originally released for Mac OS X platform in 2005 the engine has had five major releases and is currently available for both Windows and Mac OS X 15 Unity features componentbased architecture advanced 3D engine that can target various graphic APIs on several platforms NVidia PhysX engine and for physics simulation and various other frameworks Gameplay logic in this project is implemented in C which is supported as default scripting language by Unity other option being the UnityScript It is a mature and highlevel language originally developed by Microsoft which is integrated into Unity through the Mono framework 16  The choice of Unity for prototyping does not limit the options for selecting different game engine or building a custom one for the final game but given Unitys track record of being the platform of choice in number of highquality titles using it for the actual production is a viable option  24 	Requirements  To evaluate the technical needs for AI integration a set of requirements was created based on the game concept which outline the expected features of the game These are highlevel nonfunctional requirements which also outline the systemlevel design  241 Common Features  There is a set of features which are not specific to AI players but are also used by human players in the game A historical challenge for 4X games has been the amount of micromanagement which increases as the game progresses The amount of micromanagement is directly proportional to the size of game world complexity of game economy and length of the game This has potential to frustrate players and when they have to spend excess amount of time dealing with lots of repetitive lowlevel tasks instead of focusing in larger scale strategies and planning  There are various ways to reduce this micromanagement which have been added to recent 4X games and there are a few ones that considered when defining the requirements The quality of artificial intelligence to which mundane tasks are delegated to is important because if a poor implementation makes players feel that automation makes worse decisions than they would themselves do it would discourage them from using automation altogether and make the attempt to reduce micromanagement void  Colony management  There are a few aspects in managing colonies which can be delegated to automatic colony governor AI  • Production management  • Work force distribution  • Tax rates  • Import and export balancing  • Security level  It should be noted that having the support for automatic control of these properties of colony does not prevent the human player from altering or disabling any of them if heshe feels like it Also depending on how much configuration will be exposed to the player any of these could be parametrized with userspecified customization  Automated units  Another good opportunity to reduce mundane tasks for the human player is the automation of certain unit actions  • Worker automation build new improvements and adjust existing ones  • Automatic exploration reveal unexplored space and patrol previously explored areas  The above tasks are good fit to be implemented with AI automation as the scope of strategic choices made in them are focused on certain isolated parts of the game and thus do not depend on the highlevel strategic plans player may have The worker automation has the possibility to determine best possible improvement actions based on the economic status of the players empire and colonies and can adapt also previously made improvements to match the most recent situation Exploration is also by itself an isolated function where different input patterns can be fed to the exploration AI to prioritize areas to explore for example based on evaluation of military threat on influence map  242 Computer Player Specific Features  The rest of AI features are specific for the computer player and they are used to simulate the actions a human player would be performing in the game  Longterm Planning  The computer player needs to be able to choose and strive for various longterm goals most important one being desired victory condition The AI should be able to reach this goal by dividing the plan into smaller subplans and adjusting them to match the constantly changing conditions which are affected also by other players during the game  AI State Persistency  All the data used by the AI should be serializable so that the game state can be saved and loaded at any time without disrupting the functionality of the computer players decision making  Diplomacy  The AI should have ability to make diplomatic decisions including declaring war and creating alliances It should have possibility to do those choices in informed manner with knowledge about the other players economic and military power past trustworthiness and any other game parameters ie common ideologies personalities etc that might make the other player more or less favorable  243 Optional Features  There is a set of features which should also be evaluated though they are not required by the core gameplay and thus can be considered optional If implemented they do though have possibility to enhance playability value for more hardcore players  Tactical combat  When battles are initiated between fleets they are by default be resolved using simplified simulation which considers only the ship statistics numbers combat bonuses and other predetermined factors affecting the battle This can be enhanced by introducing tactical combat in which entire battle is fought as turnbased minigame and moves of computer players are handled by tactical AI Human player can either choose himselfherself all moves against the opponent or activate an automatic battle mode in which the same AI features will fight the battle also for himher  Ground combat  The fight over control of colonies on planets is handled through ground combat By default the results of these battles will be determined by the ground troop technology level number of troops and other possible combat bonuses There is possibility to further expand this battle into more finegrained ground combat simulation similar to the tactical combat in space where players would have more opportunities to control individual platoons of troops Practically this would be similar to the previously detailed tactical mode and could leverage some of the technologies but would happen on planet surface instead of space 	  3 	Technology  Since the early applications of AI in computer games the number and complexity of technologies involved have gradually grown partially with help from advancements in processing performance but also due to research done in the field of artificial intelligence research done for academic purposes The technologies introduced in this section are chosen by their potential usability in the game being created allowing the scope of thesis to focus on the most relevant ones  General Architecture  When creating AI for strategy game attention should be given to the design of overall architecture technologies involved how they are bound together and how they impact the gameplay According Ian Millington turnbased strategy games share many aspects with realtime strategy games with most important ones being Pathfinding Decision Making Tactical and Strategic AI and Group Movement 17 pp 809815 as shown below in Figure 7  Figure 7 An example of AI architecture for turnbased strategy games 17 p 815  Figure 7 shows a possible architecture for turnbased strategy games although exact model has variations depending on the gameplay elements involved in the design The technologies presented in this thesis use that model as a starting point with adjustments done to suit the exact requirements of this project  31 	Introduction to Traditional Board Game Techniques  As previously briefly mentioned in Chapter 21 one of the earliest applications for computer game AI was the game of chess Since those early days a large amount of time and effort has been spent on research and studying AI for several board games which have provided both academic and practical challenge for the researchers During the past decades a set of algorithms has gained foothold to become the foundation shared by many of the board game AIs and a few key concepts are introduced in this chapter 17 p 647  Game Tree  The most important concept for the majority of board games is the game tree which represents the game states as nodes in the tree and all possible moves as branches leading from nodes to new possible states With this data structure the goal for the AI is to choose one of the branches as a move it should make and needs to use various algorithms to find out which move is the best one it can make 18 pp 1629  Minimax  When evaluating the game tree the basic idea is to use a heuristic to give each possible move a score which indicates how good the move would be for the player in singleplayer games the heuristic could for example be the number of moves to finish a game The score of each move bubbles back up in the search tree and after scoring all possible moves the AI just needs to choose the move that has the best score However when two or more players are involved the evaluation algorithm should not only try to find the best score for the player but also acknowledge that the opponent will try to choose a move that yields the least score for the player Thus when bubbling up the scores the minimum score should be picked for enemy moves and player should choose move which give the largest of the minimum scores hence the name Minimax of the algorithm 18 pp 3039 An example of the basic principle in minimax algorithm can be seen below in Figure 8  In this case the best move with 2ply search would be p2 because it would end up with score of 5 for the player Other moves end up with lower score because move p1 would allow opponent to do move e11 resulting with score of 2 and p3 would allow move e31 with score of 4 both of which are lower than the smallest score of 5 given by move e23  The challenge with game trees is the balance between ply depth and processing power requirements the deeper the tree is the number of possible moves usually increases exponentially and so does the time required to process it On other hand if the search depth is too short the AI cannot predict the game events far enough in advance and might not be able to predict possible killer moves which might decide the winner of the game in long run There are various methods which have been developed to help and speed up searching the game trees including AlphaBeta Pruning Killer Heuristic AlphaBeta Windows and Transposition Tables 18 pp 4060 17 p 651  Applications in turnbased strategy games  Although there are similarities between board games and strategy games the complexity and number of possible moves during each turn in strategy games causes the size of game tree to grow so large that using traditional board game AI algorithms such as minimax becomes unfeasible in most situations There are however certain cases in which using some of the techniques are beneficial such as using cost estimation similar to the game tree scoring approach with task planning which can be used for example in decision making for research construction troop movement and military actions 17 pp  688670  32 	Virtual Player  Traditionally the AI has been implemented in games through the concept of AI agents which are usually divided in two types Characters and Virtual Players Characters appear as visible entities in the game that player can usually interact with they be as simple as ghosts in Pacman more advanced enemies like demons in Doom or other NPC opponents They can either be directly involved in the gameplay or just exist to add to the ambience and general immersion of the game 4 pp 1112  Virtual Players on other hand do not usually have physical representation in the game world but instead replace other human opponents in game and assume the tasks and responsibilities of that player The classic example for this kind of AI is used in board games such as Chess where the Virtual Player decides which moves it should make in the same way as a human player would This model applies to turnbased strategy games where the individual units in the game do not usually contain any intelligence but all decisions are made by the Virtual Player Exception to this are realtime strategy games where the behavior of individual units has important role in the game but that subtype of strategy games is out of the scope of this thesis 4 pp 1617  For this thesis the design of the Virtual Player is the central outcome of the research project as it is an umbrella concept that encapsulate all the supporting technologies studied in this project  321 MultiTier AI Framework  Building on the principle of the strategy game AI architecture shown earlier in Figure 7 the multitiered AI framework approach is based on separating responsibilities of the AI to individual levels The structure framework resembles the military hierarchy in which highlevel AI sets the general strategy and goals which translate to commands that are given to the next level which set their own goals to be able to fulfill those orders This continues until the individual unit level is reached where the commands are turned into actions performed by the units 17 p 544 An example of this topdown command hierarchy is shown below in Figure 9  Figure 9 Example of multitier AI for military strategy game 19  In the example some possible situational projects are shown on their source level with destination visualized The roles of these individual levels used in the model are described below in Table 1  Table 1 Roles of levels in the multitiered AI model 19  Level  Role  Strategic Intelligence  Knowledge of the entire empire management of grand strategy goals global resource levels research and diplomacy  Operational Intelligence  Divided into activity groups can track also noncombat activities such as economic and diplomatic operations  Tactical Intelligence  Information about encountered opponents geography resources  Individual Units  Pathfinding unit movement combat  There are also other possible ways of building the hierarchy in the framework including bottomup approach where individual units are autonomous and higher levels of the hierarchy just provide intelligence and general information about the game world It should be noted that the framework allows information anyway to pass in both directions in the model depending on how the gameplay requirements imposed on the AI 17 p  544 19  33 	Pathfinding  One of the most important features in nearly any game dealing with a map with entities that should be able to navigate on it is pathfinding There are various maps and grids that can be considered to be node graphs as seen in Figure 10 to which graph search algorithms can be applied to Of the examples shown hexagonal grid is used for presenting the game world in this project  Figure 10 Examples of node graphs created from maps for pathfinding  The basic case is the ability to find shortest route from point A to point B To do this there are several graph and tree search algorithms the most common ones used in games are summarized in Table 2  Table 2 Comparison of pathfinding algorithms 4 p 171  Algorithm  Description  Benefits  Drawbacks  Breadth first search  Simple algorithm but does not consider movement cost  Most simple pathfinding algorithm  Not optimized  Does not always return shortest path  Dijkstras algorithm  Improvement on the breadth first search which adds path cost  Guaranteed to find shortest path  Not optimized  Astar A*  Combines Dijkstras algorithm with a casespecific heuristic value to the cost estimation function  Guaranteed to find shortest path  Heuristic helps optimize the search  No major drawbacks  There are numerous other algorithms but due to simplicity this study only considers the commonly used ones shown in the above table Because the maps in the game are mostly generated at runtime have a large number of cells and are highly dynamic the possibility to precompute navigation data is limited  331 A* Algorithm  The Astar A* algorithm is one of the most used pathfinding algorithms in games as it is relatively simple to implement and has good performance Because of this it was chosen as the default pathfinding algorithm for the game An example of A* pathfinding case is shown below in Figure 11  Figure 11 A* Pathfinding on hexagonal grid with Manhattandistance heuristic  The pathfinding starts from the hex on lefthand side of figure with black dot and target cell is indicated with checkmark on righthand side of the grid Red color indicates cells with higher movement cost of 5 while other cells only cost 1 to move through The search begins by putting hex coordinate of starting cell into the priority queue During each iteration the first item is removed from the priority queue one with highest priority priority is calculated for each neighbor cell which has not yet been processed and each of them are added into the queue To calculate the priority the A* algorithm uses priority formula shown in Equation 1 20 below   	 		     	1  In which fn is the resulting priority gn is movement cost for this specific cell and hn is the additional A* heuristic value In this example the Manhattan distance to the target cell is used After all neighbors have been inserted to the queue the iteration proceeds to next step The search ends when either the target position has been found as one of the neighbors or when the priority queue runs out of items ie when there is no solution In the above figure green cells outlined show the resulting optimal path to target position It should be noted that if the heuristic function hn  0 then the search behaves equally to Dijkstras algorithm which A* was extended from 20  332 Optimizations  In some cases the number of search nodes from which pathfinding lookup is queried may end up being too sparse causing an excessive amount of time being spent on performing the query In this case optimization is needed and there are a few approaches that may be beneficial depending on the use case  Hierarchical Pathfinding  To speed up searching paths in large sets of nodes it may be possible to combine physically adjacent nodes as groups so that pathfinding operates initially on the higherlevel group nodes from which it progresses to lower levels after highlevel path is found Depending on the type and size of original node tree for example in large openspace maps this combining of node clusters can be extended further to higher levels creating a hierarchy of search trees which is base idea in hierarchical pathfinding 17 p 265  Zone Mapping  In some cases there may be search trees which have completely disjoint start and goal nodes When this happens a A* search would end up having to look through all connected nodes in the tree just to find out that there is no solution for the path query In zone mapping a special floodfill algorithm is used to identify isolated regions in the search tree with result of this process stored in a zone map With this cached data it is possible to know in advance whether there is any solution before having to attempt doing the path query 4 p 197  34 	Decision Making  One of the key requirements for the AI is the ability to make decisions which translate to actions executed by the computer player The relationship between input data and action request effects is visualized below in Figure 12  Figure 12 Decision making schematic by Ian Millington 17 p 303  There are several algorithms and techniques for this purpose which share the core idea of having internal or external knowledge sources as input which are processed to action requests as output which affect the internal or external state of the game 17 pp 301302 This section introduces the most common decisionmaking methods which have applications on multiple levels on the MultiTier AI model  341 Finite State Machines  Finite State Machines FSMs are one of the key concepts used in computer games A state machine is composed of a set of states and rules defining transitions between those states In a FSM only one state is active at a time and switching to other states only happens when one of the transitions out of current state is requested 6 pp 165166 A simplified FSM for scout unit is shown below Figure 13  Figure 13 A possible FSM for controlling scout behavior  There are three states in the FSM to either idle explore or return to nearest colony and four transitions controlled by two external flags which indicate whether there are areas to explore and whether the unit is at colony  In the ideal FSM model transitions are handled internally using the data provided to the FSM and are thus selfcontained However in some cases using the input data as sole trigger to state changes might not be enough for example when UI triggers user input which needs to immediately alter the state of a unit In this situation an external transition can be triggered procedurally which is used to set the state of FSM directly without use of a predefined transition condition 4 pp 5052  Hierarchical State Machines  The Hierarchical State Machines HFSMs extend the basic principle of FSMs by adding the possibility of using subFSMs which are basically state machines nested inside of a parent FSM They add flexibility to the state transition options through the ability to continue parent FSM flow after finishing execution of the subFSM Another benefit is the possibility to split more complex states into substates 17 pp 327330 Figure 14 below shows a simple example case for Hierarchical FSM  Figure 14 Example of hierarchical FSM for worker unit  In this case a worker unit would by default toggle between idle state and build and repair tasks but at the appearance of enemy unit would interrupt any task it was doing and seek cover After the threat of enemy would be cleared the Hierarchical FSM would resume the worker automation SubFSM and its active state would automatically be the one which was interrupted earlier  342 Decision and Behavior Trees  There are a couple of common techniques which have the advantage of both being simple to implement and to use Decision Trees and Behavior Trees They are usually used to control NPC actions in games but they have also potential use cases for decision making in military units in strategy games They are both treelike structures which have the benefit of being able to be shown as a visual presentation of the decisionmaking process to the AI designer 17 pp 303309 21  Decision Trees  The Decision Trees DTs help making choices based on the world state at a specific time through the use of tree built of decision branches which lead to actions Usually the decision nodes are binary and have only two branches but it is possible to make selections with more than two options The decisionmaking process starts from the root and simply just moves down to the next branch depending on the outcome of each decision node This flow is very similar to traditional ifthenelse control flow in highlevel programming languages but the nodes as usually expressed explicitly as data structures which can be defined either in code or using data model which the AI designer can modify 17 pp 303309 The above Figure 15 below shows how the previously proposed FSM logic in Figure 14 might be converted into a DT  Figure 15 The decision logic from Figure 14 converted into a decision tree  At root decision the presence of enemy nearby would be checked first in which case the worker unit would move to cover If no enemy were nearby the tree evaluation would continue to next decisions to check if either type of tasks would be available for it if not the idle action would finally be picked as the last option  Decision Tree Learning  One interesting aspect of DTs is that they can be generated using machine learning from input of observation and action sets which represent the desired outcomes for each world state observed There are various possible methods for accomplishing this but the commonly used ones are based on the Iterative Dichotomiser 3 ID3 algorithm It uses the measurement of entropy to calculate information gain from available attributes for selecting the most relevant decision factor as the next node in the decision tree and continues this process until all action nodes have been created 17 pp 593597 For example the following observations could be used as input for the ID3 algorithm  Table 3 Input observations for ID3 algorithm  Build Task Available  Repair Task Available  Enemy Near  Action  Yes  Yes  Yes  Cover  Yes  Yes  No  Build  Yes  No  Yes  Cover  Yes  No  No  Build  No  Yes  Yes  Cover  No  Yes  No  Repair  No  No  Yes  Cover  No  No  No  Idle  In each iteration the algorithm first uses Equation 2 to calculate the entropy of the entire set of actions 17 pp 593597   	 		** 	2  02 In first iteration the formula gives the following entropy values for the entire set and available subsets  Es  175  	Ehavebuildtask  1 	 	Enobuildtask  15  	Ehaverepairtask  15 	 	Enorepairtask  15  	Eenemyneary  0 	 	Eenemynear  15  Those results can now be used in Equation 3 to calculate the information gain from the subsets 17 pp 593597   	3 	4	89	3  02 Which results in the following information gain values for the attributes  Ghavebuildtask  05  Ghaverepairtask  025  Genemynear  1  With the above results the algorithm chooses the attribute with highest information gain value as input for decision node which in this case would be the presence of a nearby enemy After this the algorithm repeats the same process for the both subsets of the observation data adding new child nodes until all relevant branches have been created The DT which was created by ID3 algorithm from the sample observations is shown below in Figure 16  Figure 16 Decision tree generated by ID3 algorithm  The nodes outlined with red circles were the ones where calculation of information gain was performed Note that no decision node was added for the Yes branch of Enemy near check because neither of the two remaining attributes contributed any information gain to the decision and thus were not required at all  Some software frameworks even include Decision Tree Learning as part of their feature set for example Apple offers builtin support generating decision trees using machine learning in their GameplayKit framework on iOS macOS and tvos platforms as part of their basic Decision Tree implementation 22  Behavior Trees  Another graphstyle decisionmaking method is Behavior Tree BT which generalizes the previously introduced Decision Tree concept Any DT can be represented as BT but the modularity and extensibility combined with their simplicity is what makes them powerful 17 pp 5253 23 The difference between DT and BT trees is shown below in Figure 17  Figure 17 A Decision tree expressed as behavior tree with identical logic 23  Unlike DTs which are solely composed out of decision and action nodes BTs have a multitude of possible node types which are generally divided into interior nodes also known as composite nodes which have one or more children and leaf nodes which have no child nodes The child nodes in BT are ordered in priority order usually visualized from left to right which dictates the evaluation order of the nodes The processing starts from root node and progresses down to child nodes in the tree depending on the node types in the tree All nodes have a precondition which can have three possible return values success failure and running The most commonly used nodes in BTs and their return values are listed below in Table 4 23  Table 4 Most common behavior tree node types 23  Node  Type  Precondition return values  Success  Failure  Running  Action  Leaf Node  Upon comple tion  When cannot complete  During comple tion  Condition  Leaf Node  If true  If false  Never  Sequence  Interior Node  If all children  succeed  If one child fails  If one child returns Running  Selector  Interior Node  If all children  succeed  If all children fail  If one child returns Running  Parallel  Interior Node  If  M children succeed  If > N – M children fail  If neither Success or Failure condition is met  Each time the BT is ticked the tree is traversed down and the node preconditions checked until the active task node is reached which performs the action in during this particular tick The return value is backpropagated up in the hierarchy to the parent nodes which depending on their behavior decides what to do ie possibly evaluate next child in interior nodes and return the appropriate value back to their parent node This progress continues until the return value reaches the root of tree which is returned to the original caller 17 pp 5253 23  Thanks to the flexible structure any type of nodes can be added to the BT for example UtilityBased Systems can be leveraged to create a utility selector node which can use internally utility scoring to choose the appropriate child to execute 24 Also reusable parts of BTs can be shared as subtrees reducing amount of work needed to create duplicate behaviors  343 Fuzzy Logic  The traditional computer logic is based on Boolean algebra which infers absolute values of either true or false as the only possible conditional states There are however certain cases in which a more finegrained evaluation is required for example to assess the threat of an enemy fleet and choose appropriate actions based on the analysis of the situation For this one fuzzy logic can be used in which the absolute true and false states are replaced by a membership degree which is expressed as normalized value between  00 and 10 17 pp 344345 The overview of how fuzzy logic is used is shown below in Figure 18  Figure 18 Fuzzy process overview 6 p 192 17 pp 344354  The process starts with fuzzification of input data after which fuzzy rules can be applied to the fuzzy sets and results can be obtained through defuzzification which converts the data back into crisp values 6 p 192  Fuzzification  The Fuzzification is done through of membership functions which convert predefined ranges of values into fuzzy set membership degrees Commonly used membership functions include grade reverse grade triangular and trapezoid functions but other functions can also be used if necessary The number of membership degrees is not bound by the number of inputs as same input values can be assigned to multiple membership sets at the same time 6 pp 193198  Fuzzy Rules  After conversion to Fuzzy Sets the membership degrees can be combined using Fuzzy Rules which are built using Fuzzy Axioms which resemble the operators used in the traditional Boolean logic 6 pp 200201 The most commonly used operators are listed below in Table 5 Comparison between Boolean and Fuzzy Logic operators  Table 5 Comparison between Boolean and Fuzzy Logic operators 6 p 200  Defuzzification  After combining the values using Fuzzy Rules the membership degrees need to be extracted from the Fuzzy System to be usable in the game There are few commonly used methods for doing this  • Highest Membership  • Membershipbased Blending  • Center of Gravity  The best method to use depends on how the data is used For a simple Boolean decision the Highest Membership method should be enough but if there is need to aggregate output strength other methods are needed Although the Center of Gravity is often favored it comes with increased overhead due to the need to integrate surface areas of the membership regions The blending approach usually is good enough and is much quicker to use 17 pp 347351  Use case Threat Assessment  One good application of Fuzzy Logic in strategy games is using it for threat assessment and classification purposes A simple example case for this is shown below in Figure 19  Figure 19 Threat assessment example case for Fuzzy Logic  In the above case the AI wants to evaluate the presence of enemy fleets around its colony to adjust its defensive stance if needed As input data it uses the proportional ratio between its defensive strength and strength of enemy fleets in Equation 4   	*< 	log	|DG0||DAE0|9AB9 F 	4  Where Ae is the list of enemy fleet attack strengths Ao is list of own fleet attack strengths and do is the defensive strength of the colony With the use of log2n in formula the exponential change in ratio between own and enemy strength can be converted into a linear value which the threat assessment can more practically be applied to When considering all units within 2 hexes distance from the own territory borders applying the situation in Figure 19 to the previously introduced Equation 4 results in the following values in Equation 5   		 	5  To use this input value two membership functions with three fuzzy sets in each are defined as shown below in Figure 20  	Weaker   Equal   Stronger	Bad  Average    Good 	15	1	05	0	05	1	15	40	20	0	20	40	60	80 	pratio	reputation  Figure 20 Membership functions for force strength ratio and diplomatic reputation  The three fuzzy sets in the pratio membership function evaluate the threat which is considered to be minimal when the enemy fleet strength is less than 50 pratio  1 of defensive strength and very high when it is over 200 pratio  1 At equal strengths pratio  0 the threat is considered to be medium The three fuzzy sets in the reputation membership function are defined to represent the diplomatic reputation of enemy so their trustworthiness can be included in the evaluation of probability for their aggression Performing the fuzzification of values pratio  066 and reputation  5 with the membership functions in Figure 20 results with the following membership degrees  Table 6 Output membership degrees of the fuzzification  Fuzzy Set  Degree of Membership  Weaker  0  Equal  034  Stronger  066  Bad  025  Average  075  Good  0  The get the threat level from these values the following fuzzy rule matrix is used to map the values to Low Medium and High threat levels  Table 7 Fuzzy rule matrix for combining the force size ratio and reputation  Weaker  Equal  Stronger  Bad  Medium  High  High  Average  Low  Medium  High  Good  Low  Low  Medium  These above rules can be written as the following logic expressions  mLow  mWeaker mMedium  mWeaker mHigh  mEqual   mAverage   mBad mBad  mWeaker   mEqual mStronger   mGood  mAverage    mEqual   mStronger  Stronger   mGood   mGood mAverage    mBad m Populating the above expressions with the previously calculated membership degrees for the fuzzy sets results in the following values for the threat membership  mLow  maxmin0 075 min0 0 min034 0  0 mMedium  maxmin0 025 min034 075 min066 0  034 mHigh  maxmin034 025 min066 025 min066 075  066  When the highest membership selection is used to determine the threat level it can be concluded that mHigh has the highest membership value of 066 meaning that the threat level is high and decision making can act on strategic and diplomatic level accordingly Another option would be using the membership blending method to calculate a numeric threat level value out of the membership values if needed to for example adjust internal diplomatic stance level  Other applications  Besides the threat assessment other applications for Fuzzy Logic in strategy games also include Bayesian Network probability reasoning 6 pp 253254 and decision making in RuleBased Systems 17 p 354 Fuzzy State Machines can use Fuzzy Logic to do blending between states allowing smooth transitions based on the Fuzzy transition conditions 17 pp 364369 This technique is sometimes used for example to do animation state blending like in Unitys Mecanim Other gaming genres such as racing can also benefit from the way Fuzzy Logic can be used to control vehicle steering but that is out of the scope of this thesis 6 pp 205207  344 RuleBased AI  Rulebased Systems sometimes also known as Expert Systems have existed in the AI field since 1970s They are sometimes considered a doubleedged sword as although they allow the experts to share their knowledge of the situation and reasoning about how to handle it there is the downside that rule sets to define this knowledge must be created by those experts Although rulebased systems have many applications outside gaming industry such as in financial medical and industrial software there is also use for this approach also in games 4 p 134 Their strengths include the ability to use extensive rule sets to capture highlevel knowledge of various complex problems 4 pp 139140 and the capability to make decisions in unexpected situations which cannot be easily handled with more simple approaches such as decision trees 17 p 403 The Figure 21 below shows the basic structure of rulebased system  Figure 21 Overview of RuleBased System 4 p 138 17 p 404  As pictured this system consists of main components called the Rule Set Inference Engine and Working Memory Each of these parts are described below in more detail  Rule Set  The actual knowledge of a problem is encoded in various rules which are kept in the Rule Set also known as the Knowledge Base Each rule has two parts the condition which must be satisfied for the rule to be fired and action which defines what happens when the rule is triggered The condition can evaluate facts in Working Memory in various ways and rules can also be enabled and disabled when needed The action can alter facts in Working Memory but can also control which rules are active and even stop the processing completely 4 pp 134135  Working Memory  All facts known by the RuleBased System are kept in the Working Memory which works as a database for the Inference Engine Although the format of facts is not limited they are usually stored as Boolean numeric string and enumeration values 4 pp 134135  Inference Engine  The actual processing of rules happens in the Inference Engine which checks the rule conditions of the rule set and either selects the first match or uses the arbiter to choose which action to trigger The processing takes place in iterations which continue until either no more facts are changed in the database or a stop action is encountered Usually forward chaining rule matching is used but sometimes backward chaining can be used which matches the rules based on their outcome effect of actions on facts instead of conditions trying to find a starting state that can derive the expected result 17 pp 407408 RuleBased Systems are however notorious for suffering performance issues when large rule sets are used which need considerable processing time There are some optimizations for this process the Rete Algorithm being one of the best known of them 4 pp 134135 17 pp 422423  Arbiter  Sometimes the system contains a separate arbiter component which decides which rule is triggered if multiple rules are matched simultaneously during one iteration Possible common approaches include using first applicable rule least recently used rule random rule most specific conditions and dynamic priority arbitration 17 pp 418419  Use case Inferring tech tree state through rulebased reasoning  As rulebased systems allow inferring new facts from existing ones through the rules one possible use for them in 4X strategy game is the capability for AI to use knowledge about enemys possession of a single technology to deduce the state of other technologies in the tech tree This use case has been adapted and refined from the example provided by Bourg and Seemann 6 pp 214218 The Figure 22 below shows a possible subset of tech tree  Figure 22 Inferring state of tech tree from knowledge of a single technology  In the tree technologies are connected by arrows leading from prerequisite technologies on higher levels down to the subsequent technologies on the next level The following rules can be generated from this tech tree  IF defensetech2 THEN colonytech1Yes AND defensetech1Yes  IF weapontech2 THEN weapontech1Yes  IF shiptech2 THEN weapontech1Yes AND shiptech1Yes  IF colonytech2 THEN defensetech2Yes  IF defensetech2 THEN defensetech2Yes  IF weapontech3 THEN defensetech2Yes AND weapontech2Yes  IF shiptech3 THEN weapontech2Yes AND shiptech2Yes  As the AI has learned that enemy has a ship equipped with Missiles 2 upgrade shown green in Figure 22 and thus has researched the Weapon Tech 3 technology it can use the above rules to infer which other technologies are consequently possessed by the player as prerequisites This starts by putting the fact weapontech3 into the Working Memory and running the first iteration in Inference Engine which finds a match for the following rule  IF weapontech3 THEN defensetech2Yes AND weapontech2Yes  This rule adds the facts defensetech2 and weapontech2 into the Working Memory The inference engine runs next iteration to evaluate the rules and continue the process until no more new facts are added to the Knowledge Base at which point the processing has finished In Figure 22 the technologies set to Yes during this process are indicated in red  Other potential uses in strategy games could be predefining certain events for scenarios triggers and possibility to control unit and strategic behavior with AI scripts implemented by AI programmer andor designer  345 Utility Theory  The idea behind the Utility theory has existed for a long time and has a history predating even computers in the economics field where it is used to study consumer behavior and choices The core concept in Utility Theory is scoring each action or state in the utility model with a uniform value which represents the usefulness of each choice in the given context To allow scores of multiple sources to be comparable the utility values are normalized using methods appropriate to the given input data and the scores can be combined from multiple sources to end up with final utility score which can be used to select the appropriate action A simplified overview of the information flow inside a Utility System is shown below in Figure 23 25  Figure 23 Simplified flow of information in a Utility System  This flow of information inside Utility System can be roughly divided into the following phases  Phase 1 Converting Game Values into Utility Factors  There is no harddefined way of converting data into Utility Factors the only rule is that all factors must have the same scale so that they can be combined together and be comparable with each other There are certain generally used methods for converting arbitrary game values shown below in Figure 24 Other methods may also be used depending on what is required by the usecases of the utility factors 25  	Simple Cutoff	Linear	Quadratic	Logistic 1111 08080808 06060606 04040404 02020202 0000 00015 Figure 24 Commonly used formulas for calculating utility factors 25  Phase 2 Combining Utility Factors  Usually there are more than one factor affecting the desirability of actions and to get final utility scores for each of them they are combined Commonly used methods include calculating average of utility factors multiplying them together picking the smaller or larger of them or reversing the factor by inverting it These operations can also be chained after each other and exposing them as a visual graph editable by designers can be a very powerful tool 25  Phase 3 Picking the Best Action  After each one of the actions have been given a final utility score the AI selects which of them it should execute The most straightforward way is to just pick the one with greatest utility but it might in some cases lead to repeatable or predictable AI behavior This can be overcome in some cases by using weighed random approach where a random selection is made from possible actions with the utility score used as weight to give the actions with higher utility score a better chance of getting picked This can also be combined with bucketing also known as Dual Utility AI in which the actions are categorized and assigned a bucket based on the effect they have For example when choosing production goal in a colony the military units could be assigned in one bucket and colony improvements in another one With this approach when building army has highest utility it guarantees that a military unit is produced but the type of unit can be randomized 25  The utility scoring has a strong resemblance to fuzzification in fuzzy logic and they share some principles especially in the way game values are converted into the internal representation in both approaches They are also both good for promotion emergent behavior in AI when used properly by the AI designer 25  Use Case Diplomatic Decision Making  In the 4X strategy games one possible use case for utility reasoning might be choosing an action during a diplomatic negotiation with another player A simplified case for this is outlined below in Figure 25  Figure 25 A possible Utility System for diplomatic decision making  This situation assumes that the players have currently signed a peace treaty which offers three possible actions proposing alliance signing a research agreement or declaring war There are also certain game values exposed to the Utility System opponent reputation score for scientific benefit of research agreement and military strengths of own and opponent armies Also the AI personality goals are exposed as diplomatic victory science victory and military victory priority values There values are then converted to Utility Factors using operators defined by the designer or AI programmer which end up as utility values of the possible output actions as shown in Figure 25 This gives each of the actions a utility score and if picking the action based on highest utility the choice in this case would be declaring war against the opponent  It should be noted that in an actual production implementation various other factors should be considered such as how much the player likes or dislikes the opponent when declaring war and what impact it would have on the players own reputation how likely the opponent is to enter an alliance or research agreement before offering them and many others  Using utility scoring for decision making is especially fit for the type of strategy games that this project represents as due to the nearly infinite number of possible moves per turn there is no way to score individual actions in a purely deterministic way In this situation the reasoning of utility of actions allows the AI to make educated bestguess choices based on the available data of the game state 25 The utilitybased approach is also highly versatile thanks to its simple concept which helps it combined with a number of other techniques such as implementing utility selector in behavior trees 24 and applying utilitybased costs in colony production goal trees 26  35 	Influence Maps  To allow strategic analysis of map and game world various types of influence maps can be utilized to give the AI environmental awareness Some examples of use cases for this data are listed below 27  • Pathfinding can include influence as part of heuristic to avoid or favor certain areas  • Weak spots in enemy influence can be used to target attacks in planning of higherlevel military operations or to prioritize reinforcing own territory  The basic structure and function of influence maps has similarities to cellular automata in which uniform grids of values are modified by certain rules as a function of time usually based on the values of surrounding tiles One classic example is Conways Game of Life which uses a very simple set of rules although cellular automata has many other higherlevel uses such as city simulation in SimCity 17 pp 536537 The data in influence maps can be composed of multiple layers of data including for example 17 pp  499512  • Tactical Analysis o Friendly and enemy unit and pointofinterest threat generation  • Terrain Analysis o Defensive andor movement bonuses from terrain  o Map visibility which can be used to either increase the threat of unknown or to prioritize exploration  • Learning o Past events recorded on map such as unit kills ie frag map  Some of the data such as terrain analysis is by default spread on the influence map layer uniformly and can be used as input as such Some other data though like tactical positions such as unit threat are localized to single spots in the map and their influence needs to be distributed on the layer to be usable To do this there are a couple of common options available shown in Table 8  Table 8 List of common influence calculation methods 17 pp 502505  Method  Description  Limited Radius of Effect  Influence is applied on map as a function of distance to the unit with fixed falloff  Convolution Filters  The unit influence is applied on map using twoor threedimensional filter matrix for example using Gaussian blur  Map Flooding  The unit influence is propagated on map using Dijkstra or A* algorithm  It is also possible to use variations of the above methods depending on the source data and how the influence map is used in the game This involves usually finetuning by the AI programmers and designers to find a good balance for the influence which benefits the AI in decision making For example if certain areas of map are not visible to the player it is good idea to take the factor of unknown into account when calculating influence this however means that each AI player needs to run its own analysis of the influence map in contrast to a game state where all players have the same knowledge of unit positions and strengths in which case the data could be shared 17 pp 505507  An example of a simple influence map is visualized in Figure 26  Figure 26 An example of map of unit threat influence on hexagonal grid  This example shows a hexagonal grid with three units belonging to each player A and B of equal influence value In this case the unit influence values were propagated on the map using normalized Gaussian blur convolution filter applied through a rank 3 tensor on cubic hex projection plane q  r  s  0  Spatial Database  One possible way to represent the different sources of data affecting influence is the use of spatial database as suggested by Paul Tozour In his approach the data is applied to distinct layers in a generalized way with some possible examples of data layers listed below 28  • Openness layer  • Cover layer  • Area searching layer  • Lineoffire layer  • Light level layer  The layers can be combined using various algorithms at runtime for example using a formula like the one in Equation 6 to calculate dynamic desirability layer from other source layers 28   	FQRSTAUSSVW  *QQRR	×	XXY*AXW	×	RVAVSXXQT 	6  One of the possible benefits of using this layering of data is the increased tendency of emergent behavior in AI unit coordination through the use of shared data structures 28  Strategic Dispositions  When units are being categorized in order to identify strategic dispositions the information in spatial database can be used to aid this purpose The knowledge can be used in tactical analysis and decision making for example to identify weak spots which can be engaged in enemy territory or areas in own defences that need to be reinforced 29  An example of evaluation of strategic dispositions is shown below in Figure 27  Figure 27 A possible grouping of units for analyzing strategic dispositions  Figure 27 shows a case where a simplified map of fleet and colony influences has been propagated on the map using limited radius with fixed falloff and clusters of unit have been grouped using a simple algorithm which selects the strongest units and units in their immediate vicinity to be part of their group The total strengths of each group of units is known and thus their threat level can be estimated using fuzzy logic methods similar to the ones demonstrated earlier in Chapter 343 This data can be combined with the influence map for example by calculating the gradient of influence level between nearby grouped units In this case a higher gradient would indicate higher tension between units which can be used as input data to the tactical analysis algorithm which directs the units in groups to either engage enemy unit groups or to reinforce the defenses on local territory  The actual implementation of selection of actions depends on the iterative experimentation by designers and the AI programmer but could for example use utilitybased scoring based on the input factors gained from the analysis  Tactical Pathfinding  The influence maps can also be used in pathfinding to allow the units to consider possible threats when planning the route to the target position The tactical pathfinding can be implemented easily by adjusting the heuristic function of the A* pathfinding algorithm for example by adding penalty based on enemy threat level on the influence map which makes the units evade dangerous areas of the map giving the AI movement choices are stronger impression of intelligence One challenge in this approach however is the care needed when applying changes to the scoring heuristic function to avoid increasing the cost of pathfinding processing time too much 30  A possible use case for this approach in a 4X strategy game might be for example the need to plan route for worker unit across unclaimed space with recently observed enemy movement In this case the pathfinding should avoid areas which would most likely to lead to encounter with enemy  36 	GoalOriented Behaviors  With the previously described methods it is possible to build an AI that can evaluate the current game state and choose appropriate actions which appears sufficiently intelligent in casual gameplay However it is especially important in strategy games for the AI to be have longterm strategy and goals which makes AIs actions and decisions more meaningful and thus giving more challenging and meaningful gameplay experience for the players To accomplish this various forms of GoalOriented Behaviors GOBs can be implemented which can give the AI not only immediate internal needs which it aims to fulfill but also the capability of chaining multiple actions together in order to reach more complex goals 17 pp 376377  This section introduces a few key technologies that can be used to implement this kind of behavior which is useful in the higher layers of the multitier AI mode including production and research planning which involves also coordination between different higherlevel agents  361 GoalOriented Action Planning  The idea behind GoalOriented Action Planning GOAP has long history having roots in the Stanford Research Institute Problem Solver STRIPS which was created already as early as in the 1970s 21 GOAP planning uses backwardchaining search which means that it uses the desired goal state as starting point and traces the action sequence which leads to the starting state There are a few basic building blocks in this approach 31  Goal  A goal represents the desired final state which the planner should attempt to reach Each goal has a set of conditions which must be satisfied for the goal to be reachable  Action  There is a predefined set of actions each of which represent what the AI can do Each action has set of preconditions and effects the preconditions define what the world state should be for the action be doable and the effects define how the world state is changed by this action  Plan  The final plan is a sequence of actions leading from the current world state to the desired goal state  World State  The GOAP planner uses symbolic representation of world to perform search in statespace This abstraction allows both preconditions to be matched against the world state and effects can also be used to apply changes to the simulated states  Planning process The simplistic approach  When running plan formulation the GOAP planner is given the desired goal state a list of possible actions and the current world state which is abstracted from the concrete game world into the symbolic presentation The Figure 28 below shows a simplified overview of the planning process in statespace during the plan formulation  Figure 28 Abstract illustration of GOAP planning process  The planner starts from the desired goal state adding its conditions into the list of unsatisfied world properties During each iteration the planner searches for actions which have effects that match the unsatisfied world properties Each of the possible actions is picked as a possible node in the search graph and evaluated recursively by applying the effects to world state and adding the preconditions of the action to the list of unsatisfied world properties When the planner reaches a state where all the world properties are satisfied it has reached the initial state and thus has found a valid plan or if no more actions can be matched in which case there is no solution After a valid plan has been found it is made active and the AI attempts to follow it However if any alterations are made to the world state during the plan execution replanning is required and thus the planning process is run again 31  Adding action costs to the plan formulation  Sometimes just knowing a possible sequence of actions for reaching the goal does not suffice as there might be other lowercost paths leading to it The types of cost factors depend on the use case for example time money or health  When cost is added to the actions the search space can be considered as a weighed graph which can be evaluated using A* algorithm with the expected cost used as heuristic for the search formula Figure 29 below shows part of a possible search tree which might be formed during GOAP planning  Figure 29 A partial statespace search tree for GOAP  A benefit of the statespace presentation is that as it is practically a game tree structure certain traditional board game AI techniques can be applied to it For example to prevent unnecessary time spent on evaluating duplicate subtrees the evaluated states can be stored in a transposition table with the hash of symbolic world state used as cache key Other benefits include the possibility of using AlphaBeta Pruning and the Killer Heuristic  Iterative Deepening A* IDA*  When A* is used for pathfinding each graph node is only evaluated once and there is limited number of nodes to explore However with GOAP planning there is no limit on how many times a single action may be performed leading to infinitely long action sequences To avoid this Iterative Deepening A* IDA* which is a variant of Iterative Deepening Search IDS algorithm can be used for traversing the state graph 17 pp  376401 The progress of IDA* search is shown below in Figure 30  Figure 30 Iterative Deepening A* search example  The IDA* search works by defining a cutoff value which is the maximum cost until which the search iteration terminates On each iteration the regular depthfirst search is run until the current cutoff limit is reached If the target node was not found the cutoff value is increased and search is run again thus iteratively progressing further in the search tree each time 17 pp 376401 The above Figure 30 shows roughly how this iterative progress works  362 Hierarchical Task Networks  Although sharing some concepts with STRIPS planning the Hierarchical Task Network HTN approach assigns the current world state as starting point and uses available tasks to construct the plan through forwardchaining task decomposition This is opposite to previously introduced GOAP which uses backwardchaining search in the statespace graph to find plan leading from goal state to the initial world state 32 The Figure 31 below shows an overview of a simple HTN planning system adapted for games  This HTN planning system is divided into the following components  HTN Domain  The essential part of HTN planning system is the HTN domain which contains all tasks available for solving the particular problem There are two main types of tasks  • Primitive Tasks which are the basic building blocks of the plan They contain an operator which defines the actual lowlevel task for the game condition which uses the world state properties to evaluate whether the task can be executed and effects which alter the planner world state  • Compound Tasks which contain multiple methods of executing a particular task Each of these methods have set of preconditions that dictate which of the methods if any gets chosen to be decomposed into the plan based of the current world state  The tasks available in the domain form a hierarchy hence giving the name for this planning approach  World State and Sensors  Like in the GOAP approach the planner uses an internal world state to simulate effects of tasks in the game using the resulting state in primitive task conditions and compound task preconditions to control the planning process Sensors work as adapters providing the simulated world state from actual game state  Planner  The planner does the actual planning work which starts from the root task in the HTN domain which gets inserted into the list of tasks to process in beginning of this process after which the iterative planning process is started On each iteration the first item in the list of tasks to progress is dequeued and processed If the item in list is a primitive task its condition gets run and if satisfied the task gets appended to the final plan If the task is a compound task the preconditions of the methods get run to select the appropriate method to decompose This decomposition enqueues the tasks in the method in front of the list of tasks to progress The iterations continue until the list of tasks to progress is empty An example planning case is shown below in Figure 32  Figure 32 Illustration of a simplified HTN planning example  After the planning process is completed the planner has the final plan which can be passed to the plan runner Depending on the structure of the HTN domain it is possible that the planning may in some cases fail to provide any valid plan at all  Plan Runner  After the planner has created a plan the plan runner starts executing the plan during gameplay keeping track of currently active task in the plan and checking the task conditions during this process If the world state gets changed unexpectedly during plan execution ie when the state does not match the conditions of task executed next in the plan the HTN is forced to do a replan to run the planning process again  363 Composite Tasks  One approach to handling goaloriented behavior is the use of the Composite Task architecture Originally implemented in 1995 for a realtime strategy game it has since found use in CSXII Tactical Combat Simulator used by the US Army 33 The Figure 33 below shows the basic structure of Composite Tasks  Figure 33 Structure of Composite Tasks  The main concept in Composite Task model is the ability to split a highlevel main goal into smaller subgoals creating a hierarchy of tasks This flexibility allows expressing even very complex goals and how to satisfy them using combination of lowlevel actions  The Composite Tasks consist of the following components  • Composite Tasks which can contain either other Composite Tasks or Simple Tasks  • Simple Tasks which contain one or more Actions  • Actions which are individual atomic operations that the AI can perform  The execution of tasks starts from the root task which evaluates its child components in priority usually lefttoright order until the entire hierarchy has been walked through The benefits of Composite Tasks include design simplicity datadriven content and generalized evaluation process 33  364 MultiUnit Planning with Hierarchical PlanSpaces  The traditional planning methods are well suited for planning actions for a single actor when the number of possible actions stays in reasonable amount However when planning actions for multiple units at once for example for military incursions the statespace searching suffers from combinatory explosion This means that the number of possible combinations of actions grows exponentially exceeding the available processing power and thus becoming unusable To solve this problem the planning can be done in planspace instead of statespace 34 The Figure 34 below shows abstract illustration of the difference between statespace and planspace planning  Figure 34 Comparison of statespace and planspace planning 34  With this approach the planning is started from highlevel task which is further refined into lowerlevel tasks and eventually individual unit actions 34  The planner  Instead of keeping track of possible states the planner uses list of possible plans and scores them based on their expected cost using the same A* algorithm like in other graphbased planners On each iteration planner dequeues the most promising plan ie the one with lowest estimated cost select the appropriate planner methods and their alternative approaches to get a list of new possible plans to branch off from this plan Each of these new plans get refined by the planner method after which their estimated cost is calculated by the task cost estimation function and they are queued into the list of possible open plans This iterative process runs until either a planner succeeds by finding a complete plan in the queue or fails by running out of plans to refine 34  The tasks  The plan is composed of a hierarchy of tasks that can either be compound tasks which can be further refined to other tasks or primitive tasks which represent individual unit actions The planning domain is defined by list of possible tasks which belong to specific scope in the domain based on their position in the task hierarchy Some possible tasks are listed below in Table 9  Table 9 Some possible tasks for the planspace planning adapted from 34  Scope 	  Task examples  Mission  Highlevel mission task  Objective  Capture Colony Defend Colony  Group  Form Up Eliminate Colony Defenses Attack Invaders  Tactic  Bombing Run  Units  Attack Fleet  Individual Unit  Move Attack Wait Bomb Defend Deploy Troops  The individual tasks set of inputs and outputs which are used to link unit states between sequential tasks usually providing the output state of previous task as input of the next task When tasks are chained sequentially the preceding tasks are required to be completed before the next task in sequence can be activated This allows the planner methods to control which tasks can be executed in parallel and sequential order  Planner methods  The actual refining of tasks is done by planner methods which take the current plan and task to be refined as input and provide a refined plan for the planner The planner methods only apply to specific tasks and their complexity ranges from simple single task output to complex combination of tasks They work by creating a number of subtasks for the task being refined thus expanding the current plan to lower level The planner methods matching the tasks in Table 9 are listed below in Table 10  Table 10 Some possible planner methods adapted from 34  Scope  Planner method examples and responsibilities  Mission  Allocate units  Objective  Define activities assign units to groups  Group  Execute tasks as groups  Tactic  Synchronize tactical activity  Units  Arrange cooperation between units  Individual Unit  Define the actions  The planspace graph  As mentioned earlier the planner maintains a list of all possible complete and noncomplete plans as it searches through the planspace A part of this planspace graph is illustrated below in Figure 35  Figure 35 Illustration of the planspace graph adapted from 34  In the illustration each plan is shown as a branch in the planspace graph with their associated cost estimate The green plans are in the closedlist of plans that have been refined and red plans are in the queue of open plans The tasks inside each plan show how deep the particular plan has been refined green indicates tasks that has been refined white shows the task being refined now and red tasks are unrefined tasks  Use case Planning attack on enemy colony  The hierarchical planspace planning has various uses in a 4X strategy game and this example focuses on a simple case of attack on enemy colony The player has four fleets available to be allocated for this mission a battleship fleet a destroyer fleet a bomber fleet and group of troop transports The resulting plan that can be generated using the example tasks listed previously in Table 9 is shown below in Figure 36  Figure 36 An example plan for invasion of enemy colony  The planning starts by creating the capture colony task on objective layer which contains information about the target colony its defenses and the AI players available fleets for the mission After this the objective gets further refined into set of grouplevel tasks  Forming up the fleets eliminating defenders and invading the colony  The form up task can take advantage of influence map and tactical pathfinding to pick the best positions for the fleets and use this data to create the tasks for fleet movement to those positions It can also consider the vulnerability of certain unit types such as troop transports when it assigns these positions  The task for eliminating defenders can have various alternatives depending on the type and number of defenders in the colony presence of enemy fleet creates the need for attacking enemy fleet and existence of any planetary defenses requires using bombers to eliminate them when other fleets are protecting the bombing run These tasks are further refined down into actions for the appropriate fleet types available  The last task in capturing the colony is planetary invasion which in straightforward way creates the troop deployment actions for troop transports and assigns the other fleets to defend the transports  37 	Diplomatic Reasoning  A game featuring players with the ability to engage in diplomatic relationships with each other imposes a certain set of requirements for the AI  Forming the opinion of other players  The most important part of diplomatic interaction is the ability to evaluate opinions about other players how they have behaved in the past what they are expected to do what things the players agree and disagree about how the military scientific economic and social status are evaluated etc This includes the mechanism how the actions of players affect these opinions and how these opinions end up shaping diplomatic relationships into friendships alliances enmity or animosity A method for handling opinions is presented later in Chapter 371  The ability to estimate percussions of actions  When it comes to making diplomatic decisions the AI player needs to be able to understand effects of its actions With the opinion system in place the AI can use the expected opinion changes in goal tree search with combination of utility scoring to evaluate its actions and choose the one which yields the highest score With this approach the AI can utilize knowledge of army sizes the players opinions about each other and other factors such as personality weights in making educated guess for results of the actions  Illusions of a character with personality behind the AI player  If all AI players would make decisions based on the same goals and using the same scoring methods there would be little variation between the different types of players in the game rendering the AI behavior more predictable and boring and removing any differences in behavior among the opponents With certain preset weights given to each of the AI players their decisions can be influenced to be focused on unique goals giving each of them a more distinct personality This also allows a human player who wants to play the game with a certain strategy to seek alliances with AI players which have goals and priorities matching hisher own goals  Longterm goals and persistence  The AI needs to have logical goals and the ability to make longterm decisions to help forming alliances and other agreements with other players The combination of playerspecific weights and opinion system create a natural foundation for this process  371 Opinion Systems  The original approach to Opinion Systems as used by Adam Russell is focused on allowing individual NPC agents in game world to shape their opinion about other players based on their actions but the core mechanism he proposed can be adapted for controlling the opinions of virtual players in AI about other players 35 The Figure 37 below shows adaption of Russells Opinion System for a 4X strategy game diplomacy  Figure 37 Opinion System adapted for 4X strategy games  The main differences from his model are the replacement of NPC characters with virtual players and replacement of global opinion with visibility and weights used to select audience of the deeds and other minor adjustments to handle local opinion transformations  Opinion State  The central piece of data in the Opinion System is an opinion state which contains the opinion in either discrete or numeric format This could for example be one players trustworthiness opinion about another player ranging from 10 to 10 These opinion states can be either simple singletrack values or multidimensional opinions which are composed of more than one value affecting the opinion state 35  There are both positive and negative aspects of the multidimensional approach the positive features include orthogonality greater variety of effects better match to natural language and having more information The downsides however include being more brittle at design changes increased confusion difficulty in visualization and challenges in quantization 35 Table 11 lists a subset of the possible opinion values that can be used in 4X strategy game diplomacy with multidimensional approach involving four different opinion values  Table 11 Examples of some potential opinion values in diplomacy  Opinion Value  Meaning of 10  Meaning of 10  Scariness  Unthreatening  Terrifying  Trustworthiness  Deceitful  Honest  Sentiment  Loathed  Admirable  Aggression  Pacifist  Warmonger  This approach has the benefit of giving diplomacy more depth for example if a player has strong army and thus high scariness score but low sentiment score for past offenses another player might be unwilling to enter into a trade pact even if they would fear the opponents army 35  Actions and Deeds  The deeds originate from either direct or indirect actions made by the player Direct actions might include declaration of war or using spy to perform a sabotage mission while massing troops could be induced as indirect action measured using threat analysis and influence maps It is possible that one action causes multiple deeds such as declaring war during peace treaty would also raise the break treaty deed Some examples of possible deeds are listed in Table 12 below with their associated audiences and weights  Table 12 Some possible deeds and their weights  Deed  Affected Opinion  Audience  Weights  Target  Allies  All  Declare War  Aggression  Public  05  05  01  Break Treaty  Trustworthiness  Public  05  04  02  Demand Tribute  Sentiment  Public  02  02  02  Sabotage  Sentiment  Allies  02  01  Mass Troops  Scariness  Private  01  Trespass Territory  Scariness  Visibility  01  Mass Genocide  Sentiment  Public  05  05  05  There are four different audience types in this model  • Private Only target player receives the deed notification  • Allies Target player and its allies receive the deed notification  • Public All players receive the deed notification  • Visibility Players who have currently visibility of the affected map square get notified  The deed audience type is specific to each deed type and posting a deed requires either a target player or target location depending on the type  Deed Log  The deeds are posted into Deed Log through filtering and logging pipeline which is used to allow for example to temporarily suspend delivery of certain deeds and to track statistics about the deed posts The Deed Log not forwards the deeds to the subscribers of deed events but also keeps track of past deeds and has a list of persistent deeds for example trespassing enemy territory could be handled as a persistent deed which posts the deed notification every turn until it gets deactivated when the units leave enemy territory 35  Audience Selection  The original audience type and the target passed with deed are used to pick the destination of the deed and each of the recipients get notified  Local Transformation  Before the deed is used to affect a players opinion it goes through local transformation which is specific to each deed type In this modified 4X strategy game model this transformation process is used to apply in certain cases multiplier to the deed weight based on the notification recipients existing opinion about the target player For example lets assume that player A demands tribute from player B incurring sentiment penalty for player Bs opinion about player A If there exists player C which has negative opinion about player B then player C would use negative multiplier for local transformation about the deed weight leading to positive sentiment offset for player Cs opinion about player A 35  Deed Effects  When the deed notification eventually receives the player it has to have effect on the receiving players opinion This change in opinion is invoked as transient offset through various possible offset functions one of which is specific to each deed type Example of a transient offset function is shown below in Figure 38  Figure 38 Opinion transient offset function example by Adam Russell 35 p 544  The function has a runin time during which the deed offset increases until it reaches the peak offset and highest effect on the opinion After this it decreases during the runout time and when the transient offset function ends the final offset is left as the permanent change in opinion This allows for example a genocide deed to have strong permanent impact on opinion but a trespassing of territory yields only a temporary change which dissipates gradually 35  To prevent excess accumulation of transient offset effects on opinion by repeated deeds the effect frequencies can be regulated by adding a minimum time between repeated deed effects for a single player 35  38 	Customizing AI  Although it is possible to implement AI completely by hardcoding it within the game code there are motivations for making AI customizable which are twofold For the first the development of AI is collaboration between programmers and designers and to support this process the designers should be able to project their visions in the game with as little friction and delay as possible And for the second by providing flexible methods of modifying and creating additional content to the game user community and players can create custom mods and other expansions which can provide additional gameplay value for other players of the game 4 pp 99107 In this section some common approaches for providing AI customizability are considered from this projects point of view  Black Box and White Box Approaches  The AI system implementation philosophies can be generally characterized into two groups White Box and Black Box systems The White Box systems offer more flexibility are good for team of multiple people collaborating and they allow designers to work more independently Black box systems on other hand are good for singleperson implementation but force designers to have greater dependency on programmers for providing implementations for the blackboxed behaviors Neural Networks are one example of Black Box systems which take discrete input and provide output through trained processing in the hidden layer 4 pp 100107  Sometimes mixing the two is possible for example reusable components in White Box systems requiring less customization are usually better suited as black box components such as Pathfinding logic The choice of better approach often depends on the project needs for example implementing AI components as Black Box systems might be good for a team with many programmers but in a team with many designers a White Box System would be more preferable 4 pp 100107  381 DataDriven Design  The key idea in DataDriven Design is detaching the AI behavior and logic from the game code into a separate data model which can be independently modified by the designer without need for programmer intervention There are various ways of accomplishing this for example with FSM state masks custom parameter configurations for individual AI agents external definition of rules for RuleBased Systems and Scripting Languages 4 pp 109111  Custom Tools  One way to increase the designers power in AI development is through the development of custom tools for creation debugging and visualization of AI in the DataDriven Design model The benefits of this are the increased flexibility easier maintenance and balancing and increased usability through UIs tailored specifically for the designers needs Drawbacks however include the upkeep required from the tool programmers and extra care needed for version control handling 36  These tools can either be completely specific to the data required for a single usecase or they can be more general such as tools for visualizing and editing Decision Trees Behavior Trees or rules in RuleBased Systems The tools can either be external applications or they can be built into the game allowing adjusting of the AI data in realtime without need of exporting data and restarting the game 4 pp 104111 The drawbacks of scripting however include the need for script development tools for designers and the possible performance overhead due to script interpretation at runtime Both of these problems can be alleviated by using existing scripting tools which have already good tools for scripting and are already optimized to be embedded as part of game 37 Although there is no limit in which language can be used for scripting there are a few most commonly used options in game development Some game engines come with a builtin scripting system such as UnrealScript in the Unreal Engine and Torgue Script in the Torque Engine The use of opensource scripting engines such as Lua or Python is also very popular among game developers Visual Scripting in which the logic is visualized graphically to the designer instead of a textbased presentation is also one possibility and commercial libraries such as PlayMaker Visual Scripting for Unity3D exist to support this approach And if none of these options is suitable for a particular game developers can opt to create a completely custom scripting language and engine tailored for the particular needs of their use case 4 pp 112130 38  For this project the choice of customization was narrowed to a combination of casespecific custom data models and behavior scripting using an opensource scripting engine A custom script engine was out of the scope of this thesis so the choice of scripting language was further narrowed down to selection between Lua and Python with motivations explained in more detail below Both Lua and Python are scripting engines with a good reputation of being well suited for embedding into games due to their free license easy integration and stable language specifications which are backed by strong existing developer communities 4 pp 114130 As this project uses Unity3D and C for implementation of the prototype there are two popular frameworks which provide support for adding scripting languages on top of it IronPython for Python scripting 39 and Moonsharp for Lua scripting 40 In Lua the language itself has been designed to be flexible and extensible so although by default there is no support for ObjectOriented OO paradigm such as classes inheritance and encapsulation they can be added through the use of metatables The interpreter itself does not support multithreading so care has to be taken by either running the interpreter only in a single thread using mutual exclusion for access control or having multiple interpreters on separate threads The language syntax itself is very similar to Clike languages with shallow learning curve and the standard library is very small and easy to learn 17 pp 449450 Python has native support for OO programming and it excels when it comes to mixing the script language with native languages The language syntax depends heavily on indentation but is generally considered one of the easiest languages to read and learn There is a very large number of libraries available for Python but in runtime the language suffers from size and speed issues 17 pp 451452 Based on the above considerations Lua integration using MoonSharp was chosen for this project main reasons being the easy integration and lightweight runtime which were key for the nature and scope of the prototype As the majority of how the AI works is hidden from the player there is often temptation to cut corners short either to artificially increase the level of difficulty provided by the AI or perhaps just to save time in development Some games are notorious for having cheating AI and it may be a big spoiler for the gameplay experience and source of frustration if the player feels that his opponent is exploiting an advantage heshe cannot match 6 pp 34  Some examples of cheating might be the ability of AI to ignore visibility status of map thus having always full knowledge of the location and arrangement of all enemy units or adding a certain resource production multiplier to the computer players production which would be tied to the AI difficulty level chosen by the player 41 For this project the use of features giving the AI player unfair advantage over human player is prohibited and instead the difficulty variation is done through combination of regulating AI aggression on easier levels adding randomness to the decision scoring and otherwise tweaking the way AI makes choices based on the same information that is available to the player Unlike many other realtime game genres with strict performance requirements the 4X strategy games have the benefit of being more relaxed when giving the AI processing time thanks to their turnbased gameplay model However care needs to be taken to balance between how much computing time is given to the AI to not cause too big slowdown in the gameplay especially in lategame situations in larger game worlds when many AI agents owned by multiple AI players might be operating This section introduces certain techniques to optimize and balance the processing time given to the AI features A key feature in managing the time consumed by AI is division of the AI logic into manageable tasks and using some method to control the execution of them In a singlethreaded execution model there is a fixed maximum amount of time that can be spent for performing other tasks as rendering of the game usually happens on the same thread A generalpurpose execution management system can be used to not only run the AI code but to also control the time given to many other background tasks in the game such as asset downloading audio and physics processing 17 pp 693725 A basic scheduling approach is to assign tasks to be executed on certain frames using a simple algorithm such as execution frequency Figure 39 below shows how tasks A B and C might be executed when scheduled on relatively prime frequencies In a loadbalancing system the scheduler keeps track of time spent for each task ensuring that processing time on each tick does not exceed the maximum limit In addition the expected running time of tasks can be estimated which can be used to predict how long the task takes and thus help scheduling it for execution Scheduling Groups can also be used to divide tasks into groups which share a certain scheduling algorithm such as One way to control the time consumed by pathfinding and to prevent CPU spikes caused by excessively large pathfinding queries is to divide a single path query operation to multiple steps a technique known as timesliced pathfinding With this approach the pathfinder is given fixed amount of time to process during each tick and when the time is exhausted the state of pathfinder is saved in such way that the query can be resumed when the pathfinder is again given processing time One challenge with this approach is that if state of map affecting the result changes during the query the results may not be guaranteed to be valid How much this shortcoming affects the feasibility of the approach is however dependent on the use case of the pathfinding 43 In the simple manual scheduling scenario all tasks are run on a single thread in cooperative fashion This requires both the AI tasks to behave well in terms of how long they take to execute on a single frame and the scheduler has responsibility on how to divide the workflow during each tick Another approach for allocating CPU time not only for AI tasks but potentially to other components in the game engine too is the use of multithreading Traditionally this approach has had the downsides of requiring synchronization of data structures and performance penalty caused by context switching but in the past decade the advent of multicore CPUs even in the lowestend mobile devices has created a situation where the use of multithreading to leverage the new hardware features gives performance gains that outweigh the disadvantages 17 pp 693725  Although the performance of Central Processing Units CPUs has increased constantly during the history of computers the power of graphics processing units GPUs has exploded in the past decades to be a very viable option for performing largescale computation With the introduction of GeneralPurpose computation on the GPU GPGPU APIs such as OpenCL and DirectCompute this power has become easily accessible to developers including AI programmers This allows great chance to speed up the AI by offloading applicable parts of it to the GPGPU processing 44 As pathfinding is one of the core methods used in gridbased strategy games seeking to optimize it through GPU offloading is one of the most promising applications for this technology Recent research on has demonstrated that A*type pathfinding can be implemented with GPGPU with a much higher performance compared to traditional CPUbased search 45 The GPU offloading in Unity projects can be achieved with the use of Compute shaders which provides an API for running GPGPU programs within Unity applications The downside of Compute shaders is they require using the latest versions of graphics programming APIs and graphics hardware supporting them 46 There are also a couple of methods which could be useful in the development of AI for strategy games but they were considered not to fit in the scope of this project due to a combination of practical and schedule challenges They do however have potential to be explored in future research and are listed here to explain why they were rejected at this point Explaining away in which a known result fact is used to reason the probability of presence of source facts leading to it including characteristics of independence and conditional dependence The challenge in Bayesian networks is that for them to be useful the probabilities used for inferring states need to be acquired by either gathering the information during simulated gameplay or through training of the network This means that the problems they are used to solve need to be fairly uniform as alteration of gameplay and rules affecting the probabilities causes the network to produce invalid decisions One solution for this could be allowing the network to adapt to the users gameplay by training it during actual gameplay but this might make the AI show too much emergent behavior and thus lead to undesirable decisions which might not be expected by the AI programmer or programmer causing other parts of the AI to behave unexpectedly and appear broken to the player The concept of Neural Networks is one of the machine learning techniques which has gained a lot of popularity in the AI research in the recent years and there are even some commercial games which have utilized this approach The Neural Networks try to simulate the function of neurons in actual human brain with a simulated mathematical model In this model the Neural Network is divided into three layers Input layer Hidden layer and the Output layer with each of those layers containing sets of neurons The number on each layer dependent on the use case of the network The source data is fed into the input layer from which a feedforward process passes the information through the hidden layer all the way until it reaches the output layer During training the output data is evaluated and backpropagated to the weights in the hidden layer and repeated until the Neural Network outputs the expected values After this the trained network can either be used in the final AI implementation or the training process can be infinitely continued to allow the AI to be able to adapt to player behavior during the gameplay 6 pp 269315 The major challenge in the Neural Networks is like previously outlined in the evaluation of Bayesian Networks the need for training and high risk of unexpected emergent behavior and these features make especially debugging testing and balancing gameplay difficult 6 p 271 It should be noted that it is possible to leverage these techniques in games successfully but due to the time and effort required for these approaches compared to the potential gain they were not included in the scope of this thesis In this section the produced high level technical design is presented in addition to outlining the best practices for implementing it Due to the highly iterative process in game development this solution should be considered as a starting point for prototyping and most likely goes through various alterations and fine tuning as the development proceeds It should however provide an understanding of what kind of options are being considered as realistic goals for the project On high level the entire AI is encapsulated as the virtual player which interfaces with the game by taking the world state as input and providing set of actions as output Figure 41 below shows an abstract overview of this information flow To give a better understanding of this process below is a brief explanation of the purpose of each step in this illustration This abstraction of the AIs understanding of the game state and its operations allows better modularity and helps preventing cheating by allowing the AI to use only the same information and actions which a human player would be able to utilize in the game Furthermore with this architecture it is possible to use certain parts of AI features for human players to allow worker automation colony governors and other features that might be automated to reduce the dreaded micromanagement which might be present in largescale games When implementing the AI a good approach for controlling the complexity is the division of the AI to dedicated components The central part of the virtual player is the multitier AI model which follows the principles set earlier in Chapter 321 The responsibilities of each tier level are capsulated to managers each of which has a dedicated role in the decisionmaking hierarchy These managers and individual agents in lower tiers of the model utilize various AI tools which are available for them throughout the tier levels as a separate toolbox This division to components is illustrated below in Figure 42 The HTN planner in this component utilizes a highly abstract highlevel strategic decisionmaking domain which composes to various abstract tasks which are assigned to the other managers The Diplomacy and Expansion Managers also provide data for the planner to help in the decision making The research goal selection uses in straightforward way the active goal given by the Strategy Manager to choose which field of research should be given the research priority For tracking the state of enemy research the Research Manager uses functionality of RuleBased Systems inference engine allowing it to use knowledge of enemy fleet types colony improvements and existing knowledge of research The Diplomacy Manager is responsible for handling any diplomatic actions requested by the Strategy Manager and it also provides necessary data for the highlevel HTN planner such as the opponent reputation based on opinion values The overview of the Diplomacy Manager is shown below in Figure 45 One central component of the Colony Manager is the goal pool it contains all possible goals and at any given time each colony is assigned one of these goals which it needs to fulfill As the Colony Manager gets the highlevel goals from the Strategy Manager it makes sure that the currently assigned goals have the highest utility for reaching this highlevel goal As the Colony Manager is also responsible of tracking global resource production and allocation this allows it to not only directly respond to specific production goals but also to balance the resources between individual colonies to avoid potential production or growth bottlenecks The mission planning relies strongly on the Tactical Analysis Server to find both weak and strong areas in the friendly or enemy territories which allows it to create missions not only for incursions to enemy space but also for reinforcing own defenses This planning process also includes allocation of available military units to specific missions When a potential mission is formulated the allocated units and the mission goal is forwarded to the military coordinator which handles planning of the actual mission and decomposition to individual unit actions The Expansion Manager is responsible for handling both the expansion of players empire with colony ships and using worker fleets to improve the existing structures owned by it The overview of the Expansion Manager is shown below in Figure 48 The goals given to colony ships and worker units are directly assigned to the individual units which use their own decisionmaking model to fulfill them The core of Military Coordinator is the Hierarchical PlanSpace HPS Planner which uses the planner methods and tasks provided by the coordinators data model Functionality of this HPS planning process was detailed earlier in Chapter 364 If the planning succeeds the resulting plan containing tasks for each individual fleet is executed by the coordinator which provides goals for the individual units during the plan execution until mission is either finished or failed The decisions made at this level are mapped directly to the commands given to fleets during the AI players unit movement turn during the gameplay The worker automation can be also used by the human player to toggle partial Expansion Manager and AI fleet behavior on and off when needed Each colony owned by the AI player is managed by the Individual Colonies AI module which takes the goals given by the Colony Manager and translates them into actions that can be performed by the AI for that particular colony Overview of the Individual Colonies module is shown below in Figure 51 The AI toolbox contains various generic AI algorithms and tools introduced earlier in the chapter 3 which are utilized by the various other modules throughout the multitier levels of the AI decisionmaking model The most important principle of the toolbox is that each of the technologies used by the AI is encapsulated into a reusable component which allows integration with other AI code through predefined interfaces This not only allows keeping the componentspecific code separate from other AI logic but also allows better possibility for scripting language integration explained later in Chapter 44 Unlike in realtime strategy games which have strict limits on the CPU budget for AI execution time per frame the nature of turnbased games allows certain level of flexibility in the implementation of AI processing model Although not optimal in the proposed prototype all AI actions can be isolated to the AI players turn thus reducing the pressure to interleaving AI processing with other players turns However even when executing all AI code of a particular player in one batch certain steps must be taken to avoid undesirable behavior of the game such as stalling the game interface during the AI processing In Unity the default method of implementing asynchronous behavior is through the use of Coroutines which resemble cooperative multitasking Each Coroutine gets executed on the Unitys main thread and they are responsible for handing over the execution voluntarily by yielding Although this approach is easy to implement it has the drawback of needing careful planning of when to yield the execution to avoid either clogging too much CPU time on a single Coroutine pass or wasting CPU time by yielding excessively often Other challenge of this approach is the care needed when designing the AI model to allow Coroutine yielding throughout different components Another way of implementing the asynchronous execution is through the use of C threads Although the use of threading allows independent execution from the main thread and possibility of benefiting from multicore CPUs they have the drawback of needing synchronization between any shared data structures and as Unitys API is not threadsafe would be limited to executing only the parts of C code which are not using Unity features In this prototype the Coroutine method was chosen to be used When balancing the AI behavior and features it would be unnecessarily time consuming to have QC play multiple games against an AI after each modification done to it by the programmers or designers To alleviate this problem automated testing can speed up a lot of this process by allowing simulation of entire games played by variable number of AI players A set of parameters can be configured for this process Also different variations of AI using their own versions of data models could be potentially matched against each other to directly evaluate the effect of modifications of the model data on AI performance with minimal iteration time Each automated game produces a transcript of all actions performed during the game and the resulting outcome of the game This recorded journal of the game can be replayed in either highor lowlevel detail which allows designers to pinpoint which actions by the AI were undesired and give them ideas which features would need to be improved In the proposed AI model the individual manager modules in the multitier have the potential to benefit from being abstracted from the other AI code through use of LUA scripting engine The AI toolbox which have very little need if any for specialization can be easily utilized by scripts as standalone components exposed to the scripting environment removing the need to write any lowlevel AI code in the scripting language As explained in Chapter 382 the MoonSharp library was chosen to be used in the prototype but due to schedule challenges the experimentation of scripting integration was not finalized in time for this thesis and remains on theoretical level Following the principles of the Data Driven Design introduced in Chapter 381 many parts of the AI model are isolated into data models which can be customized either by custom tools or direct manipulation of the data in question These datamodels are listed below in Table 13 All of the above data models can be stored in either JSON or XML presentation which can be further exposed to the designers with an UI through custom tools where needed This allows building a foundation for the AI behavior before devoting time to building the custom tools It should be noted that some data such as the rules used for technology inference can be derived from other game data This chapter explains what was developed during the project and evaluates how well the project output matched the initial goals The biggest challenge for the implementation of the complete AI player was the lack of a 4X game engine and creating one singlehandedly for the purpose of this thesis would have exceeded the feasible time limits available to finish the project and risked generating excessive workload beyond the scope of the project The outcome of this thesis can be categorized in two distinct parts The most important outcome of the project was the highlevel design of the AI player which should be used as a starting point and guideline for the implementation of the actual AI code in the actual game as presented earlier in Chapter 4 as the proposed solution This technical design contributed to the selection of parts suitable for independent prototyping which are detailed later in Chapter 52  It should be noted that the greatest value of the highlevel design comes from this profound research of AI field done for the theory part of this thesis as the time spent on this process will be saved in any possible future implementation of actual games  As the evaluation of this technical design was limited to general assessment of its usefulness in the scope of this thesis due to lack of a complete game prototype the results of the prototyping were used to reinforce and assess the feasibility of this highlevel design Although the technical design represents the virtual player feature of the strategy game all necessary parts of the design can be applied for playerassisting governor features by considering the human player as a complete AI player with certain decisionmaking modules disabled This allows the full influence mapping threat evaluation and other features to provide necessary data for the assisted features such as production planning advisor recommendations scientific research goal suggestion etc As per the original game design the map prototype was based on the hexagonal grid This grid type provides better movement and distance handling by eliminating the different between diagonal and axial distances but in contrast requires more complicated handling of hexagonal coordinates A hex coordinate utility was developed during the project to help managing this challenge This first version provided a good foundation for the map and pathfinder code although some bugs were found and fixed in the later version Building upon the map prototype used for the pathfinding testing previously the functionality was extended to include support for multilevel spatial database to map player influence and the combined effects for visualization on the map Additionally the pathfinding engine was adapted to leverage the spatial database information to allow experimenting with tactical pathfinding Appendix 2 shows screenshots of the various spatial database layers being visualized This version of prototype was equipped with a primitive representation of military units which appear as circles on the map Each of the units has the following properties The aforementioned spatial database layers were used just for testing purposes and actual game could use any number and any combination of layers in whichever way the game designers might feel useful Tactical pathfinding was created as a simple extension to the original pathfinder using the influence data in the spatial database to show benefits of this approach The screenshots in Appendix 3 show three different pathfinding modes with equal start and goal locations  Simple Each node is considered to have fixed cost of 1 which makes the pathfinder attempt finding the route with lowest number of map cells from start to the goal node The screenshot also shows that this approach makes the pathfinder visit very low number of map cells thanks to the A* algorithm which prioritizes cells closer to the target Terrain movement cost The pathfinder now considers movement cost of terrain when doing the path query showing a different resulting route Also a much higher number of map nodes was visited during this query Enemy threat avoidance Again the same start and goal nodes are queried but this time the scoring of path is done using data from the enemy influence layer although boosted 10 times to allow including terrain movement cost as a secondary scoring method which creates a completely different path which nicely skirts around the areas around enemy influence The actual formulas for combining influence data from spatial database for map cells are highly customizable and may need a lot finetuning depending on the use case for the game For example it may need careful consideration on how much effort the AI should put into avoiding enemy if resulting path leads to excessively long detour However for the prototype purpose the combinations used seem to work quite well A simple inference engine was created to demonstrate the basic concepts of rulebased system functionality This implementation uses heavily Unity features such as storing rulesets in ScriptableObjects and editor extensibility to allow running the inference engine in either edit or runtime mode Screenshots of the inference engine test are shown in Appendix 4 which were taken from the Unity Editor mode The first screenshot shows the ruleset asset and the associated test rules used in the prototype The other screenshots show state of the inference engine after each user action starting from resetting the inference engine followed by singlestepping it one step at a time until the inference process was finished The debug output of the Unity console is also included in the screenshots which shows the effects of inference process during each step Although the basic inference process works and demonstrates the functionality of associated algorithms a more generic implementation supporting dynamic rule generation would probably be most useful for the actual productionquality game especially if used for technology inference like proposed in the highlevel design The motivation for this thesis originated from the interest in game artificial intelligence and the 4X strategy gaming genre This combined with the ongoing trend of constantly increasing mainstream popularity of gaming gave further incentive for this research as a way to create a design which could benefit future development projects requiring this type of knowledge As possibilities for exploring this field of technology were not available at the workplace for the timeframe of this thesis the project was implemented as a personal undertaking focusing strongly on theoretical research The main goal of this thesis was to research various AI technologies in order to create a plan for integrating AI for a turnbased 4X strategy game and a secondary goal was to do prototype implementation of it The theoretical part of the study was finished but the implementation was limited to prototyping only certain areas of the researched technologies The technology research phase proved highly informative and a lot of knowledge was gained for creating a feasible highlevel design for the AI implementation During the prototyping phase the technologies that were experimented with also showed a lot of their potential also in practical setting thus reinforcing the credibility of the parts of design which they were associated with Although there were certain shortcomings in the practical output of the thesis the theoretical knowledge gathered has high value for not only the author personally as game developer but hopefully also for other people facing similar challenges who might be reading this One major improvement in future would be the full implementation of AI in the actual game As the AI design is theoretical and highlevel there are most likely many challenges in the practical implementation which will be reflected back on the design of the AI model This includes for example considering the actual gameplay design of the game and the processing performance of the target platform Also the further work should not be limited by the research done in this thesis but should also be open to exploring possibilities of both future technologies and reevaluating the existing technologies such as Bayesian and Neural Networks which were ruled out of the scope for this thesis The purpose of this thesis was to design and develop a piece of software to improve Google Drive shared folders in organizational use The main reason why the topic was chosen is that organizations that use Google Drive as a centralized data storage are especially vulnerable to a potentially vast amount of work in case of an accidental deletion of items I immediately started to figure out a topic for my upcoming Masters thesis As Google Apps was one of the main tools we used in my workplace I figured it could provide an interesting topic in some form As all software Google Apps also proved to include design flaws partially addressed by at least one third party vendor Because the vendor did not include a solution which would have been feasible to our company and most of our customers I decided to develop a software solution which would be feasible for reselling and provide a good topic for my thesis The design and development was more challenging than anticipated but so was the actual report in a thesis form as most of the development was done before the written report All figures tables and appendices were made completely by the author All elements in this report were made with Google Docs Google Sheets and Google Drawing excluding screenshots Due to limitations in Google Docs the final document was rebuilt with Microsoft Word Last but not least Id like to give a loving thank you for my wife Tiina for taking care of our daughter while allowing me to work on this project besides my regular day job This thesis focuses on the design and development of a piece of software which improves the functionality of Google Drive shared folders for organizational use The goal of the project was to design and develop a software solution which monitors and sets file ownerships within a shared folder in a way that accidentally deleted files are easy to restore The software is to be sold as a service The finished product was named HSWDrive The study discusses the use of shared folders in Google Drive Google Drive is an internetbased computer file storage service also known as cloud storage developed by Google Inc It is part of a wider suite of webbased applications called Google Apps which is reasonably priced for companies Google Apps for Work and available free for consumers with a free Gmail account and education facilities Google Apps for Education The current service model was launched on 2012 although the same base has existed since 2010 when it was still known as Google Docs Google Drive is currently used by roughly 240 million active users 1 The basic idea of a cloud storage is that a user has hisher own password protected account which the user uses to add modify or delete hisher personal files With the use of cloud storage the enduser does not have to carry any removable mass media CD  pendrive  hard drive with himher as long as heshe has access to the internet Because internet connectivity has spread so wide in todays world it is usually unnecessary to carry the files with you physically Due to the fact that a user can share a file or folder to other users or user groups in Google Drive it is also possible to use Google Drive as a centralized enterprise data storage To establish a centralized data storage a user must create a folder and share it to all other users which require access to the same files When a user adds files to a shared folder the ownership of the files remain to the user who added the files Otherwise any user could fill up the disk space of another user When a file is deleted only the file owner can restore it from the trash bin Because of this logic a problem arises when someone accidentally deletes a file from the shared folder The original owner has to be found and the owner must restore the file back to the shared folder This could potentially cause a lot of manual work in case a whole file structure has been deleted This challenge can be bypassed by creating a software solution which automatically monitors and logs all the changes done within a Google Drive shared folder By changing the ownership of each file to a single dedicated user account the files are easy to restore from one user interface in case of an accidental deletion Although Google Drive provides a platform to share files within a company there is one major problem in the concept when a file is deleted from a shared folder by someone who is not the original owner of the file it disappears from the shared folder and is left orphaned An orphaned file is a file which has an owner but does not have a location so it can only be found by the owner of the file by conducting a search It cannot be found in any particular folder not even in the trash In case of an accidental deletion of large folder structures this causes a significant problem in restoring the deleted files and folders This has happened to the case company and its customers thus a solution to avoid it in the future is required 2 As to this date the case company has found only one widely recognised commercial service to address this problem AODocs Although AODocs is much more than just a system to prevent accidental deletions the case companies required functionality of AODocs is to convert any Google Drive folder to a controllable file server with advanced options The problem with AODocs is that since they released a new version in the last quarter of 2014 it has not been possible for users to delete or add files to a shared folder thru the Google Drive interface thus forcing the users to use the AODocs interface or a separate browser plugin for adding and deleting files The case company used AODocs for a trial period of 30 days at the end of 2014 and was almost ready to purchase it but after noticing some properties which would have required the endusers to change working habits a questionnaire was conducted within the company to decide whether AODocs was suitable or not for this environment The questionnaire included the following questions Based on the results of the survey 8 users out of 9 answered yes to the first three questions All users preferred Google Chrome as the initial application to access Google Drive but some users also used the Google Drive app on mobile devices and Google Drive synchronization client to have a local copy of the files At this point it was clear that the employees did not like the idea that each and every user in the organization should either install a browser plugin or learn a new system to add modify or delete files in a shared Google Drive folder According to the free comments section of the questionnaire AODocs was taking too much control over the user experience which led to the conclusion that AODocs was not a feasible solution for the case company The software must be easy to implement on other folders and organisations thus making it easy to sell as a service Because this type of functionality requires continuous monitoring of the folders and files the finished software must run on a dedicated server The aim of this project was to design and develop a software solution which monitors and sets file ownerships in a way that accidentally deleted files are easy to restore After the initial requirements development server and Google Developer credentials were obtained the software was designed and developed using the agile software development method and objectoriented PHP language Most of the programming was done with the combination of TextMate and Transmit TextMate was used to write the code and Transmit was used to upload changes to the server Programming was done in various locations mostly in the developers home or workplace due to the fact that software design The Google Developer Console credentials were used to gain access to the Google Drive API Google Drive API was used to send commands and perform queries to Google Drive The actual software was be developed using the PHP language PHP was chosen due to the developers vast experience in it and because Google provides PHP client libraries for the Google Drive API This project was limited to Google Apps for Work and Google Apps for Education environments so it does not work with free Gmail accounts due to limitations of Google Drive API Unlike in AODocs a separate interface will not be created for handling files All file and folder handling and restoration of deleted files can be done directly in Google Drive although a separate file restoration interface was made for easier restoration of large file structures All files saved in the shared folder must be owned by someone within the company of the shared folder Files which are owned by external collaborators were not included in the scope of the software although this might be possible in a future release Section 2 explains more about the technical aspects of Google Drive Google Drive API and the programming language of the developed software Section 3 describes the requirements for the developed software to function Section 4 describes the program logic and includes illustrating tables for the logic and database structure Section 5 describes the evaluation for the finished software Section 6 lists all encountered problems while developing the software Section 7 expresses the current and future possibilities for licensing and distribution of the software and Section 8 includes measurement results and analysis of the finished product and discusses of the possible future insights To better understand the technical aspects of Google Drive and the development of the software solution the used technologies and software logic are described more thoroughly in the following sections Google Drive was introduced in April 2012 even though the core service did exist before this as a file storage system for the Googles webbased office suite The early webbased office suite was called Google Docs and it included functionality for word processing spreadsheets and presentations After the launch of Google Drive these were separated into three individual apps Google Docs Google Sheets and Google Slides respectively Google Drive is used to store and share files and folders in the cloud It is directly integrated to Gmail Googles email system which enables file sharing directly via the email interface The use of Google Drive for file sharing addresses the widely known problem of large email attachments filling mailboxes By only sending a link to the file the recipients mailbox does not get filled by large attachments and the recipient is able to download the attachments on demand The downside of this method is that the file can only be downloaded as long as the sender does not delete the original file By having files in a cloud storage there is no need to carry any physical storage media All changes made to the files are usually viewable by all privileged users and devices within a few seconds delay either from the web interface or in a locally synchronized folder when using a separate synchronization client software A noticeable difference between Google Drive and many other Cloud Storage services is that Google Drive uses tags instead of folders Although files appear to be inside folders they are actually just tagged with the folder names By using tags it is possible to have a file located in various locations at the same time This method is very similar to using hard links in a Unixbased operating system such as Linux The only practical difference between a hard link and tag is that hard links are done in the operating systems file system level while tags are done in a separate database By utilizing tags instead of traditional folders it is possible to save a considerable amount of disk space because there is only one copy of any given file even if it is located in various locations of various users 3 Although having a separate backup system is important it is not that crucial in such as vast system as Google Drive because of the various safety and security implementations All data is replicated between various data centers around the globe and each data center is equipped with emergency power generators which enable data integrity in case of a natural catastrophe The data centers operate on custom built servers which are exclusively built by Google and run a stripped version of the Linux operating system 4 All overwritten files are automatically added to a separate revision history which allows file restoration in case of an accidental change in a file Deleted files are added to the trash bin before final deletion which allows users to restore them in case of an accidental deletion Some files are even recoverable by Google Support and the organization administrator for a few days after emptying the trash bin For extended data integrity it is possible to take data backups from Google Drive to a local computer or a third party cloud storage such as Backupify 5 6 Google Drive was originally intended for web browser use due to its integration with Docs Sheets and Slides As the number of users increased Google released the synchronization client for Windows and Mac OS X The idea of the synchronization client is to have a local copy of all or selected files within Google Drive This way users are able to access files without opening the web browser According to the experience of the case company many users face the problem of not understanding that deleting or moving items from a locally synchronized folder also deletes the items from Google Drive This functionality is one of the main drivers for this project As of today there is no official version of the synchronization client for Linux All files and folders in Google Drive are always owned by someone Even though teams and companies use Google Drive to collaborate within shared folders the files within shared folders are by default always owned by the person who creates or uploads the file Each file consumes disk space only from the owner regardless of the file location The file ownership can be transferred to another user within the same organization by the current owner or programmatically by using Google Drive REST API After a file has been transferred to a new user it will free the consumed disk space from the previous user and consume it from the new userHaving files with different owners in a shared folder can create a problem when a user account is deleted Because the files are owned by the user deleting a user will also delete the files owned by the user within a shared folder Another possible problem arises when a file is deleted from such a shared folder Because the file is owned by some user it can only be found from the owners recycle bin which makes it difficult to restore in a multiuser environment Each Google Drive user has a My Drive folder which is also called the root or root folder Google Drive allows files and folders to be shared and to exist in multiple locations at the same time These locations are called parents in the Google Drive REST API Having files and folders in multiple locations simultaneously is a handy feature but in theory allows a recursive loop A recursive loop occurs when a parent folder is added into its child folder The problem of a recursive loop is that a folder structure is infinite because the child folder includes its parent folder In case a folder structure with a recursive loop would be synchronized as a local copy to the end users computer it would cause an infinite number of items which would rapidly fill the computers hard drive Besides filling the end users hard drive it would also make deleting the folder structure troublesome because most file systems start the deletion process by calculating the number of items to be deleted If the number of items is infinite the calculation process also takes an infinite amount of time to advance thus never completes or causes an error message A graphical representation of a recursive loop can be seen in Figure 1 Each user can also add shared files and folders to the users own root folder for easier access The root folder of each Google Drive is called My Drive Even though a shared folder is owned by someone else each collaborator can add it into any folder under My Drive as long as the user has write access and the location does not cause a recursive loop In case the user does not have write access or tries to create a recursive loop the operation fails and an error message is shown to the user When adding files to a shared folder the file access permissions are inherited from the parent Thus when adding items to a shared folder it is not required to explicitly share them to other users unless the targeted user does not have access to the shared folder Google Drive automatically increments access permissions to any object within a shared folder to match the access permissions of the parent folder Manually added permissions are not removed in this process  In case a user with write access deletes files or folders from a shared location the items will only be removed from the current parent location but not from the owners My Drive which is considered as the root folder If the shared location was the only location for the deleted item it becomes orphaned An orphaned file or folder does not exist in any folder because it does not have any parent locations An orphaned item is not considered to be deleted or trashed it just no longer exists in any location The only way to find an orphaned file or folder is to conduct a search in the Google Drives web interface Finding an orphaned file can be challenging but it becomes even more burdening in case the files owner is unknown The visibility of files and folders in My Drive is illustrated in Figure 2 2 As illustrated in Figure 2 files which reside in an orphaned folder such as File A in Folder A are also potentially hard to find due to the parent folder being orphaned Even though File B1 is situated in the orphaned Folder A it can still be easily found due to its existence in Folder B which is situated in My Drive Although having files directly in the root of My Drive is not considered as best practice for file organization it is still directly visible to the user When someone deletes a singleparented item from a shared folder one of the two situations occurs If the deleting user is the owner of the item the folder will be added to the owners trash from where it is no longer accessible to collaborators in any form before the user restores the item If the deleting user is not the owner of the item the item is deleted from the current location but is still accessible to the owner and other users who are explicitly granted access via manually sharing the item Even though access is still possible it is still as an orphaned file and needs to located by searching via the web interface of Google Drive Google Drive REST API is an essential part of Google Drive SDK which includes all the required commands protocols and tools to interact with Google Drive By having an API developers around the world are able to extend the functionalities of Google Drive The API officially supports the following languages Go Java JavaScript NET Nodejs PHP Python  Ruby Besides these languages it is also possible to use the API with any other programming language as long as the language supports REST calls over HTTPS The downside of using an unsupported language is that there is very little documentation to support development for such languages 7  Google provides up to one billion 1 000 000 000 free Google Drive API requests per project on a daily basis and more can be requested The number of API requests is limited to 50 requests per second per user Most operations are done as the file storage user of an organization Depending on the requirements additional API requests might be prone to financial expenses Google Drive API consists of 13 different resources to access Google Drive functionalities A resource can be thought as an object in terms of programming The available resources are Files About Changes Children Parents Permissions Revisions Apps Comments Replies Properties Channels and Realtime The Files resource includes methods to list view follow create modify and delete files and folders When using the listing method this resource cannot be used directly to list files within a folder The listing method works in a very similar way than using the search bar in the web based Google Drive interface with the addition of having more options such as the ability to limit a search only to Google Photos or reducing the number of returned objects The About resource is very simple in terms of methods because it only includes one method to get information about a user such as the used amount of data in Gmail Google Drive and Google Photos The About resource also includes a vast amount of Google Drive related information such as sharing policies some Google Drive API settings and the file ID for the users root folder also known as My Drive The Changes resource includes methods to get list and watch changes done to files and folders Every time a change has been done in the users Google Drive a new change item is created which includes basic information about the file which has been changed The use of a change log for monitoring file modifications is considerably faster than recursively scanning for each file in a folder structure because only changed files are listed The Children resource includes methods to list remove and add children objects to and from a folder To use the Children resource a file ID of a parent folder must be given before being able to use this resource Also folders are considered as files in the Google Drive API thus folders also have a file ID This is the most used resource in this project as it is required to list files within folders while checking for user ownerships This resource cannot be used to delete or trash files or folders When deleting a child object it only deletes the object from the selected parent To actually delete a file or folder the Files resource must be used The Parents resource provides methods to list add and remove parents locations of any item Like the Children resource this resource cannot be used to actually delete files or folders It is only used to add or remove a parent from any child object This resource is utilized as part of checking if a file is situated within a shared folder The Permissions resource includes methods to list modify and remove permissions from items Each permission setting for every item has its own permission ID The permission resource defines the permission role of an item In case of modifying existing permissions for a file or folder the current permission ID for the given item and user combination must be obtained by listing the permission objects for a given file In case of changing the ownership of an item the user needs to have a permission object assigned to the given item If the user does not have any permission objects assigned to a given item one needs to be created before the user can be converted as the owner of the given file This resource is one of the most used resource of this project as the primary focus is to centralize the ownership of files to one storage user The Revisions resource includes methods to list modify and delete revisions of a file Revisions are stored automatically for each file by Google Drive when a file has been updated Although the possibility to restore file contents may be an option in the future this project does not currently focus in restoring file contents thus this resource is not utilized at all The Apps resource includes methods to list and view apps which have been enabled in the users Google Drive As this project is not focused in listing for existing external Google Apps it is not used at all The Comments resource includes methods to list modify add and delete comments to file created in Google Docs Google Sheets or Google Slides Comments are not currently supported in other types of files As this project does not include functionalities to view or alter comments in separate files it is not used at all The Replies resource includes methods to list modify add and delete replies to comments in file created with Google Docs Google Sheets or Google Slides As this project does not include functionalities to view or alter comments in separate files it is not used at all The Properties resource includes methods to list create modify and delete custom properties for drives stored in Google Drive The properties can be public to all apps or private to just one app As this project does not focus in file properties of separate files it is not used at all The Channels resource is a resource which is created when a File resource is set to be followed with the Watch method The Channels resource can be stopped with the Stop method thus ending the watching or following of a file Due to requirement of following all items within a given structure this resource does not have any practical use in the current project The Realtime resource includes methods to get and update realtime API models which are associated to a selected file It can be used to monitor changes to a file in realtime thus allowing external applications to reflect changes The Google Drive REST API uses mainly OAuth 20 for authorization of the software against Googles servers OAuth 20 is the successor of Oauth 10 and an open standard for allowing secure delegation of data resources The main idea of Oauth is to allow delegated access to only some part of resources not to everything 8 It is used for access authorization by many notable cloud service providers such as Google Microsoft Amazon and Dropbox 9 To establish a successful connection between the software and Google Apps instance a client ID and client secret must be generated within the Google Development console The client ID must be granted appropriate permissions for Google Drive on each Google Apps instance which will be hosting a shared folder The permissions are granted from the Google Apps administration console by an administrator of the Google Apps instance Once the client ID has been created and permissions are granted the software is able to authenticate and gain authorization to Googles servers by using Oauth 20 The actual authentication and authorization and communication between the software and Googles servers is usually done in the following steps First the software requests an access token from Google The access token is received upon a successful authentication A successful authentication is achieved by using user consent In this example user consent is achieved with the client ID and client secret which were generated in the Google Development Console Assuming the authentication was successful the software receives a session object and an access token which is valid for a limited time The session object is used as a proof of authentication by the software when sending queries and commands to Googles servers The access token is used to request a new token before the original token expires 10 PHP is a widely used serverside scripting language which was officially introduced in 1995 by Mr Rasmus Lerdorf PHP started as a simple procedural language but it has been further developed to an advanced objectoriented language 11 The latest version branch of PHP is 56 12  Even though PHP is an objectoriented language it is still not categorised as a programming language due to the fact that PHP applications are not usually compiled to bytecode PHP applications are typically processed directly on a server by a PHP interpreter  MySQL is a relational database management system RDBMS owned by Oracle Corporation MySQL was initially released in 1995 by MySQL AB MySQL is the worlds second most used relational database management system and the most used opensource relational database management system The latest stable version as of September 30th 2015 is 5627 13  MySQL is widely used in website development but it can also be used for other applications such as serverside applications and even as a local database for individual applications Since Oracle bought MySQL in 2008 a new opensource fork of MySQL was created known as MariaDB to address concerns about keeping MySQL free and under the GNUGPL license The intent of MariaDB is to maintain a high compatibility with MySQL for easier transition in case the development of a free MySQL is seized MariaDBs current lead developer is Michael Widenius who was one of the founders of MySQL AB Based on the findings of the inquiry done in the case company at the end of 2014 most users preferred Google Chrome as the application to access the web interface of Google Drive Some users also used mobile apps and the Google Drive synchronization client to have a local copy of all files within a shared Google Drive folder Based on this information the developed software needs to work discreetly without affecting current working habits or applications File restoration should be easy enough for regular office workers although restoration should be done by an experienced administrator for best results The requirements for the development environment are not high because the software is initially used only to monitor a very limited number of shared folders within Google Drive The official requirements of the selected operating system Debian GNULinux are also so minor that any modern PC is sufficient for development purposes 13 While the development environment does not need to be powerful the production environment on the other hand needs to be powerful enough to provide a good user experience Initial benchmarks Appendix 1 indicate that the monitoring of one shared folder consumes an average of 3 to 6  of CPU resources on a typical virtual machine with 1 vCPU and 4 GBs of RAM This result was measured by recursively listing all files within a shared folder After changing the softwares logic from recursively listing items directories into a changebased scanning the CPU consumption was reduced to a stable 3  of CPU usage Appendix 2 The changebased scanning lists only files which have been changed since the last scan With this change the number of required files to list was reduced exponentially thus reducing the amount of consumed resources Based on these measurements one relatively slow virtual machine is able to withstand the monitoring of at least 32 concurrent shared folders within Google Drive This assumption was made on the basis that each new shared folder to monitor would cause an average of 3 percentage points increase in CPU usage By multiplying 3 percentage points by 32 a 96  CPU usage is achieved The actual consumption should be considerably lower due to the fact that the servers operating system itself could consume up to 2  of CPU while on standby without any shared folders to monitor due to automatic updates indexing and connectivity checks A more specific measurement can be achieved with a larger number of folders to monitor The CPU consumption is directly linked to the number of API calls API requests in Appendix 1 and API Response in Appendix 2 done via Google Drive API as can be conducted by comparing the CPU usage to the number of API calls per second The software requirements to run the developed software can be divided roughly in two sections serverside requirements and clientside requirements Serverside requirements must be met for the software to be able to work in the server Clientside requirements are to be met for the user interface to work for the enduser who intends to restore file structures to an earlier state As for the serverside requirements Google Drive API requires Google Developer Console authorization credentials to function which can be obtained from Google Developers Console The Google Drive API PHP client library requires at least PHP version 53 to function and the user interface requires at least Apache 20 to function The targeted development and production environment is Debian GNULinux wheezy which already includes Apache 22 and PHP version 54 so these requirements are automatically fulfilled just by choosing this operating system 14 The clientside requirements are exactly same as using Google Drive itself because all file controlling is made within Google Drive or with a separate file restoration interface which originates partly from Google Drive Google Drive requirements include any of the following web browsers Chrome Firefox Internet Explorer Safari only on a Mac as long as the browser is the newest or second newest version release 15 All interaction between the software and Google Drive REST API is done via regular HTTPS requests over port 443 so there is no major requirements for the internet connection speed or firewall settings Due to the potentially vast number of requests per shared folder instance the connection needs to be stable and able to withstand multiple requests within a small timeframe Initial measurements Appendix 1 indicated that the monitoring of one shared folder handles about 2 API requests per second and requires about 012 Mbs of download and about 0035 Mbs of upload speed to interact seamlessly with the Google Drive REST API After changing the software logic from a full recursive scan to a changebased scan the number of API requests was significantly reduced to about 02 requests per second on average A full recursive scan was named as full synchronization and a changebased scan was named as delta synchronization Appendix 2 The requirements for this project consist of three main categories First the hardware requirements for the server which runs the software application must be fulfilled Second software requirements must be fulfilled to provide the necessary platform for programming and using the file restoration interface Third running the software and connectivity requirements must be fulfilled to provide a consistent quality of service for the software Debian GNULinux was chosen as the operating system for the server platform According to the official system requirements guide of Debian GNULinux any modern computer would satisfy the basic hardware requirements Software requirements are divided in two sections serverside requirements and clientside requirements Serverside requirements are automatically met by having the latest version of Debian GNULinux and client side requirements are automatically met if the endusers are able to access Google Drive via a web browser since the restoration interface is only plain HTML with the exception of the instance creation interface which uses Google Drive file picker for selecting a folder to be monitored Because the measured network usage was very low connectivity requirements are quite modest in terms of speed Due to this any modern broadband connection should suffice More important than the connection speed would be the reliability of the connection Long connection outages might lead into a situation where a file has been created and deleted before it has been noticed by the software Because the software is intended to run quickly and serve multiple customers reliably 365 days a year all the minimum requirements must be exceeded with significantly better hardware and network connections While considering data security aspects it is also a best practice to use the latest stable release of required software services and tools This section begins by explaining the methodology used to design and develop the software After the methodology section the technical details are explained more thoroughly The project started with the knowledge of a problem with Google Drives shared folders in organizational use The first step was to find out of any already existing commercial products which could resolve the current problem According to a conducted questionnaire in the case company the only commercial product was not considered feasible The questionnaire provided a good starting point for the design of the requirements for a new software to overcome the encountered problem The software was designed and developed in various locations and with various devices in a timeframe of nine months As the agile development method was chosen the initial design did not include too strict guidelines on how the software should act in different situations While developing the software design plans were constantly changed to reflect the encountered problems As a starting point the project required a dedicated server with a PHP runtime environment a MySQL database a Google Apps for Work account and Google Developer credentials A virtual server with the product name of n1standard1 was bought as a Google Cloud Computing service The n1standard1 was the cheapest standard typed virtual server from Google which was still considered to be more than enough for the software to function in a pilot phase The required runtimes and databases were installed on the server separately The required Google Apps for Work account and Google Developer credentials were obtained from Google before the project started Due to the developers many years of experience with the PHP language it was chosen as the language to develop the software Although the developer had a considerable amount of knowledge in PHP some additional information had to be studied via the official PHP website during the development As PHP is categorized as a scripting language it did not require any code compilation and all changes were updated in realtime The developer did not have any previous experience with the Google Drive REST API so information regarding this had to be gathered from the Google Drive REST API reference page The software was developed to run on the server as scheduled task so it does not need any user interaction The evaluation of this project was done by simulating reallife situations of file and folder deletions while trying to restore folder structures after a deliberate deletion The ease and speed of file restorations were the primary metrics of the evaluation for this project Both metrics were measured by assigning endusers of pilot companies the task of restoring files which were deleted from a shared folder The evaluation methods of the finished product are described in the Solution Evaluation section User authentication and authorization process was developed by utilizing Googles Users PHP API This means that for a user to access the system the user has to be in the super administrators group of the respective organization The authentication process was developed to function in the following way First the user fills a registration form to sign up for the service After registering the user receives a followup email with instructions on how to permit the software access to the users Google Apps domain After the user has permitted access to the software the user is able to login to the softwares user interface The user interfaces login page does not process any credentials but redirects the user to a Googles login page When the user logs in to hishers Google Account the user will be redirected back to the softwares user interface as an authorized user All traffic between the endusers web browser and the developed software is processed via a secure HTTPS HTTP over SSL connection with the TLS 12 protocol which is the same technology as used in most commercial online banking systems The user interface was designed to allow Google Apps for Work administrators a oneclick restoration of a file structure to a given time The interface was programmed to allow access only to registered users with the Super Administrator role for their Google Apps for Work domain Once the user has registered to HSWDrive the user must login to the system via Googles authentication system Googles authentication system provides HSWDrive the users administrative role which is used to verify access privileges Authorized users are given the following options Create a new shared folder instance modify the organizations current instances and restore an existing instance to a certain point of time The date restoration interface only restores item locations not the actual data In case the content of a file needs to be restored to a certain date Google Drives builtin revision control can be used Items are also never removed even though a restoration would be done to a time when a certain item did not exist Table 1 illustrates the previous statements in a more understandable way As designed the restoration interface did not include anything else than a item listing for the selected date and current date a date selector and a restoration button The restoration button was implemented with a confirmation alert to prevent accidental restorations Figure 3 illustrates the file restoration interface in practice As can be seen in Figure 3 the logout link includes the email address of the currently logged user The email address is directly linked to the currently logged Google Apps user and in case the user wishes to login again a new authentication must be done against Googles authentication system The lefthand side column displays the current status of the selected folder while the righthand side column displays the status on the given date The main purpose of the designed software is to change the ownership of all files within a shared folder to one centralized user for easier file restoration Once the files are owned by one centralized user the files cannot be permanently deleted by other users as explained in the technology behind Google Drive and the developed software section The centralized user is called as the data storage user Due to the nature of Google Drive API various steps had to be taken in order to change the owner of an item within a shared folder Because the ownership of an item cannot be changed centrally by one administrative user the software has to act on behalf of the previous owner to grant ownership to a new user Figure 4 illustrates how the owner changing process was achieved in the developed software Based on the flowchart in Figure 4 the first step is to list all permission objects for the given item A permission object is an instance of the Permission resource which was explained in the Google Drive REST API section Once the list of permission objects has been obtained the software looks for the permission object with the owner role Based on the found object the software is able to detect the username of the files owner If the file is owned by an external organization or the storage user is already the current owner the file is bypassed After obtaining the owners username the software checks if the storage user already has a permission object the ID for that object is stored for future use In case the storage user does not have a permission object the software creates a permission object for the storage user and stores the ID for the newly created object Once the software has the permission object for the current and future user the software creates a new Google Drive service as the previous owner and transfers the ownership to the storage user Most of the programming was done with the PHP language in combination with a MySQL database for file status recording According to the original plan a PHP based programming framework called Symfony2 was intended to be used for the project As the software logic advanced it seemed Symfony2 would have been too resource consuming for such software Thus plain PHP was finally selected to be the programming language for this project Some sections of the user interface required the use of JavaScript As the project advanced it was clear that the original plan of using at least some sort of PHP framework would have been better in terms of manageability All functions were achieved but as a result of using plain PHP the software became more complex to update in the future If the software becomes a success in terms of sales a next logical step would be to rewrite most of the code in some PHP framework to support easier updates in the future MySQL was selected as the database engine The database consists of four separate tables The first table includes a list of all instances An instance is basically just a row of information consisting of the basic information of the centralized data user company information file ID for the shared folder and file IDs for the lost files folders The second table consists of the status of files in the shared folder This table includes the basic information of each file such as the file name instance ID and last update time The third table is structurally identical to the file table but only includes the history of old files Every time a file is being updated the third table shows the status history of each file as can be deducted from the table below The fourth table consists of registered users Only registered users are allowed to add modify and restore instances to an earlier stage Only registered users are allowed to add modify and restore instances to an earlier stage Each row in the instances table represents a shared folder for a single organization The shared folder may have numerous subfolders and files The instances table also defines the location for the files with the status of Escaped Released or Orphaned which are all moved under respectively named folders in the Lost and found folder for that instance The file tables and instance table are connected to each other with the instance ID parameter by using the builtin relation model of MySQL InnoDB This relation is indicated with an asterisk character in Table 2 The main idea of this database scheme is to have all the up to date file information in one table and another table is used only for logging purposes By having a link to the instance ID if is easy to query for files which belong to a single instance Each row in the users table includes basic information about the user The isAdmin column of the user table describes whether the user has full administrative privileges for the whole system Users with full administrative privileges are automatically granted full access to all organizations instances Unlike most database systems there is no password column in the users table The reason for this is that authentication is done via Googles Users PHP API Login passwords are never stored or handled within HSWDrive Each item file or folder in the database has a status Due to the fact that one item can reside in multiple locations within Google Drive one item might be listed multiple times in the database if the item is in multiple shared folders that are monitored by HSWDrive The items are separated by an instance ID which indicates to the folder that is being monitored Possible item statuses are ok deleted orphaned escaped released trashed or external Table 3 explains thoroughly what each  The item is accessible by the centralized data user but ownership has been given away and the item is no longer in the shared folder This type of item is automatically added to the released folder As stated previously all changes to items are scanned once every minute Each scan checks the following for all items Is the item the root folder of the shared folder has the item been deleted does the item exist in the database what are the current parents are the parents currently within the shared folder is the item owned by the storage user is the file owned by the same Google Apps organization is the file trashed have the parents changed and has the items name changed To decide what to do to a file in case of a detected change the table of Appendix 3 is used to determine the next action and file status The software was named as HSWDrive It was developed to function in the following steps First a company authorizes the software to have access to its Google Drive Next the company decides and provides the software a shared folder and a centralized data user account which will perform as the central data storage owner In the third step the software performs a recursive scan for the whole shared folder The software lists all files and folders stored in the shared folder and adds or updates the files in a database table While scanning the files the software changes the ownership of each file to match the centralized data user account Only files owned by someone in the same organization will be modified After these two steps are done the software starts to monitor all changes done within the shared folder From this point on it will perform a full scan only once daily but individual changes are monitored every minute In a perfect world the monitoring alone would be enough but due to possible errors in file ownership updates or network connectivity issues a daily scan was scheduled to overcome possible update errors The endproduct is a fullyfunctional serverside software which runs as scheduled tasks via crontab The software is programmed with the PHP language and it uses the Google Drive API for interaction with Google Drive The stages of development are roughly in the following steps First the prerequisites software hardware credentials must be met second follows the initial development and deployment third comes piloting and feedback and last comes the final release The finished product is sold as a service so the customer never gets any executable files or code The software consists of two separate main components to handle files Delta synchronization and full sync The delta synchronization component only checks for changes done to files in the given folder since the last change The full synchronization component runs a recursive scan of all files and folders within the shared folder A delta synchronization is run every minute while a full synchronization is run only once daily The reason for this division is that neither of the components alone are effective alone A delta synchronization is very quick to run because it detects only the last changes On the other hand in case of a slight network failure in the delta sync some files might not be processed The full synchronization is less prone to miss files in case of a slight network error but it might take a very long time to scan a large file structure Another downside of a full synchronization is that it consumes a lot more network and processor resources due to scanning each and every single file in the folder structure Due to the increasing number of shared folder instances two minor components were made to run all folder instance synchronizations simultaneously The main benefit of running a synchronization for all folders simultaneously is the noticeable increase of speed compared to running the synchronization on all folders consecutively Besides the finished product a deployment guide was made for administrators to easily setup HSWDrive on their organization This was done to make deployment as easy as possible considering not being able to use Google Apps Marketplace which would have been the easiest solution Due to time constraints the deployment guide was considered to be more worth the effort than submitting a beta staged software publicly to Google Apps Marketplace The product quality was evaluated in terms of functionality ease of deployment ease of use and speed of item restoration in case of an accidental file or file structure deletion The functionality evaluation was considered as successful if the software is able to keep track of all files and file changes within a shared folder Besides being able to keep track of files and changes the software must also be able to function quickly to provide a fluent user experience A delay of 5 minutes was chosen to be the maximum accepted time for a change to be recorded within normal use although the expected average delay is less than one minute These could be tested by making changes to items in a shared folder and confirming that the changes are registered within the softwares database Ease of deployment was measured by providing the customer administrators a deployment guide and observing if the customer is able to deploy the software to their organization with minimal help Ease of use and speed of item restoration was measured by asking regular users to deliberately delete files and attempt restoration via the HSWDrive file restoration interface This type of evaluation was considered to be the only way to concretely see if the software is reliable fast and easy enough for production use with future customers Besides evaluating the basic requirements of the solution practical testing was done in the following categories resource consumption speed of use and ease of use with a total of 11 shared folders from various organizations The solution was stress tested for resource usage by deploying various shared folders from various organizations for pilot use After the deployment resource usage was monitored via the servers own reporting tools and Google Developer Console The detailed measurements can be found in the Results and Analysis section Speed was monitored by adding items in a shared folder and calculating the time that the software requires to process the item The file processing speed was observed afterwards by using Google Drives activity pane for the selected items This method provided a reliable way to measure speed without having to compare update time between Ease of use was tested in two sections Implementation and usage The implementation section was tested by providing the administrators of customer organizations a deployment guide and testing if the customers were able to deploy the software by themselves The usage section was tested by giving the pilot customers a tasks to recover files by using the restoration interface If the customer was able to implement the software with minimal help the requirement for ease of deployment was considered as a success If a user was able to restore files to any earlier location with minimal help the requirement for ease of use was considered a success A separate quick guide Appendix 4 was given to the endusers to help in the deployment and recovery process As most software development projects also this project had problems which were not anticipated in the design phase This section describes the encountered problems and how they were resolved While testing the functionality of the first working version of the software a high amount of system resource usage was measured particularly in CPU network and disk usage Appendix 1 While this did not cause a major problem in the speed or reliability of the software with one case company it would have had a greater impact in an environment of multiple companies and shared folders Because the software is aimed to be sold as a service it is expected to handle various instances within one physical server without having noticeable latency or reliability issues After a logic survey of the software the cause of high resource usage was pinpointed to the logic of recursive file scanning As the initial version of the software used to scan through all files within a shared folder every hour it caused a major resource usage peak every hour Due to the large number of files within the case companies shared folder this scan took almost an hour for each scan To overcome this problem a change in program logic was implemented in the following way Instead of scanning through all the files every hour only the changes made to files within a shared folder were monitored after the initial recursive scan This was possible by using the Changes resource provided by the Google Drive REST API With this logic all changes could be detected each minute and still the average resource usage would was measured to be only 10  compared to the original resource usage Appendix 2 Because the Google Drive API PHP Client library is updated very frequently the documentation provided by Google did not always comply with the actual functions of the client library This caused some significant problems in making the software work correctly These problems were exceeded by investigating the source code of the client library Requests for correlation of the documentation were sent to Google The final decision of choosing not to use Symfony2 framework unlike originally planned caused a significant increase in programming time because many functions had to be rewritten in the event of a change to the database structure When this deficiency was noticed the software was already developed so far that moving it to a framework was not considered to be worth the effort The software was finished without a framework but if time and budget allows it will be rewritten within Symfony2 framework in the future While starting the project it was not obvious that even seemingly simple operations such as changing the owner of a file required multiple phases to achieve Even though the software can be granted full administrative privileges to an organizations Google Drive each operation is done subjectively with a user account This means that instead of changing the owner of a file needs to be done as the current owner As an example It was not possible to just define a new owner to a file with one simple command To change the owner of a file the first operation was to list all permission objects for the file After obtaining a list of permission objects each object had to be scanned for the username and permission level While scanning the permission objects the current owner had to be selected to confirm if the file was already owned by the storage user In case it was not the second phase was to check if the storage user had any permission object at all In case the storage user did not have any permission object one needed to be created Once the current owner was found and the storage user had a permission object the ownership could finally be transferred from the owners permission object to the storage users permission object The transfer had to be done as the previous owner Another complexity aspect of Google Drive API was that all items are considered as files Understanding that within Google Drive folders are actually just empty files with children objects was a new revelation For simplicity item was selected as the word to represent any file or folder in this thesis Due to the fact that one item can coexist in multiple folders it was required to understand the concept of parent objects and children objects Parent objects can be thought as the locations for the item while child objects can be thought as files within a folder Naturally only folder objects can have children objects After having the software online for a few days some pilot companies reported on lost files which could only be found by some users in the respective organization These reports led to an investigation which indicated that although the lost items were in the corresponding folder they would not be visible to some users The users who experienced these problems had sufficient privileges to the respective folder but still some files would appear have vanished The underlying reason for this has been unknown up to this date but a workaround has been implemented to overcome this problem The workaround to overcome the issue was to reshare the folder to the users who are experiencing problems After the folder was reshared no further complaints were received by the users This section covers basic information about possible licensing terms and distribution methods for the developed software solution Even though licensing terms and distribution methods were not the focus of this project they are relevant or future reference As mentioned in the previous chapters the software is made to be sold as a service so a separate license agreement is not made between the buyer and the seller On the other hand the customer is required to accept a service contract which describes the content and price of the subscription The pricing and contract terms are not the focus of this thesis so they will not be discussed any further The initial target group consists of companies which have bought Google Apps for Work from the case company Due to this target group most agreements are initially done via telephone or email while the distribution is done remotely as a service After the software has been distributed to the initial target group it will be targeted to other companies in the future Although the commercial use of the developed software is not the main focus of this thesis it is still a focus point of the future Once the software has been successfully tested for at least 6 months on various test companies it can be considered to be a ready for commercial use To truly make it commercial it has to be published in Google Apps Marketplace for easier distribution The following procedure must be done in order to publish the app First a payment system needs to be integrated to the registration form This can be done with PayPal or another similar payment system After that it is required to register as a developer to Google Apps Marketplace Registering requires a 5 USD payment to Google After registering the second phase is to take screenshots of the software and create a manifest file The manifest file should include basic information about the software such as the name description and icons After a manifest file has been created and screenshots are taken the third phase is to submit the manifest file screenshots and application URLs to Google Developer Dashboard After submitting the app the fourth phase is to send a request for listing to Google Apps Marketplace by filling the Google Apps Marketplace Listing Review Request form Once the form has been submitted Google reviews the software and decides whether or not it is acceptable for publishing If the software is considered as acceptable the fifth phase is to test the installation of the software via Google Apps Marketplace If everything works as intended the final phase is to start marketing the software via various internetbased channels 14 Although the software itself is intended to be sold as a service for monetary gain the source code may be published with the GNU General Public License GNU GPL or similar license thus granting other developers individuals and organizations the freedom to use modify and copy the source code There are numerous reasons why this is considered as a good practice First all the developers programming knowledge has been gained from free resources including most of the API resources and code snippets Based on this it would only be fair to give something back Second publishing the source code would allow other developers to provide feedback and recommendations to improve the code Third publishing the source code could also function as public demonstration of excellence thus potentially provide work opportunities in the future Finally as the software is intended to be run on a server if is very unlikely that small business organizations would have the resources or knowledge for deploying the software while larger business organizations usually tend to avoid open source solutions without a guarantee of support If GNU GPL is chosen as the license model for the software each source code file should include a preamble indicating that the source code is under the GNU GPL license For the time being no decisions have been made regarding the licensing of the software This decision will be made in the future when the software has been further developed and implemented to a larger scale of customers This section discusses the results and outcome of the finished software solution The amount of work and resource consumption is the main focus while the summary section includes the final measurements The development of the software was done within 9 months which was within the set timeframe and did not cause any delays to other work matters The software had a total of 4194 lines of PHP code and 865 lines of CSS code for the user interface The original assumption of required PHP code was less than 1000 lines so the amount of implementation work was exceeded multiple times The main reason for the vast amount of code was clearly because of not using a PHP framework which would have minimized the required amount of manually written code If the original plan of using Symfony2 Framework had been followed most of the manually made PHP methods could have been automized and database structure changes could have been updated from one central location instead of updating all functions manually in the case of a change in the database structure The software was measured to function with very minimal resource consumption By increasing the number of shared folders to be monitored from one to eleven the only noticeable resource increase was observed in the CPU usage As indicated in Figure 5 the CPU usage with one folder was measured to be approximately 3  increasing the number of folders to eleven increased the CPU usage to approximately 17  This indicates that the server should withstand at least 32 concurrent folders while still having a 50  CPU consumption average on the current virtual machine Taking in account that the used virtual machine is the slowest standard typed virtual machine within the Google Cloud Computing Platform it is just a matter of upgrading to a faster virtual machine in case more processing power is required By observing Figure 6 it is clear that even with 11 shared folders to monitor network and disk usage activity had almost zero impact to the server with less than 50 Kbps network usage and 30 KBps disk usage on an average Some traffic spikes were observed but even those had very minimal constraint to the server The only significant increase of resource consumption was measured as the number of API requests done to the Google Drive API as can be observed in Figure 7 While the first version of the software consumed an average of 2 requests per second for one shared folder the improved version with a changebased scan only consumed an average of 02 requests per second for one shared folder The measurement of 11 shared folders consumed an average of 7 requests per second which increases that the average of one folder would be 064 requests per second This sudden increase of requests can be explained with the fact that most of the latest customers had just migrated to Google Drive thus adding new files on a daily basis Once the new customers have finished their migration the average number of requests should be considerably lower When considering that the maximum number of daily API requests is one billion 1 000 000 000 there is still capacity for 1000 shared folders even with the measured peak value of over almost one million daily requests The success rate of requests was measured as 100  which indicates in a flawless network connection and in the adequacy of quota for API requests As the requirements indicated that the software had to be easy to use various tests were performed by giving customers the task to deploy an instance and restore deleted items According to the tests all 5 customer administrators were able to restore deleted files successfully The deployment phase was considered as more difficult to most customers due to the advanced setting changes within Google Apps Administrator Console Only 2 users out of 5 users were able to deploy the software by themselves Taking in consideration that most of the customers were not technically experienced this was still considered as a success in terms of deployment easiness Once the software is deployed in Google Apps Marketplace the deployment will be considerably easier for users with a less technical background While building the solution many new aspects of Google Drive were learned including many which are not obvious to the regular user Before the project was started it was quite unclear how Google Drive handles file deletions in shared folders Even with the knowledge of Google Drives file handling logic it might still be challenging to explain the operations logic to regular users in a clear way due to its ownerbased file system In the beginning of the project Google Drive seemed like any other file system where files can be listed copied and modified by simple commands As the development advanced it was clear that the sophistication and design of Google Drive was done in a very different way since each operation to a file required multiple phases to accomplish the desired outcome Most of the functions were combined to PHP methods which made the operations easier in the future but the amount of work required to build these functions was far greater than was anticipated while starting the project One of the new discoveries was the fact that a file which is not deleted but inaccessible to a user is considered to be deleted in the Google Drives point of view A software solution was designed and developed according to the initial requirements The software was named as HSWDrive and evaluated successfully with various pilot customers The software runs a full synchronization to all HSWDrive instances on a daily basis and a delta synchronization every minute File structure restorations were tested and proven to work as designed Even though the software has been proven to work as expected with a set of customers further analysis and bug tracking is required before publishing it in Google Apps Marketplace for wide distribution The software is already in active use by the case company and many of its customers Further development for new features such as file revision restoration and support for externally owned files is scheduled for the near future The aim of this project was to design and develop a software solution to allow easy restoration of accidentally deleted files in a Google Drive shared folder The software was designed finished tested and deployed to various pilot customers within 9 months which was considered to be within an acceptable timeframe The development would have been faster with a PHP framework but on the other hand it probably would have had consumed more system resources If time allows the software will be rewritten in a PHP framework in the future The software was a success in terms of functionality and demand as it was proven to function fluently on various pilot companies Ease of use was also measured to be satisfactory as all pilot users were able to restore files and 40  were able to deploy the software independently Some technical issues were discovered during initial deployment but once these were overcome no significant deficiencies were observed The software is still in a testing phase but after a more comprehensive and successful testing period of at least 6 months the software can be submitted to Google Apps Marketplace for easier deployment and sales purposes A future release of the software will include the possibility to restore files not only to earlier locations but also earlier states Due to Google Drives automatic revision history this can be achieved without having to store file data in the softwares own database Another functionality to be developed in the future is the possibility to restore files owned by another organization Currently this is not allowed because HSWDrive only handles files owned by the same organization and file ownerships cannot be transferred between organizations due to Google Drives limitations Even though it is not possible to take ownership of such files it is still possible to monitor and log the files in case of an accidental deletion The research started in 2012 to study current business needs of the company and how the Business management tools and methods are helping to address the needs the research began with a study about the best practices and current practices in using agile methods and tools in different teams The main objective of the research was to analyse the current practices in the teams for project planning and execution and identify the ways that are needed to make the visibility of the project situation clear to all the stakeholders in the project and with a purpose to reduce delays in the project While the research moved on with the study and discussion with many individuals in the team it turned up to be a great opportunity to learn the business management methods and tools from the information shared in the discussions Also with several years of experience in the industry in different kind of roles I had an opportunity to express and relate the best ways of using the tools and methods which would help any individual in software development organisations similar to the one in the case study The research should give a fairly good view of best practices in Agile methods and tools and what is missing currently in practices correcting the missing practices could reduce delay in meeting the project deadline It brings in a clear picture of combining the latest agile methods and tools to address the current needs of the industry The solution provided will be helpful for better utilisation of tools and stay updated with the current situation of the projects at any time This research would have not been possible without many individuals whom I wish to give my sincere thanks and to mention my mother who travelled to Finland from India on time when I needed her help to go for studies and my supervisors Jukka Kainulainen and Thomas Rohweder The aim of the present thesis was to study the current ways of using agile methods and practices in the case company and find the ways to improve the current practices in order to have better visibility of the project status Accept360 is the tool used for project planning and execution in the case company hence the study concentrated on understanding the current practices in using the tool and finding out the gaps that exists in the current practices in comparison with the best practices of using the Agile methods and tools Agile approaches are used in software development to help businesses respond to unpredictability To address the uncertainties and have good visibility of the project status the agile methods provide rules and a standard ways of usage which would help the project stakeholders to stay updated with the project status In the research the best practices in agile method of software development were studied first and some of the best practices and methods that would best suit the organisation were identified Next the current practices were studied in the selected teams in the case organisation by conducting contact interviews with the key stakeholders from which the researcher collected information about the current ways of using the agile methods and tools The interviews also gave information about the opinions about the tool which is being used currently for project planning and execution how the tool addressed the current needs for project planning and execution and what were the suggestions from the key stakeholders for improving the practices in the tool and methods The data collected about the current practices was then compared with the best practices and the gaps between them were identified from which the proposals to improve the visibility of the project situation were created The study resulted in three key theme proposals The first theme is to follow the already agreed practices and guidelines which is missing currently The second theme is using the Agile Software Product Management method and the third theme is implementing Scrumban practice for project planning and execution The last two themes mainly recommended ways for having a separation between requirement and engineering management The proposals were found to be very useful input for further process improvements to have better visibility of the project status and address the current key issues in the case organisation Earlier Nokia Oy comprised following business groups Smart Devices Mobile Phones Locations and Commerce and Nokia Siemens Networks Later Nokias Smart device and Mobile Phones groups were then bought by Microsoft and merged into one to create a new group Microsoft Mobile Oy ever since the merger over the past three years the group has created incredible results awardwinning phones and amazing services that have made Nokia Windows Phones the fastestgrowing smartphones in the world In Microsoft Mobile Oy Applications Software teams which belong to Smart Devices unit are the teams which creates valuable and innovative solutions that adds value to the smartphones Projects executed by the teams in Applications Software Team require dynamic changes and updates in requirements always Projects are executed in challenging situations where the market is highly competitive and customer needs are changing rapidly in order to meet such demands and supply the best products for the customers and also to deliver products with high standard and quality it is required to make sure that the projects are planned and executed with high productivity and good utilisation of the resources available also it is required that the project situation is visible to all the audience of the project at any point of time so that the needs for executing the project successfully are meet as early as possible and there is minimum delay in delivering the project Even after having a welldefined processes and tools for project planning management and execution there are delays in some projects Hence there is a good need for evaluation of how the project planning tool and methods that are used in the projects and how efficiently the tools are used to track the progress of the project The main objective of this thesis is to study the best practices using the agile methods and tools in the company Accept360 is being used mainly for project planning and tracking the current way of usage of Accept360 tool need to be analyzed and the gaps that exist between best practices and the current practices need to be identified and solutions that could fill this gaps need to be researched Research question of this thesis is How the visibility of the project situation could be kept updated at any point of time What are the best ways to stay on track of the current situation of the projects and so be able to manage the issues appearing earlier and minimize the delivery slippages The outcome of the study is to propose the solutions which include guidelines for the efficient usage of best practices artefacts tools and methods sample process framework that would address the needs of the projects and that are needed to help everyone involved in the project to have a clear visibility of the project status This chapter explains research approach this includes the steps that were done to identify the research problem analyse the current state and the steps that were done to find solution for the research problem As first step the details about the case company was studied as a part of the course on Business management methods and tools it included complete study of the companys strategy mission visions goals current growth needs and KPI measurements In the study it was found that the current importance for the company is to minimise the delays in delivering the software for which the visibility of the project status plays a key role unless the needs of the project is visible to all the stack holders involved  at any point of time it is not possible to address them earlier In ASW team Accept360 tool is used as the key tool for tracking and executing the projects hence a research on how the tool is currently being used in the teams does the tools address all the needs of the projects is there any improvements needed in currents way of working in the teams needed to have better visibility of the project situation sounded a good topic of research for the thesis Hence a study about the Visibility of the project situation was decided as topic of the research A flowchart in Figure 1 was created with steps for finding the solution for research problem As second steps the research topic was discussed with the senior manager in the company and it was considered that this topic of research would yield a good review and guide for identifying the gaps that exists in the currents practices of usage of the tool and methods for Project Planning and Execution and the objective of the thesis was set Since Project planning and tracking is done using the Accept360 tool study of the tool was planned first  Initially couple of meetings were planned to discuss the available information about Accept360 tool and how the research need to be done During the first meeting the guidelines available for the Accept360 tool were discussed and previous presentations that were used to provide guidelines for the teams for using the Acccept360 tools were gone through In the second meeting a short walk through of contents in the Accept360 for some of the projects were viewed and discussed Then identification of the teams and the group of people to be contacted for current state analysis was done Next identification of a team that is using Accept360 in compliance to the guidelines was done it was found that the Team S in Sandiego has created the content in Accept360 in a best format that it could be used as benchmark for comparative study of Accept360 content As a third step a study of best practices in the software development methods and practices was done using books articles companys project guidelines and the training materials for the Accetp360 tool and practices Next access to the training website for the Accept360 tool which is an exact trial ground for the usage of the tool in the same way as real situation was requested with access to the tool studied the features supported in the tool experimented the usage of the tool to understand the complexities and merits of the tool Next a basic set of questions and topics for discussion were designed for the interview From the work experience in Nokia Research and Development centre since 2006  Nokia research and development organisation until and around 2007 had been following the waterfall model for its software development process after that time there were many trainings and changes in the way of working for moving towards Agile methods The main drawback with waterfall model is it follows the procedure in which when each stage of the project is completed the developers go on to the next stage and they do not validate the previous step when they go forward so at the end if the final product turns out to be with some flaws there is less chance to go back and fix the issues if the flaws were found at the end to go back and fix the problems means it would be possible only by scratching the whole project there is no chance for change hence extensive planning on expected outcome is done at the initial stage and then the project is executed after the planning is done the design and requirements are documented very well the documentation helps when there are very less resources available to continue in the project help to add new resources quickly In waterfall model client would know what to expect but at the same time there is a high risk that if the planning at the beginning had some faults that could end up in risking the whole project and it also does not consider the evolving needs of the customers testing is done only at the end of the project so if errors are found at this stage it could be that they will stay for rest of the project and could not be fixed Waterfall method is good only if the initial requirements are very clear and expected outcome is well defined Agile methods were introduced as solution to many problems in the waterfall methods this method allows changes to be done at any stage of the project initial requirements are kept very simple and a very simple product is made initially later changes are done as per the client needs and latest improvements in the industry The products development is done in small cycles in stages at the end of each cycle the requirements are prioritized and updated testing is done at every sprint so that the errors are identified at every stage and the prioritised errors are taken care in next development cycle this also produces good quality product over comprehensive documentation at every stage this means that the product is in quality at each stage and could be released at the end of any stage this ensure that the deadline for releasing the product is always met At the same time there is more care needed to make sure that every stages in the project are not series of coding stage and end up in a way that the initial planned requirements and product and final outcome are totally different Agile method of development is good if the product is intended for a rapidly changing market This also requires highly skilled independent and adaptive developers to be involved in the project Agile methodologies promise that the product reaches the market faster with good quality of software fulfilling customer needs There are several software development methods introduced based on the principles that are followed in different organizations For example the Lean Software Development method was introduced based on the principles followed in Toyota manufacturing unit which are based on continuous improvement and elimination of waste but these are more generic principles that this could also be applied in the agile methods The table below from the reference 1 lists different agile methods and principles mentioned in the reference 1 Scrum is a light weight software development process consisting of implementing a small number of customer requirements in two to four week sprint cycles Schwaber 1995 XP consists of collecting informal requirements from onsite customers organizing teams of pair programmers developing simple designs conducting rigorous unit testing and delivering small and simple software packages in short twoweek intervals Anderson et al 1998 Adaptive software development or ASD involves product initiation adaptive cycle planning concurrent feature development quality review and final quality assurance and release Highsmith 2000 LEAN involves eliminating waste amplifying learning deciding as late as possible delivering as fast as possible empowering the team building integrity in and seeing the whole Poppendieck  Poppendieck 2003 In general agile methods may vary in applying certain practices However the methods emphasis on producing working software in small iterations and utilising resources efficiently Main features of the agile software development could be summarized as three key things as mentioned in the reference 4 Feature orientation Reactive development and Evolving Project scope Feature orientations is to have main focus on the producing features faster with a goal to deliver the working functionality that brings most value to the customer Reactive development is reacting to a change rather than planning ahead and keeping the decision making as late as possible Examples of reactive practices are refactoring adjusting requirements priorities and release scope after each iteration Evolving project release scope is the main distinguishing features of agile development changing from a fixscope approach to a more openended approach In the traditional fixed scope approach much effort is spent on defining and planning the content of a product release of a project upfront In agile the release scope is emerging in the process of development rather than planned ahead A prioritised list of requirements serves as an initial input which is a kind of wishlist of a release scope The release scope is expected to be refined and updated at the end of each planned cycles of work in order to accommodate changes and new information that was learned in the latest stage This practice is in line with the reactive development property described above Scrums main principle is implementing a small number of requirements in a short cycle It includes mainly the following ways Selfcorrection with inspection making everything visible or known to the stakeholders for example plans schedules issues and progress more clear at every point of time as they are changing all the time Stop and Review the product and the process Scrum operates this way At the beginning of each sprint the team reviews what it should do Then the team selects what it can turn into the release of a potentially working functionality by the end of the sprint Then the team start to work independently with no interruption to make the best efforts for the rest of the sprint At the end of each sprint the Team demonstrates the release of functionality it built so that all the stakeholders can inspect and do adaptation to the project In Scrum the project is started with a vision that needs to be developed The vision initially is stated in market terms and not much in technical terms The items which are to be developed to deliver this vision are made as a list called product backlog items As scrum suggests one person is responsible for delivering the items planned to those who are funding the project and the person in scrum terms is called Product owner The product owner is responsible for delivering the vision in such a way that maximises their return of investment for that the owner creates a plan which includes the product backlog Product Backlog is a list of functional and nonfunctional requirements that when turned into functionality will deliver the vision of the project The requirements that are important to create most value for the product is at the top priority in the list of requirements The prioritized requirements is a starting point and the contents priorities and grouping of the requirements into releases this is usually expected to change the moment the project starts The updates and changes in business requirements and the performance of the team to build the requirements into functionality decides the changes in the requirement list and its content Scrum team is a selforganizing team The team plan their own work the work plan is visible as sprint backlog items it is a good practice if each functionality to be delivered is appearing as a sprint backlog item this helps the team to view the sprint backlog item as a plan and also as a reference for the other team members as they work In this way it ensure that the work to be done is thought through This would make sure that the daily scrums are more meaningful as the work plan is clearer Scrum master role is recommended in Scrum one person in the team act in this role to ensure that the scrum rules are followed and the team do not cut corners Scrum recommends it as a thumb rule that the team is given the prioritised backlog items based on which the team decide by its own on what need to be done next and it is required that the scrum master in the team ensures that the team does not skip the process of scrum and also can help remove the impediments but at the same time the person does not have the authority over the team but only acts to help shape the development processes in the team and make sure that the team brings out the good results and make sure the team is not going off track Scrum team location is recommended to be in a collocated team space which is achieved by removing the cubicles and let team members to face each other and communicate often this eliminates isolation and misunderstandings Mandatory meetings for scrum execution are the Sprint planning meeting Daily Scrum meeting Sprint review meeting and Sprint retrospective meeting these meetings are supported and encouraged by the Scrum master and these meetings are emphasised to get the maximum benefit of the scrum process framework In Scrum all work are done in small cycles called sprint which usually is a one to four weeks cycle Sprint Planning meetings usually have two parts in the first half the Product Owner presents the team with the list of requirements that are prioritised and that are expected to deliver a functionality in the release at the end of the Sprint this list is called Sprint Backlogs During the second half of the meeting the team is expected to discuss and plan their work tasks are created by the team for completing the Sprint backlog items after this meeting the sprint is started and it is time boxed to the number of days the sprint is supposed to be Daily Scrum meeting is held daily for 15 minutes in this meeting the all the team members meet at one place and each team member answers to the three questions what was done since last daily scrum What is planned to be done from now till the next daily scrum And what are the impediments that are on the way to complete the planned items in the sprint backlog The purpose of this meeting is to synchronize the work with everyone in the team and to address and schedule for more meetings if needed to go forward in completing the sprint goals Sprint review and the Retrospective meetings are held at the end of each sprint In the sprint review meeting the team presents the developed functionality or implementation to all the stakeholders of the project this is an informal meetings this helps to bring all the stakeholders together and have an understanding and view what is needed next for the project and then plan for it Sprint retrospective meeting is held after the Sprint review meeting and before the next planning meeting This meeting is to encourage the team to revise the process they adapted in the sprint and make sure it is within the scrum process framework so that the next sprint is more effective and interesting for everyone in the team User Stories Epics and Themes are the three work items which are mainly used to define the items that needs to be worked on by the scrum team Theme in scrum is the highest level in the story hierarchy and describes a view of a tangible product which can be a trading application or an abstract goal such as performance tuning A product owner breaks down a theme into one or more epics Epic represents a group of related user stories or a block of requirement that is not yet been rationalized into stories Sometimes a large user story is also called as an epic A story is a brief statement of a product requirement or a business case Typically stories are expressed in plain language to help the reader understand what the software should accomplish Product owners create stories A scrum user then divides the stories into one or more scrum tasks Scrum recommends to remove the unneeded artefact such a design documents hence the work items descriptions serve as a source of information about the project Projects are estimated based on the three key things the resources scope and time Estimation is a challenging task to do when there are uncertainties of way of execution and process followed are changing but when there is a defined process it could make estimation easier Time spend for estimation could go waste if the estimation are not useful or appropriate but the possibility to have accurate estimate is very less as there is some amount of uncertainties in the projects dependencies in projects which are driven for innovation and change of existing technologies so spending more time in estimating the project will not return a good value for investment but spending a little time to estimate even if it is less accurate would be more efficient to manage the project From the above Figure 4 indicates that the accuracy value starts to degrade when the amount of time spend in estimation is going beyond certain limit Estimate is still an estimate Spending more time in estimation is not going to make the estimate more accurate And to reach high level of accuracy in the curve which means to move away from base line only a little efforts is needed so a less amount of time spend in the estimation would increase the accuracy level As mentioned in the reference Agile teams tend to stay in the left side of the Figure 4 as they acknowledge that estimates could not be done accurately but still encourage the idea of estimation with less time and recognise those estimates that give big gain As agile process is to deliver frequently a fully working tested and integrated software they always are in a situation that they have a reliable plan Usually the user stories are not fully grained down to fine level so they are not of same order of magnitude and differ in their size Hence by aggregating some stories into themes and writing some stories as epics a team is able to reduce the effort they will spend on estimating However its important that they realize that estimates of themes and epics will be more uncertain than estimates of the more specific smaller user stories User stories that will be worked on in the near future for example in the next few iterations need to be small enough that they can be completed in a single iteration These items should be estimated within one order of magnitude It is a good practice to use the sequence 1 2 3 5 and 8 for this estimation User stories or other items that are likely to be clearer than a few iterations can be left as epics or themes These items can be estimated in units beyond the 1 to 8 range sequence recommend earlier To accommodate estimating these larger items it is good to add 13 20 40 and 100 to the preferred sequence of 1 2 3 5 and 8 In Scrum project there are four reports that need to be created by the end of each sprint The first lists the Product Backlog at the start of the previous Sprint The second lists the Product Backlog at the start of the new Sprint The third the Changes report details all of the differences between the Product Backlogs in the first two reports The fourth report is the Product Backlog Burn down report The Changes report summarizes what happened during the Sprint what was seen at the Sprint review and what adaptations have been made to the project in response to the inspection at the Sprint review Why have future Sprints been reformulated Why was the release date or content reformulated Why did the team complete fewer requirements than anticipated during the Sprint Where was the incomplete work reprioritized in the Product Backlog Why was the team less or more productive than it had anticipated All of these questions are answered in the Changes report The old and new Product Backlog reports are snapshots of the project between two Sprints The Changes report documents these differences and their causes A collection of Changes reports over a period of time documents the changes inspections and adaptations made during that period of time Burn down Report This Burn down report measures the amount of remaining Product Backlog work in the units of story points for each of them on the vertical axis and the time scale by Sprint days on the horizontal axis A Story Point is a subjective unit of estimation to estimate User Stories It represent the amount of effort required to implement a user story Using Story Points for estimation is better than estimating in hours or days as it is an estimation done using relative sizing by comparing one story with a sample set of previously sized stories Relative sizing across stories tends to be much more accurate over a larger sample than trying to estimate each individual story for the effort involved The Fibonacci series 1 2 3 5 and 8 is most commonly preferred to categorize efforts in scale of 1 2 4 8 16 points and so on The Product Owner plots remaining quantity of Product Backlog work at the start of each Sprint By drawing a line connecting the plots from all completed Sprints a trend line indicating progress in completing all work can be drawn By figuring out the average slope over the last several Sprints and velocity can be determined occurring when the trend line intersects the horizontal axis This is an important report It would graphically present to management how the factors of functionality and time were interrelated This is good to be included in the project reports but can be an appendix Kanban was initially designed and used in manufacturing units and then it had its way in to many other fields as well and in software development too the basic implementation model of Kanban described in the reference 7 gives a clear idea of steps needed to a Kanban in any field Kanban itself is a scheduling tool which replaces the traditional daily and weekly scheduling Kanban scheduling is an execution tool rather than planning tool Kanban does not replace the planning process but rather takes the information from planning and uses it to create the Kanban a Value stream which is explained later in this section Kanban is considered as a tool which helps to identify and remove the project dysfunction The word Kanban is translated as visual cards Figure 6 Kanban Implementation Model Kanban model above has the seven steps for implementing Kanban for a production organisation but the same steps would apply for any organisation like software development units The steps are 1 Conduct data collection 2 Calculate the Kanban size 3 Design the Kanban 4 Train everyone 5 Start the Kanban 6 Audit and maintain the Kanban 7 Improve the Kanban Step 1 Conduct Data Collection This Phase is to collect the data necessary to characterize the development process This is conducting value stream mapping VSM for the entire organisation which is to determine which development processes would be good candidates for implementing pilot Kanban scheduling systems Step 2 Calculate the Kanban Size This step is to calculate the size of the Kanban Initially calculate the Kanban work item size based on current conditions not based on future plans or desires The initial calculations will utilize the development requirements the productivity rate and risks involved Step 3 Design the Kanban This step is designing the stages and flow in the Kanban the value stream Once the Kanban quantities required to support development requirements based on current conditions is calculated it is time to design the Kanban The completed Kanban design will answer the question of how you will implement the Kanban The design will consider The end product of this step should be a plan for implementation of the Kanban including implementation actions action assignments and schedule milestones Step 4 Train Everyone The people involved has to be trained about how the system will work and on their role in the process the process and the visual signals has to be explained in a training Also the rules are reviewed during the training It is aimed for taking the participants through whatif scenarios to help them understand their roles and the decisionmaking process The training is focused on operating the Kanban Step 5 Start the Kanban Before Kanban scheduling is implemented all the visual management pieces are kept in place To avoid confusion and make training much easier the signals are set up control points are marked and the rules are completed and coordinated As the Kanban is deployed it is good to anticipate problems that may impact success and take action to prevent or mitigate these problems During the deployment stage develop a scheduling transition plan determining the exact point for the change and the amount of efforts required to make the change Step 6 Audit and Maintain the Kanban After the Kanban starts the next step is of the process auditing the Kanban When the Kanban is designed the person who will audit it is also identified Typically the auditor will be watching how the scheduling signals are handled and whether output stays satisfactory When the auditor finds problems then the problems need to be fixed immediately by the responsible party to maintain the integrity of the Kanban design The auditor will have to look at future requirements to make sure the Kanban quantities meet expected demand Step 7 Improve the Kanban Finally After the Kanban gets running look at how to improve the Kanban to reduce new work items waiting to enter the stream Resist the urge to just start pulling items Check how the flow is running and pull the necessary items immediately After this onetime adjustment only reduce the quantities based on improvements made to the development process Determine the amount that can be reduced by using the calculations used in sizing the Kanban to calculate the new quantities  The main idea of the Kanban is to have a flow of stages a running value stream which has different level it is the part of step to design the Kanban and is shown in the Figure 7 below  Figure 7 Kanban Value Stream  Kanban works in a way that the items are pulled into each stage of the value stream in a flow Each stage in the stream have two states Queue and Execution state when an item is moved from one stage to another it first stays in Queue and it is then moved to Execution state And there is a limit set for the total number of items in each stage this is called work in progress limit this limit is set based on the capacity of the team so this makes it a value pulling system to keep the flow steady with work in progress limits in each stage being uniform when the items in each stage exceeds the work in progress limits or if the items in a stage is empty it is a signal that the flow needs attention also makes the progress more visible to all stakeholders  342 Kanban Work Items and Terms  Most of the agile practitioners use the term iterations which is time boxed cycle in which the selected user stories are completed In Kanban which is more specifically designed for Lean practitioners the term iterations is replaced by Minimum Marketable Features MMFIn Kanban the team work on the MMF with no time limits for each MMF user stories and for each stories the scenarios are created Each Story is a card that represents functionality and Scenario represents the action related to each functionality  Figure 8 Kanban Work Items  Delivery Rate The rate at which units of work work item pass through the value stream or part of the stream  Lead Time the term is Development Delivery Rate Measured in units per dayhoursecond the two terms described above can be related as  Lead Time  Work In Progress  Delivery Rate  Delivery Rate Work In Progress  Lead Time  Valueadding Time The total time spent on valueadding activities for one unit of work  Valueadding activities exclude waiting and superfluous work  Resource Efficiency A measure of the utilisation of a given resource ie the ratio between the time working on adding value in the system to the total time available  Flow Efficiency A measure of timeutilisation on a given unit of work ie the ratio of the Valueadding Time to the System Lead Time  Backlog 	A nonWIPlimited queue containing work items awaiting service by the initial activity in a Kanban system  Work Item The item controlled in the Kanban system Effort Required Determines the approximate size of work in personunits of time May be a negotiated function of desired quality  Cadence The rhythm of the production system Not necessarily an iteration Kanban still allows for iterations but decouples prioritization delivery and cycle time to vary naturally according to the domain and its intrinsic costs The average transit time of a work item through a Kanban system  Activity Valueadding work that can be determined as complete Includes activity queue a set of resources and a WIP Limit Represents an allocation of the effort required to complete a work item  Next Work Item Selection Function Rule for selecting the next work item from a queue when an activity has less work than its WIP limit depends on both Class of Service and Value Function and leads to specific flow behaviours  Class of Service CoS Provides a variety of handling options for work items A CoS may have a corresponding WIP limit for each activity to provide guaranteed access for work of that class A CoS WIP limit must be less than the activitys overall WIP limit Examples are expedite datecertain and normal CoS may be disruptive such as expedite and is the only way to suspend work in progress  Value Function Estimates the current value of a work item within a CoS for use in the selection algorithm Can be simple null value function would produce FIFO or a complex multiple kanbansystem multifactor method considering shared scarce resources and multiple costrisk factors The means of prioritizing work items  Activity Queue Holds work items within an Activity that are awaiting processing The sum of items in process and items in activity queue must be within the WIP limit for each CoS  WIP Limit Limit of work items allowed at one time within an activity Prefer this term to flow units in process or similar Measured in units Which term should be used for the rate at which units pass through the system or part of the system Velocity in Scrum Delivery Rate and Throughput are all used frequently probably Delivery Rate is more common in the Kanban community though I have a slight preference for Throughput It is only one word and it applies equally to a subset of the system as to the final delivery part  Visible Representation A common visual indication of work flow through the activities Often a columnar display of activities and queues May be manual or automated Shows status of all workinprogress blocked work WIP limits it is a characteristic that provides transparency enabling better management Difficult to model  Flow Metrics Includes cumulative flow charting and average transit lead time  343 Kanban Execution  In Kanban work is pulled from the back rather than pushed from the front Limiting the number of items in any one flow stage at a point of time with work in progress limit WIP is the key factor which drives the Kanban execution forcing WIP limits encourages team members to stop at one point when the WIP limit has reached and everyone looks in to the issue together until it is solved before they move on to work on the next item In this ways the impediments and roadblocks are eliminated as early as possible Kanban contains an embedded process for handling items that need to be expedited through the flow fixed delivery dates and work type splitting  Work is assumed to be broken down to a roughly similar size In the Kanban Board not only the flow of stories or scenarios are represented but the MMF itself also could be added in the flow and checked Prioritisation of the backlog is performed just in time JIT  Figure 9 Kanban Board  344 Kanban Reporting  The key means to check the progress of work in Kanban team is using Cycle Time Lead Time and Cumulative Flow chart Cycle time is the time taken for a unit in terms of a user story or a scenario to pass through all the stages in the flow Cycle time starts when the flow starts and when any one of the unit is out the flow to a completed The main difference between the cycle time and Lead time is the unit is marked in the later the time between when on particular unit it entered to the flow and the same item is out of the value stream as complete  In the Figure10 below is the view of the Cumulative Flow Diagram by Kanban Tool  Coloured areas on the diagram represent work in progress for each stage of a process  Figure 10 Kanban Cumulative Flow Chart  By looking at the vertical distance of a chart we can define how many items are currently in progress The horizontal distance shows how long it takes for a task to be completed Measuring the horizontal distance on a Cumulative Flow Diagram allows you to monitor the Cycle Time according to which you can make a prediction of when all the work in progress will be done Vertical distance helps you to set the right work in progress limits  Cumulative Flow Diagram should run smoothly Large steps and flat horizontal lines indicate impediments to flow or lack of flow Variations in the gap or bands stand for bottleneck situations which usually occur due to irrelevant work in progress limits This means that the number of tasks in each column should remain at the same level over the time In addition too many tasks in the queue mean either problems with finishing work on time or that the employee on the next stage cannot deal with work  35 Hybrid Project Management Approach  Hybrid Project Management approach is a method in which the requirements and release planning are done in waterfall method this methods combining the waterfall and agile method for the project execution as explained in the picture below is referenced from the article in reference 2 which In the first sprint the project planning and proposal is done in a traditional way of analysis design and documentation and the product backlog items are made in the second spring the agile scrum method is followed in the team to critical path prototyping is made and presented to the team and the stakeholders and also tested and bugs are found and the product backlog is groomed according to the finding in the sprint and the new backlog along with the bugs are made sprint backlog for the next sprint in which the final prototyping is made and demonstrated to all the teams involved in the organisation Application development teams uses the approach similar to this hybrid project management method  Figure 11 Hybrid Project Management Skeleton  36 Agile Maturity Model  Over the last decades the CMMI models has been used most predominantly to measure the quality and maturity level of the organization ever since recent times when the agile software development came in to existence in most of the software development companies where the customer needs are changing rapidly there has been many studies done to analyse the adaptability of CMMI for the agile method of working  Main objectives Agile software development methodology are lower cost high productivity and satisfied customer The CMM tends not to focus the software process on an organizations business objectives in their software process improvement programme 7 Also most companies small to large companies found it is too difficult to reach higher levels in the CMM 7 The study also mentioned that the CMM improvement path is not always smooth the efforts generally took longer and cost more than expected While agile software development methodology is targeted to lower cost Some of the KPAs have been found difficult to apply in small projects 7 This may be because CMM was originally structured for big enterprises 7 CMM addresses practices such as document Policies and procedure that large organizations need because of their size and management structure 7 Hence there is new innovation which were created to measure the levels of Agile software development practices which is called Agile software maturity model  The Figure 12 below from the reference 7 states the different levels in AMM in simple terms from the article the levels could be defined as below Level 1 Initial Level is where the organisation unstructured and has no process improvement goal Level 2 In this level the organisation is has project or software planning customer or stakeholders orientation practices Level 3 Defined Level is about having Customer satisfaction Software quality and development practices this level denotes a more focus on practices related to customer relationship management frequent deliveries pair programming communication coding testing and quality of software Level 4 Improved Level is to have People orientation and project management Practices Companies at this maturity level are in a position to collect detailed measure of the software development process or practices and product quality both the software development practices and products are quantitatively understood and controlled using detailed measurements examination of risk and respect to the team who is going to develop the system The AMM at level 4 maturity aims to help developers or managers to respect for the coworkers or people involved in the project identify and improve problems related to team sustainable pace and organising team by itself This is achieved by an assessment of current process and to identify where weakness lie Level 5 Mature level is when the organisation has Performance Management and Defect prevention practices in place Companies at this level continually improve their processes through quantitative feedback from the process and form testing innovative ideas and technologies  Figure 12 Agile Maturity Model Levels  37 Agile Software Product Management  The software product management SPM includes the process of managing the requirements defining the release defining the context for the products involving internal and external stakeholder and this topic also includes many other areas many process are followed as this is more driven by market driven requirements engineering and currently there is very little of scrum done in the requirements engineering The article is a case study on software product management described in the reference 3 proposes the use of an agile SPM method based on SCRUM It also prescribes the use of two different sprints one for requirements engineering and the other one for development engineering  Figure 13 Agile Software Product Management  38 Accept360  Organisation which belonged to Nokia smart phone division before the merger with Microsoft used the Accept360 tool as the key tool for requirements and release management The tool has features to support requirement engineering and also the software development engineering It is a web based tool which is supported in Internet explorer and Firefox browsers in Desktop computers  381 Accept360 Elements  Accept360 has defined elements for requirement and release management they are called Roadmaps module Requirements Module and Teams Module Roadmaps Module is used to for managing the release milestones and artefacts related to that Requirements module provides features to manage the contents of the releases Team module provides support to plan and allocate the resources for working on the planned contents of the releases  382 Accept360 Team Element Ranking Tab  Ranking tab contains user interfaces for managing team sprint backlogs  It has the team element for the planning and managing the sprint backlogs The ranking tab has two tabs the Backlog pane and Sprint Pane The Backlog pane lists the backlog items for the project which could be easily dragged to the Sprint Pane which contains the list of selected backlog items for the selected sprint  383 Accept360 Recommended Practices  For setting up the Accept360 for an application it is recommended that the contents added for any application follows a certain recommended format it is recommended that the backlogs are added in a standard hierarchy and the structure of the contents is recommended to be as in the Figure 14 Each application has a folder with the application name under which sub folders are created for each version of the application and application driver if it exists   Figure 14 Accept360 Structure in Practice  384 Accept360 Responsible Actors  As explained in the Figure 15 at every stage of the project the contents needs to be updated in the Accept360 by the program manager or the project manager at some stage or by both of them to together And it emphasis that he Project manager has the overall responsibility to ensure that the contents in the tools are up to date  Figure 15 Accept360 Actors  385 Accept360 Agile Task board for Scrum features  Agile task board provides the features that are needed for supporting the task planning by the scrum teams Accept360 tool has support for complete scrum process Key components that are used in scrumming with Accept360 are Feature Subfeatures Stories and Tasks The Product owner creates the Feature SubFeature and Story contents The scrum master has access to Stories and Tasks During the Sprint planning the stories are pulled for each sprint and the Tasks required to complete the stories are created by the scrum master after discussing with the team members And the Developers have access to update the status of the tasks  Feature in the Accept360 can be compared to the Theme in scrum terms but more equivalent in usage is an application as a whole is represented as a feature and the SubFeature as Epic which is a list of all the requirements for the application When the subfeature is drafted it is then proposed for implementation and stories are created and then the stories are assigned to the teams as sprint backlog items which then gets a story point assigned for it For each stories the tasks are created and is assigned to the team members Below table represents the lifecycle states that are available in Accept360 and only some as states are used in defining the states of the requirements in the ASW projects  SW Lifecycle  SW  Draft  Initial status when a new item is created first time not yet ready for further actions and not yet in any backlog with schedule  Proposed  NOT USED  Candidate  NOT USED  Committed  Development team has committed to deliver item by Planned Delivery Date for certain programs  Implemented  Item has been implemented and tested by the responsible team and given to the integration team  Done  Item is ready ie it has been implemented tested and integrated  On hold  NOTUSED  Rejected  Item rejected  Table 1 Requirements LifeCycle  386 Accept360 Kanban features  Requirements can be synchronized from Accept360 Team Backlog to Backlog column of the Kanban Board sorted by Rank order From there it is possible to prioritize items ranking is updated to Accept360 UI accordingly see whos responsible Track changes Balance workload and limits also it is possible to configure team development states as needed each column can be bind to Accept360 Lifecycle progress can be also followed in Accept360 it is possible to set WIP limits for each state to avoid unfinished tasks and visualize the bottlenecks emphasizes to have a work item 100 done instead of having many 80 done is has to be noted that current Accept Kanban Board version supports only Requirements management not Tasks  387 Accept360 Nzilla Support  Accept360 supports importing Open bugs from Nzilla to the corresponding team It helps the development teams to give relevant information from Nzilla about the bug to enable them to Rank the related Bug against other Stories on the Team Backlog It is a Oneway integration and Defect master always in Nzilla Nzilla enables Lifecycle mapping and synchronize between tools to improve visibility To integrate Nzilla with Accept360 a separate structure has to be created in Accept In scope Programs and Component level automatically Synchronized using Nzilla Database IDs Any name changes will be automatically updated this stop the solution breaking and make deployment to teams easier Only downside is there will be some extra Bug Bags and folders for teams that may not yet be using the solution  39 San Diego Team Practices  The teams in San Diego had been able to prove that they have standard way of usage of the Accet360 tool to visualize the needs and progress of the project in every stage of the project Here is the snapshot of the way the team has defined the subfeatures which includes a clear steps and functions that needs to be done in order to execute a project starting from the initial prototyping until the planned feature or application is available in the market  The items in the Requirements module have the prefix to each item which explain the common activity that is need to be executed in the project and also there are items that explain the main stages which a project would go through so it helps to visualise the project situation with the items in the Accept360 tool  Goals of every functionality are also clearly included in the list of subfeatures and stories The team follow the template available in Accept360 tool below is the screenshot of the template  Also the teams follow the mixed Scrum and Kanban methods The teams have sprint planning Daily scrum meeting and Retrospective meetings And meetings are helped whenever it is appropriate and needed and the team has a regular practice of keeping the Accept360 updated always  310 Making Most of Scrum and Kanban  As the saying goes No tool is complete and No tool is perfect it only depends on how it is used this applies to both scrum and Kanban The value of a tool is that it limits the options A process tool that lets user do anything is not very useful this process could be named Do Whatever but having a Do The Right Thing process is guaranteed to work  Scrum and Kanban have both of their own pros and cons so it is good to understand the difference between the two tools so that we could make the best use of both the tools  Difference between Scrum and Kanban  Scrum  Kanban  Time boxed iterations prescribed  Time boxed iteration optional Can have separate cadences for planning release  and process improvement Can be event driven instead of time boxed  Team commits to a specific amount of work for this iteration  Commitment optional  Uses Velocity as default metric for planning and process improvement  Uses Lead Time as default metric for planning and process improvement  Cross functional teams prescribed  Cross functional teams optional Specialist teams allowed  Items must be broken down so they can be completed within 1 sprint  No Particular item size is prescribed  Burn down chart prescribed  No Particular type of diagram is prescribed  WIP limited indirectlyper sprint  WIP limited directly per workflow state  Estimation prescribed  Estimation optional  Cannot add items to ongoing iteration  Can add new items whenever capacity is available  A sprint backlog is owned by one specific team  A Kanban board may be shared by multiple teams or individuals  Prescribes 3 roles POSMTeam  Doesnt Prescribe any roles  A scrum board is reset between each sprint  A Kanban board is persistent  Prescribes a prioritized product backlog  Prioritization is optional  Table 2 Difference between Scrum and Kanban  311 Scrumban  Scrumban is a result of the thinking to get most value of the scrum process like Kanban Scrumban is a pullbased system where the team no longer plans out the work that is committed to during the planning meeting and instead continually grooms the backlog The same Scrum meetings can and should still take place but the cadence of them can be more contextdriven The real key factors for Scrumban though is ensuring that work in progress WIP is still limited  Figure 16 Scrumban Sprint and Value Stream  Scrumban works with the Workinprogress limits not Sprints With Scrum the amount of work that is ongoing is limited by the Sprint time commitment But in Scrumban with no specific time commitment the team must limit itself through the use of WIP limits on columns within their task board The goal is always to move tickets in a flow from left to right on the board If too many issues are in progress the team is at risk of not finishing anything to high quality standards Instead there should be a maximum number of tickets allowed per column If the number of tickets in that column ever exceeds the maximum the entire team should swarm onto that column and help move tickets on This should happen no matter what functional role a team member fills  Scrumban still supports the scrum ways of having the meetings and time boxed deadlines for the release of the features planning meetings should take place as often as they are needed When the team is unable to regularly pull stories off the top of the backlog at their normal pace a planning meeting is necessary Review meetings helps to improve the way of work with feedbacks Reviewing work with clients and customers is the only way that development teams can get the feedback necessary to properly adapt what they are working on Clients tend to prefer that these are held at a regular cadence  Retrospective meetings These can vary when held but a general rule of thumb is to hold a retrospective after every review This is the most useful part of the Agile process and should be given the proper place for that  Daily standup meetings in the Scrum world follow a simple pattern The team takes 15 minutes and each person says a what heshe did yesterday b what heshe is working on today and c what is blocking any of that work In practice this boils down to redundant statuses that recount information available on the teams task board For Scrumban a more effective method is to refocus on the flow of tickets on the board That same pattern of yesterdaytodayblocked can be transferred to the tickets themselves—the group moves through each column and briefly discusses each ticket and what is necessary to move that ticket rightward on the board This provides far more context to the team and informs every one of any major architectural or design decisions  Metrics can certainly be useful it interprets the complex process that is going on in the team it is not just a single value or number to be expected to know how the team is progressing and it could help visualize the situation the team is going through on the way to achieve the target The term Velocity the amount of story points a Scrum team completes in a single Sprint is such a metric that incentivizes lower quality at the end of a Sprint as a team scrambles to finish every last story they committed to When the number fluctuates as is common with a newer team the stakeholders begin to question the outputs of the team and even the effectiveness of Agile itself In Scrumban the metric is cycle time instead of velocity This is the length of time a ticket takes to complete measured from when it is first began Over time a statistical analysis of all tickets in the project can yield a mean cycle time and standard deviation This can be a useful planning tool at a macro level as it is trivial to add up the number of stories and multiply by mean cycle time  312 Summary of Best Practices  From the study and discussion about the best practices it is found that the scrum method is used mostly in the software development teams as it is very good for planning and scheduling meetings but it also recommends that the sprint has to be pre planned and should not be interrupted in between and the backlogs need to be reset which makes it to a situation that we cannot keep more buffer for backlogs to be tried and everything need to be reset at the end  Next that Kanban is used most widely in software development organizations which works with Work in progress limits are the visual indicator for the execution and planning of activities And also recommends that that designing of the Kanban and steps for designing should be followed which if not followed would result in a situations that he project situation could not be in control  Next the thinking of making most out of the Scrum and Kanban has been received as a good approach to make use of good things in both the methods and which is best suitable for the needs in software development organizations this could well be implemented with the techniques defined in the Scrumban  Next the hybrid project management models is a good method which gives a way to have the traditional way of software development methods and also have scrum included but at that same time this does not address the needs that the planning need to be dynamic not only the execution is dynamic so to have agile mode of working for planning the requirements and developing the requirements that agile product management method is very suitable  Next the study of the Accept360 tool details that the tool has a lot of features in it but it is built in a way that the tool need to be adapted but adding more plugins to it for each feature to be able to utilise it more efficiently   From the study of usage of the Accept360 in the team in Sandiego it is found that the content what is used to describe the project requirements or the backlogs play a key role in visualizing the project status and also it make it clear that defining the contents of the backlogs and work items in a standard way it is done in the team would make it more helpful to know the needs and status of the project easily  4 Current State Analysis  Current state analysis was done in the teams in Espoo  two teams were analysed and in each team the Product Manger Project Manager  Development Team Lead and The Quality Lead were contacted and interviewed on the following topics Project Details Team Information Agile PracticesProject Planning and Execution InformationProject SituationsSuggetions for desired Improvments in Accept360 tool The interview topics and questions are listed in the Appendix 1   41 Analysis of teams in Espoo  411 Analysis in Team X  Team X is working on creating a new version of an existing application so this application is expected to replace the old application mainly with the new user interface and improved usability of features the development phase of the application would need a complete cycle from proto typing the new user interface and adding all the functionalities to the new applications This means that the efforts needed for developing the working application could be estimated to an appropriate value which would help define a process to follow  The team uses the scrum process for planning and execution of the activities Accept360 tool is used during the sprint planning and review meetings UI specification document and the story description in the Accet360 serves as the documentation needs of the projects as they are updated with appropriate information UI Design team shares the updates and synchronize their work for their sprints with the spirits of the development team sprints and meetings Also a short description of key features of the application is documented in share point documents management page  Team follows two weeks sprints and at the end of the sprint there is a demo and preplanning session for the next sprint Releases and Testing are done weekly during the project execution stage  Team uses the Agile task board in Accept360 for managing the tasks also the Nzilla is used to check the issues and bugs are also worked on as tasks There are no daily meetings in the team it is expected that each team member is located in the same location and has to contact the development lead or another team members to know what has to be done and what is expected  The discussion with the key stake holders of the project the Lead Program Manager Project Manager Lead Developer and Quality lead is available in Appendix 24  412 Analysis in Team Y  Team Y is working on developing value addition features for the existing application the Team includes many small teams every feature addition and implementation is initiated whenever the decision to have the feature in the application is accepted and is prototyped by one of the small teams and is integrated to the application  Team uses the One Note as a tool for planning and updating the backlogs The release plan requirements list and status is tracked in OneNote pages Accept360 is not used for these purposes the reason explained was the releases happened ever month and the features are released in very small time  Team uses the UI specification as the document to know the details of the projectAccept360 could not be relied on for the details of the project as it is not fully updated all the time  Sprints are mostly weekly sprints sprint meetings are held every week and Adhoc meetings are organised whenever there is need At the end of every week an open agenda meeting is held which is for having an open discussion about anything related to project and or give presentation about the progress Testing actives are done on weekly basis  The team had previously tried using the Kanban for their planning and scheduling before but it was not very helpful as they did not have control of the activates and work items all the time as it ended up in just having a board with full of task cards and nothing going in planned way and things went just out of hand but what it is important to note here is the Kanban followed was not a designed to be adopted for the overlaying process in the organization  The discussion with the key stake holders of the project the Lead Program Manager Project Manager Lead Developer and Quality lead is available in Appendix 58  42 Summary of gaps between best practices and current practices  One of the main aim of the Agile methodology is to have clear visibility of the project at any point of time so that any kind of resources are needed deliver that project as planned are addressed at the earliest this could be achieved only if the teams follow the prescribed practices of the standard methods they adapted to follow or follow a methods which would not prescribe more rituals but still is would make the situation more visible to all the stake holders But currently the teams do follow standard methods but skip some of the recommendation in the methods  Though the teams are very dedicated to deliver the products and functionalities the team members are not interested to follow the best practices and tend to work in more selfcentred way because of which certain practices which would help all the other team members to perform well are not taken in to consideration The teams have their own way of working than following the standard practices in the way of working  From the study it is found that there are some practicalities that are not followed in the teams a clear understanding of responsibilities and who is responsible for updating the tool is missing This is also obvious from the contents in the items in the Accept360 tool which is not up to date for some projects  Also there are some needs that are missing in the current processes and tools for example the Product backlogs have to be synchronized with the tasks to add the tasks for the sprints there need to be a story to add task but in practice the development teams have more tasks that need not necessarily have to be a requirement backlog item but is needed for their day to day activity The requirements engineering and the software development engineering are in this way tied more closely in the tool Even though there need to be synchronization between them the tasks in the development engineering team need to be created according to the development activities to get the requirements implemented this would need different terminologies in defining the tasks that would not fall into different the category of work items and tasks definitions  The Agile task board is updated only at the end of the sprint which explains the board is not actively used in the team the reason for this found from study is that the way of working is not the same in the flow of states in the task board also there are tasks which could go longer than the sprint time  but in scrum after every sprint they had to be reset and added to next sprint or put as impeded which would give a picture that there the things are not going well but in general there are some dependencies that the tasks need to stay between the sprints for it be completed in a better way it is because the order in which the task need to be worked on changes that some tasks need to be put on hold for some time waiting for other tasks to be complete or started As the sprint is time boxed and needs a reset of the tasks and backlogs for the sprint after every sprints it leads to a situation that the tasks are updated in a way that their status is not related to actual status and is updated for the purpose of resetting the sprint backlogs which scrum recommends So there is a need for other methods and tools like Kanban which would help the developers to keep the tasks across sprints without resetting it at the end of each sprints But it has also been experimented in the Team Y that Kanban method alone was not helping enough as there is a need to time box the release of features at some point of time and the releases are tied to the release of the devices to the market this needs some kind of iterative approach as well for planning the activities hence there is a need for using some of the features from the scrum method and Kanban tool to get the needs of the project execution addressed completely  5 Proposal  51 Theme Proposals  As summarized in the best practices and current state analysis the key areas where the gap exits in knowing the status of the project is how the contents of the project planning and execution tool are defined and used the difference in work items needed for requirements engineering and development engineering and difficulties in combing the time boxed sprint development method with the continuously improvement and integrated development mode of working These gaps could be address better by the themes that are explained in the below subheadings which would help fill the gaps that exists and help improving the visibility of the project status to all stakeholders of the project  511 Usage of Structured Contents for Work Items  Work Items as explained in the best practices are the items like epics stories tasks mmf and activities The description of these items could serve as documentation for the project and this could include details about goals features of the product being developed All the work items would fall in one of the categories of the activities that are commonly needed to create the end product like Functional requirements Non Functional requirements User interface design and development Core technology implementation and interfacing Performance validation and improvements Testing and Error correction The organisation in case company in which the research is being done has a recommendation and guide for creating and maintaining the work items The guide provides information about required structure for defining the work items naming and versioning conventions avoiding duplicates in contents this guide could be used as reference to maintain the contents in a standard way The guide also includes details about environment needs to use the tool such as browser requirements for the tool plugins needed to have a better performance of the tool and it includes a template which explains the backlog creation ethics  Initiatives to use the guidelines and keeping the work items updated would help improve the visibility of the project status so that by looking at the content of the subfeature stories and tasks it is possible to have a clear picture of the situation of the project For example the Team S in Sandiego have the contents defined in a specific way they follow different syntax for defining the work items so that when looking at the contents in the Subfeatures Stories and Tasks it is possible to understand the stage of the project So keeping the contents more meaningful in the recommended standard way could make the visibility of the project situation more clear to all the stake holders involved  For example the Figure 17 show the sample content from the best practices studied in the Sandiego team this could be used as reference for creating the items for the scrum team backlogs The template has suffixes such as Story  Demo story UI design Miscellaneous UXUI Spike Automated Testing Exploratory Reliability Compatibility Interoperability Non Functional Certification Testing and other stages the project need to carry on for finally delivering the project These suffixes would give more information about what is planned and being done at that stage of the project so this helps to have more details of the activities carried on in the project and it would help to provide needed resources for those activities based on their needs  Figure 17 Template for Work Items in Scrum team  512 Usage of Agile Sotware Product Management  The software product management SPM as explained in the reference 3 proposes the use of scrum in product management It also prescribes the use of two different sprints one for requirements engineering and the other one for development engineering  For a scrum process the backlogs are the key instruments and there are two different kind of backlog items involved in software development the Product backlog which is the prioritized list of requirements that includes all relevant needs of the product that is being planned to be developed Some of these Product backlog items are then copied by the development team as their sprint backlogs called Development Sprint backlogs which are then split in to stories and tasks  Though there is a tight dependency between the two backlog items the backlog items picked up by the development teams are reset every sprint and different tasks are created newly Hence there is a good need that these two backlogs items are used differently by the software management teams and the software development teams Currently scrum is followed only by the development teams so as explained in the reference 3 the article suggests the use of agile scrum methods in defining the Product backlogs would be more efficient to manage the change and synchronize the changes and progress in both requirements engineering and development engineering  From current state analysis interviews in the case company  it show that the existence of the difference in requirement engineering and development engineering needs for managing the backlogs even though there has to be synchronized usage backlogs between these two teams their functional needs demand the need for having two different sprints or cycles for the same product  513 Usage of Scrumban  The requirements engineering teams which involve the project management and planning requires clearly what is prescribed in scrum the sprints roles and ceremonies but at the same time the development engineering teams need a stream which is more like Kanban to keep their flow of work steady and visible so Scrumban which recommends work pull system which is a scheduling method based on demand and at the same time recommends to have the scrum way of planning and checking the situation by the aid of meetings would best suit the needs in the organisation in study  There are also many teams involved in one project and each team have their own planning and execution cadence the Figure 18 is a sample framework which explains the involvement of different teams with different work cadence defined for the same project and more relevant for scrumban method The frame work is also based on the concept explained in the reference 9 which is good for complex implementation Currently there is only one cadence which is defined for the development team and other teams like requirements management and UI design teams which are very tightly working with their deliverables as input to the development engineering team do not have a designed cadence for delivering their deliverables and they are provided in a dynamic and not planned way so defining a cadence and process method for all the key teams that are involved in the project would make the interaction and situations much clearer and easier for planning and executing the activities of the development team and for the whole project as well  As an example there are three teams in the sample framework first team is the team which is the key team that creates and defines the requirements of the features of the product being developed the product needs several other teams also to be involved very closely to get the product developed fully the development engineering team and the UI design teams are the other key teams which work on the creating the product as in the Figure 18 the Requirements team follow the scrum methods they define and provide the backlogs for the other teams and also schedule the planning and status meetings which could for example happen in a three weeks cadence and the development team have a cadence of two weeks for releasing the working software product and the UI design team works in flexible cadence of delivering the UI design document whenever there is a need for new method of defining the user interface all these three teams has to work independently but still depend on very closely on each other so having a framework designing like in the sample framework in the Figure 18 would help planning and coordinating the teams in better way also this would the help visualize the dependencies on deliverables between teams and define them in a structured ways and this would help all the stakeholders involved in the project to visualise the status of the projects more clearly  Figure 18 Sample Framework with Scrumban Method  In Scrumban the story and tasks card need to be built and maintained appropriately otherwise it would not yield any value as in the Figure 19 below the story cards or the subfeatures in green are the items which are worked on by the requirement management team which mostly involve the project managers and the development team lead the stories are selected at the beginning of each sprint and story board is reset at the beginning of each sprint  Figure 19 Sample Scrumban Board  Scrumban for the development team works with the Workinprogress limits not Sprints With Scrum the amount of work that is ongoing is limited by the Sprint time commitment But in Scrumban with no specific time commitment the team must limit itself through the use of WIP limits on columns within their task board The goal is always to move tickets in a flow from left to right on the board If too many issues are in progress the team is at risk of not finishing anything to high quality standards Instead there should be a maximum number of tickets allowed per column If the number of tickets in that column ever exceeds the maximum the entire team should swarm onto that column and help move tickets on This should happen no matter what functional role a team member fills  For the Software product management Scrumban still supports the scrum ways of having the meetings and time boxed deadlines for the release of the features planning meetings should take place as often as they are needed When the team is unable to regularly pull stories off the top of the backlog at their normal pace a planning meeting is necessary Review meetings helps to improve the way of work with feedbacks Reviewing work with clients and customers is the only way that development teams can get the feedback necessary to properly adapt what they are working on Retrospectives meetings could be held when needed or in regular intervals so that team has an opportunity to give feedback about whole process involved and give input about what changes they want to have to in order to improve the Efficiency of the team  Scrumban can restore working time to the development team and avoids unnecessary meetings And most importantly it can limit the teams work in progress so that they can finish what they start to a high standard Scrumban can remove overhead stress for the development team increase efficiency and increase the overall satisfaction  52 Summary of Proposal  From the study it is understood that the usage of Accept360 tool for day to day activities of the project execution is not preferred due to the fact that the tools is very slow for accessing and updating the content The tool is more suitable for planning the releases and maintaining features for devices globally but it does not address the need for the software development projects that has quick and dynamic life cycle But as long as the tools is used and since the tool supports many features of the agile development methods the guides and training documents available for the tool are created to address the process followed in the organisation in case study over years following the guidelines in the same way it is recommended would help the teams to use the tool more efficiently for planning and execution also would help all the stakeholders involved in the project to have the clear visibility of the project situation at any point of time   From the current state analysis it shows that there is interests to use other methods and tools in addition to methods and features supported in the Accept360 tool and Accept360 is considered to be helpful more for requirement management teams rather than for software development teams so it is good if the teams could make use of other tools suggested by the stake holders interviewed for current state analysis tools like scrum works pro which already has synchronization with the Accept360 or the teams could try the tools like Jira and Version One for their activities and synchronize their sprint backlogs with the Product backlogs in the Acept360 but it is important to mention here that for synchronizing backlogs with other tools the efforts and resources needed would be more Hence it is good to further research on what tool could serve more good for the organisations need  Since in the organisation in case study  there is more clear work type splitting of requirements engineering and software development engineering the Agile methods in requirements engineering would provide more visibility in project situations and requirements creation and grooming would be carried on in more structured and dynamic way  Also from reference 8 and other online research papers recommend the usage of both Scrum and Kanban together for having more visibility of the project situation and progress It would increases the productivity and efficiency of the team  6 Conclusions  61 Summary  This chapter gives the general overview of the thesis it is a look through of all the steps done in the research process Validate the research by comparing the objective with the results how applicable and generalized are the results in another context And also check how reliable the research process is if the methods and materials used in data collection and analysis are appropriate and whether this research could be carried on further by another person in the same way it is done currently  62 Practical Implications  The thesis was aimed to study the gaps that exists in current practices in software development methods and tools in the organisation in case company in comparison with the best practices in agile software development methods and tools that are available from different sources and summarised in the literature part of the thesis and to propose the solutions would fill the existing gaps and would provide the ways to improve the visibility of the situations The current state analysis was done by discussions with many individuals in different kind of roles in three different teams These discussion turned out to be a great source of knowledge and insight into the organisational process mainly it gave an insight and good understanding of project planning and execution methods Everyone involved were in highly responsible roles hence the discussions were more clearly depicting the actual responsibilities and needs in a project which helped to frame the proposals that would address the needs to fulfil their responsibilities in an improved way and help all the key stakeholders in a project to have better visibility of the project status  The research was mainly based on the current ways of working current issues and needs that were desired by the team members The proposals are improvements that are needed in three different area of project planning and execution methods and tools and proposals are framed based on the research articles in the references The feedback about the proposal was very positive that people involved in the requirement management showed interests in the proposals and they wanted to have similar practices proposed in this research  The current research focus only in finding the gaps that exists in the methods and tools and proposes the key ways that could be used to improve the visibility of the project status and the research proposes the sample approaches that could be tried to have better visibility of the project situations which also help in reducing the delays in the project deliverables The proposed sample approaches need to be designed further and piloted according to the needs in the teams in the organisation under case study  63 Evaluation  631 Objective Vs Outcome  The main objective of the thesis was to find the ways that would improve the visibility of the project situation to all the stakeholders involved in the project In the study three main proposals were created to improve the visibility of the situation First proposal is to improve the contents in the requirements management tool so that the contents have the description of what is being done and what is the goal and outcome of the work item for each requirement Second is to have a separate scrum process for requirement engineering which is called Agile Scrum Product Management which lets the backlogs to be engineered with a process rather than continuously so that they are created with better clarity rather than in an Adhoc way which definitely would give clearer picture and estimate of the project situations Third proposal is using the Scrumban which is having a scrum process for main process of requirements engineering and the other engineering teams like development engineering teams would work in accordance with their requirements from scrum team with a key control agent as Work in progress limits The development engineering teams would have a Kanban board designed in a way that their work flow is checked by work in progress limits and is not reset every time the scrum sprint ends in this way the actual situation of their work progress could be seen in the work item flow and is more realistic The outcome of all the above three proposals would create a situation where the process and the all the stake holders in the project are more closely involved which would eventually improve the visibility of the project situations Reliability checking for the thesis is done mainly based on the validating how well the methods and the materials used and well the outcome addresses the objective of the thesis The Methods and materials used in this thesis is mainly from the referenced articles and books and the teams and people involved are from the organisation in which the case study is done The objective and the outcome of the study are closely related to the real situation and more relevant to the study topic In the literature part contents are designed in a way that the agile methods and practices are highlighted and explained in detail that this report would serve the purpose of referencing as a guideline to design and improve them Also current state analysis were conducted in a more generalised way with more generic question about project planning and execution so that the inputs from those interviews would give a generic overview of the process and the needs for improvements in the project in this way this study has initiated a research which would further be continued in an elaborate mode for example by clearly designing the scrum team in accordance with the recommended scrum ceremonies and designing the Kanban as per the needs of the project and apply them in scrumban method and validate the same with experiments in more teams piloting the proposals are some of the key research areas for future studies  The thesis is validated using following methods the Triangulation Prolonged engagement in the field and Member Checking further in this section Also the answer to the research question How the visibility of the project situation could be kept updated at any point of time What are the best ways to stay on track of the current situation of the projects and so be able to manage the issues appearing earlier and minimize the delivery slippages Is answered in the proposal section and is addressed completely Hence this states that the research has a consistent and logical approach in providing what is promised in as objective  Triangulation is a validity procedure where the researcher search for convergence among multiple and different sources of information to form the themes and categories in the study As a validity procedure in this method the researcher uses a systematic process of sorting the data to find themes by eliminating the overlapping areas In this methods the researcher rely on multiple evidences rather than single incidents or data point in the study The data collected include three different teams and ten individual discussion also the best practices were studied from several articles company guides and books  Next method that is used for validating the study is Prolonged Engagement in the field since the thesis was carried out in the same organization where the researcher is working since many years and one of the teams which were involved in the study is where the researcher worked at the time of study for more than seven months  as Fetterman 1989 contends that working with people day in and day out for long periods of time is what gives ethnographic research its validity and vitality p46 the study has more vitality  Member Checking method of validation involves getting feedbacks from the participants The feedback from the participants who were key stake holders in the organization in the case study and the Supervisor of the thesis who is in the role of the Operations Manager of the organization under case study were collected The participants feedback states that the study was very useful in giving details about the agile methodologies and best practices in more elaborative way The feedback from the supervisor was that the discussion and contents in the thesis were useful in understanding the current practices and also the proposals in the thesis are closer to the changes that were expected by many key stack holders in the organization and was helpful to align and improve the current practices for addressing current key issues Hence in the thesis the qualitative method used for data collection has provided with more valid information of the current state the data from the discussions are valid and used to build the proposal The research does not contain any personal opinions of the researcher but is fully based on the data collected from the key stake holders in the organisation and the articles and book in the references The researcher has used the experience in the organisation to be selective in collecting the data so that the data is more relevant for the research which has brought very good result in proposing the solution framework 
